<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:10:11 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-10070/HBASE-10070.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-10070] HBase read high-availability using timeline-consistent region replicas</title>
                <link>https://issues.apache.org/jira/browse/HBASE-10070</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In the present HBase architecture, it is hard, probably impossible, to satisfy constraints like 99th percentile of the reads will be served under 10 ms. One of the major factors that affects this is the MTTR for regions. There are three phases in the MTTR process - detection, assignment, and recovery. Of these, the detection is usually the longest and is presently in the order of 20-30 seconds. During this time, the clients would not be able to read the region data.&lt;/p&gt;

&lt;p&gt;However, some clients will be better served if regions will be available for reads during recovery for doing eventually consistent reads. This will help with satisfying low latency guarantees for some class of applications which can work with stale reads.&lt;/p&gt;

&lt;p&gt;For improving read availability, we propose a replicated read-only region serving design, also referred as secondary regions, or region shadows. Extending current model of a region being opened for reads and writes in a single region server, the region will be also opened for reading in region servers. The region server which hosts the region for reads and writes (as in current case) will be declared as PRIMARY, while 0 or more region servers might be hosting the region as SECONDARY. There may be more than one secondary (replica count &amp;gt; 2).&lt;/p&gt;

&lt;p&gt;Will attach a design doc shortly which contains most of the details and some thoughts about development approaches. Reviews are more than welcome. &lt;/p&gt;

&lt;p&gt;We also have a proof of concept patch, which includes the master and regions server side of changes. Client side changes will be coming soon as well. &lt;/p&gt;

</description>
                <environment></environment>
        <key id="12682280">HBASE-10070</key>
            <summary>HBase read high-availability using timeline-consistent region replicas</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="enis">Enis Soztutar</assignee>
                                    <reporter username="enis">Enis Soztutar</reporter>
                        <labels>
                            <label>needs_releasenote</label>
                    </labels>
                <created>Tue, 3 Dec 2013 00:44:39 +0000</created>
                <updated>Mon, 10 Oct 2016 20:08:21 +0000</updated>
                            <resolved>Fri, 26 Jun 2015 22:33:06 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                    <fixVersion>1.2.0</fixVersion>
                                    <component>Admin</component>
                    <component>API</component>
                    <component>LatencyResilience</component>
                        <due></due>
                            <votes>11</votes>
                                    <watches>99</watches>
                                                                                                            <comments>
                            <comment id="13837162" author="enis" created="Tue, 3 Dec 2013 00:50:35 +0000"  >&lt;p&gt;Attaching a design doc for the feature. Comments welcome. &lt;/p&gt;</comment>
                            <comment id="13837174" author="vrodionov" created="Tue, 3 Dec 2013 01:02:06 +0000"  >&lt;blockquote&gt;
&lt;p&gt; Of these, the detection is usually the longest and is presently in the order of 20-30 seconds. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Any particular reason, why? &lt;/p&gt;</comment>
                            <comment id="13837187" author="jmhsieh" created="Tue, 3 Dec 2013 01:07:48 +0000"  >&lt;p&gt;This is great.  I&apos;ve been giving this a lot of thought recently and doing some experiments to see how feasible this is.  &lt;/p&gt;</comment>
                            <comment id="13837189" author="sershe" created="Tue, 3 Dec 2013 01:12:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any particular reason, why? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To avoid false positives due to e.g. GC... we wait for some time before we consider RS dead&lt;/p&gt;</comment>
                            <comment id="13837192" author="jmhsieh" created="Tue, 3 Dec 2013 01:14:53 +0000"  >&lt;p&gt;Here&apos;s a link to my outline of the feature, looking forward to comparing the designs and concerns. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;a href=&quot;https://docs.google.com/document/d/1q5kJTOA3sZ760sHkORGZNeWgNuMzP41PnAXtaCgPgEU/edit#heading=h.pyxl4wbui0l&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1q5kJTOA3sZ760sHkORGZNeWgNuMzP41PnAXtaCgPgEU/edit#heading=h.pyxl4wbui0l&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13837210" author="vrodionov" created="Tue, 3 Dec 2013 01:29:04 +0000"  >&lt;p&gt;How about simple beacon process which is not affected by GC and sends &quot;I am alive&quot; messages to Zk every, say 100ms? HBase can spawn this process on start up. One can easily  detect server failure (including VM swapping) in less than 1 sec in most cases. If server swaps than it should be marked as unavailable anyway (swapping is bad). Just saying. &lt;/p&gt;</comment>
                            <comment id="13837221" author="enis" created="Tue, 3 Dec 2013 01:47:52 +0000"  >&lt;p&gt;Jonathan, great that your doc have very similar ideas to the one proposed in this issue, if not for a different main driver use case. We are focussing on read availability as a primary goal, and leave the primary promotion as a future goal (in the last section). Instead of a primary / shadow mark, we are proposing a replica_id, which inherently contains that information, plus enables to have more than one secondaries (shadows). In the region changes section, I detailed how we can do keep up with the primary region in a three-way proposal. As detailed, we do not want to impose any restrictions on the co-location of region replicas for the primaries hosted on the same server. Thus, in a wal-tailing case, we do not want to have every RS tailing the logs of every other RS. That is why I think it makes more sense to have this only in a wal-per-region world. Otherwise, I think we can tap into the replication and log replay work to tail our own logs and replicate to secondaries. &lt;/p&gt;

&lt;p&gt;Any details you can share for the experiments you did? &lt;/p&gt;</comment>
                            <comment id="13837234" author="jmhsieh" created="Tue, 3 Dec 2013 01:57:40 +0000"  >&lt;p&gt;High level comparison after first read: These are  are quite complimentary on goals and initial focuses.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;enis focuses on stale read availability for phase 1, then reduced recovery time for phase 2.  Jon focuses on reduced recovery time first, and then punts on stale read availability for a phase 2.&lt;/li&gt;
	&lt;li&gt;enis provides 3 options for log recovery, jon focuses on what enis calls the &quot;wal tailing&quot; approach.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The requirement that I&apos;m most concerned about &lt;b&gt;recovering consistent reads as quickly as possible&lt;/b&gt;.  I find the memstore / wal tail appealing most because we are essentially focusing the most on this current area of weakness.&lt;/p&gt;

&lt;p&gt;At the moment I&apos;ve done some experiments on the wal tailing approach to see how much overhead it causes on the nn. I don&apos;t think we need to have a wal-per-region world to make it feasible &amp;#8211; we only need to make sure that our shadow region are more a more like shadow regionservers  &amp;#8211; only have one or two of the replicas read the logs and make them the preferred places for the shadows.  This basically acts as a constraint for the selection of where secondaries are assigned.   &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; in your doc does &quot;group-based assignment&quot; mean assigning multiple regions on a single transaction?   or is this hdfs affinity groups?  (if it is, then I agree, we need hdfs affinity groups for this to be efficient).  However, I don&apos;t think we get into a situation where all RS&apos;s must read all other RS&apos;s logs &amp;#8211; we only need to have the shadows RS&apos;s to read the primary RS&apos;s log.&lt;/p&gt;</comment>
                            <comment id="13837247" author="devaraj" created="Tue, 3 Dec 2013 02:23:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vrodionov&quot; class=&quot;user-hover&quot; rel=&quot;vrodionov&quot;&gt;Vladimir Rodionov&lt;/a&gt;, we&apos;d like to have the clients see the least downtime for their queries when the primary is not reachable for any reason (including temporary network partition). We want to be doubly sure that we are marking the server dead at the appropriate time - not too soon and not too late. That&apos;s why 20 seconds or so in a cluster of, say, 100 nodes, seems like a good value for a session timeout. Also, in practice there have seen cases where a node appears to be fine but then in reality it isn&apos;t (faulty disk and things like that) and that increases the latency of the responses. We are trying to address the use case where clients are willing to (knowingly)tolerate the staleness of the reads.&lt;/p&gt;

&lt;p&gt;But yeah we should be able to poll for existence of the RS process/node (from a separate process local or remote) and remove the ZK node when we discover that the RS process is down. Discussions around these issues are in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5843&quot; title=&quot;Improve HBase MTTR - Mean Time To Recover&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5843&quot;&gt;&lt;del&gt;HBASE-5843&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13837266" author="enis" created="Tue, 3 Dec 2013 02:58:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;Enis Soztutar in your doc does &quot;group-based assignment&quot; mean assigning multiple regions on a single transaction?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was trying to refer to not having co-location constraints for secondary replicas whose primaries are hosted by the same RS. For example, if R1(replica=0), and R2(replica=0) are hosted on RS1, R1(replica=1) and R2(replica=1) can be hosted by RS2 and RS3 respectively. This can definitely use the hdfs block affinity work though. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However, I don&apos;t think we get into a situation where all RS&apos;s must read all other RS&apos;s logs &#8211; we only need to have the shadows RS&apos;s to read the primary RS&apos;s log.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I am assuming a random distribution of secondary regions per above. In this case, for replication=2, a region server will have half of it&apos;s regions in primary and the other in secondary mode. For all the regions in the secondary mode, it has to tail the logs of the rs where the primary is hosted. However, since there is no co-location guarantee, the primaries are also randomly distributed. For n secondary regions, and m region servers, you will have to tail the logs of most of the RSs if n &amp;gt; m with a high probability (I do not have the smarts to calculate the exact probability) &lt;/p&gt;</comment>
                            <comment id="13837271" author="devaraj" created="Tue, 3 Dec 2013 03:04:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;, WAL per region (WALpr) would give you the locality (and hence HDFS short circuit) of reads if you were to couple it with the favored nodes. The cost is of course more WAL files... In the current situation (no WALpr) it would create quite some traffic cross machine, no?&lt;/p&gt;</comment>
                            <comment id="13837279" author="jmhsieh" created="Tue, 3 Dec 2013 03:20:39 +0000"  >&lt;p&gt;This discussion might be better to have on the dev@ list then here.  Shall we start a thread there?&lt;/p&gt;</comment>
                            <comment id="13837408" author="jmhsieh" created="Tue, 3 Dec 2013 06:57:11 +0000"  >&lt;p&gt;I&apos;ve posted responses to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt; on dev@&lt;/p&gt;</comment>
                            <comment id="13837440" author="k4j" created="Tue, 3 Dec 2013 08:16:27 +0000"  >&lt;p&gt;Have we considered using Apache Helix. It provides most of the features required to achieve this. At LinkedIn, we have solved a similar problem for Espresso (&lt;a href=&quot;http://www.slideshare.net/amywtang/espresso-20952131&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.slideshare.net/amywtang/espresso-20952131&lt;/a&gt;)  where we have primary and secondary for each shard. Secondary is used to read when primary is unavailable. Helix also promotes the secondary to primary when the primary goes down.&lt;/p&gt;</comment>
                            <comment id="13837844" author="devaraj" created="Tue, 3 Dec 2013 16:20:32 +0000"  >&lt;p&gt;@Kishore, one of the guiding thoughts is that HBase core should support this feature natively rather than use another infrastructure.&lt;/p&gt;</comment>
                            <comment id="13849978" author="enis" created="Tue, 17 Dec 2013 01:32:20 +0000"  >&lt;p&gt;Update: as discussed, we have put a proof-of-concept implementation for a working end-to-end scenario, and would like to share that to get some early reviews and feedback. If you are interested on the technical side of the changes, please check the patch/branch out. Please note that the patches and the branch is far from being clean and complete, but otherwise clean enough to understand the scope of changes and areas that are touched. This also contains the end-to-end API&apos;s at the client side (except for execution policies). We will continue to work on the patches to get them in a more mature state, and recreate and clean up the patches for reviews, but at any stage, reviews / comments are welcome. We will keep pushing the changes to this repo / branch until the patches are in a more stable state, at which point, we will work on cleaning up and shuffling the patches to be more consumable by reviews. &lt;/p&gt;

&lt;p&gt;The code is at github repo: &lt;a href=&quot;https://github.com/enis/hbase.git&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/enis/hbase.git&lt;/a&gt;, and the branch is hbase-10070-demo. This repository is based on 0.96.0 for now. I&apos;ll also attach a patch which contains all the changes if you want to take a closer look.  &lt;br/&gt;
This can be build with:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
git clone git@github.com:enis/hbase.git
cd hbase 
git checkout hbase-10070-demo 
MAVEN_OPTS=&lt;span class=&quot;code-quote&quot;&gt;&quot;-Xmx2g&quot;&lt;/span&gt; mvn clean install &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; assembly:single -Dhadoop.profile=2.0  -DskipTests  -Dmaven.javadoc.skip=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; -Dhadoop-two.version=2.2.0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The tar ball generated would be hbase-assembly/target/hbase-0.96.0-bin.tar.gz&lt;br/&gt;
The hadoop version that should be used for real cluster testing is 2.2.0. &lt;/p&gt;

&lt;p&gt;What&apos;s there in the repository:&lt;br/&gt;
1. Client (Shell) changes&lt;br/&gt;
The shell has been modified so that tables with more than one replica per region can be created:&lt;br/&gt;
  create &apos;t1&apos;, &apos;f1&apos;, &lt;/p&gt;
{REGION_REPLICATION =&amp;gt; 2}&lt;br/&gt;
One can also &apos;describe&apos; a table and that will have the replica configuration in the response string:&lt;br/&gt;
  describe &apos;t1&apos;&lt;br/&gt;
One can do a &apos;get&apos; with the eventual consistency flag set (we haven&apos;t implemented the consistency semantics for the &apos;scan&apos; family in this drop):&lt;br/&gt;
  get &apos;t2&apos;,&apos;r6&apos;,{&quot;EVENTUAL_CONSISTENCY&quot; =&amp;gt; true}&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;NOTE THE quotes around the EVENTUAL_CONSISTENCY string. Will fix this soon to work without the quotes.&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;br/&gt;
Outside the shell, the API to do with setting the willingness to tolerate eventual consistency is Get.setConsistency and the returned Result can be queried if it is stale or not via Result.isStale&lt;br/&gt;
&lt;br/&gt;
2. Master changes&lt;br/&gt;
The one main change here is about creation and management of replica HRegionInfo objects. The other change is to make the StochasticLoadBalancer aware of the replicas. During the assignment process, the Assignment Manager consults the balancer to give it a plan for the assignment - here the StochasticLoadBalancer ensures that the plan takes into account the constraint - primary/secondary not assigned to the same server, same rack (if more than one rack configured).&lt;br/&gt;
&lt;br/&gt;
3. RegionServer changes&lt;br/&gt;
The one main change here is to be able to open regions in readonly mode. The other change here is to do with the periodic refresh of store files. The configuration that sets this up is (this is disabled by default):
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &amp;lt;property&amp;gt;
   &amp;lt;name&amp;gt;hbase.regionserver.storefile.refresh.period&amp;lt;/name&amp;gt;
   &amp;lt;value&amp;gt;2000&amp;lt;/value&amp;gt;
 &amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;br/&gt;
4. UI changes&lt;br/&gt;
The UIs corresponding to the tables&apos; status and the regions&apos; status have been modified to say whether they have replicas.&lt;br/&gt;
&lt;br/&gt;
There are unit tests - TestMasterReplicaRegions,TestRegionReplicas,TestReplicasClient,TestBaseLoadBalancer and some others. &lt;br/&gt;
&lt;br/&gt;
There is also a manual test scenario to test out reads coming from the secondary replica:&lt;br/&gt;
1. create a (at least) two node cluster.  &lt;br/&gt;
2. create a table with replica 2. From HBase shell:&lt;br/&gt;
  create &apos;t1&apos;, &apos;f1&apos;, {REGION_REPLICATION =&amp;gt; 2}
&lt;p&gt;3. arrange the regions so that the the primary region is not co-located with meta or namespace regions.&lt;br/&gt;
 You can use move commands in HBase shell for that: &lt;br/&gt;
 move &apos;4392870ae8ef482406c272eec0312a02&apos;, &apos;192.168.0.106,60020,1387069812919&apos;&lt;/p&gt;

&lt;p&gt;4. from the shell do a couple of puts and then &apos;flush&apos; the table from the shell&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hbase(main):005:0&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i in 1..100
hbase(main):006:1&amp;gt; put &apos;t1&apos;, &lt;span class=&quot;code-quote&quot;&gt;&quot;r#{i}&quot;&lt;/span&gt;, &apos;f1:c1&apos;, i
hbase(main):007:1&amp;gt; end
hbase(main):009:0&amp;gt; flush &apos;t1&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;5. suspend the region server which is hosting the primary region replica by sending kill -STOP signal from bash:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
kill -STOP &amp;lt;pid_of_region_server&amp;gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;6. get a row from the table with eventual-consistency flag set to true.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 get &apos;t2&apos;,&apos;r6&apos;,{&lt;span class=&quot;code-quote&quot;&gt;&quot;EVENTUAL_CONSISTENCY&quot;&lt;/span&gt; =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;7. put should fail&lt;br/&gt;
The (6) and (7) steps should be done quickly enough otherwise the master would recover the region!! (Default ZK session timeout is 90 seconds)&lt;/p&gt;</comment>
                            <comment id="13872572" author="stack" created="Wed, 15 Jan 2014 20:47:36 +0000"  >&lt;p&gt;Here&apos;s a few comment on the JIRA content.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In the present HBase architecture, it is hard, probably impossible, to satisfy constraints like 99th percentile of the reads will be served under 10 ms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Should this be an architectural objective for HBase?  Just asking.  Our inspiration addressed the 99th percentile in a layer above.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Of these, the detection is usually the longest and is presently in the order of 20-30 seconds.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We should work on this for sure.  Native zk client immune to JVM pause has come up in the past.  Would help all around (as per the Vladimir comment above)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However, some clients will be better served if regions will be available for reads during recovery for doing eventually consistent reads. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Radical!  Our DNA up to this has been all about giving the application a consistent view.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For improving read availability, we propose a replicated read-only region serving design, also referred as secondary regions, or region shadows. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could this be build as a layer on top of HBase rather than alter HBase core with shims on clients and CPs?&lt;/p&gt;

&lt;p&gt;Do you envision this feature being always on?  Or can it be disabled?  If the former (or latter actually), what implications for current read/write paths do you see?&lt;/p&gt;

&lt;p&gt;This is far along.  I&apos;m late to the game.  Sorry about that.  Let me take a look at design and github.&lt;/p&gt;</comment>
                            <comment id="13872683" author="enis" created="Wed, 15 Jan 2014 22:15:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;Should this be an architectural objective for HBase? Just asking. Our inspiration addressed the 99th percentile in a layer above.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think we should still focus on individual read latencies and try ti minimize the jitter. Obviously, things like hdfs quorum reads, etc are helpful in this respect, and we also plan to incorporate that kind of work together with this. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We should work on this for sure. Native zk client immune to JVM pause has come up in the past. Would help all around (as per the Vladimir comment above)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed. But MTTR is orthogonal I think. In a region being single-homed world, there is no way you can get away without some timeout. We can try to reduce it in cases, but a network partition can always happen. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Radical! Our DNA up to this has been all about giving the application a consistent view.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep, we are not proposing to change the default semantics, just giving the flexibility if the tradeoffs are justifiable on the user side. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could this be build as a layer on top of HBase rather than alter HBase core with shims on clients and CPs?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think the most clean way is to bake this into HBase proper. These are some of the reasons we went with this instead of proposing a layer above: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Regardless of eventual consistency for writes, Replicated read only tables or bulk-load only tables are one of the major design goals for this work as well. This can and should be addressed natively by HBase I would argue. The eventual consistency work just extends this further on a use case basis.&lt;/li&gt;
	&lt;li&gt;RPC failover + RPC cancellation is not possible to do from outside (or at least easily)&lt;/li&gt;
	&lt;li&gt;A higher level API cannot easily tap into LB to ensure that region replicas are not co-hosted.&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;Do you envision this feature being always on? Or can it be disabled? If the former (or latter actually), what implications for current read/write paths do you see?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The branch adds REGION_REPLICATION which is a per-table conf, and get/scan.setConsistency() API which is per request. The write path is not affected at all. On the read path, we do a failover (backup) RPC similar to &lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/Berkeley-Latency-Mar2012.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/Berkeley-Latency-Mar2012.pdf&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="13872684" author="enis" created="Wed, 15 Jan 2014 22:16:07 +0000"  >&lt;p&gt;Thanks for looking BTW. &lt;/p&gt;</comment>
                            <comment id="13872752" author="stack" created="Wed, 15 Jan 2014 23:01:12 +0000"  >&lt;p&gt;Feedback on the design:&lt;/p&gt;

&lt;p&gt;&quot;...some&#160;clients&#160;will&#160;be&#160;better&#160;served&#160;if&#160;regions&#160;will&#160;be&#160;available&#160;for&#160;reads&#160;during&#160;recovery for&#160;doing&#160;eventually&#160;consistent&#160;reads&quot;&lt;/p&gt;

&lt;p&gt;Has anyone asked for this feature on the list or in issues?  You have a hard user for this feature or is this speculative work?  If so, is the user/customer looking explicitly to be able to do stale reads or is giving them stale data a compromise on what they are actually asking for (Let me guess, HA consistent view).  If HA consistent view is what they want, should we work on that instead?&lt;/p&gt;

&lt;p&gt;What delivery timeline are we talking?  Is this is for 1.0 hbase?  Or is it post 1.0?  If the later, maybe we want to go more radical than what is being proposed here.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However,&#160;some&#160;clients&#160;will&#160;be&#160;better&#160;served&#160;if&#160;regions&#160;will&#160;be&#160;available&#160;for&#160;reads&#160;during&#160;recovery for&#160;doing&#160;eventually&#160;consistent&#160;reads.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Should such clients use another data store, one that allows eventually consistent views?  Clients having to deal with sometimes stale data will be more involved (I like this deduction from the F1 paper &quot;Designing applications to cope with concurrency anomalies in their data is very error-prone, time-consuming, and ultimately not worth the performance gains&quot;).&lt;/p&gt;

&lt;p&gt;Pardon all the questions.  I am concerned that a prime directive, consistent view, is being softened.  As is, its easy saying what we are.  Going forward, lets not get to a spot where we have to answer &quot;It is complicated...&quot; when asked if we are a consistent store or not.&lt;/p&gt;

&lt;p&gt;This document is nicely written.&lt;/p&gt;

&lt;p&gt;Here are some random comments as I go through the doc:&lt;/p&gt;

&lt;p&gt;+ Could we implement this feature with some minor changes in core and then stuff like clients that can do the read replica dance done as subclass of current client &amp;#8211; a read replica client &amp;#8211; or at a layer above current client?&lt;br/&gt;
+ Why notions of secondary and tertiary?  Isn&apos;t it primary and replica only?&lt;br/&gt;
+ &quot;Two&#160;region&#160;replicas&#160;cannot&#160;be&#160;hosted&#160;at&#160;the&#160;same&#160;RS&#160;(hard)&quot;  If RS count is &amp;lt; # of replicas, this is relaxed I&apos;m sure (hard becomes soft).  Hmm... this seems complicated: &quot;If&#160;LB&#160;cannot&#160;assign&#160;secondary&#160;replicas&#160;due&#160;to&#160;above&#160;hard&#160;constraints,&#160;the&#160;replica will&#160;be&#160;added&#160;to&#160;this&#160;map.&#160;In&#160;case&#160;of&#160;new&#160;region&#160;servers&#160;joining&#160;the&#160;cluster,&#160;LB&#160;will&#160;be&#160;invoked over&#160;this&#160;map.&quot;  Suggest dropping it for first cut.&lt;br/&gt;
+ Seems wrong having RCP consious of replicas.  I&apos;d think this managed at higher levels up in HCM.&lt;/p&gt;

&lt;p&gt;This design looks viable to me and the document is of good quality.&lt;/p&gt;







</comment>
                            <comment id="13872830" author="enis" created="Wed, 15 Jan 2014 23:52:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;Has anyone asked for this feature on the list or in issues? You have a hard user for this feature or is this speculative work? If so, is the user/customer looking explicitly to be able to do stale reads or is giving them stale data a compromise on what they are actually asking for (Let me guess, HA consistent view). If HA consistent view is what they want, should we work on that instead?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Should such clients use another data store, one that allows eventually consistent views? Clients having to deal with sometimes stale data will be more involved&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We have a customer use case which is the main driver, but while in the design stage, we are also having some interest from other prospects as well. The main use case is actually not HA consistent view. Eventual consistency is a kind of a misnomer actually, the main use case is to be able to read at least some data back in case of a failover. It is more like a &quot;timeline consistency&quot; rather than the eventual consistency a-la dynamo. That data might be stale as long as it is acknowledged as it is, but for serving the data to the web tier, there should be a better guarantee than our current MTTR story. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I am concerned that a prime directive, consistent view, is being softened. As is, its easy saying what we are. Going forward, lets not get to a spot where we have to answer &quot;It is complicated...&quot; when asked if we are a consistent store or not.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Again, the main semantics is not changed. As per the tradeoffs section, we are trying to add the flexibility for some tradeoff. Our whole CAP story is not a black-or-white choice. We are strongly consistent for single row updates, while highly available across regions (an RS going down does not affect other regions not hosted there), and eventual consistent across-DC.  &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Could we implement this feature with some minor changes in core and then stuff like clients that can do the read replica dance done as subclass of current client &#8211; a read replica client &#8211; or at a layer above current client?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Seems wrong having RCP consious of replicas. I&apos;d think this managed at higher levels up in HCM.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; do you want to chime in? &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Why notions of secondary and tertiary? Isn&apos;t it primary and replica only?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I should revisit wording I guess. I think we are using the terminology primary &amp;lt;=&amp;gt; replicaId =0, secondaries &amp;lt;=&amp;gt; replicaId &amp;gt; 0, and secondary &amp;lt;=&amp;gt; replicaId = 1,  tertiary &amp;lt;=&amp;gt; replicaId = 2, etc. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;Two region replicas cannot be hosted at the same RS (hard)&quot; If RS count is &amp;lt; # of replicas, this is relaxed I&apos;m sure (hard becomes soft). Hmm... this seems complicated: &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed. I&apos;ll update the doc reflecting that. Initially, I though we should do underReplicated regions kind of thing, but it requires more intrusive changes to AM / LB since we have to keep these around etc. Now the LB design is simpler in that, we just try to not co-host region replicas, but if we cannot (in case replication &amp;gt; # RS or # racks etc) we simply assign the regions anyway. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This design looks viable to me and the document is of good quality.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;</comment>
                            <comment id="13872848" author="stack" created="Thu, 16 Jan 2014 00:01:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;It is more like a &quot;timeline consistency&quot; rather than the eventual consistency a-la dynamo.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let us call it &apos;timeline consistency&apos; from here on out then while it still in the inception phase. Let us not allow &apos;eventually consistent&apos; anywhere near our lexicon else we will only confuse.&lt;/p&gt;
</comment>
                            <comment id="13873009" author="enis" created="Thu, 16 Jan 2014 03:46:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;What delivery timeline are we talking? Is this is for 1.0 hbase? Or is it post 1.0? If the later, maybe we want to go more radical than what is being proposed here.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Forgot to answer that earlier. I think it will depend on the timing of 1.0 vs timing of this branch. I think we can get all of the patches (for phase 1) in a mature and reviewed state in 1-2 months. Then at the time of the merge proposal, we can see how far along we are at the 1.0 release candidate. AFAIK, current trunk does not contain anything that is not there in 0.98, so we that means whether we should go with 1.0 = a mature version of 0.98 approach or 1.0 = mature version of 0.99 approach. I prefer the latter, but that is up for discussion in the mailing list I guess. &lt;/p&gt;</comment>
                            <comment id="13875417" author="stack" created="Fri, 17 Jan 2014 23:44:39 +0000"  >&lt;p&gt;Ok on the timing.  You know how I feel about 1.0 &amp;#8211; sooner rather than later &amp;#8211; but hopefully this feature gets done in time.&lt;/p&gt;

&lt;p&gt;Looking at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt;, I have a &apos;design level&apos; concern so let me raise it here rather than there.  Let me repeat a comment I made there:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;After thinking more on this, I &apos;get&apos; why you have the replicas listed inside in the row rather than as rows themselves &lt;span class=&quot;error&quot;&gt;&amp;#91;in hbase:meta&amp;#93;&lt;/span&gt;.  The row in hbase:meta becomes a proxy or facade for the little cluster of regions one of which is the primary with the others read replicas.  If that is the case, lets recognize it as so and make proper accommodation in the code base and model.&lt;/p&gt;

&lt;p&gt;Problems I see are:&lt;/p&gt;

&lt;p&gt;+ HRegionInfo now is overloaded.  Before it was the info on a specific region.  Now it is trying to serve two purposes; its original intent and now too as a descriptor on the region-serving &apos;cluster&apos; made of a primary and replicas.  Lets avoid overloading what up to this has had a clear role in the hbase model.&lt;br/&gt;
+ The primary holds the &apos;pole position&apos; being the name of the region in meta.  The read replicas are differently named with the 00001 and 00002, etc., interpolated into the middle of the region name.  I suppose doing it this way &apos;minimizes&apos; the disturbance in the code base but I&apos;m worried this naming exception will only confuse though it minimizes change.  Why would the primary not be named like the replica regions?  &lt;/p&gt;

&lt;p&gt;On the latter I can hear a reply that goes, &quot;For those who do not need read replicas, then they will be unaffected&quot;, which I would counter ensures that this feature will forever be ghetto and no one will use it because it unexercised.&lt;/p&gt;

&lt;p&gt;Trying to ensure that we do not paint ourselves into a corner and to avoid the ghetto, looking beyond read replicas to full-on quorum read/writes, I can imagine we&apos;d need some means like the above where the hbase:meta row name is not longer the physical name of a region but rather a logical name.  The primary region in the quorum in read replicas is the region number 00000 but doing quorum read/writes, the leader will need to be able to change over the life of the quorum.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Going forward, all regions get an index?  By default the index is zero.  When replicas or quorum members, the indices distingush members.  When read replicas the region with index 0 is primary.  When a quorum, the index has no special meaning.  In the past we have had two naming conventions for regions live side by side in the one live cluster.  We could do it again.&lt;/p&gt;</comment>
                            <comment id="13875452" author="devaraj" created="Sat, 18 Jan 2014 00:54:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;Ok on the timing. You know how I feel about 1.0 &#8211; sooner rather than later &#8211; but hopefully this feature gets done in time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah.. couple of us are on it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;After thinking more on this, I &apos;get&apos; why you have the replicas listed inside in the row rather than as rows themselves &lt;span class=&quot;error&quot;&gt;&amp;#91;in hbase:meta&amp;#93;&lt;/span&gt;. The row in hbase:meta becomes a proxy or facade for the little cluster of regions one of which is the primary with the others read replicas. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s great. A copy-paste of what I said in the RB on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt; for others&apos; reference.&lt;br/&gt;
&quot;I and Enis had debated this as well. The consensus between us was that we don&apos;t need to add new META rows for the replicas. After all, the HRI information is exactly the same for all the replicas except for the replicaID. In the current meta, we already have a column for the location of a region. It seemed logical to just extend that model - add newer columns for the replica locations (and similarly for the other columns like seqnum). That way everything for a particular user-visible region stays in one row (and makes it easier for readers to know about all replica locations from that one row). Regarding special casing, yes there is some special casing in the way the regions are added to the meta - create table will create all regions (if the table was created with replica &amp;gt; 1), but only the primary regions will be added to the meta. The regionserver - when it updates the meta with the location after it opens a region invokes the API passing the replicaID as an argument - the column names are different based on whether the replicaID is primary or not. These are pretty much the special cases for the meta updates.&quot;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;HRegionInfo now is overloaded. Before it was the info on a specific region. Now it is trying to serve two purposes; its original intent and now too as a descriptor on the region-serving &apos;cluster&apos; made of a primary and replicas. Lets avoid overloading what up to this has had a clear role in the hbase model.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;By doing it the way we have in the patch on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt;, it seems to reflect what&apos;s going on - &quot;HRI is a logical descriptor and a facade for a bunch of primary &amp;amp; replicas&quot;. That&apos;s how we store things in the meta and how we reconstruct HRIs from the meta when needed.&lt;br/&gt;
There are possibly other approaches of doing this. E.g. Extend HRegionInfo as, say, HRegionInfoReplica and maintain the information about replicaID there, and/or change all the relevant methods to accept HRegionInfoReplica and potentially return this as well in relevant situations. The issue there is those approaches would be very intrusive and we would still need special cases for replicaID == 0 or not. Not confident how much we would gain there. Is it too much to ask to change the view of what a HRI means (to what you say above). Anyway, let me ponder a bit on this...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The primary holds the &apos;pole position&apos; being the name of the region in meta. The read replicas are differently named with the 00001 and 00002, etc., interpolated into the middle of the region name. I suppose doing it this way &apos;minimizes&apos; the disturbance in the code base but I&apos;m worried this naming exception will only confuse though it minimizes change. Why would the primary not be named like the replica regions?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t mind naming the primary regions similar to the replicas. This might mean tools that currently depend on the name format would break even if the cluster is not deploying tables with replicas (you guessed that response &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) But yeah, if you go the full Paxos route, the &apos;primary&apos; could be anyone in the replica-set and there it makes sense to have all members in the set to have an index.&lt;/p&gt;</comment>
                            <comment id="13875471" author="stack" created="Sat, 18 Jan 2014 01:23:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;....Is it too much to ask to change the view of what a HRI means&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No.  As long as we are clear what we are about.&lt;/p&gt;

&lt;p&gt;It is not clear in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt; that HRI&apos;s role is changed from description of actual region to instead, description of a region &apos;cluster&apos;.  It needs to be made more plain.&lt;/p&gt;

&lt;p&gt;Regards...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;...Regarding special casing, yes there is some special casing in the way the regions are added to the meta - create table will create all regions (if the table was created with replica &amp;gt; 1), but only the primary regions will be added to the meta. The regionserver - when it updates the meta with the location after it opens a region invokes the API passing the replicaID as an argument - the column names are different based on whether the replicaID is primary or not. These are pretty much the special cases for the meta updates.&quot;...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I went back to the design doc and it is too high level for me to &apos;see&apos; the new hbase:meta schema changes and the list of special casings.  I think a description of the new hbase:schema would help.  A list of the &apos;special-casings&apos; would help too but might be easier just looking at code.  A macro-level list perhaps?&lt;/p&gt;

&lt;p&gt;Is &apos;replica&apos; and &apos;replica id&apos; the right term given we want to first put in place mechanisms that can be used for more than just &apos;read replicas&apos;.  Region &apos;set&apos; and set &apos;member&apos;?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This might mean tools that currently depend on the name format would break even if the cluster is not deploying tables with replicas&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Once before, we had two naming schemes in the cluster and via various tricks &amp;#8211; whether there is a &apos;.&apos; on the end &amp;#8211; it was possible to have the cohabit the same cluster instance.  In this case, it might not have to be so radical.  We might just relax the regex that looks at encoded name such that if we encounter a region with more than the usual 20 bytes of md5, then those extra bytes are set member index (no need of a delimiter &amp;#8211; delimiter messes up your sort which may not be an issue if rows in hbase:meta are no longer the actual names of regions).&lt;/p&gt;





</comment>
                            <comment id="13875478" author="enis" created="Sat, 18 Jan 2014 01:35:24 +0000"  >&lt;p&gt;Thanks Stack for looking into this. Valid feedback. &lt;br/&gt;
I think we can model the desired region model with something like this: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Region { table, startKey, endKey, regionId, encodedName} 
RegionReplica {region, replicaId}
RegionReplicaState {offline, split}
RegionReplicaLocation {server, seqId}
RegionLineage { [splitDaughter], [mergeParent] } 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 
&lt;p&gt;I think the challenge is that, currently HRI and the meta layout + the client level API&apos;s will need to be redesigned from scratch if we want to fully switch to this model. Then for example table.getRegions() would only return Region&apos;s, but table.getRegionReplicas() would return smt different. The region assignments and everything should switch to being RegionReplica based rather than HRI based. &lt;/p&gt;

&lt;p&gt;If we keep HRI ~= Region + RegionReplicaState, then the region location API&apos;s, and assignment have to be managed via RegionReplica objects and the APIs have to be overloaded (since there won&apos;t be HRI -&amp;gt; location anymore). Right now what we have is HRI ~= RegionReplica + RegionState. I guess we can spend some time to see whether this is possible without refactoring major portions of the code base, but I fear the answer might be what my intuition says. &lt;/p&gt;

&lt;p&gt;For paxos / quorum case, I think we can keep the special treatment of replicaId == 0 =&amp;gt; primary for now. If we later change the write model, then we can have a leader definition, but the leader would not necessarily mean replicaId = 0. Even in that case, we have to differentiate between a server hosting a specific replica which still required static replicaId or similar. The special case where we do not add the replicaId to the string form of HRI is for not requiring a meta + hdfs regioninfo rewrite. I guess we can add it there, but add special case handling for parsing back. Would that work? &lt;/p&gt;

</comment>
                            <comment id="13875492" author="stack" created="Sat, 18 Jan 2014 02:09:30 +0000"  >&lt;p&gt;You may not have seen my response above where I think we should not talk of replicas in our model all.  We should call the region instances something else and then in a layer above do designations such as &quot;primary&quot; or &quot;leader&quot; and &quot;read-only replica&quot; or &quot;follower&quot;.   If you agree with me, the &apos;desired region model&apos; you propose in your comment is no longer appropriate.&lt;/p&gt;

&lt;p&gt;Sorry Enis, I don&apos;t get your second paragraph above.  Pardon me.  It is a failing on my part.  I&apos;m having trouble untangling what the &apos;Right now&apos; refers to, whether it is trunk or your work on this feature so far.... and what &apos;whether this is possible&apos; refers to. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I guess we can add it there, but add special case handling for parsing back. Would that work?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t done the research but am thinking that a wholesale move to a new region naming scheme for all new region creations better than a scheme where we have &apos;replicas&apos; named with one format and everything else another.&lt;/p&gt;




</comment>
                            <comment id="13875521" author="lhofhansl" created="Sat, 18 Jan 2014 03:13:10 +0000"  >&lt;p&gt;Let&apos;s stick with small quick releases. I think that has worked well with 0.94.&lt;br/&gt;
If it&apos;s ready we pull it in 1.0, if not we can get it into 1.1 or (depending on size) into 2.0.&lt;br/&gt;
As always, just my $0.02.&lt;/p&gt;

&lt;p&gt;Lastly, are we pushing into a domain that is at odds with HBase? It seems the use case targeted would be the domain of Cassandra and similar (yes, I used the C-Word). Consistent reads ... &lt;em&gt;sometimes&lt;/em&gt; ... is weird unless the semantics on when that happens are absolutely crystal clear.&lt;/p&gt;</comment>
                            <comment id="13876066" author="devaraj" created="Mon, 20 Jan 2014 01:07:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; The user specifies he can tolerate the stale reads via flags in the read api. The Results are also tagged as such and so he can inspect whether the result is stale or not. In other words, the user has full control still.&lt;/p&gt;</comment>
                            <comment id="13876087" author="devaraj" created="Mon, 20 Jan 2014 02:04:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, I agree with you that the notion of &quot;replicaID == 0&quot; being a primary replica, etc. should be maintained in a layer outside HRegionInfo. HRegionInfo could come with an &apos;index&apos; and the &apos;index&apos; should be an inherent part of the HRI&apos;s identification (in the name, etc.). The layer outside could associate &quot;index == 0&quot; with &quot;primary&quot; replica, etc.. Will submit a patch along these lines.&lt;/p&gt;</comment>
                            <comment id="13878234" author="enis" created="Wed, 22 Jan 2014 04:10:48 +0000"  >&lt;p&gt;Attaching a more detailed design proposal for the first subtask about HRI + meta changes after the discussions with Stack. &lt;/p&gt;

&lt;p&gt;In the doc, we list two approaches which can be pursued depending on the pros/cons section. We&apos;ll continue to modify the patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt; to see which one is more feasible. &lt;/p&gt;

&lt;p&gt;Please feel free to review + leave comments at the doc. &lt;/p&gt;</comment>
                            <comment id="13883707" author="enis" created="Tue, 28 Jan 2014 03:24:28 +0000"  >&lt;p&gt;I&apos;ve updated the subtask &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt; with a new patch, and also updated the design doc for adding an alternate third proposal which I think we can iterate on it for the final shape. Please review the patch / doc for whether it makes sense or not. &lt;/p&gt;</comment>
                            <comment id="13886974" author="devaraj" created="Thu, 30 Jan 2014 19:42:20 +0000"  >&lt;p&gt;Straightforward patch..&lt;/p&gt;</comment>
                            <comment id="13888061" author="enis" created="Fri, 31 Jan 2014 19:38:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;Straightforward patch..&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I guess this should go to the subtask &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10348&quot; title=&quot;HTableDescriptor changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10348&quot;&gt;&lt;del&gt;HBASE-10348&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="13892918" author="enis" created="Thu, 6 Feb 2014 02:09:51 +0000"  >&lt;p&gt;I have created the branch in svn since we have some patches ready for commit. The branch is &lt;a href=&quot;https://svn.apache.org/repos/asf/hbase/branches/hbase-10070&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/repos/asf/hbase/branches/hbase-10070&lt;/a&gt;, which will be the base for eventual merge back into trunk. The plan is to only commit reviewed patches there with at least 2 +1&apos;s. All subtasks of this jira are scheduled to be committed at that branch. &lt;/p&gt;

&lt;p&gt;We might have to do a final rebase, but hopefully that would be only mechanical and would not need any reviews. &lt;/p&gt;</comment>
                            <comment id="13892924" author="enis" created="Thu, 6 Feb 2014 02:17:28 +0000"  >&lt;p&gt;I also created a version &quot;hbase-10070&quot; to mark the issues that have been committed to the branch. We can set the fix versions of the jira with that patch. Not sure whether it is best practice to resolve those jiras or not. I just left them open for now. &lt;br/&gt;
The first sub task &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10347&quot; title=&quot;HRegionInfo changes for adding replicaId and MetaEditor/MetaReader changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10347&quot;&gt;&lt;del&gt;HBASE-10347&lt;/del&gt;&lt;/a&gt; is committed at the branch. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10348&quot; title=&quot;HTableDescriptor changes for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10348&quot;&gt;&lt;del&gt;HBASE-10348&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10354&quot; title=&quot;Add an API for defining consistency per request&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10354&quot;&gt;&lt;del&gt;HBASE-10354&lt;/del&gt;&lt;/a&gt; will shortly follow. &lt;/p&gt;</comment>
                            <comment id="13906327" author="devaraj" created="Thu, 20 Feb 2014 00:02:18 +0000"  >&lt;p&gt;Let&apos;s resolve the subtask issues after commit to branch.&lt;/p&gt;</comment>
                            <comment id="13906332" author="sershe" created="Thu, 20 Feb 2014 00:05:28 +0000"  >&lt;p&gt;I did just that w/&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10356&quot; title=&quot;Failover RPC&amp;#39;s for multi-get &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10356&quot;&gt;&lt;del&gt;HBASE-10356&lt;/del&gt;&lt;/a&gt;. Don&apos;t forget to change fix version though&lt;/p&gt;</comment>
                            <comment id="13906335" author="enis" created="Thu, 20 Feb 2014 00:08:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;Let&apos;s resolve the subtask issues after commit to branch.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ok. I resolved the issues that were committed to hbase-10070, and changed the fix versions to only include the branch name. Once we merge the branch to trunk, we can add the fix versions.&lt;/p&gt;</comment>
                            <comment id="13906469" author="jmhsieh" created="Thu, 20 Feb 2014 01:49:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;I also created a version &quot;hbase-10070&quot; to mark the issues that have been committed to the branch. We can set the fix versions of the jira with that patch. Not sure whether it is best practice to resolve those jiras or not. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds good to me.  When we did snapshots, we committed and resolved the affects and target version to the branch version.  When we successfully merged we changed it to the normal version number (back then 0.95).  As an example, take a look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7339&quot; title=&quot;Splitting a hfilelink causes region servers to go down.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7339&quot;&gt;&lt;del&gt;HBASE-7339&lt;/del&gt;&lt;/a&gt;&apos;s history.&lt;/p&gt;</comment>
                            <comment id="13906506" author="enis" created="Thu, 20 Feb 2014 02:28:10 +0000"  >&lt;p&gt;Thanks Jonathan. We&apos;ll use the same approach here. 8 issues resolved already. &lt;/p&gt;</comment>
                            <comment id="13964492" author="tychang" created="Wed, 9 Apr 2014 18:12:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; Is the 3rd option of Async WAL replication feature already implemented? &lt;/p&gt;</comment>
                            <comment id="13965150" author="enis" created="Thu, 10 Apr 2014 09:09:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;Enis Soztutar Is the 3rd option of Async WAL replication feature already implemented?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not yet, we are finishing the phase 1 as identified in the doc. Only the scan support is left for that. Afterwards, we plan to continue on implementing the asyn wal approach with a design proposal of it&apos;s own. Are you interested in using async wal replication separately, or you are asking whether the feature as a whole is available? &lt;/p&gt;</comment>
                            <comment id="13965533" author="tychang" created="Thu, 10 Apr 2014 16:51:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; I am interested in the feature as a whole.  I can see Get a single row is already supported. How about the get(list&amp;lt;Get&amp;gt;)? I seems cannot find it. Will that be supported later together with scan in phase 1? &lt;/p&gt;</comment>
                            <comment id="13965556" author="sershe" created="Thu, 10 Apr 2014 17:15:52 +0000"  >&lt;p&gt;list get is also supported, although some changes need to be committed to address corner cases, like &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10794&quot; title=&quot;multi-get should handle replica location missing from cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10794&quot;&gt;&lt;del&gt;HBASE-10794&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13965779" author="tychang" created="Thu, 10 Apr 2014 20:00:08 +0000"  >&lt;p&gt;I am using Enis git branch of 10070, it has commit until Jan 30. That is probably why I don&apos;t see it? I checked the 10070-working branch, it has up to Feb 13, seems still not update enough. Do you know which branch has the multi get support? &lt;/p&gt;</comment>
                            <comment id="13965781" author="yuzhihong@gmail.com" created="Thu, 10 Apr 2014 20:02:59 +0000"  >&lt;p&gt;Tianying, here is the branch:&lt;br/&gt;
&lt;a href=&quot;https://svn.apache.org/repos/asf/hbase/branches/hbase-10070&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/repos/asf/hbase/branches/hbase-10070&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13965916" author="tychang" created="Thu, 10 Apr 2014 22:01:13 +0000"  >&lt;p&gt;Got it. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zhihyu%40ebaysf.com&quot; class=&quot;user-hover&quot; rel=&quot;zhihyu@ebaysf.com&quot;&gt;Ted Yu&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13981722" author="enis" created="Fri, 25 Apr 2014 22:47:16 +0000"  >&lt;p&gt;Changed issue title, according to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10354&quot; title=&quot;Add an API for defining consistency per request&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10354&quot;&gt;&lt;del&gt;HBASE-10354&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14004159" author="mantonov" created="Wed, 21 May 2014 00:35:53 +0000"  >&lt;p&gt;Guys, sorry for coming to this topic really late, but there&apos;re some considerations I&apos;d like to bring up (I&apos;m looking at whatever is committed currently in the branch).&lt;/p&gt;

&lt;p&gt;I think about the client API changes needed for consistency (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10354&quot; title=&quot;Add an API for defining consistency per request&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10354&quot;&gt;&lt;del&gt;HBASE-10354&lt;/del&gt;&lt;/a&gt;) in light of the consensus-based effort which aims to having multiple active replicas, and how to make those 2 approaches smoothly work together so multiple active replicas could be built just upon this solid foundation, perhaps there may be more flexible alternative.&lt;/p&gt;

&lt;p&gt;Instead of defining enum Consistency &lt;/p&gt;
{strong, timeline}
&lt;p&gt; and hooking into Get and Scan,  and defining several possible internal strategies on how to send RPCs based on that (&quot;primary timeout&quot;, &quot;parallel&quot;, &quot;parallel with delay&quot; ) may be we can define pluggable strategy on how to execute RPCs? Similar to HDFS FailoverProxyProvider, which can be defined in the client&apos;s config.&lt;/p&gt;

&lt;p&gt;This way we can use pluggable:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;no-op&quot; provider, to have behavior like what we have now in trunk&lt;/li&gt;
	&lt;li&gt;timeline provider, which would work as described here&lt;/li&gt;
	&lt;li&gt;provider which can work with multiple active replicas and round-robin between them if some of them fail.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="14004184" author="cos" created="Wed, 21 May 2014 01:19:23 +0000"  >&lt;p&gt;Can I ask what definition of &lt;b&gt;timeline consistency&lt;/b&gt; are we using here? I am a little bit uncomfortable with it as I don&apos;t know what exactly is implied by this term.&lt;/p&gt;

&lt;p&gt;To explain further on this: in case of a consensus based replication (described in the Design Document attached to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10909&quot; title=&quot;Abstract out ZooKeeper usage in HBase - phase 1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10909&quot;&gt;&lt;del&gt;HBASE-10909&lt;/del&gt;&lt;/a&gt;) we are claiming that all writable active replicas are &lt;b&gt;one copy equivalent&lt;/b&gt; or strong consistency across replication that reached the same GSN. In case of this JIRA, the &lt;b&gt;strong consistency&lt;/b&gt; with just a single writable replica (and no RO slaves) has the same semantic. I believe by providing a pluggable fail-over policy implementation we will guarantee that &lt;b&gt;strong consistency&lt;/b&gt; in case of a consensus based replication has the same semantical meaning as in case of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10070&quot; title=&quot;HBase read high-availability using timeline-consistent region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10070&quot;&gt;&lt;del&gt;HBASE-10070&lt;/del&gt;&lt;/a&gt;. In other words, we&apos;ll provide the implementation of the semantic instead of a documentation of a such.&lt;/p&gt;

&lt;p&gt;Relaying to an earlier Stack&apos;s comment:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Pardon all the questions. I am concerned that a prime directive, consistent view, is being softened. As is, its easy saying what we are. Going forward, lets not get to a spot where we have to answer &quot;It is complicated...&quot; when asked if we are a consistent store or not&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;shall we try to provide a harder consistency guarantees, while covering the weaker ones en-route?&lt;/p&gt;</comment>
                            <comment id="14005044" author="devaraj" created="Wed, 21 May 2014 18:27:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; yes, we did think about a pluggable client provider. We put that as future work since we wanted to focus on implementing all the bits and pieces for the timeline consistency first.&lt;/p&gt;

&lt;p&gt;@cos, the timeline definition is here - &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10513&quot; title=&quot;Provide user documentation for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10513&quot;&gt;&lt;del&gt;HBASE-10513&lt;/del&gt;&lt;/a&gt;. We already have the timeline stuff working (and tested). We could already hide the fact that a consensus based strong consistency (when that is available) is being used by the servers behind the default consistency level which is STRONG.&lt;/p&gt;

&lt;p&gt;In summary, yes, we should consider a pluggable provider but I think that would be an incremental step from what has been done.&lt;/p&gt;

&lt;p&gt;What has been done so far addresses timeline consistency. I also look at the work we have done as building blocks for consensus based strong consistency.&lt;/p&gt;</comment>
                            <comment id="14005073" author="apurtell" created="Wed, 21 May 2014 18:41:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can I ask what definition of timeline consistency are we using here? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe the Yahoo PNUTS paper is where many have first heard the term &apos;timeline consistency&apos;. Daniel Abadi summarizes it at &lt;a href=&quot;http://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html&lt;/a&gt; as &quot;where replicas may not be consistent with each other but updates are guaranteed to be applied in the same order at all replicas&quot;, which I think is very concise.&lt;/p&gt;

&lt;p&gt;If we claim this about &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10070&quot; title=&quot;HBase read high-availability using timeline-consistent region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10070&quot;&gt;&lt;del&gt;HBASE-10070&lt;/del&gt;&lt;/a&gt;, is that accurate? (I think yes.) &lt;/p&gt;

&lt;p&gt;If so, the documentation committed on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10513&quot; title=&quot;Provide user documentation for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10513&quot;&gt;&lt;del&gt;HBASE-10513&lt;/del&gt;&lt;/a&gt; doesn&apos;t summarize the term as cleanly in my opinion. Might be good to update the lead in to the timeline consistency part of the doc.&lt;/p&gt;</comment>
                            <comment id="14005082" author="stack" created="Wed, 21 May 2014 18:50:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;and hooking into Get and Scan, and defining several possible internal strategies on how to send RPCs based on that (&quot;primary timeout&quot;, &quot;parallel&quot;, &quot;parallel with delay&quot; ) may be we can define pluggable strategy on how to execute RPCs? Similar to HDFS FailoverProxyProvider, which can be defined in the client&apos;s config.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; So, rather than have the client ask for level of &apos;consistency&apos; in the API, instead, the replica interaction would be set on client construction dependent on the plugin supplied?&lt;/p&gt;

&lt;p&gt;In the API at the moment we have STRONG and TIMELINE (What happens if I ask for TIMELINE and cluster is not deployed with read replicas?  Ignored?).  If we were to add QUORUM_STRONG, are we thinking that a client should be able to choose amongst these options?  Will that fly?  At the moment, as noted, we have amended Get and Scan.  We&apos;ll have to amend all ops if we follow the path of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10513&quot; title=&quot;Provide user documentation for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10513&quot;&gt;&lt;del&gt;HBASE-10513&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;How hard to evolve from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10513&quot; title=&quot;Provide user documentation for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10513&quot;&gt;&lt;del&gt;HBASE-10513&lt;/del&gt;&lt;/a&gt; to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; suggestion?&lt;/p&gt;</comment>
                            <comment id="14005198" author="enis" created="Wed, 21 May 2014 20:36:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;I believe the Yahoo PNUTS paper is where many have first heard the term &apos;timeline consistency&apos;. Daniel Abadi summarizes it at &lt;a href=&quot;http://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dbmsmusings.blogspot.com/2010/04/problems-with-cap-and-yahoos-little.html&lt;/a&gt; as &quot;where replicas may not be consistent with each other but updates are guaranteed to be applied in the same order at all replicas&quot;, which I think is very concise. .... Might be good to update the lead in to the timeline consistency part of the doc.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed. Yes, the name was inspired by the PNUTS model. I like the concise form. I&apos;ll add it to the doc. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;we can define pluggable strategy on how to execute RPCs&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The Consistency enum is mostly concerned about semantics, while execution layer (RPC) is concerned about latency + performance. Even within a given Consistency model, you may want different execution strategies I think (like for TIMELINE consistency, parallel and parallel with delay, or go to first replica, then second, then third, etc). In the committed code in branch, the consistency model implies hard coded execution model. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What happens if I ask for TIMELINE and cluster is not deployed with read replicas? Ignored&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good question. The execution strategy for TIMELINE replicas is to do the primary RPC first, then if no response after some delay (10ms by default) do the RPCs for secondaries. If the region has only 1 replica, there won&apos;t be any RPC, so it will just wait for the primary RPC to response back.  &lt;/p&gt;</comment>
                            <comment id="14005257" author="vrodionov" created="Wed, 21 May 2014 21:23:43 +0000"  >&lt;p&gt;Can somebody explain  (in a few sentences) how does the approach, described in this JIRA, differ from the following one:&lt;/p&gt;

&lt;p&gt;1. Have master cluster for writes/reads&lt;br/&gt;
2. Have secondary cluster for reads only&lt;br/&gt;
3. Set up replication master-&amp;gt; secondary&lt;br/&gt;
4. Keep replication lag under control.&lt;br/&gt;
5. Add secondary cluster support to HBase client library.&lt;/p&gt;

&lt;p&gt;From the H/W usage point of view both approaches require additional resources anyway.  &lt;/p&gt;</comment>
                            <comment id="14005271" author="devaraj" created="Wed, 21 May 2014 21:38:18 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;~vladrodionov&amp;#93;&lt;/span&gt;, this solution is complementary to the multi-dc HA. It addresses regionserver failures within a single cluster. Not sure if it was obvious or not, but this solution doesn&apos;t introduce more copies of hfiles in the cluster. There is still a single copy of the hfiles in a given cluster. The &quot;Tradeoffs&quot; section in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10513&quot; title=&quot;Provide user documentation for region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10513&quot;&gt;&lt;del&gt;HBASE-10513&lt;/del&gt;&lt;/a&gt; talks about the other resource overheads added if this solution is used.&lt;/p&gt;</comment>
                            <comment id="14005780" author="mantonov" created="Thu, 22 May 2014 09:22:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt;,  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;,  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yeah, the reason I brought it up is that unlike changes for example in LB, this is public (yet evolving) API, so just wanted to double-check that we don&apos;t expose to client code details which would limit us later.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Even within a given Consistency model, you may want different execution strategies I think (like for TIMELINE consistency, parallel and parallel with delay, or go to first replica, then second, then third, etc). In the committed code in branch, the consistency model implies hard coded execution model.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sure, any consistency model (except current behavior i guess) would benefit from being customizable.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, rather than have the client ask for level of &apos;consistency&apos; in the API, instead, the replica interaction would be set on client construction dependent on the plugin supplied?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Either &quot;rather&quot; or &quot;both&quot;, I guess. If we could say that level of consistency (strong, timeline or quorum-strong) could be defined in config files per-client (not per-operations), we would be able to avoid having this enum. But we consider that being able to define consistency level per-operation is mandatory, right?&lt;/p&gt;

&lt;p&gt;In that case I&apos;m thinking of the following model:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;deploy pluggable policy at client side which which decide on RPC requests, this policy would be used globally for all requests as default&lt;/li&gt;
	&lt;li&gt;consider the Consistency enum (and point it out in both user and dev level docs) as a &quot;hint&quot;, used only to be able to customize individual scans or gets, and probably add note in class documentation that cluster may ignore the flag set if the feature isn&apos;t available?&lt;/li&gt;
	&lt;li&gt;current timeline consistency model doesn&apos;t assume quorums for write, so I think it makes sense to add QUORUM_STRONG in enum.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="14006238" author="enis" created="Thu, 22 May 2014 18:17:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;But we consider that being able to define consistency level per-operation is mandatory, right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;deploy pluggable policy at client side which which decide on RPC requests, this policy would be used globally for all requests as default&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Right now there is no alternate implementation for normal RPCs, and only 1 model for TIMELINE RPCs. When we have alternate implementations, we can make it pluggable (and configurable per operation). &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;current timeline consistency model doesn&apos;t assume quorums for write, so I think it makes sense to add QUORUM_STRONG in enum.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t like adding this now. Once we have a corresponding implementation for the proposed quorum write we can add it to the enum later. &lt;/p&gt;</comment>
                            <comment id="14006293" author="vrodionov" created="Thu, 22 May 2014 18:43:58 +0000"  >&lt;blockquote&gt;
&lt;p&gt;In the present HBase architecture, it is hard, probably impossible, to satisfy constraints like 99th percentile of the reads will be served under 10 ms&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I did some quick math. To blame bottom 1% on RS down event, means, that, on average (with 1 min MTTR),  we should see ~ 14-15 RS down events per cluster per day (1440 minutes). I think it is way above what we have in a real life. Just a quick math. I am not saying that this is not worth doing, but it will not give us 10ms 99% (this is what I am pretty sure about). Just saying. This is only for RS down type of failures. We all know that HBase cluster may experience other types of temporary disabilities, which may affect read request latency:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;blocked writes under heavy load (probably  reads as well?) - not sure. Solution : tune configuration and throttle incoming requests&lt;/li&gt;
	&lt;li&gt;blocked reads due to blocked writes (no available handlers to serve incoming requests). Solution: have different pools for write/reads or use priority on RPC requests (new feature, correct?)&lt;/li&gt;
	&lt;li&gt;excessive GC (sometimes) - Solution: off heap, off heap, off heap.&lt;/li&gt;
	&lt;li&gt;something else, I forgot or not aware about?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;but all of these should be and must avoided in a properly configured and tuned cluster. &lt;/p&gt;

&lt;p&gt;So, this is basically, to mitigate serious events (RS down) , not transient ones. To improve read request latency distribution there is one classic solution that works for sure - cache, cache, cache. &lt;/p&gt;



</comment>
                            <comment id="14006296" author="mantonov" created="Thu, 22 May 2014 18:45:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; yep, sounds good to me. Adding new value in enum should be backward-compatible.&lt;/p&gt;</comment>
                            <comment id="14006312" author="vrodionov" created="Thu, 22 May 2014 18:53:12 +0000"  >&lt;p&gt;May be my point was not clear ... I agree that read HA (how many 9&apos;s by the way) is good feature to have, but this won&apos;t give us what the developer declared in the description section. The relatively high MTTR is not the major source of a bad 90%+ request latency in HBase.   &lt;/p&gt;</comment>
                            <comment id="14006317" author="cos" created="Thu, 22 May 2014 18:54:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;but it will not give us 10ms 99%&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In reality, nobody with a skin in the game is seriously considering 99% availability. Even 99.999% isn&apos;t suitable for some. Would you consider 5.7 minutes annualized to be a plausible danger?&lt;/p&gt;</comment>
                            <comment id="14006335" author="vrodionov" created="Thu, 22 May 2014 19:05:03 +0000"  >&lt;p&gt;I apologize &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;, I call you - developer &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. &lt;/p&gt;</comment>
                            <comment id="14006343" author="vrodionov" created="Thu, 22 May 2014 19:08:56 +0000"  >&lt;blockquote&gt;
&lt;p&gt;In reality, nobody with a skin in the game is seriously considering 99% availability. Even 99.999% isn&apos;t suitable for some. Would you consider 5.7 minutes annualized to be a plausible danger?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How does this contradict my statement?  HA is great feature, but it has nothing to do with improving read requests latency distribution.&lt;/p&gt;</comment>
                            <comment id="14006358" author="octo47" created="Thu, 22 May 2014 19:24:22 +0000"  >&lt;p&gt;If you can spread read load of hot region across shadows - read latencies can go down due of less contention, especially in case avoiding reading from primary rs. &lt;/p&gt;</comment>
                            <comment id="14006367" author="mantonov" created="Thu, 22 May 2014 19:30:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;HA is great feature, but it has nothing to do with improving read requests latency distribution.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In this jira we&apos;re saying that we can&apos;t provide good latency in 99 (.999?) % cases for the following (but not limited, as there&apos;re also GC pauses etc) reason: when region replica fails (RS fails) the requests time out, or just take really long time. And this feature is addressing this aspect. So this jira aims (as I understand) to give HA (possibly stale) replicas, with added benefit of reduced latency.&lt;/p&gt;</comment>
                            <comment id="14268636" author="vrodionov" created="Thu, 8 Jan 2015 00:59:46 +0000"  >&lt;p&gt;What is the current status of this feature? Is it on hold? Abandoned because of FB HydraBase &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12259&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-12259&lt;/a&gt; being ported to HBase?&lt;/p&gt;</comment>
                            <comment id="14268652" author="devaraj" created="Thu, 8 Jan 2015 01:16:19 +0000"  >&lt;p&gt;As commented here - &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11183?focusedCommentId=14221856&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14221856&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-11183?focusedCommentId=14221856&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14221856&lt;/a&gt;, we did implement the second phase of the work (most of the unresolved jiras here pertain to that). We are in the process of porting the patches to master and 1.1 branches. As it stands, this addresses different use cases than what HydraBase addresses (and yes, we should document the differences in the two).&lt;/p&gt;</comment>
                            <comment id="14269562" author="vrodionov" created="Thu, 8 Jan 2015 17:00:42 +0000"  >&lt;p&gt;Thank you, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="14596299" author="busbey" created="Mon, 22 Jun 2015 17:37:54 +0000"  >&lt;p&gt;what&apos;s current status on this, specifically in branch-1 for the 1.2 release?&lt;/p&gt;</comment>
                            <comment id="14599687" author="devaraj" created="Wed, 24 Jun 2015 16:30:38 +0000"  >&lt;p&gt;I think all the planned work is complete on this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=busbey&quot; class=&quot;user-hover&quot; rel=&quot;busbey&quot;&gt;Sean Busbey&lt;/a&gt; and all the code is there in 1.1. We need to update the docs to reflect the current state though. &lt;/p&gt;</comment>
                            <comment id="14599716" author="busbey" created="Wed, 24 Jun 2015 16:45:58 +0000"  >&lt;p&gt;Great! Can we get a jira going for the docs work? We can work towards getting it done while 1.2 RCs are in-progress.&lt;/p&gt;</comment>
                            <comment id="14602100" author="enis" created="Thu, 25 Jun 2015 22:55:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Great! Can we get a jira going for the docs work? We can work towards getting it done while 1.2 RCs are in-progress.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13973&quot; title=&quot;Update documentation for 10070 Phase 2 changes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13973&quot;&gt;&lt;del&gt;HBASE-13973&lt;/del&gt;&lt;/a&gt;. Will resolve this one once that is in. It is the last remaining item. &lt;/p&gt;
</comment>
                            <comment id="14603714" author="enis" created="Fri, 26 Jun 2015 22:33:06 +0000"  >&lt;p&gt;All subtasks done for some time. Time to resolve this. Thanks everyone.&lt;/p&gt;</comment>
                            <comment id="15062597" author="busbey" created="Thu, 17 Dec 2015 19:20:18 +0000"  >&lt;p&gt;Could someone add a release note for this?&lt;/p&gt;</comment>
                            <comment id="15561917" author="wang ken" created="Mon, 10 Oct 2016 10:27:18 +0000"  >&lt;p&gt;Is there any documentation about how this feature affect co-processors, specially endpoint co-processors?&lt;/p&gt;</comment>
                            <comment id="15563309" author="enis" created="Mon, 10 Oct 2016 19:56:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is there any documentation about how this feature affect co-processors, specially endpoint co-processors?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No effect to endpoint coprocessors. Only the get requests or scan requests are performed with backup RPC requests to replicas. The endpoint RPCs will only go to the primary replica. The coprocessors which inject into preGet() etc will get called though, whenever the client does an RPC to the primary replica or secondary replicas. &lt;/p&gt;</comment>
                            <comment id="15563339" author="techbuddy" created="Mon, 10 Oct 2016 20:08:21 +0000"  >&lt;p&gt;So, that means the EndPoint co-processors could still fail if the primary replica is down.Right?&lt;br/&gt;
In other words, they are not benefitted by read HA, which probably makes sense as long as the endpoint co-processors&lt;br/&gt;
are used for write operations. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12775848">HBASE-13063</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12721560">HBASE-11367</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12459886">HBASE-2357</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12702258">HBASE-10785</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12699545">HBASE-10703</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12702428">HBASE-10791</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12703252">HBASE-10817</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12775848">HBASE-13063</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12697129">HBASE-10605</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12707741">HBASE-10957</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12695003">HBASE-10525</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12616659" name="HighAvailabilityDesignforreadsApachedoc.pdf" size="255810" author="enis" created="Tue, 3 Dec 2013 00:50:35 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12689107">HBASE-10347</subtask>
                            <subtask id="12689109">HBASE-10348</subtask>
                            <subtask id="12689112">HBASE-10350</subtask>
                            <subtask id="12689113">HBASE-10351</subtask>
                            <subtask id="12689116">HBASE-10352</subtask>
                            <subtask id="12689125">HBASE-10354</subtask>
                            <subtask id="12689126">HBASE-10355</subtask>
                            <subtask id="12689127">HBASE-10356</subtask>
                            <subtask id="12689129">HBASE-10357</subtask>
                            <subtask id="12689136">HBASE-10359</subtask>
                            <subtask id="12689148">HBASE-10361</subtask>
                            <subtask id="12689153">HBASE-10362</subtask>
                            <subtask id="12694844">HBASE-10513</subtask>
                            <subtask id="12694896">HBASE-10517</subtask>
                            <subtask id="12696125">HBASE-10572</subtask>
                            <subtask id="12697317">HBASE-10616</subtask>
                            <subtask id="12697464">HBASE-10620</subtask>
                            <subtask id="12697742">HBASE-10630</subtask>
                            <subtask id="12697843">HBASE-10633</subtask>
                            <subtask id="12697844">HBASE-10634</subtask>
                            <subtask id="12698557">HBASE-10661</subtask>
                            <subtask id="12698711">HBASE-10672</subtask>
                            <subtask id="12698745">HBASE-10674</subtask>
                            <subtask id="12699502">HBASE-10701</subtask>
                            <subtask id="12699547">HBASE-10704</subtask>
                            <subtask id="12699545">HBASE-10703</subtask>
                            <subtask id="12701037">HBASE-10729</subtask>
                            <subtask id="12701115">HBASE-10734</subtask>
                            <subtask id="12701379">HBASE-10743</subtask>
                            <subtask id="12701994">HBASE-10778</subtask>
                            <subtask id="12702428">HBASE-10791</subtask>
                            <subtask id="12702475">HBASE-10794</subtask>
                            <subtask id="12702998">HBASE-10810</subtask>
                            <subtask id="12703272">HBASE-10818</subtask>
                            <subtask id="12703252">HBASE-10817</subtask>
                            <subtask id="12704138">HBASE-10858</subtask>
                            <subtask id="12704142">HBASE-10859</subtask>
                            <subtask id="12707336">HBASE-10942</subtask>
                            <subtask id="12707741">HBASE-10957</subtask>
                            <subtask id="12707827">HBASE-10961</subtask>
                            <subtask id="12715593">HBASE-11214</subtask>
                            <subtask id="12714353">HBASE-11183</subtask>
                            <subtask id="12716946">HBASE-11261</subtask>
                            <subtask id="12720717">HBASE-11332</subtask>
                            <subtask id="12727138">HBASE-11511</subtask>
                            <subtask id="12727142">HBASE-11512</subtask>
                            <subtask id="12728935">HBASE-11567</subtask>
                            <subtask id="12729007">HBASE-11568</subtask>
                            <subtask id="12729027">HBASE-11569</subtask>
                            <subtask id="12729031">HBASE-11571</subtask>
                            <subtask id="12694326">HBASE-10491</subtask>
                            <subtask id="12729032">HBASE-11572</subtask>
                            <subtask id="12729035">HBASE-11574</subtask>
                            <subtask id="12729255">HBASE-11580</subtask>
                            <subtask id="12737210">HBASE-11842</subtask>
                            <subtask id="12739518">HBASE-11903</subtask>
                            <subtask id="12739738">HBASE-11908</subtask>
                            <subtask id="12742317">HBASE-12012</subtask>
                            <subtask id="12755567">HBASE-12485</subtask>
                            <subtask id="12757142">HBASE-12561</subtask>
                            <subtask id="12757147">HBASE-12562</subtask>
                            <subtask id="12762496">HBASE-12714</subtask>
                            <subtask id="12731155">HBASE-11634</subtask>
                            <subtask id="12778083">HBASE-13121</subtask>
                            <subtask id="12780223">HBASE-13169</subtask>
                            <subtask id="12781338">HBASE-13213</subtask>
                            <subtask id="12822466">HBASE-13515</subtask>
                            <subtask id="12840675">HBASE-13973</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 3 Dec 2013 01:02:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>361537</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1qblr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>361835</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>