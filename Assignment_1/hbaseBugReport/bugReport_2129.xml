<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:59:12 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2129/HBASE-2129.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2129] Simple Master/Slave replication</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2129</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We should first build a very simple replication mechanism to validate our assumptions and get a feel of what replication is in this very distributed context. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12445592">HBASE-2129</key>
            <summary>Simple Master/Slave replication</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12421518">HBASE-1295</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jdcryans">Jean-Daniel Cryans</assignee>
                                    <reporter username="jdcryans">Jean-Daniel Cryans</reporter>
                        <labels>
                    </labels>
                <created>Fri, 15 Jan 2010 04:26:15 +0000</created>
                <updated>Fri, 20 Nov 2015 13:01:33 +0000</updated>
                            <resolved>Thu, 1 Jul 2010 22:42:55 +0000</resolved>
                                                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="12800538" author="jdcryans" created="Fri, 15 Jan 2010 04:44:20 +0000"  >&lt;p&gt;This first patch adds a new contrib called &quot;mdc_replication&quot;. It is master/slave and only supports 1 slave. The modifications done to the core project are only there to enable the sub-classing and extended functionality (like ZKW which has more friendly methods).&lt;/p&gt;

&lt;p&gt;The package.html file gives an overview of the features and describes how to deploy it on 2 clusters. Every user table is replicated, there&apos;s no scoping. I also uses Hadoop RPC as it was already integrated, in the future the plan is to use a non-versioned RPC mechanism.&lt;/p&gt;

&lt;p&gt;Replication works by first collecting the HLog.Entry in a queue on the master cluster and are sent in a batch every 10 seconds by default. On the slave cluster those edits are stored in a log file and then replayed by a separate thread. It is currently blocking so while the edits are played no other batch of edits can come in.&lt;/p&gt;

&lt;p&gt;A region server decides on which RS to replicate to by scanning the /rs directory in the slaves ZK folder and then randomly chooses a subset of nodes according to a default ration of 10%. This is to make sure that we can handle clusters of different sizes. When the RS actually replicates a batch of edits, it first chooses a random node from the subset. This is a cheap way to do try to even the load on the slave cluster.&lt;/p&gt;

&lt;p&gt;If a region server dies on the slave side, the RS on the master gets a new subset and retries with a (hopefully) new node.&lt;/p&gt;

&lt;p&gt;If a region server dies on the master side, the RS getting a region from it will first replicate every edit found in the oldlogfile.&lt;/p&gt;

&lt;p&gt;Apart from the new unit tests, I tested this patch on a single machine by putting two fully distributed clusters of 1 region server that used the same HDFS and ZK quorum (the machine is a i7, 12GB RAM, 2x500GB in RAID1). I was able to run PE sequentialWrite 1 on the first cluster and read all the edits on the slave cluster.&lt;/p&gt;

&lt;p&gt;There is a big potential for refactoring (ReplicationSink works a bit like a HLog) in this patch and some parts are done with something bigger in mind (multiple slaves for example). So, at this point, comments on the general layout are very welcomed but nitpicks won&apos;t be very useful since it this code is meant to change a lot.&lt;/p&gt;</comment>
                            <comment id="12800705" author="hammer" created="Fri, 15 Jan 2010 14:08:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;replayed by a separate thread&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Are there any strategies to parallelize this operation for busy clusters?&lt;/p&gt;</comment>
                            <comment id="12800727" author="jdcryans" created="Fri, 15 Jan 2010 15:53:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;Are there any strategies to parallelize this operation for busy clusters?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, this is one of the things that I scaled down in order to present a patch sooner. Currently the ReplicationSink cannot get behind edits coming from the master cluster since its blocking the replication of new entries. The next step is to allow appenders while the file is being read (logs replayed), then once parallelized (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2066&quot; title=&quot;Perf: parallelize puts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2066&quot;&gt;&lt;del&gt;HBASE-2066&lt;/del&gt;&lt;/a&gt;) puts are available to use them in that thread.&lt;/p&gt;

&lt;p&gt;The plan for the logs themselves in ReplicationSink is to be able to let the thread get behind on replaying edits, for example because the slave cluster is very busy, and that if the region server dies to be able to use those logs to replay them somewhere else.&lt;/p&gt;</comment>
                            <comment id="12800813" author="apurtell" created="Fri, 15 Jan 2010 18:22:01 +0000"  >&lt;p&gt;J-D, what are your thoughts about moving this from master-slave to multi-master. I.e, I might want to have 5 installations each replicating between each other. We can add scoping later, not a big deal to worry about that only after something is working and validated. &lt;/p&gt;</comment>
                            <comment id="12800814" author="apurtell" created="Fri, 15 Jan 2010 18:23:24 +0000"  >&lt;p&gt;In fact, regarding scoping, if you don&apos;t need it don&apos;t worry about it, just let me add it as an option when we need it here. &lt;/p&gt;</comment>
                            <comment id="12800821" author="jdcryans" created="Fri, 15 Jan 2010 18:31:55 +0000"  >&lt;p&gt;@Andrew&lt;/p&gt;

&lt;p&gt;The way I plan it, every region server can support both a ReplicationSource and a ReplicationSink. With the right configuration in zookeeper, you could easily get a master-master setup but also a chain of replication (cluster A replicates to cluster B which replicates to cluster C). In the case of master/master, you have to manage to not replicate edits from which you are the origin (I know you know that). I guess you could encode that information in KV (routing information?).&lt;/p&gt;

&lt;p&gt;WRT scoping, I left it out of this particular Jira but it is one of the next item on my todo list along with live configuration of replication streams and being able to ask a cluster to replicate edits from time X (which is partly enabled by &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2070&quot; title=&quot;Collect HLogs and delete them after a period of time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2070&quot;&gt;&lt;del&gt;HBASE-2070&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="12800829" author="apurtell" created="Fri, 15 Jan 2010 18:47:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;In the case of master/master, you have to manage to not replicate edits from which you are the origin (I know you know that). I guess you could encode that information in KV (routing information?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was thinking to try overloading HLogKey with source routing information. So the replicator can skip sending the edit to a replication peer if it is listed in the source path for the item. Implies globally unique identifiers for peer clusters but that&apos;s not a big deal. Having an admin assign one can be part of the replication config process. Could be just a byte, treated as unsigned. I do not expect there will be deployments with &amp;gt; 256 peer HBase clusters for a while. (Imagine that...)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;WRT scoping, I left it out of this particular Jira but it is one of the next item on my todo list&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, cool.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;along with live configuration of replication streams and being able to ask a cluster to replicate edits from time X &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice!&lt;/p&gt;</comment>
                            <comment id="12800834" author="jdcryans" created="Fri, 15 Jan 2010 18:57:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;I was thinking to try overloading HLogKey with source routing information&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let&apos;s do that (in another Jira). The identifier could be the one used in Zookeeper to describe peers (in this patch, particularly in the file add_peer.rb, I use the name &quot;test&quot; as it only supports 1 slave) so if the admin commands are done carefully it could be very doable. Do you think this would be part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1728&quot; title=&quot;Column family scoping and cluster identification&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1728&quot;&gt;&lt;del&gt;HBASE-1728&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="12800852" author="apurtell" created="Fri, 15 Jan 2010 19:23:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you think this would be part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1728&quot; title=&quot;Column family scoping and cluster identification&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1728&quot;&gt;&lt;del&gt;HBASE-1728&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That makes sense. Already HLK is being extended with scoping information in that issue. We can broaden it just a bit. &lt;/p&gt;</comment>
                            <comment id="12828423" author="jdcryans" created="Tue, 2 Feb 2010 03:20:17 +0000"  >&lt;p&gt;New patch that requires &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2178&quot; title=&quot;Hooks for replication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2178&quot;&gt;&lt;del&gt;HBASE-2178&lt;/del&gt;&lt;/a&gt; and that adds fixes for locking in ReplicationSink and better handling of HTablePool.&lt;/p&gt;</comment>
                            <comment id="12828427" author="ryanobjc" created="Tue, 2 Feb 2010 03:24:28 +0000"  >&lt;p&gt;with the discussion up on hbase-dev, does it make sense to federate this into github?&lt;/p&gt;</comment>
                            <comment id="12828437" author="jdcryans" created="Tue, 2 Feb 2010 04:23:47 +0000"  >&lt;p&gt;@Ryan&lt;/p&gt;

&lt;p&gt;I&apos;m not sure... Replication is a very core feature IMO and the reason we would put it elsewhere first is because we don&apos;t want to include this experimental code yet. I would even be in favor of integrating it into core right away and put a final boolean that would offline the code completely if set to false (like appends in dfs).&lt;/p&gt;</comment>
                            <comment id="12829335" author="jdcryans" created="Thu, 4 Feb 2010 00:14:12 +0000"  >&lt;p&gt;Today I set up a replication stream between 2 clusters in 2 different datacenters, each cluster has 20 nodes and it&apos;s replicating on average 50 000 rows per second. I will let it run for some time to see how it goes.&lt;/p&gt;

&lt;p&gt;While setting it up I noticed the following:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If a zoo.cfg is present in conf/, it completely overrides any value you may have set directly on a HBC object so using add_peer.rb doesn&apos;t work. As a workaround I moved all the configurations from that file to hbase-site.xml and deleted the file.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When using the add_peer.rb script, it&apos;s really important to give the exact hbase.zookeeper.quorum specified in hbase-site.xml or the master cluster will think he&apos;s a slave. It would be nice to add that notice in the script when it confirms the addresses.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When a region server is choosing peers it prints out a message like this: &quot;Considering 19 rs, with ratio 1.9&quot;. In this case we should read something more like &quot;Considering 2 rs, with ratio 0.1&quot;. Fix that.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;When setting up the second cluster, you either have to distcp the hbase folder in HDFS from the first cluster or recreate all the tables empty. If doing the latter, it can be a real pain if you have a lot of tables and you don&apos;t keep the DDL scripts around so I wrote a jruby script that reads all the HTableDescriptors from the master cluster and passes them to a HBaseAdmin configured with the address of the second cluster. I could add it in the patch.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12829903" author="jdcryans" created="Fri, 5 Feb 2010 01:26:08 +0000"  >&lt;p&gt;This patch adds a comment in the documentation about the zoo.cfg problem, tells the user to make sure to use exact quorum addresses and adds a new script to copy all tables description. The scripts share a lot of code, this should eventually be all merged into a single hirb-like shell.&lt;/p&gt;</comment>
                            <comment id="12830186" author="jdcryans" created="Fri, 5 Feb 2010 17:38:36 +0000"  >&lt;p&gt;I just committed this to trunk. I will soon open new issues to add more features.&lt;/p&gt;</comment>
                            <comment id="12847692" author="apurtell" created="Sat, 20 Mar 2010 02:12:10 +0000"  >&lt;p&gt;Broken on trunk after commit of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2283&quot; title=&quot;row level atomicity &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2283&quot;&gt;&lt;del&gt;HBASE-2283&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Needs some rework. ReplicationHLog.doWrite assumes a WAL edit contains only a single KV. But &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2283&quot; title=&quot;row level atomicity &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2283&quot;&gt;&lt;del&gt;HBASE-2283&lt;/del&gt;&lt;/a&gt; made row edits atomic. Multiple KVs corresponding to multiple columns may be in a log entry now, and not all columns may have the same scope. &lt;/p&gt;</comment>
                            <comment id="12847693" author="jdcryans" created="Sat, 20 Mar 2010 02:15:56 +0000"  >&lt;p&gt;I asked Stack to disable replication in trunk so that it doesn&apos;t bother the other devs. I&apos;m currently working on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2223&quot; title=&quot;Handle 10min+ network partitions between clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2223&quot;&gt;&lt;del&gt;HBASE-2223&lt;/del&gt;&lt;/a&gt; which is basically Replication 2.0 and completely rewrites the ReplicationSource part (a patch is under internal review at the moment but I now have to migrate it to the new trunk). So I would like to keep this issue closed.&lt;/p&gt;</comment>
                            <comment id="12884470" author="jdcryans" created="Thu, 1 Jul 2010 22:42:55 +0000"  >&lt;p&gt;This is back in trunk since &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2223&quot; title=&quot;Handle 10min+ network partitions between clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2223&quot;&gt;&lt;del&gt;HBASE-2223&lt;/del&gt;&lt;/a&gt; was committed.&lt;/p&gt;</comment>
                            <comment id="15017830" author="lars_francke" created="Fri, 20 Nov 2015 13:01:33 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12454998">HBASE-2178</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12434494" name="HBASE-2129-v2.patch" size="75205" author="jdcryans" created="Tue, 2 Feb 2010 03:20:17 +0000"/>
                            <attachment id="12434913" name="HBASE-2129-v3.patch" size="77434" author="jdcryans" created="Fri, 5 Feb 2010 01:26:08 +0000"/>
                            <attachment id="12430349" name="HBASE-2129.patch" size="101553" author="jdcryans" created="Fri, 15 Jan 2010 04:44:20 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 15 Jan 2010 14:08:41 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32418</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hgif:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99937</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>