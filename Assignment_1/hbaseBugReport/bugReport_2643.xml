<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:03:35 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2643/HBASE-2643.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2643] Figure how to deal with eof splitting logs</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2643</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;When splitting the WAL and encountering EOF, it&apos;s not clear what to do. Initial discussion of this started in &lt;a href=&quot;http://review.hbase.org/r/74/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/74/&lt;/a&gt; - summarizing here for brevity:&lt;/p&gt;

&lt;p&gt;We can get an EOFException while splitting the WAL in the following cases:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The writer died after creating the file but before even writing the header (or crashed halfway through writing the header)&lt;/li&gt;
	&lt;li&gt;The writer died in the middle of flushing some data - sync() guarantees that we can see &lt;em&gt;at least&lt;/em&gt; the last edit, but we may see half of an edit that was being written out when the RS crashed (especially for large rows)&lt;/li&gt;
	&lt;li&gt;The data was actually corrupted somehow (eg a length field got changed to be too long and thus points past EOF)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Ideally we would know when we see EOF whether it was really the last record, and in that case, simply drop that record (it wasn&apos;t synced, so therefore we dont need to split it). Some open questions:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Currently we ignore empty files. Is it ok to ignore an empty log file if it&apos;s not the last one?&lt;/li&gt;
	&lt;li&gt;Similarly, do we ignore an EOF mid-record if it&apos;s not the last log file?&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12465884">HBASE-2643</key>
            <summary>Figure how to deal with eof splitting logs</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nspiegelberg">Nicolas Spiegelberg</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Jun 2010 17:07:28 +0000</created>
                <updated>Fri, 20 Nov 2015 12:41:55 +0000</updated>
                            <resolved>Fri, 3 Sep 2010 05:58:42 +0000</resolved>
                                    <version>0.89.20100621</version>
                                    <fixVersion>0.89.20100924</fixVersion>
                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12875502" author="tlipcon" created="Fri, 4 Jun 2010 06:23:54 +0000"  >&lt;p&gt;Updated description to a summary of the issue at hand - refer back to original review for original discussion.&lt;/p&gt;</comment>
                            <comment id="12897697" author="richlackey@roamingcloud.com" created="Thu, 12 Aug 2010 11:19:52 +0000"  >&lt;p&gt;There is an assumption that no RuntimeException will occur within splitLog. While such an exception is remote, it is possible. Should a RuntimeException occur, then it will percolate through HMaster, which will not join the cluster.&lt;/p&gt;

&lt;p&gt;This is a general condition. The assumption in HDFS is that this cannot occur (or will be caught by upper layer), which is to say that none of the lower layers catches Exception to prevent the RuntimeException, e.g., NullPointerException, from percolating through. If the SequenceFile contains garbage (or has been corrupted), then the opportunity for the underlying DataInputStream to throw a RuntimeException increases.&lt;/p&gt;

&lt;p&gt;The solution is to add a catch for Exception in splitLog and consider the log corrupt.&lt;/p&gt;</comment>
                            <comment id="12897748" author="stack" created="Thu, 12 Aug 2010 13:45:29 +0000"  >&lt;p&gt;Good stuff Richard.  If a runtime exception splitting logs, we should fail to start the cluster?  It means likely dataloss.  Or, there is a flag in hbase now &amp;#8211; its in the splitlog stuff &amp;#8211; IIRC which says fail-if-any-error OR try-and=keep-going across errors.  If this flag is set, we should catch the RuntimeException, log it and not start the master.  Otherwise, we should log the exception and then let the master proceed.&lt;/p&gt;</comment>
                            <comment id="12897776" author="richlackey@roamingcloud.com" created="Thu, 12 Aug 2010 14:42:18 +0000"  >&lt;p&gt;Since the exception is likely to be the result of a corrupted log, I interpreted that to be within the realm of the flag setting &amp;#8211; a generalization of the intent. &lt;/p&gt;

&lt;p&gt;It seems like adding the catch, logging the exception, and following the flag setting, would add more predictive behavior. At a minimum it provides more documentation at startup, which should permit problem source isolation.&lt;/p&gt;</comment>
                            <comment id="12902110" author="nspiegelberg" created="Tue, 24 Aug 2010 22:22:07 +0000"  >&lt;p&gt;We have encountered this EOF problem in our test cluster this week.  Is there a use case where an EOF could lead to data loss instead of just indicating data truncation due to connection failure?  HDFS throws a ChecksumException IOE with corrupt disk data, so EOF should only indicate application-level corruption.  It seems like we should handle the EOF case differently than normal IOEs and proceed even when &apos;hbase.hlog.split.skip.errors&apos; == false.&lt;/p&gt;</comment>
                            <comment id="12902206" author="nspiegelberg" created="Wed, 25 Aug 2010 00:32:44 +0000"  >&lt;p&gt;IIIRC:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;5:09pm&amp;#93;&lt;/span&gt; nspiegelberg: I&apos;m just wondering if there&apos;s a reason for us to not continue through an EOF&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:09pm&amp;#93;&lt;/span&gt; St^Ack: I can&apos;t think of one&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:10pm&amp;#93;&lt;/span&gt; St^Ack: what you say is reasonable nspiegelberg&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:10pm&amp;#93;&lt;/span&gt; St^Ack: least till we learn otherwise&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:10pm&amp;#93;&lt;/span&gt; St^Ack: I have to go lads... pick up kid&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:10pm&amp;#93;&lt;/span&gt; kannan: i agree... we could check to see if the IOE was an EOF and continue on....&lt;/p&gt;</comment>
                            <comment id="12902574" author="nspiegelberg" created="Wed, 25 Aug 2010 18:51:15 +0000"  >&lt;p&gt;Changed the LOG from warn to info level after internal review&lt;/p&gt;</comment>
                            <comment id="12905802" author="stack" created="Fri, 3 Sep 2010 05:58:42 +0000"  >&lt;p&gt;OK.  Lets go w/ this (We keep going of EOF in any WAL during spilts even if &apos;hbase.hlog.split.skip.errors&apos; == false.).  I wrote the &apos;decision&apos; into new WAL chapter in the hbase &apos;book&apos;.  Thanks for the patch Nicolas.&lt;/p&gt;</comment>
                            <comment id="12905934" author="stack" created="Fri, 3 Sep 2010 15:22:00 +0000"  >&lt;p&gt;Here is what the doc of the eof handling looks like currently (For Nicolas).  To be improved.&lt;/p&gt;</comment>
                            <comment id="12914570" author="jdcryans" created="Fri, 24 Sep 2010 18:12:50 +0000"  >&lt;p&gt;Adding this to the latest 0.89&lt;/p&gt;</comment>
                            <comment id="12914596" author="jdcryans" created="Fri, 24 Sep 2010 19:08:31 +0000"  >&lt;p&gt;While testing 0.89.20100924, I got this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Tests run: 14, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.948 sec &amp;lt;&amp;lt;&amp;lt; FAILURE!
testEOFisIgnored(org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit)  Time elapsed: 0.179 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
java.io.IOException: hdfs://localhost:62668/hbase/hlog/hlog.dat.0, pos=1012, edit=9
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.addFileInfoToException(SequenceFileLogReader.java:165)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:137)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:122)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.parseHLog(HLog.java:1564)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1323)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1210)
        at org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.testEOFisIgnored(TestHLogSplit.java:317)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:140)
        at org.apache.maven.surefire.Surefire.run(Surefire.java:109)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)
        at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readFully(DataInputStream.java:180)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1937)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:135)
        ... 35 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And looking at the code it doesn&apos;t check if the source of the IOE is a EOF. I don&apos;t see it failing on trunk, was it handled in the scope of another jira?&lt;/p&gt;</comment>
                            <comment id="12914601" author="nspiegelberg" created="Fri, 24 Sep 2010 19:13:46 +0000"  >&lt;p&gt;@jd : This bug was introduced by &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2889&quot; title=&quot;Tool to look at HLogs -- parse and tail -f&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2889&quot;&gt;&lt;del&gt;HBASE-2889&lt;/del&gt;&lt;/a&gt;.  It is psuedo-fixed in the trunk &amp;amp; I have a more permanent fix up on the JIRA.  Use reflection to maintain the original error type and added pertinent info.&lt;/p&gt;</comment>
                            <comment id="12914635" author="jdcryans" created="Fri, 24 Sep 2010 20:23:29 +0000"  >&lt;p&gt;Thanks Nicolas!&lt;/p&gt;</comment>
                            <comment id="15017110" author="lars_francke" created="Fri, 20 Nov 2015 12:41:55 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12470400">HBASE-2889</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12472139">HBASE-2933</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12472511">HBASE-2935</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12453070" name="HBASE-2643.patch" size="4059" author="nspiegelberg" created="Wed, 25 Aug 2010 18:51:15 +0000"/>
                            <attachment id="12453782" name="ch03s02.html" size="3975" author="stack" created="Fri, 3 Sep 2010 15:22:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 4 Jun 2010 06:23:54 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26401</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hikn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100271</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>