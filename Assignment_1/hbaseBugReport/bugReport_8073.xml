<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:51:47 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-8073/HBASE-8073.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-8073] HFileOutputFormat support for offline operation</title>
                <link>https://issues.apache.org/jira/browse/HBASE-8073</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;When using HFileOutputFormat to generate HFiles, it inspects the region topology of the target table. The split points from that table are used to guide the TotalOrderPartitioner. If the target table does not exist, it is first created. This imposes an unnecessary dependence on an online HBase and existing table.&lt;/p&gt;

&lt;p&gt;If the table exists, it can be used. However, the job can be smarter. For example, if there&apos;s far more data going into the HFiles than the table currently contains, the table regions aren&apos;t very useful for data split points. Instead, the input data can be sampled to produce split points more meaningful to the dataset. LoadIncrementalHFiles is already capable of handling divergence between HFile boundaries and table regions, so this should not pose any additional burdon at load time.&lt;/p&gt;

&lt;p&gt;The proper method of sampling the data likely requires a custom input format and an additional map-reduce job perform the sampling. See a relevant implementation: &lt;a href=&quot;https://github.com/alexholmes/hadoop-book/blob/master/src/main/java/com/manning/hip/ch4/sampler/ReservoirSamplerInputFormat.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/alexholmes/hadoop-book/blob/master/src/main/java/com/manning/hip/ch4/sampler/ReservoirSamplerInputFormat.java&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12636487">HBASE-8073</key>
            <summary>HFileOutputFormat support for offline operation</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12636666">HBASE-8084</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="ndimiduk">Nick Dimiduk</reporter>
                        <labels>
                    </labels>
                <created>Tue, 12 Mar 2013 00:37:19 +0000</created>
                <updated>Sat, 18 Jun 2016 00:06:22 +0000</updated>
                                                            <fixVersion>2.0.0</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>mapreduce</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                <comments>
                            <comment id="13599536" author="lhofhansl" created="Tue, 12 Mar 2013 00:42:21 +0000"  >&lt;p&gt;+1 on this. HFileOutputFormat requiring an online HBase cluster is annoying.&lt;/p&gt;</comment>
                            <comment id="13599550" author="yuzhihong@gmail.com" created="Tue, 12 Mar 2013 01:14:26 +0000"  >&lt;p&gt;+1.&lt;br/&gt;
Nice feature.&lt;/p&gt;</comment>
                            <comment id="13913397" author="jinghe" created="Wed, 26 Feb 2014 19:44:56 +0000"  >&lt;p&gt;A good feature. &lt;br/&gt;
An example is the WALPlayer. When replaying WALs to HFiles, it requires a live table with the same table name in the WAL, which makes it not useful as a offline tool.&lt;/p&gt;</comment>
                            <comment id="13954518" author="jinghe" created="Sun, 30 Mar 2014 00:04:09 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Sampling the input data to provide split point is a big plus, and will involve more work. &lt;/p&gt;

&lt;p&gt;How do you think the following improvement to give an option to first get rid of the dependency on an online HBase?&lt;/p&gt;

&lt;p&gt;Instead of replying on a live HTable from the table name, we can just go to the file system disk to get the table descriptor and region infos.&lt;br/&gt;
From there we have all the info we need, e.g compression, region boundaries, etc.  &lt;br/&gt;
In this case, we will only reply on a hbase.root.dir and the table name.&lt;/p&gt;


</comment>
                            <comment id="13955406" author="ndimiduk" created="Mon, 31 Mar 2014 17:21:26 +0000"  >&lt;p&gt;That&apos;s an interesting idea. What about FS permissions? There&apos;s no assumption that a job-submitting user would have read access to the HBase user&apos;s data. Plus, it requires the data structure be available &amp;#8211; ie, access to the same hdfs as is running HBase. The appeal of sampling is that all it requires of HBase is knowledge of the on-disk file format. Nothing about the destination is necessary. The complexity isn&apos;t that bad, just a second (short) MR job.&lt;/p&gt;</comment>
                            <comment id="13955554" author="jinghe" created="Mon, 31 Mar 2014 19:15:38 +0000"  >&lt;p&gt;I happen to work on enabling security on our Hadoop/HBase cluster recently.  Good point on the FS permissions. &lt;/p&gt;

&lt;p&gt;We could also expose additional API for HFileOutputFormat.configureIncrementalLoad() so that we outsource and provide user/caller flexibility to input split point or other info to, so that HFileOutputFormat does not has to figure out and do this internally.&lt;br/&gt;
For example, the user/caller will do the sampling of their data and input the info to HFileOutputFormat.configureIncrementalLoad().&lt;br/&gt;
The user can even provide a trivial input, e.g. these are my intended split points &lt;/p&gt;
{0, 100000, 200000, ...}</comment>
                            <comment id="13955565" author="ndimiduk" created="Mon, 31 Mar 2014 19:27:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;We could also expose additional API for HFileOutputFormat.configureIncrementalLoad() so that we outsource and provide user/caller flexibility to input split point or other info to, so that HFileOutputFormat does not has to figure out and do this internally.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, this is a good step. The partitions file could be passed to TOP directly, and the same file could be parsed to count the number of reducers.&lt;/p&gt;</comment>
                            <comment id="14014453" author="jinghe" created="Sat, 31 May 2014 01:31:08 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I attached a draft patch that basically follow the idea to provide more options to configureIncrementalLoad().&lt;br/&gt;
Similar to createTable() with HTableDescriptor and splitKeys as input arguments,  this removes the requirement of a live HTable, and provides an option for the caller to provide with static input info.&lt;br/&gt;
Please share if this is an acceptable step. Thanks.&lt;/p&gt;</comment>
                            <comment id="14014526" author="hadoopqa" created="Sat, 31 May 2014 04:56:05 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12647743/HBASE-8073-trunk-v0.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12647743/HBASE-8073-trunk-v0.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12647743&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 6 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +  static void configureCompression(HTableDescriptor tableDesc, Configuration conf) throws IOException {&lt;br/&gt;
+  static void configureBlockSize(HTableDescriptor tableDesc, Configuration conf) throws IOException {&lt;br/&gt;
+  static void configureBloomType(HTableDescriptor tableDesc, Configuration conf) throws IOException {&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9657//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14015185" author="jinghe" created="Mon, 2 Jun 2014 05:48:15 +0000"  >&lt;p&gt;Attached v1. Fixed the test case errors: need to mock a HTableDescriptor in the two test cases.&lt;/p&gt;</comment>
                            <comment id="14015234" author="hadoopqa" created="Mon, 2 Jun 2014 07:49:36 +0000"  >&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12647858/HBASE-8073-trunk-v1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12647858/HBASE-8073-trunk-v1.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12647858&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 6 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9662//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14484741" author="ndimiduk" created="Wed, 8 Apr 2015 05:23:55 +0000"  >&lt;p&gt;Looks like we&apos;ve let this one sit. Any chance of an updated patch &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jinghe&quot; class=&quot;user-hover&quot; rel=&quot;jinghe&quot;&gt;Jerry He&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14484776" author="anoop.hbase" created="Wed, 8 Apr 2015 05:56:37 +0000"  >&lt;p&gt;This looks to be an important improvement. &lt;/p&gt;</comment>
                            <comment id="14491181" author="jinghe" created="Sat, 11 Apr 2015 20:11:06 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Are you ok with the basic approach in my last few comments and the patch?  I can update or rebase the patch.&lt;/p&gt;</comment>
                            <comment id="14492013" author="anoop.hbase" created="Mon, 13 Apr 2015 07:33:09 +0000"  >&lt;p&gt;Seems the approach in V2 is ok.  It allows to specify the splits as well as other imp table/cf attributes like compression, block size etc.  The DFS level read would have allowed it to be read by code, but the user permission can be really an issue.   As this change allows to pass the splits, one can have a smarter i/p data sampler code sit in front and make the splits as pass to configureIncrementalLoad().&lt;/p&gt;</comment>
                            <comment id="14492975" author="ndimiduk" created="Mon, 13 Apr 2015 19:58:36 +0000"  >&lt;p&gt;Reviewing comments and the patch. I think this is a good incremental step, but doesn&apos;t achieve my intended use-case. What I had in mind is an ETL operation that&apos;s generating some dataset (incremental or otherwise). It&apos;s run from some infra that&apos;s decoupled from the online serving tier (ie, maybe generating the dataset with EMR/S3 while serving infra is on-prem). The ETL process knows it&apos;s generating HFiles, but that&apos;s about it. We have the data set, we know what the rowkeys will be &amp;#8211; there&apos;s no reason to make the user think about split points.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12714238">HBASE-11170</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12647743" name="HBASE-8073-trunk-v0.patch" size="14928" author="jinghe" created="Sat, 31 May 2014 01:28:49 +0000"/>
                            <attachment id="12647858" name="HBASE-8073-trunk-v1.patch" size="16699" author="jinghe" created="Mon, 2 Jun 2014 05:47:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 12 Mar 2013 00:42:21 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>316979</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 35 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1ip27:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>317320</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>