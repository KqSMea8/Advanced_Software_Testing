<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:03:19 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2611/HBASE-2611.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2611] Handle RS that fails while processing the failure of another one</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2611</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2223&quot; title=&quot;Handle 10min+ network partitions between clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2223&quot;&gt;&lt;del&gt;HBASE-2223&lt;/del&gt;&lt;/a&gt; doesn&apos;t manage region servers that fail while doing the transfer of HLogs queues from other region servers that failed. Devise a reliable way to do it.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12465318">HBASE-2611</key>
            <summary>Handle RS that fails while processing the failure of another one</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12421518">HBASE-1295</parent>
                                    <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="v.himanshu">Himanshu Vashishtha</assignee>
                                    <reporter username="jdcryans">Jean-Daniel Cryans</reporter>
                        <labels>
                    </labels>
                <created>Mon, 24 May 2010 23:54:38 +0000</created>
                <updated>Tue, 14 Oct 2014 10:01:33 +0000</updated>
                            <resolved>Tue, 29 Jan 2013 18:49:15 +0000</resolved>
                                                    <fixVersion>0.94.5</fixVersion>
                    <fixVersion>0.95.0</fixVersion>
                                    <component>Replication</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>19</watches>
                                                                <comments>
                            <comment id="12870922" author="jdcryans" created="Tue, 25 May 2010 00:37:37 +0000"  >&lt;p&gt;My solution would require the use of 4 &quot;types&quot; of znodes:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Intention: When a node fails, a machine first writes locally the intention of locking the other&apos;s&lt;/li&gt;
	&lt;li&gt;Lock: After writing the intention a node &quot;locks&quot; another&apos;s folder of queues by creating a znode that contains its startcode&lt;/li&gt;
	&lt;li&gt;Tag: After sucessfully locking a znode, the winning node puts a znode locally that gives the locked RS&apos; start code and lists the queues that are going to be copied&lt;/li&gt;
	&lt;li&gt;Delete: After copying everything that was listed in a tag, the node creates this znode aside the lock to mark those queues as being under deletion (because there&apos;s no atomic recursive delete in ZK)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Then let&apos;s say we have the following machines:&lt;/p&gt;

&lt;p&gt;Machine A: The first machine that dies&lt;br/&gt;
Machine B: The machine that&apos;s trying to failover A but that fails while doing it			&lt;br/&gt;
Machine C: The machine that&apos;s trying to failover B and that acquires the lock successfully			&lt;/p&gt;

&lt;p&gt;Here&apos;s now what should happen to node C when B fails after step X.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Machine B fails:&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Machine C&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After writing intention&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Reads the intention, sees it doesn&apos;t own the lock, removes the intention and proceeds with failover of just B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After writing lock&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Reads the intention, sees the lock is owned by B, repeats the failover process going to the deepest node&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After writing tag&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;same, but takes care not to copy what&apos;s in the tag when doing the failover of B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After copying some znodes&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;same behavior, as long as the tag is there what&apos;s in it is considered dirty&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After writing delete marker&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Reads the intention, sees the lock, sees the delete marker, finishes deleting all znodes, deletes the tag, then failovers B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After deleting the local tag&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Reads the intention, sees the znode is gone, deletes the intention znode, failovers B&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;After deleting the intention&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Basic failover&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                            <comment id="12918256" author="jdcryans" created="Tue, 5 Oct 2010 22:17:52 +0000"  >&lt;p&gt;Punting, won&apos;t have time to do it for 0.90.0&lt;/p&gt;</comment>
                            <comment id="13047535" author="stack" created="Fri, 10 Jun 2011 22:45:43 +0000"  >&lt;p&gt;Moving out of 0.92.0. Pull it back in if you think different.&lt;/p&gt;</comment>
                            <comment id="13134549" author="ctrezzo" created="Mon, 24 Oct 2011 22:00:44 +0000"  >&lt;p&gt;@J-D&lt;/p&gt;

&lt;p&gt;If you don&apos;t mind, I was thinking about taking a crack at this using your 4 &quot;types&quot; of znode strategy. I&apos;ll start working on a sketch patch.&lt;/p&gt;

&lt;p&gt;At a first glance, it seems as though most of the code changes are going to be in ReplicationSourceManager.NodeFailoverWorker.run().&lt;/p&gt;</comment>
                            <comment id="13134568" author="ctrezzo" created="Mon, 24 Oct 2011 22:19:56 +0000"  >&lt;p&gt;...and of course ReplicationZookeeper.&lt;/p&gt;</comment>
                            <comment id="13135292" author="jdcryans" created="Tue, 25 Oct 2011 18:03:38 +0000"  >&lt;p&gt;Actually it would be nice if it was in a separate utility package since atomically moving a znode &quot;folder&quot; recursively would be a very useful function in general. It might even already exist on the net.&lt;/p&gt;</comment>
                            <comment id="13138637" author="ctrezzo" created="Fri, 28 Oct 2011 18:54:29 +0000"  >&lt;p&gt;I think adding the ability to atomically move a znode and all its child znodes might be a pretty invasive change. I couldn&apos;t seem to find any utility package for this on the net, but there is a patch in Zookeeper (&lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-965&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;ZOOKEEPER-965&lt;/a&gt;) implementing atomic batch operations that is scheduled for 3.4.&lt;/p&gt;

&lt;p&gt;I thought about the problem a little bit, and after conferring with Lars, I think we might not need the atomic move (although it would definitely make it simpler).&lt;/p&gt;

&lt;p&gt;Below is some pseudo code for the algorithm I came up with. It is very similar to what you suggested above. Both intentions and locks are tagged with the region server they point to (i.e. locks are tagged with the rs that holds them, and intentions are tagged with the rs they intend to lock). Intentions are at the same level in the znode structure as locks. It is a recursive, depth first algorithm.&lt;/p&gt;

&lt;p&gt;Questions/comments/suggestions always appreciated.&lt;/p&gt;

&lt;p&gt;Chris&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;

&lt;span class=&quot;code-comment&quot;&gt;//&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method is the top-level failover method (i.e. NodeFailoverWorker.run())
&lt;/span&gt;failOverRun(FailedNode a) {
  recordIntention(a, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;);
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(getLock(a, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;)) {
    &lt;span class=&quot;code-comment&quot;&gt;//transfer all queues to local node
&lt;/span&gt;    moveState(a, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;);
  }
  &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
    deleteIntention(a, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
  }
  replicateQueues();
}

moveState(NodeToMove a, CurrentNode c, TargetNode t) {
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(lock exists on a) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(lock on a is owned by c) {
      moveStateHelper(a, c, t);
    }
    &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;code-comment&quot;&gt;//someone &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; has the lock and is handling
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;//the failover
&lt;/span&gt;      deleteIntention(a, c);
    }
  }
  &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(queue znodes exist) {
      &lt;span class=&quot;code-comment&quot;&gt;//we know that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; node has queues to transfer
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(getLock(a, c)) {
        moveStateHelper(a, c, t);
      }
      &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        deleteIntention(a, c);
      }
    }
    &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;code-comment&quot;&gt;//we know that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; node is being deleted
&lt;/span&gt;      deleteState(a);
      deleteIntention(a, c);
    }
  }
}

moveStateHelper(NodeToMove a, CurrentNode c, TargetNode t) {
  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(every intention b of a) {
    moveState(b, a, t);
  }
  &lt;span class=&quot;code-comment&quot;&gt;//we need to safely handle the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; where we &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; to copy
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;//queues that have already been copied
&lt;/span&gt;  copy all queues in a to t;
  deleteState(a);
  deleteIntention(a, c);
}

deleteState(NodeToDelete d) {
  &lt;span class=&quot;code-comment&quot;&gt;//there is no need to traverse down the tree at all
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;//because at &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; point everything below us should have
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;//been deleted
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;//
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;//we need to safely handle the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; where we attempt to delete
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;//nodes that have already been deleted
&lt;/span&gt;
  delete entire node;
}

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13138739" author="yuzhihong@gmail.com" created="Fri, 28 Oct 2011 20:54:21 +0000"  >&lt;p&gt;In moveState(), if lock on a is owned by c, should lock be released after moveStateHelper() returns ?&lt;br/&gt;
I guess lock release can also be done at the end of moveStateHelper().&lt;/p&gt;</comment>
                            <comment id="13138762" author="ctrezzo" created="Fri, 28 Oct 2011 21:17:42 +0000"  >&lt;p&gt;I should have specified that in deleteState(), the line &quot;delete entire node&quot; deletes the entire znode replication hierarchy for that region server. This would include the lock znode, which is essentially releasing the lock at the end of moveStateHelper().&lt;/p&gt;

&lt;p&gt;Thanks!&lt;br/&gt;
Chris&lt;/p&gt;</comment>
                            <comment id="13165085" author="ctrezzo" created="Thu, 8 Dec 2011 08:29:48 +0000"  >&lt;p&gt;@J-D&lt;/p&gt;

&lt;p&gt;LarsH and I were talking about another approach to region server replication hlog queue failover yesterday, and I wanted to get some feedback on it.&lt;/p&gt;

&lt;p&gt;Currently when handling a nodeDeleted event, the live region servers only attempt to failover the node corresponding to the event. The nodeDeleted event is only fired once, so to protect ourselves from orphaning the znode state of the failed region server in a cascading failure scenario, we move the state to the znode of the region server that is performing the failover. Since we don&apos;t have an atomic way to move this state, it gets a little tricky.&lt;/p&gt;

&lt;p&gt;Instead of this approach, we could have the region server attempt to failover all failed region servers every time it receives a nodeDeleted event. For example, the nodeDeleted method could go something like this: refresh the region server list, get the list of region servers in the replication znode structure, attempt to lock and failover any region server listed in the replication znode structure that is not currently alive.&lt;/p&gt;

&lt;p&gt;The same race to lock the region server znode will occur. Only one region server will get the lock and handle the failover. Each NodeFailoverWorker that gets started could simply operate on the original dead region server znode structure. If the region server fails while preforming the failover, then both the region servers will get picked up by another region server when the nodeDeleted event for the second failure is fired. Locks would have to be ephemeral nodes to prevent permanent locking of a region server when the failover region server dies. Once the replication hlog queues are successfully replicated, the znode for the dead region server can be deleted.  &lt;/p&gt;

&lt;p&gt;On the cons side, this approach makes the handling of a nodeDeleted event a heavier weight operation.&lt;/p&gt;

&lt;p&gt;On the pros side, it makes the failover code much simpler because we no longer have to worry about moving the region server znode state around in zookeeper.&lt;/p&gt;

&lt;p&gt;Thoughts always appreciated.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Chris&lt;/p&gt;</comment>
                            <comment id="13401779" author="v.himanshu" created="Tue, 26 Jun 2012 23:12:28 +0000"  >&lt;p&gt;I looked at this issue from the perspective of using Zookeeper#multi Operation (present in 3.4). This API guarantees to do a list of Op as a single transaction, rolling back all the Ops in case any of the Op fails. I tested this functionality as a standalone case (where the transaction was to move a bunch of Znodes from one parent to another), and it works good (out of N threads which race to do the transfer, only 1 is successful). And in case of a failure, all the Ops done so far are rolled back. I can attach the sample code if required.&lt;/p&gt;

&lt;p&gt;Here is the approach I used to utilize multi for this issue:&lt;br/&gt;
a) All the active region servers tries to &quot;move&quot; the logs of peers under the dead regionserver znode. It involves creating Op objects for creating new znodes and deleting old ones. As per the multi API guarantee, only one regionserver will be successful in moving the znodes.&lt;/p&gt;

&lt;p&gt;b) The regionservers will &quot;keep on trying to move&quot; the znodes from the dead regionserver untill they are sure that the node is deleted (by the successful regionserver), or there is no log to process. This is to avoid any corner case so as not to miss the logs for the dead regionserver. The number of trials can be made configurable.&lt;/p&gt;

&lt;p&gt;c) In case of cascading failure (when the successful regionserver dies before it gets the notification from zk about the successful move), other regionservers will get this new event and will proceed as normal (will try to move all the znodes from this newly dead regionserver znode).&lt;/p&gt;


&lt;p&gt;It will be good to know what others think about this approach. Other rogue conditions that can happen?&lt;/p&gt;

&lt;p&gt;Attached is a patch based and I tested it by manually killing regionservers at random (not totally random, but killing one and then killing the successful one when it has just transferred the logs) (its difficult to kill it while transferring as its an atomic operation now). Any ideas/suggestions for more direct testing are welcome.&lt;/p&gt;</comment>
                            <comment id="13402582" author="zhihyu@ebaysf.com" created="Wed, 27 Jun 2012 21:50:16 +0000"  >&lt;p&gt;Putting patch on review board helps.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * @param opList: list of Op to be executed as one trx.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&apos;trx&apos; -&amp;gt; &apos;transaction&apos;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(opList == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; || opList.size() ==0)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Space between if and (.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    }&lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException ie) {
+      LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;multi call interrupted; process failed!&quot;&lt;/span&gt; + ie);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Restore interrupt status for the thread (same for doMultiAndWatch). Space between } and catch.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;multi call failed! One of the passed ops has failed which result in the rolled back.&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Line length beyond 100.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;
+   */
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; SortedMap&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, SortedSet&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;&amp;gt; copyDeadRSLogsWithMulti(
+      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; deadRSZnode) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;javadoc for the return value.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;This is us! Skipping the processing as we might be closing down.&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Add deadRSZnodePath to the log.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    RetryCounterFactory retryCounterFactory = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RetryCounterFactory(&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE, 3 * 1000);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I don&apos;t think MAX_VALUE is a good choice.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        SortedSet&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt; logQueue = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TreeSet&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Why is logQueue backed by a TreeSet ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;KeeperException occurred in multi; &quot;&lt;/span&gt; +
+            &lt;span class=&quot;code-quote&quot;&gt;&quot;seems some other regionserver took the logs before us.&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Add ke to the above message.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        Op deleteOpForLog = Op.delete(zNodeForCurrentLog, -1);
+        znodesToWatch.add(logZnode);
+        opsList.add(createOpForLog);
+        opsList.add(deleteOpForLog);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Please reorder the above calls so that znodesToWatch.add() is after opsList.add() calls. This would make code more readable.&lt;/p&gt;</comment>
                            <comment id="13402594" author="zhihyu@ebaysf.com" created="Wed, 27 Jun 2012 22:08:12 +0000"  >&lt;p&gt;Suppose there are (relatively) large number of Op&apos;s in opsList, the chance of collision between active region servers is high.&lt;/p&gt;

&lt;p&gt;Some experiments should be performed so that we get idea of how long this procedure takes to succeed.&lt;/p&gt;</comment>
                            <comment id="13402775" author="v.himanshu" created="Thu, 28 Jun 2012 02:00:54 +0000"  >&lt;p&gt;Thanks for the review Ted. I will upload a modified version on the rb. My initial idea of putting it here was to get some feedback on the approach.&lt;/p&gt;

&lt;p&gt;Yes, it is zk intensive as all other regionservers are competing to do the transaction. But, as soon as one is successful (the first one who create the list and issues the multi command), other regionservers which haven&apos;t had a chance to do a listChildern call on the dead regionserver znode will not see anything; and for other regionservers which have created the Ops, the very first Op will fail as the znode has already moved. Zookeeper#multi op is fail fast, it rolls back the transaction on first failure without retrying remaining Ops. I tested it on a 3 RS cluster with average load being 12-14 logs, and it usually is done within seconds after the regionserver failure is noticed. What sort of experiments you are thinking about.&lt;br/&gt;
On an another note, TestReplication passes.&lt;/p&gt;</comment>
                            <comment id="13402801" author="zhihyu@ebaysf.com" created="Thu, 28 Jun 2012 03:07:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;average load being 12-14 logs&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you make the above 10x ?&lt;/p&gt;

&lt;p&gt;Another consideration is when (which major release) zookeeper 3.4 would be listed as minimum requirement.&lt;br/&gt;
There hasn&apos;t been consensus so far.&lt;/p&gt;

&lt;p&gt;Here&apos;re all the replication-related tests:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdmin.java
src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSink.java
src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
src/test/java/org/apache/hadoop/hbase/replication/TestReplication.java
src/test/java/org/apache/hadoop/hbase/replication/TestReplicationDeleteTypes.java
src/test/java/org/apache/hadoop/hbase/replication/TestReplicationPeer.java
src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSource.java
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13402811" author="v.himanshu" created="Thu, 28 Jun 2012 03:39:33 +0000"  >&lt;p&gt;zookeeper 3.4 is there in 0.92+? What do you mean by minimum requirement? Please explain.&lt;/p&gt;

&lt;p&gt;I find the related test, queuefailover, in TestReplication. Good to know about other test classes.&lt;/p&gt;</comment>
                            <comment id="13402816" author="zhihyu@ebaysf.com" created="Thu, 28 Jun 2012 03:52:18 +0000"  >&lt;p&gt;The 3.4 is only for zookeeper client.&lt;br/&gt;
Companies (such as StumbleUpon) run 3.3.x in production which doesn&apos;t support multi().&lt;/p&gt;</comment>
                            <comment id="13402824" author="jesse_yates" created="Thu, 28 Jun 2012 04:02:28 +0000"  >&lt;p&gt;3.4 is currently only required for security and further, is not yet a stable release of ZK. That said, if it does become stable its likely to be adopted given that its been pretty solid for many people.&lt;/p&gt;</comment>
                            <comment id="13402829" author="v.himanshu" created="Thu, 28 Jun 2012 04:14:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;3.4 is only for zookeeper client.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I find this a bit confusing. Why is it so? What do we gain by this?&lt;/p&gt;

&lt;p&gt;@Jesse: TM is using secure hbase in their production (if i am not wrong). So, 3.4 seems pretty reasonable choice. Has there been any discussion on this. I would like to know more context on this.&lt;/p&gt;</comment>
                            <comment id="13402863" author="jesse_yates" created="Thu, 28 Jun 2012 05:32:21 +0000"  >&lt;p&gt;@Himanshu here is the thread I started on this on dev@ little while ago: &lt;a href=&quot;http://search-hadoop.com/m/u2D7j1yRpi72&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://search-hadoop.com/m/u2D7j1yRpi72&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;It basically comes down to the fact that it would be irresponsible to do a release of HBase that requires an unstable dependency. Yeah, TM has it in production, but that doesn&apos;t mean their usage is representative of &lt;em&gt;everyone&apos;s&lt;/em&gt;. If the ZK fellas decide that 3.4 is a stable release, then I&apos;m all for making it the requirement in 0.96, but until the guys who write the software feel like its stable, I don&apos;t think we are qualified to say it is stable. &lt;/p&gt;

&lt;p&gt;I do think its weird that we make 3.4 a dependency, but it really would be too weird (and honestly a waste of effort) to support two versions of the protocol, especially considering the trickiness of dealing with ZK clusters that may be in the process of upgrade, etc. &lt;/p&gt;</comment>
                            <comment id="13448248" author="lhofhansl" created="Wed, 5 Sep 2012 00:28:34 +0000"  >&lt;p&gt;Also see patch and discussion in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6695&quot; title=&quot;[Replication] Data will lose if RegionServer down during transferqueue&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6695&quot;&gt;&lt;del&gt;HBASE-6695&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13482984" author="lhofhansl" created="Wed, 24 Oct 2012 05:24:19 +0000"  >&lt;p&gt;I think we should really try to fix this for 0.94.&lt;/p&gt;</comment>
                            <comment id="13484717" author="lhofhansl" created="Fri, 26 Oct 2012 05:08:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ctrezzo&quot; class=&quot;user-hover&quot; rel=&quot;ctrezzo&quot;&gt;Chris Trezzo&lt;/a&gt; Wanna work on this?&lt;br/&gt;
Have a look at the discussion in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6695&quot; title=&quot;[Replication] Data will lose if RegionServer down during transferqueue&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6695&quot;&gt;&lt;del&gt;HBASE-6695&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
If we accept a bit of herding initially we can fix this with a simple change (I think)... And ZK herding is still better than data loss.&lt;/p&gt;</comment>
                            <comment id="13489029" author="lhofhansl" created="Thu, 1 Nov 2012 21:02:12 +0000"  >&lt;p&gt;Alas, looks like we won&apos;t get to this... again.&lt;/p&gt;</comment>
                            <comment id="13536506" author="lhofhansl" created="Wed, 19 Dec 2012 22:45:17 +0000"  >&lt;p&gt;Moving out again&lt;/p&gt;</comment>
                            <comment id="13553430" author="v.himanshu" created="Tue, 15 Jan 2013 02:44:35 +0000"  >&lt;p&gt;Working on it; will provide a patch soon.&lt;/p&gt;</comment>
                            <comment id="13554517" author="v.himanshu" created="Tue, 15 Jan 2013 23:33:25 +0000"  >&lt;p&gt;Patch that provides an alternative way to copy the znodes in an atomic way, using Zookeeper multi. &lt;br/&gt;
It is configurable using &quot;hbase.zookeeper.useMulti&quot; property. It does a &apos;ls&apos; on the znode and creates Operations to do the &quot;move&quot; (Create new and delete old) znodes. I tested it on a 3 node cluster and killed the server that had 200 log znodes. The other two regionserver competed and one took away the all the znodes. Ran TestReplication#queueFailover on a jenkins it passed.&lt;/p&gt;</comment>
                            <comment id="13554574" author="lhofhansl" created="Wed, 16 Jan 2013 00:41:37 +0000"  >&lt;p&gt;So that call to multi better not fail, ever. Otherwise we&apos;ll still lose track of data to be replicated.&lt;br/&gt;
There two problem currently:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Transfer of queues is only attempted once&lt;/li&gt;
	&lt;li&gt;Queues may be partially transferred&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This patch addresses the only #2.&lt;/p&gt;</comment>
                            <comment id="13554582" author="yuzhihong@gmail.com" created="Wed, 16 Jan 2013 00:52:21 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * @param znode
+   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Please finish javadoc.&lt;br/&gt;
The key of SortedMap is peer cluster Id, right ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Got exception in copyQueuesFromRSUsingMulti: &quot;&lt;/span&gt; + e);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you use comma in place of +, you would get method names.&lt;/p&gt;

&lt;p&gt;There is no empty line in copyQueuesFromRSUsingMulti(). Consider adding empty line to separate sub-steps.&lt;/p&gt;</comment>
                            <comment id="13554584" author="v.himanshu" created="Wed, 16 Jan 2013 00:56:04 +0000"  >&lt;p&gt;The call to multi is using RecoverableZookeeper#multi, which does a retry in case of &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
          &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; CONNECTIONLOSS:
          &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; SESSIONEXPIRED:
          &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; OPERATIONTIMEOUT:
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;which by default, is three. I find this approach better than the existing one.&lt;/p&gt;</comment>
                            <comment id="13554586" author="ctrezzo" created="Wed, 16 Jan 2013 01:02:09 +0000"  >&lt;p&gt;But the retries in RecoverableZookeeper are not atomic... if the region server fails in the middle of RecoverableZooKeeper.multi, the queues will not get transferred.&lt;/p&gt;</comment>
                            <comment id="13554588" author="ctrezzo" created="Wed, 16 Jan 2013 01:03:54 +0000"  >&lt;p&gt;Also, I don&apos;t think your manual test described above hits this corner case. You need at least two region server failures for this to happen. For example, region server &quot;A&quot; fails, region server &quot;B&quot; races and wins the failover of A, and then region server B fails before it finishes copying A&apos;s queue to it&apos;s own queue. Then when someone picks up B, A&apos;s original queue will not get completely replicated.&lt;/p&gt;

&lt;p&gt;Thanks for working on this though! It is a tricky one.&lt;/p&gt;</comment>
                            <comment id="13554617" author="ctrezzo" created="Wed, 16 Jan 2013 01:40:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvashish%40cs.ualberta.ca&quot; class=&quot;user-hover&quot; rel=&quot;hvashish@cs.ualberta.ca&quot;&gt;Himanshu Vashishtha&lt;/a&gt; Hmm I may have miss spoke... atomic was not the right word choice.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But the retries in RecoverableZookeeper are not atomic... if the region server fails in the middle of RecoverableZooKeeper.multi, the queues will not get transferred.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I see that as long as a multi hasn&apos;t succeeded, all region servers will continue to try and failover the queues. So the problem seems to be more along the lines of if all region servers exhaust their multi retries, then the queues would get lost.&lt;/p&gt;

&lt;p&gt;Is there ever a case in practice where we would run into this and zookeeper is not down?&lt;/p&gt;</comment>
                            <comment id="13554724" author="v.himanshu" created="Wed, 16 Jan 2013 04:04:50 +0000"  >&lt;p&gt;Chris: Thanks for taking a look.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is there ever a case in practice where we would run into this and zookeeper is not down?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can&apos;t think of any. &lt;br/&gt;
Even if that ever happens (let&apos;s say all regionservers can&apos;t connect to zk or whatever), then, we need something different (possibly beyond the scope of this jira) so any new joining regionserver take a look at existing log znodes, etc.&lt;/p&gt;

&lt;p&gt;Re: Testing:&lt;br/&gt;
Yeah, I know. But, given that it is moved in one transaction, I can&apos;t think of how to replicate it in a testing environment. Therefore, I tested to see what happens when two regionservers tries to copy the queue, and whether this approach scales well with number of logs or not. &lt;/p&gt;</comment>
                            <comment id="13554747" author="lhofhansl" created="Wed, 16 Jan 2013 05:02:28 +0000"  >&lt;p&gt;This is definitely an improvement.&lt;br/&gt;
What happens when a region server dies after it copied the queues but before it could finish shipping all the edits?&lt;/p&gt;</comment>
                            <comment id="13554752" author="v.himanshu" created="Wed, 16 Jan 2013 05:13:52 +0000"  >&lt;p&gt;Lars: Then a regionserver who does the failover will also process the leftover znodes (just like what happens currently).&lt;/p&gt;</comment>
                            <comment id="13554758" author="lhofhansl" created="Wed, 16 Jan 2013 05:26:12 +0000"  >&lt;p&gt;Cool... So as long as the multi itself does not fail we&apos;re good.&lt;/p&gt;</comment>
                            <comment id="13554761" author="v.himanshu" created="Wed, 16 Jan 2013 05:34:24 +0000"  >&lt;p&gt;Yes. I would ask, though, in what possible circumstances you foresee failure of multi()? &lt;/p&gt;</comment>
                            <comment id="13554783" author="v.himanshu" created="Wed, 16 Jan 2013 06:18:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;: I asked about possible failure scenarios because it will be great if they can be worked upon beforehand.&lt;/p&gt;</comment>
                            <comment id="13554789" author="lhofhansl" created="Wed, 16 Jan 2013 06:29:39 +0000"  >&lt;p&gt;Yeah, I don&apos;t know.&lt;/p&gt;

&lt;p&gt;But what can happen is that the region server who wins the race to take over the dead region server&apos;s queues could die before it even manages to call multi. In the case - since the ephemeral znode is only removed once - we won&apos;t ever retry to move that region server&apos;s queues again. Right?&lt;br/&gt;
So another part of the puzzle is to have a way to retry the takeover later. Back in the comments here there are various suggestions about how to do that mostly centering around having all surviving RSs try to move a dead RS&apos;s queues.&lt;/p&gt;</comment>
                            <comment id="13555213" author="v.himanshu" created="Wed, 16 Jan 2013 17:05:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;But what can happen is that the region server who wins the race to take over the dead region server&apos;s queues could die before it even manages to call multi.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not following your question. How can a regionserver wins a race before calling multi? If regionserver &quot;A&quot; fails, &lt;b&gt;all&lt;/b&gt; regionserver will call multi to do the failover, and only one (let&apos;s say &quot;B&quot;) will succeed. Now, if B also dies meanwhile (while it has succeeded in transferring the queue from zk perspective), the regionserver doing the failover for B will also process A&apos;s znodes (as they are with B now). Therefore, I don&apos;t see we really need a retry. Did I miss anything?&lt;/p&gt;</comment>
                            <comment id="13555263" author="lhofhansl" created="Wed, 16 Jan 2013 17:51:39 +0000"  >&lt;p&gt;But that is not case (unless I am misunderstanding completely). All RSs race to get the lock to take over the dead RS&apos;s queues. Once there is a winner, that RS will move the queues. So if the winning RS dies after it learn that it is the winner but before it move the queues those queues are lost.&lt;/p&gt;

&lt;p&gt;What you describe is one way to solve the problem: All RSs simply try to move the queues. That would work, but would lead to the herding effect (which I think is acceptable).&lt;/p&gt;</comment>
                            <comment id="13555488" author="v.himanshu" created="Wed, 16 Jan 2013 21:34:02 +0000"  >&lt;p&gt;Yes, your description is totally correct. So, you okay with the approach, Lars?&lt;/p&gt;</comment>
                            <comment id="13555522" author="lhofhansl" created="Wed, 16 Jan 2013 22:13:31 +0000"  >&lt;p&gt;This change is good (so +1), but it does not fix the whole problem (you&apos;re not having all RSs attempt the queue failover).&lt;br/&gt;
Maybe we do your patch in a subtask and leave this issue open.&lt;/p&gt;</comment>
                            <comment id="13555524" author="lhofhansl" created="Wed, 16 Jan 2013 22:15:11 +0000"  >&lt;p&gt;Or did you mean whether I&apos;m OK with all RSs attempting to move the queues? I&apos;m happy with that too. I think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jdcryans&quot; class=&quot;user-hover&quot; rel=&quot;jdcryans&quot;&gt;Jean-Daniel Cryans&lt;/a&gt; voiced some concerns over the incurred herding effect.&lt;/p&gt;</comment>
                            <comment id="13558487" author="lhofhansl" created="Mon, 21 Jan 2013 02:41:11 +0000"  >&lt;p&gt;Specifically, check out ReplicatoinSourceManager.NodeFailoverWorker.run().&lt;br/&gt;
First all surviving RSs race to obtain the lock:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!zkHelper.lockOtherRS(rsZnode)) {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Only one RS will continue to move the failed RS&apos;s queues.&lt;/p&gt;

&lt;p&gt;I think what we could do is this:&lt;br/&gt;
If multi is supported we just have all surviving RSs attempt to move the queues (don&apos;t bother with the lock step). If multi is as atomic as advertised that should work and only one of the RS will succeed to move the queues atomically, but all will try.&lt;br/&gt;
It seems like that should work.&lt;/p&gt;</comment>
                            <comment id="13558938" author="v.himanshu" created="Mon, 21 Jan 2013 17:50:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;: Yes, I followed the same approach in the attached patch.&lt;/p&gt;</comment>
                            <comment id="13559232" author="lhofhansl" created="Tue, 22 Jan 2013 00:32:21 +0000"  >&lt;p&gt;Hmm... Yes, you did. Sorry, somehow missed it when I looked at it first.&lt;br/&gt;
Cool then, we came to the same conclusion. Just took me much longer to get to it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;+1 on patch, it should indeed fix this problem.&lt;/p&gt;</comment>
                            <comment id="13559248" author="yuzhihong@gmail.com" created="Tue, 22 Jan 2013 00:57:20 +0000"  >&lt;p&gt;Patch v3 fills javadoc for copyQueuesFromRSUsingMulti().&lt;/p&gt;</comment>
                            <comment id="13560005" author="v.himanshu" created="Tue, 22 Jan 2013 20:51:40 +0000"  >&lt;p&gt;Thanks for the review Lars &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, and Ted for updating the patch.&lt;/p&gt;
</comment>
                            <comment id="13560036" author="yuzhihong@gmail.com" created="Tue, 22 Jan 2013 21:18:57 +0000"  >&lt;p&gt;The trunk patch depends on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7382&quot; title=&quot;Port ZK.multi support from HBASE-6775 to 0.96&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7382&quot;&gt;&lt;del&gt;HBASE-7382&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;@Himanshu:&lt;br/&gt;
Can you run the tests listed @ 28/Jun/12 04:07 ?&lt;/p&gt;</comment>
                            <comment id="13560147" author="lhofhansl" created="Tue, 22 Jan 2013 23:06:32 +0000"  >&lt;p&gt;Let&apos;s make critical so it gets in.&lt;/p&gt;</comment>
                            <comment id="13562383" author="yuzhihong@gmail.com" created="Fri, 25 Jan 2013 05:21:25 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
p0 2611-upstream-v1.patch
patching file hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java
Hunk #1 succeeded at 25 (offset -1 lines).
Hunk #2 FAILED at 41.
Hunk #3 succeeded at 858 (offset 131 lines).
1 out of 3 hunks FAILED -- saving rejects to file hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java.rej
patching file hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java
Hunk #1 succeeded at 579 (offset 19 lines).
patching file hbase-server/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java
Reversed (or previously applied) patch detected!  Assume -R? [n] ^C
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;@Himanshu:&lt;br/&gt;
Can you update the upstream patch ?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13562454" author="v.himanshu" created="Fri, 25 Jan 2013 06:05:18 +0000"  >&lt;p&gt;updated trunk patch&lt;/p&gt;</comment>
                            <comment id="13562460" author="lhofhansl" created="Fri, 25 Jan 2013 06:19:58 +0000"  >&lt;p&gt;Himanshu, you are officially my hero now. We&apos;ve been discussing this for over a year, and it looks like we&apos;re finally fixing it.&lt;/p&gt;</comment>
                            <comment id="13562479" author="hadoopqa" created="Fri, 25 Jan 2013 07:06:20 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12566463/HBASE-2611-trunk-v2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12566463/HBASE-2611-trunk-v2.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 1 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4177//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13562699" author="yuzhihong@gmail.com" created="Fri, 25 Jan 2013 14:05:50 +0000"  >&lt;p&gt;Patch v3 fixes the javadoc warning:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; map of peer cluster to log queues 
+   */
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; SortedMap&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, SortedSet&amp;lt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;gt;&amp;gt; copyQueuesFromRSUsingMulti(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; znode) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13562753" author="hadoopqa" created="Fri, 25 Jan 2013 15:10:23 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12566510/2611-trunk-v3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12566510/2611-trunk-v3.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4181//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13562755" author="yuzhihong@gmail.com" created="Fri, 25 Jan 2013 15:13:01 +0000"  >&lt;p&gt;Will integrated patch v3 later today if there is no further review comment.&lt;/p&gt;</comment>
                            <comment id="13562847" author="yuzhihong@gmail.com" created="Fri, 25 Jan 2013 17:22:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jdcryans&quot; class=&quot;user-hover&quot; rel=&quot;jdcryans&quot;&gt;Jean-Daniel Cryans&lt;/a&gt;:&lt;br/&gt;
It would be nice if you take a look at Himanshu&apos;s patch.&lt;/p&gt;</comment>
                            <comment id="13564016" author="lhofhansl" created="Mon, 28 Jan 2013 03:19:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jdcryans&quot; class=&quot;user-hover&quot; rel=&quot;jdcryans&quot;&gt;Jean-Daniel Cryans&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="13564910" author="lhofhansl" created="Tue, 29 Jan 2013 00:44:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted_yu&quot; class=&quot;user-hover&quot; rel=&quot;ted_yu&quot;&gt;Ted Yu&lt;/a&gt; Let&apos;s commit this. +1 from me.&lt;/p&gt;</comment>
                            <comment id="13564936" author="jdcryans" created="Tue, 29 Jan 2013 01:07:43 +0000"  >&lt;p&gt;Some comments:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;LOG.info(&quot;Moving &quot; + rsZnode + &quot;&apos;s hlogs to my queue&quot;);&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This could be changed to say whether it&apos;s going to be done atomically or not.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;LOG.debug(&quot; The multi list is: &quot; + listOfOps + &quot;, size: &quot; + listOfOps.size());&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is going to print a lot of object references... not sure how useful this is. Maybe just keep the size?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;LOG.info(&quot;Atomically moved the dead regionserver logs. &quot;);&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;With my first comment this becomes redundant and somewhere else it will say when the move is done anyway.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;LOG.warn(&quot;Got exception in copyQueuesFromRSUsingMulti: &quot; + e);&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Put the &quot;e&quot; in the second paramater instead of appending it to the string.&lt;/p&gt;</comment>
                            <comment id="13564967" author="v.himanshu" created="Tue, 29 Jan 2013 01:37:02 +0000"  >&lt;p&gt;incorporating JD&apos;s comments; I left one where it prints out after successfully moving the znodes. I think it is helpful as only one regionserver will print this, others will fail.&lt;/p&gt;</comment>
                            <comment id="13565004" author="hadoopqa" created="Tue, 29 Jan 2013 02:33:13 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12566875/HBASE-2611-trunk-v3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12566875/HBASE-2611-trunk-v3.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 1 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4225//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13565005" author="yuzhihong@gmail.com" created="Tue, 29 Jan 2013 02:33:42 +0000"  >&lt;p&gt;@Himanshu:&lt;br/&gt;
Mind attaching patch for 0.94 ?&lt;/p&gt;</comment>
                            <comment id="13565031" author="yuzhihong@gmail.com" created="Tue, 29 Jan 2013 03:12:28 +0000"  >&lt;p&gt;Patch v4 fixes javadoc warning w.r.t. empty @return.&lt;/p&gt;</comment>
                            <comment id="13565080" author="hadoopqa" created="Tue, 29 Jan 2013 04:59:38 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12566899/2611-trunk-v4.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12566899/2611-trunk-v4.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4228//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13565081" author="yuzhihong@gmail.com" created="Tue, 29 Jan 2013 05:05:47 +0000"  >&lt;p&gt;Patch v4 integrated to trunk.&lt;/p&gt;

&lt;p&gt;Thanks for the patch, Himanshu.&lt;/p&gt;

&lt;p&gt;Thanks for the reviews, Lars and J-D.&lt;/p&gt;</comment>
                            <comment id="13565088" author="lhofhansl" created="Tue, 29 Jan 2013 05:21:22 +0000"  >&lt;p&gt;0.94 patch. Passes TestReplication locally.&lt;/p&gt;</comment>
                            <comment id="13565089" author="hadoopqa" created="Tue, 29 Jan 2013 05:32:04 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12566908/2611-0.94.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12566908/2611-0.94.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/4229//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/4229//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13565095" author="lhofhansl" created="Tue, 29 Jan 2013 05:38:04 +0000"  >&lt;p&gt;Going to commit the 0.94 version tomorrow, unless I hear objections.&lt;/p&gt;</comment>
                            <comment id="13565131" author="hudson" created="Tue, 29 Jan 2013 06:46:36 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3820 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3820/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3820/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2611&quot; title=&quot;Handle RS that fails while processing the failure of another one&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2611&quot;&gt;&lt;del&gt;HBASE-2611&lt;/del&gt;&lt;/a&gt; Handle RS that fails while processing the failure of another one (Himanshu) (Revision 1439744)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13565341" author="hudson" created="Tue, 29 Jan 2013 13:09:33 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #382 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/382/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/382/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2611&quot; title=&quot;Handle RS that fails while processing the failure of another one&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2611&quot;&gt;&lt;del&gt;HBASE-2611&lt;/del&gt;&lt;/a&gt; Handle RS that fails while processing the failure of another one (Himanshu) (Revision 1439744)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13565640" author="lhofhansl" created="Tue, 29 Jan 2013 18:49:15 +0000"  >&lt;p&gt;Committed to 0.94... Yeah!&lt;/p&gt;</comment>
                            <comment id="13565728" author="hudson" created="Tue, 29 Jan 2013 19:57:09 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #800 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/800/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/800/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2611&quot; title=&quot;Handle RS that fails while processing the failure of another one&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2611&quot;&gt;&lt;del&gt;HBASE-2611&lt;/del&gt;&lt;/a&gt; Handle RS that fails while processing the failure of another one (Himanshu Vashishtha) (Revision 1440054)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13566296" author="hudson" created="Wed, 30 Jan 2013 08:41:27 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #102 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/102/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/102/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2611&quot; title=&quot;Handle RS that fails while processing the failure of another one&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2611&quot;&gt;&lt;del&gt;HBASE-2611&lt;/del&gt;&lt;/a&gt; Handle RS that fails while processing the failure of another one (Himanshu Vashishtha) (Revision 1440054)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13570971" author="hudson" created="Tue, 5 Feb 2013 03:58:23 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security-on-Hadoop-23 #11 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/11/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/11/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2611&quot; title=&quot;Handle RS that fails while processing the failure of another one&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2611&quot;&gt;&lt;del&gt;HBASE-2611&lt;/del&gt;&lt;/a&gt; Handle RS that fails while processing the failure of another one (Himanshu Vashishtha) (Revision 1440054)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12523002">HBASE-4401</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12605474">HBASE-6695</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12624522">HBASE-7382</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12747665">HBASE-12241</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12456177">HBASE-2223</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12566908" name="2611-0.94.txt" size="5777" author="lhofhansl" created="Tue, 29 Jan 2013 05:21:22 +0000"/>
                            <attachment id="12566510" name="2611-trunk-v3.patch" size="5994" author="yuzhihong@gmail.com" created="Fri, 25 Jan 2013 14:05:50 +0000"/>
                            <attachment id="12566899" name="2611-trunk-v4.patch" size="6106" author="yuzhihong@gmail.com" created="Tue, 29 Jan 2013 03:12:28 +0000"/>
                            <attachment id="12565884" name="2611-v3.patch" size="6485" author="yuzhihong@gmail.com" created="Tue, 22 Jan 2013 00:57:20 +0000"/>
                            <attachment id="12566463" name="HBASE-2611-trunk-v2.patch" size="5959" author="v.himanshu" created="Fri, 25 Jan 2013 06:05:18 +0000"/>
                            <attachment id="12566875" name="HBASE-2611-trunk-v3.patch" size="6070" author="v.himanshu" created="Tue, 29 Jan 2013 01:37:02 +0000"/>
                            <attachment id="12565039" name="HBASE-2611-v2.patch" size="6424" author="v.himanshu" created="Tue, 15 Jan 2013 23:33:25 +0000"/>
                            <attachment id="12533558" name="HBase-2611-upstream-v1.patch" size="12884" author="v.himanshu" created="Tue, 26 Jun 2012 23:12:56 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 10 Jun 2011 22:45:43 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32678</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 45 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0b05z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>62137</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>The fix for this issue uses Zookeeper multi functionality (hbase.zookeeper.useMulti). Please refer to hbase-default.xml about this property. There is an addendum fix at HBase-8099 (fixed in 0.94.6). In case you are running on branch &amp;lt; 0.94.6, please patch it with HBase-8099, OR make sure hbase.zookeeper.useMulti is set to false.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>