<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:37:54 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-12934/HBASE-12934.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-12934] Utilize Flash storage for flushing</title>
                <link>https://issues.apache.org/jira/browse/HBASE-12934</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Store flushing should be able to make use of hdfs storage policy.&lt;br/&gt;
One option is to allow setting storage policy for the directory path of the specified column family.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12770447">HBASE-12934</key>
            <summary>Utilize Flash storage for flushing</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12603333">HBASE-6572</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="6">Invalid</resolution>
                                        <assignee username="yuzhihong@gmail.com">Ted Yu</assignee>
                                    <reporter username="yuzhihong@gmail.com">Ted Yu</reporter>
                        <labels>
                    </labels>
                <created>Tue, 27 Jan 2015 20:01:16 +0000</created>
                <updated>Thu, 29 Jan 2015 16:49:11 +0000</updated>
                            <resolved>Tue, 27 Jan 2015 20:58:16 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="14294131" author="lhofhansl" created="Tue, 27 Jan 2015 20:28:35 +0000"  >&lt;p&gt;Still a bit skeptical. For the memstore we&apos;re writing at least 128mb chunks (unless HBase is misconfigured and we&apos;re writing due to global heap pressure, or one has too many column families).&lt;br/&gt;
SSDs and HDDs have similar throughput (although highend SSDs can be faster here). The main difference is seek latency, which 0.1ms for SSDs and typically at least 10ms for HDDs.&lt;br/&gt;
So assuming we have about 100mb/s throughput in both cases, we&apos;d take ~1.28s+0.1ms for SSDs vs. ~1.28s+10ms for HDDs, a difference of less than 1%.&lt;br/&gt;
Flushes are also asynchronous (same as compactions).&lt;/p&gt;

&lt;p&gt;The main advantage is (1) for random reads where the working set does not fit into the aggregate cache and (2) the WAL writes &lt;b&gt;if&lt;/b&gt; we&apos;re actually sync&apos;ing the WAL to disk.&lt;/p&gt;

&lt;p&gt;My statement would change if SSDs have typically higher write throughput compared to HDDs. Do they?&lt;br/&gt;
And even than, why limit this flushes then? Both flushes and compactions are asynchronous.&lt;/p&gt;</comment>
                            <comment id="14294142" author="yuzhihong@gmail.com" created="Tue, 27 Jan 2015 20:38:28 +0000"  >&lt;p&gt;Lars:&lt;br/&gt;
w.r.t. throughput, see &apos;Data transfer rate&apos; row of the table on &lt;a href=&quot;http://en.wikipedia.org/wiki/Solid-state_drive&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Solid-state_drive&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14294171" author="lhofhansl" created="Tue, 27 Jan 2015 20:52:50 +0000"  >&lt;p&gt;Yep... Seen that. We also have the bandwidth limit from the write pipeline. 125mb/s max for 1ge. 1.56gb/s for 10ge. Also I think we usually say that an HDFS output stream can write at about 40-50mb/s. Maybe an HDFS guy can provide some actual numbers.&lt;br/&gt;
So with really nice SSDs and 10ge networking between racks, I can see an advantage, if HDFS can handle, but only if we do this for flushes and compactions (i.e. the CF is essentially stored on SSDs).&lt;br/&gt;
Then again compactions are tricky, since the write-amplification will wear out the SSDs.&lt;/p&gt;

&lt;p&gt;Bottom line, this should be backed up by some perf data &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
And as I stated, other parts are more obvious: finishing &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt; and then place WALs on SSDs, or allow column families to be stored on SSDs for random read load.&lt;br/&gt;
Why don&apos;t we do that first?&lt;/p&gt;

&lt;p&gt;Happy to meet up on this and have face-2-face discussion.&lt;/p&gt;</comment>
                            <comment id="14294179" author="stack" created="Tue, 27 Jan 2015 20:58:16 +0000"  >&lt;p&gt;Resolving. Rubbish issue. No justification other than &quot;Store flushing should be able to make use of hdfs storage policy.&quot; In face of strong counter argument, still no &apos;why&apos;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted_yu&quot; class=&quot;user-hover&quot; rel=&quot;ted_yu&quot;&gt;Ted Yu&lt;/a&gt; Please stop filling issues w/o proper justification.  Wastes everyone&apos;s time.  If you need something of use to do around here, just ask.  There is plenty to do.&lt;/p&gt;</comment>
                            <comment id="14294188" author="apurtell" created="Tue, 27 Jan 2015 21:03:51 +0000"  >&lt;p&gt;I agree with Lars&apos; prioritization: write acceleration for WALs, read acceleration of read-mostly HFile data. Flush files and compaction temporaries are inconsequential. &lt;/p&gt;</comment>
                            <comment id="14294258" author="hadoopqa" created="Tue, 27 Jan 2015 21:54:41 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12694831/12934-001.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12694831/12934-001.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit f7df0990c2d321cffd7ea2e20cb7b280d8cc9db6.&lt;br/&gt;
  ATTACHMENT ID: 12694831&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 2 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.regionserver.TestBlocksScanned&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestSplitTransaction&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestColumnSeeking&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestCoprocessorInterface&lt;br/&gt;
                  org.apache.hadoop.hbase.io.encoding.TestPrefixTree&lt;br/&gt;
                  org.apache.hadoop.hbase.filter.TestColumnPrefixFilter&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestScanner&lt;br/&gt;
                  org.apache.hadoop.hbase.filter.TestDependentColumnFilter&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestResettingCounters&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestRegionMergeTransaction&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestKeepDeletes&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestStoreFileRefresherChore&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestRegionObserverStacking&lt;br/&gt;
                  org.apache.hadoop.hbase.io.hfile.TestScannerSelectionUsingKeyRange&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestScanWithBloomError&lt;br/&gt;
                  org.apache.hadoop.hbase.filter.TestFilter&lt;br/&gt;
                  org.apache.hadoop.hbase.filter.TestInvocationRecordFilter&lt;br/&gt;
                  org.apache.hadoop.hbase.filter.TestMultipleColumnPrefixFilter&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestWideScanner&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestIntraRowPagination&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestMinVersions&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Javadoc warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/patchJavadocWarnings.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//artifact/patchprocess/patchJavadocWarnings.txt&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12606//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14294475" author="devaraj" created="Wed, 28 Jan 2015 00:27:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;And as I stated, other parts are more obvious: finishing &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt; and then place WALs on SSDs, or allow column families to be stored on SSDs for random read load.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;, agree on the fact that &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt; should be done in order to see benefits that &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12848&quot; title=&quot;Utilize Flash storage for WAL&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12848&quot;&gt;&lt;del&gt;HBASE-12848&lt;/del&gt;&lt;/a&gt; potentially brings in. But could you please clarify on the &quot;storing of column families&quot; - what Ted did here would be a starting point for that? I took a quick look at the patch and it seems it handles the case of flushing but not compaction. Or, are you suggesting that we could &quot;promote&quot; certain column families&apos; files to SSDs when we see fit (manually / automatically).. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Then again compactions are tricky, since the write-amplification will wear out the SSDs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;, did you have any material that talks about that. A quick search on the web on this topic led me to some material but those were not conclusive. I was chatting with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sureshms&quot; class=&quot;user-hover&quot; rel=&quot;sureshms&quot;&gt;Suresh Srinivas&lt;/a&gt; on this and we are both interested to hear more this... (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sureshms&quot; class=&quot;user-hover&quot; rel=&quot;sureshms&quot;&gt;Suresh Srinivas&lt;/a&gt; feel free to chime in).&lt;/p&gt;</comment>
                            <comment id="14294592" author="lhofhansl" created="Wed, 28 Jan 2015 02:08:13 +0000"  >&lt;p&gt;SSD wear is well documented. I am not sure it is an issue specifically, just a concern to rule out. Would definitely use SSDs only for data that mostly read, especially with LSM trees.&lt;/p&gt;

&lt;p&gt;The title of this jira is about utilizing flash for flushing. It only does flushes, and when enabled does all flushes to SSDs. That just makes no sense to me. Why would I want to place all HFiles resulting from flushes on SSDs across the board? I can&apos;t think of a single use case where this is useful. It&apos;s the wrong way to slice this problem.&lt;br/&gt;
Flushes are asynchronous, they only become a problem when we reach the number of the blocking store files, which is a problem of &lt;b&gt;compactions&lt;/b&gt; not being able to keep, not flushes.&lt;/p&gt;

&lt;p&gt;Now...&lt;br/&gt;
If you slice it the other way and say &quot;all files written on behalf of this column family are written to SSD&quot;, then we have a story. And that would &lt;b&gt;not&lt;/b&gt; be about improving writes but about &lt;b&gt;reads&lt;/b&gt;. In that case you write all flushes and compaction of a specific column families to SSDs.&lt;br/&gt;
This jira is not doing that, nor does it provide a path to get there.&lt;/p&gt;</comment>
                            <comment id="14294599" author="lhofhansl" created="Wed, 28 Jan 2015 02:22:09 +0000"  >&lt;p&gt;Misread the patch. The flush decisions are per CF. Sorry.&lt;/p&gt;

&lt;p&gt;So if we add compactions too, this would be useful. Wanna do that?&lt;/p&gt;</comment>
                            <comment id="14294611" author="apurtell" created="Wed, 28 Jan 2015 02:31:50 +0000"  >&lt;p&gt;I didn&apos;t misread the patch. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I don&apos;t think we want to put all of the data for the whole CF on one tier, we should be deciding based on access statistics whether to migrate HFiles from one tier to the other, not just assuming a flush or freshly compacted file will benefit. Any given file could end up just being cold data. I would agree that a coarse grain &quot;policy&quot; of flushing and compacting every file of a CF would be a first step, and it might be useful if a user can judge it properly for their use case, but this would be another thing the user has to specify correctly in the schema without possibility of autotuning/dynamic decision making. So I don&apos;t think it&apos;s quite the right approach.&lt;/p&gt;</comment>
                            <comment id="14294695" author="lhofhansl" created="Wed, 28 Jan 2015 04:22:44 +0000"  >&lt;p&gt;Everything except for the 2nd paragraph in my last comment still stands.&lt;/p&gt;

&lt;p&gt;SSD wear is a concern for write heavy CFs, that this is a &lt;b&gt;read&lt;/b&gt; optimization, and that it is not useful when we only consider flushes (then we misguidedly tried to optimize for writes).&lt;/p&gt;

&lt;p&gt;And lastly that if we change the scope of this jira to keep an entire CF on SSD (presumably a user would know about pros/cons before enabling this) and make sure in the release notes that this is a read optimization for CF accessed via random reads, it would be potentially useful as a first step to what Andrew points to.&lt;/p&gt;</comment>
                            <comment id="14294742" author="stack" created="Wed, 28 Jan 2015 05:51:56 +0000"  >&lt;p&gt;Great. A bunch of effort being expended debating what this issue could be about.&lt;/p&gt;</comment>
                            <comment id="14295979" author="enis" created="Wed, 28 Jan 2015 22:35:48 +0000"  >&lt;p&gt;I was thinking the same thing that we will do a keep the files of the column family in SSD approach. We have an IN_MEMORY option for CF&apos;s. This should be similarly configured (although semantically very different). &lt;br/&gt;
Agreed with the SSD wear concerns, but in theory an LSM based mutable DB is better than a B-tree based one for wear. I believe enterprise class SSD&apos;s can also perform 2-4x better write throughput (my MBP disk can achieve 700MB/s!). In any case, I think we want to give power users the ability to start experimenting with this. Should we move the discussion into the parent jira? &lt;/p&gt;</comment>
                            <comment id="14296195" author="lhofhansl" created="Thu, 29 Jan 2015 01:11:31 +0000"  >&lt;p&gt;I think it&apos;s fine to move on with this one, as along as we do what we all appear to agree now (keep a CF on SSD). Just flushes is useless (IMHO, as usually) and hence I believe the initial idea of this jira was misguided.&lt;br/&gt;
We can keep this one closed and open a new one for that, or we can change this jira to do that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We have an IN_MEMORY option for CF&apos;s. This should be similarly configured.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Exactly. That makes it simple and easy to understand.&lt;br/&gt;
And later we can add what Andy wants, namely that we can identify hot files or regions and migrate those to SSD automatically.&lt;/p&gt;</comment>
                            <comment id="14296206" author="stack" created="Thu, 29 Jan 2015 01:19:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;....or we can change this jira to do that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Open new one. This one is destroyed.&lt;/p&gt;</comment>
                            <comment id="14296518" author="devaraj" created="Thu, 29 Jan 2015 07:34:41 +0000"  >&lt;p&gt;I am running the risk of stating the obvious but let me still summarize. As a starter, (1) we could do flushes of certain CFs that the user sees fit into SSDs (2) compactions should also happen on files in the SSDs and they should create the outputs on the SSDs. We should do both these in a single jira. In the next phase of the work, we could do the more intelligent usage based movement of storefiles to/from SSD. It&apos;ll be good to get numbers on performance.&lt;br/&gt;
I was checking with some folks on the issue of wear/tear of SSDs (when subject to the HBase IO patterns). It seems for the enterprise grade SSDs, it should not a problem.&lt;br/&gt;
Does the above sound reasonable (and yes this jira seems jinxed; let&apos;s open a new one once the summary above is agreed upon as a good starting point).&lt;/p&gt;</comment>
                            <comment id="14297121" author="lhofhansl" created="Thu, 29 Jan 2015 16:49:11 +0000"  >&lt;p&gt;That sounds good to me. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12694831" name="12934-001.txt" size="5922" author="yuzhihong@gmail.com" created="Tue, 27 Jan 2015 20:02:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 27 Jan 2015 20:28:35 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 46 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i24uxr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>