<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:33:56 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6116/HBASE-6116.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6116] Allow parallel HDFS writes for HLogs.</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6116</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-1783&quot; title=&quot;Ability for HDFS client to write replicas in parallel&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-1783&quot;&gt;HDFS-1783&lt;/a&gt; I adapted Dhrubas changes to be used in Hadoop trunk.&lt;br/&gt;
This issue will include the necessary reflection changes to optionally enable this for the WALs in HBase.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12558321">HBASE-6116</key>
            <summary>Allow parallel HDFS writes for HLogs.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="lhofhansl">Lars Hofhansl</reporter>
                        <labels>
                    </labels>
                <created>Mon, 28 May 2012 09:47:50 +0000</created>
                <updated>Thu, 2 May 2013 02:30:52 +0000</updated>
                            <resolved>Wed, 1 May 2013 05:47:13 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>31</watches>
                                                                <comments>
                            <comment id="13285503" author="lhofhansl" created="Wed, 30 May 2012 08:28:46 +0000"  >&lt;p&gt;Waiting for &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-1783&quot; title=&quot;Ability for HDFS client to write replicas in parallel&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-1783&quot;&gt;HDFS-1783&lt;/a&gt; API to stabilize before I post a patch here.&lt;br/&gt;
I don&apos;t currently have access to a real cluster, so if someone could do some testing in a real cluster with Hadoop-Trunk and HBase-Trunk, please let me know!&lt;/p&gt;</comment>
                            <comment id="13285760" author="apurtell" created="Wed, 30 May 2012 15:46:31 +0000"  >&lt;p&gt;When there is a patch I can try it out on a test cluster in EC2.&lt;/p&gt;</comment>
                            <comment id="13287358" author="lhofhansl" created="Fri, 1 Jun 2012 12:30:55 +0000"  >&lt;p&gt;Initial patch, which includes &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
This also fixes building HBase trunk with Hadoop trunk (3.0.0-SNAPSHOT).&lt;/p&gt;

&lt;p&gt;In order to test, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-1783&quot; title=&quot;Ability for HDFS client to write replicas in parallel&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-1783&quot;&gt;HDFS-1783&lt;/a&gt; needs to be applied to Hadoop (trunk) first.&lt;br/&gt;
Then build Hadoop with:&lt;br/&gt;
mvn -Pnative -Pdist -Dtar -DskipTests install&lt;br/&gt;
And then HBase with:&lt;br/&gt;
mvn -DskipTests -Dhadoop.profile=3.0 ...&lt;/p&gt;

&lt;p&gt;Parallel writes can be enable in hbase-site.xml with:&lt;br/&gt;
    hbase.regionserver.wal.parallel.writes&lt;/p&gt;

&lt;p&gt;Since this patch include &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt;, durable sync can also be enabled:&lt;br/&gt;
    hbase.regionserver.wal.durable.sync&lt;br/&gt;
    hbase.regionserver.hfile.durable.sync&lt;/p&gt;

&lt;p&gt;(all options can be set to &quot;true&quot;)&lt;/p&gt;

&lt;p&gt;@Andy: If your offer to do a quick test in EC2 still stands that&apos;d be awesome!&lt;/p&gt;</comment>
                            <comment id="13291653" author="lhofhansl" created="Fri, 8 Jun 2012 09:36:12 +0000"  >&lt;p&gt;If it makes testing easier I could attach a patch of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-1783&quot; title=&quot;Ability for HDFS client to write replicas in parallel&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-1783&quot;&gt;HDFS-1783&lt;/a&gt; against Hadoop-2.&lt;/p&gt;</comment>
                            <comment id="13397933" author="apurtell" created="Wed, 20 Jun 2012 22:09:28 +0000"  >&lt;p&gt;I ported &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-1783&quot; title=&quot;Ability for HDFS client to write replicas in parallel&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-1783&quot;&gt;HDFS-1783&lt;/a&gt; and this patch to Hadoop 2.0.1 and HBase 0.94. A stressful test on a 5 slave cluster with LoadTestTool is still stable after 8 hours. I only have EC2 resources so am not sure a relative performance benchmark would be that meaningful, but I could try.&lt;/p&gt;</comment>
                            <comment id="13398258" author="lhofhansl" created="Thu, 21 Jun 2012 07:47:18 +0000"  >&lt;p&gt;@Andrew: Awesome!&lt;br/&gt;
A comparative benchmark is still useful I think. Only if it is not too much work, as the standard deviation will be high on EC2. I think you&apos;ll see an improvement in a write heavy test.&lt;/p&gt;</comment>
                            <comment id="13398651" author="apurtell" created="Thu, 21 Jun 2012 17:59:02 +0000"  >&lt;p&gt;@Lars, sure. Parallel vs. not only I presume, or are you interested in the difference with durable sync enabled also?&lt;/p&gt;</comment>
                            <comment id="13398772" author="lhofhansl" created="Thu, 21 Jun 2012 19:23:36 +0000"  >&lt;p&gt;Any datapoint is helpful. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Parallel vs. not will be most interesting.&lt;br/&gt;
Durable sync on EC2 might be interesting as it might be slow on EC2.&lt;/p&gt;

&lt;p&gt;Whatever time permits... I know you&apos;re busy. Thanks so much for spending time on this.&lt;br/&gt;
We&apos;ll be testing too when I&apos;m back in the US.&lt;/p&gt;</comment>
                            <comment id="13400075" author="apurtell" created="Sat, 23 Jun 2012 23:34:16 +0000"  >&lt;p&gt;Attached is a comparison of pipelined vs. parallel sync differences on two identical (but different) 5 slave EC2 clusters. I modified HBase to use a new histogram metric for recording HLog sync latency and then ran a write dominant workload on each cluster using LoadTestTool for 60 minutes and captured RegionServer metrics at one second intervals.&lt;/p&gt;

&lt;p&gt;The first tab of the spreadsheet describes the experiment parameters. The second shows mean, 99th percentile, and standard deviation for pipelined syncs as reported. The third shows mean, 99th percentile, and standard deviation for parallel syncs as reported. The fourth has some simple graphs I threw together for illustration. The remaining tabs contain the detail of the captured metrics for each host.&lt;/p&gt;

&lt;p&gt;Edit: Note all metrics are in milliseconds.&lt;/p&gt;</comment>
                            <comment id="13400180" author="jmhsieh" created="Sun, 24 Jun 2012 16:40:55 +0000"  >&lt;p&gt;Wow, nice!  &lt;/p&gt;

&lt;p&gt;Is there any reason why the tests weren&apos;t run on exactly the same EC2 instances with different configuration/binaries?  &lt;/p&gt;

&lt;p&gt;Is the variation due to EC2 node variation or because of the software changes? Do you think that matters?  &lt;/p&gt;

</comment>
                            <comment id="13400185" author="apurtell" created="Sun, 24 Jun 2012 17:21:10 +0000"  >&lt;p&gt;In my experience there is little difference in variability run to run on the same instances as opposed to two parallel runs started around the same time. EC2 is not a great platform for this kind of testing so it should be run on real hardware to see if the results are replicated there. Or I could try the so called cluster compute instances. They are expensive but given the result confirming it with those could be justified &lt;/p&gt;</comment>
                            <comment id="13400195" author="lhofhansl" created="Sun, 24 Jun 2012 18:59:52 +0000"  >&lt;p&gt;Again thanks for doing this Andy.&lt;br/&gt;
Looks like the latency is indeed reduced greatly, by around 30% - judged by the mean.&lt;/p&gt;

&lt;p&gt;The slower instances now hover around 8ms as opposed to 12ms before, and the other instances around 6ms as opposed to 8ms.&lt;/p&gt;

&lt;p&gt;I&apos;m still trying to interpret the 90th percentile numbers.&lt;/p&gt;</comment>
                            <comment id="13400224" author="apurtell" created="Sun, 24 Jun 2012 22:46:28 +0000"  >&lt;p&gt;I&apos;m going to do this again tomorrow on cluster compute instances.  The results should be cleaner. &lt;/p&gt;</comment>
                            <comment id="13400801" author="apurtell" created="Mon, 25 Jun 2012 19:18:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m going to do this again tomorrow on cluster compute instances. The results should be cleaner.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I should have realized this earlier, but CC instances don&apos;t support instance store volumes, only EBS. EBS is IMO a crappy storage subsystem, in my experience instance store is about 2x slower than real hardware, but EBS can be 10x+ slower and highly variable. So this completes what I can do with EC2.&lt;/p&gt;</comment>
                            <comment id="13401016" author="lhofhansl" created="Mon, 25 Jun 2012 23:25:41 +0000"  >&lt;p&gt;@Andy: Fair enough &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
BTW, do you still have the HBase-0.94 patch you made (so I do not have to do the same work)?&lt;/p&gt;</comment>
                            <comment id="13401154" author="apurtell" created="Tue, 26 Jun 2012 04:10:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;BTW, do you still have the HBase-0.94 patch you made (so I do not have to do the same work)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Attached as &apos;apurtell-patches.zip&apos;. The second HBase patch won&apos;t apply directly, in this code base the histograms are scaled to milliseconds, but the changes are easy to apply by hand.&lt;/p&gt;</comment>
                            <comment id="13401158" author="lhofhansl" created="Tue, 26 Jun 2012 04:21:19 +0000"  >&lt;p&gt;Awesome. Thanks!&lt;/p&gt;</comment>
                            <comment id="13404359" author="lhofhansl" created="Sat, 30 Jun 2012 01:27:20 +0000"  >&lt;p&gt;I am having a hard time quantifying any advantage from this patch with my tests in our DEV cluster.&lt;br/&gt;
So I no longer think that this is a worthwhile avenue to follow.&lt;/p&gt;

&lt;p&gt;I used PerformanceEvaluation. I hacked it to be able to test with smaller packets and/or autoflush enabled, and in no scenario did I see a statistically significant advantage when this patch was enabled.&lt;/p&gt;

&lt;p&gt;Will close as &quot;Won&apos;t fix&quot; unless somebody else can think of other ways of testing this.&lt;/p&gt;

&lt;p&gt;@Ted: Maybe you can do the test you test you for multiple WALs?&lt;/p&gt;</comment>
                            <comment id="13404523" author="stack" created="Sat, 30 Jun 2012 15:55:13 +0000"  >&lt;p&gt;@Lars So, you don&apos;t see a perf boost writing in parallel?  Where is the 30% that Andrew was getting on EC2?  You don&apos;t see it on your dev cluster?  (You fellas at SF use toy hardware or what?)&lt;/p&gt;</comment>
                            <comment id="13404536" author="lhofhansl" created="Sat, 30 Jun 2012 16:11:36 +0000"  >&lt;p&gt;This was with a 6 DN/RS cluster with real HW. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Whatever scenario I tried the parallel write path was never faster.&lt;/p&gt;

&lt;p&gt;I tried with PerformanceEvaluation and defaults and a presplit table (i.e. 1000 byte values, no autoflush, etc). In that case I basically just saturated the client&apos;s network (~ 92mb/s in a 1gig link). I tested with --nomapred.&lt;br/&gt;
I then tried with a single region, to see if that one RegionServer would seen an advantage.&lt;br/&gt;
When that did not show any gains, I hacked PerformanceEvaluation to let me use smaller - 100 byte - value and to optionally also enable autoflush.&lt;br/&gt;
Now the network on the client is no longer saturated.&lt;br/&gt;
In that case parallel writes were actually slower, which really surprised me, as I had assumed that many individual puts that are all written to the WAL would show a big post for parallel writes.&lt;/p&gt;

&lt;p&gt;It&apos;s possible test my testing methodology is flawed.&lt;/p&gt;</comment>
                            <comment id="13404551" author="stack" created="Sat, 30 Jun 2012 16:37:17 +0000"  >&lt;p&gt;Dumb?: For sure the //writing was enabled?  If saturated network, I could imagine it not making a diff but if spare bandwidth.... I&apos;d think it&apos;d show through.  The //write would make us saturate the network on an interface before a &amp;#8211; (pipeline) write would I suppose.  And its easy enough for PE to saturate network IIRC anyway, w/o //writes.&lt;/p&gt;</comment>
                            <comment id="13404563" author="lhofhansl" created="Sat, 30 Jun 2012 16:53:22 +0000"  >&lt;p&gt;Yeah... I&apos;ll add a bit more diagnostics logging to make 100% sure as this definitely surprised me.&lt;br/&gt;
The length (latency) of the pipe from the client to the RS is long (at least in my test) as compared to the length of pipes between the RSs and the DNs.&lt;br/&gt;
Also the writing of the blocks is already interleaving with writing to the OS buffers, so the pipeline might not have had as much as an effect as expected.&lt;/p&gt;

&lt;p&gt;I&apos;ll also test together with durable sync, as this could show a different pattern.&lt;/p&gt;</comment>
                            <comment id="13404592" author="apurtell" created="Sat, 30 Jun 2012 18:17:09 +0000"  >&lt;p&gt;It&apos;s worth double checking the results with real hardware, but those are what matter IMO.&lt;/p&gt;</comment>
                            <comment id="13404593" author="apurtell" created="Sat, 30 Jun 2012 18:18:17 +0000"  >&lt;p&gt;Also, one difference is I ran LoadTestTool from the master node.&lt;/p&gt;</comment>
                            <comment id="13404614" author="stack" created="Sat, 30 Jun 2012 20:29:09 +0000"  >&lt;p&gt;@Lars If you remove hbase from the equation, what do you see?  (There is an hfile PE tool beside the PE tool... does same thing IIRC but just w/ hfiles)&lt;/p&gt;</comment>
                            <comment id="13404621" author="lhofhansl" created="Sat, 30 Jun 2012 20:52:49 +0000"  >&lt;p&gt;I&apos;ll try this on Monday. I&apos;ll also run PE from within the DC.&lt;/p&gt;</comment>
                            <comment id="13646396" author="lhofhansl" created="Wed, 1 May 2013 05:47:13 +0000"  >&lt;p&gt;Closing. Feel free to reopen if you think there is merit in this.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12549194">HBASE-5699</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12553865">HBASE-5937</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12554266">HBASE-5954</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12502330">HDFS-1783</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12530542" name="6116-v1.txt" size="17821" author="lhofhansl" created="Fri, 1 Jun 2012 12:30:55 +0000"/>
                            <attachment id="12533432" name="apurtell-patches.zip" size="15018" author="apurtell" created="Tue, 26 Jun 2012 04:10:04 +0000"/>
                            <attachment id="12533195" name="pipelined-vs-parallel-comparison.zip" size="3973511" author="apurtell" created="Sat, 23 Jun 2012 23:34:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 30 May 2012 15:46:31 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>241812</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 33 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02dan:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11755</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>