<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:08:31 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3234/HBASE-3234.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3234] hdfs-724 &quot;breaks&quot; TestHBaseTestingUtility multiClusters</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3234</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We upgraded our hadoop jar in TRUNK to latest on 0.20-append branch.  TestHBaseTestingUtility started failing reliably.  If I back out hdfs-724, the test passes again.  This issue is about figuring whats up here.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12479898">HBASE-3234</key>
            <summary>hdfs-724 &quot;breaks&quot; TestHBaseTestingUtility multiClusters</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hairong">Hairong Kuang</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Sun, 14 Nov 2010 06:51:46 +0000</created>
                <updated>Fri, 20 Nov 2015 12:43:02 +0000</updated>
                            <resolved>Tue, 30 Nov 2010 05:24:55 +0000</resolved>
                                                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12931820" author="stack" created="Sun, 14 Nov 2010 13:03:27 +0000"  >&lt;p&gt;The new hadoop jar would seem to be responsible for the now failing TestHFileOutputFormat too.  If I put in place an hadoop jar that is minus hdfs-724, it all passes.  Let me attach output of TestHBaseTestingUtility with full DEBUG enabled in case anyone has nought better to do of a sunday but peruse DEBUG spew.&lt;/p&gt;</comment>
                            <comment id="12931821" author="stack" created="Sun, 14 Nov 2010 13:05:32 +0000"  >&lt;p&gt;To get the above log, I changed the log4j that tests see as follows:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Index: src/test/resources/log4j.properties
===================================================================
--- src/test/resources/log4j.properties (revision 1034940)
+++ src/test/resources/log4j.properties (working copy)
@@ -42,6 +42,6 @@
 
 #log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG
 
-log4j.logger.org.apache.hadoop=WARN
+log4j.logger.org.apache.hadoop=DEBUG
 log4j.logger.org.apache.zookeeper=ERROR
 log4j.logger.org.apache.hadoop.hbase=DEBUG
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then ran this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$  mvn clean test -Dtest=TestHBaseTestingUtility
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12931917" author="streamy" created="Sun, 14 Nov 2010 23:11:32 +0000"  >&lt;p&gt;This is snippet from attached log that seems to be the first failure...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2010-11-14 05:00:08,938 DEBUG [DataStreamer for file /user/stack/.logs/pynchon-432.lan,63324,1289739567228/192.168.1.69%3A63324.1289739568182 block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_-4366233055961732763_1007 wrote packet seqno:1 size:795 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 05:00:08,938 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.BlockReceiver(393): Receiving one packet for block blk_-4366233055961732763_1007 of length 774 seqno 1 offsetInBlock 0 lastPacketInBlock false
2010-11-14 05:00:08,938 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.BlockReceiver$PacketResponder(737): PacketResponder 0 adding seqno 1 to ack queue.
2010-11-14 05:00:08,938 DEBUG [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_-4366233055961732763_1007 responded an ack: Replies for seqno 1 are SUCCESS
2010-11-14 05:00:08,938 DEBUG [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_-4366233055961732763_1007 waiting for local datanode to finish write.
2010-11-14 05:00:08,938 DEBUG [ResponseProcessor for block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are FAILED
2010-11-14 05:00:08,939 WARN  [ResponseProcessor for block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProcessor exception  for block blk_-4366233055961732763_1007java.io.IOException: Bad response 1 for block blk_-4366233055961732763_1007 from datanode 127.0.0.1:63316
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2542)

2010-11-14 05:00:08,939 WARN  [DataStreamer for file /user/stack/.logs/pynchon-432.lan,63324,1289739567228/192.168.1.69%3A63324.1289739568182 block blk_-4366233055961732763_1007] hdfs.DFSClient$DFSOutputStream(2616): Error Recovery for block blk_-4366233055961732763_1007 bad datanode[0] 127.0.0.1:63316
2010-11-14 05:00:08,941 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_-4366233055961732763_1007 java.io.EOFException: while trying to read 795 bytes
2010-11-14 05:00:08,941 INFO  [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(844): PacketResponder blk_-4366233055961732763_1007 0 : Thread is interrupted.
2010-11-14 05:00:08,941 INFO  [PacketResponder 0 for Block blk_-4366233055961732763_1007] datanode.BlockReceiver$PacketResponder(907): PacketResponder 0 for block blk_-4366233055961732763_1007 terminating
2010-11-14 05:00:08,941 WARN  [RegionServer:0;pynchon-432.lan,63324,1289739567228.logSyncer] hdfs.DFSClient$DFSOutputStream(3293): Error while syncing
java.io.IOException: All datanodes 127.0.0.1:63316 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2666)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:2157)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2356)
2010-11-14 05:00:08,942 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@54cee271] datanode.DataXceiver(377): writeBlock blk_-4366233055961732763_1007 received exception java.io.EOFException: while trying to read 795 bytes
2010-11-14 05:00:08,943 FATAL [RegionServer:0;pynchon-432.lan,63324,1289739567228.logSyncer] wal.HLog(1083): Could not append. Requesting close of hlog
java.io.IOException: Reflection
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:147)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.hflush(HLog.java:1059)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:983)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:145)
        ... 2 more
Caused by: java.io.IOException: All datanodes 127.0.0.1:63316 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2666)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:2157)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2356)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12931926" author="streamy" created="Sun, 14 Nov 2010 23:57:51 +0000"  >&lt;p&gt;From a new run, here is all of the activity of the block that causes the Abort...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;
2010-11-14 15:44:00,042 DEBUG [IPC Server handler 4 on 40196] namenode.FSDirectory(273): DIR* FSDirectory.addFile: /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 with blk_4643386764144918409_1009 block is added to the in-memory file system
2010-11-14 15:44:00,042 INFO  [IPC Server handler 4 on 40196] namenode.FSNamesystem(1482): BLOCK* NameSystem.allocateBlock: /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005. blk_4643386764144918409_1009
2010-11-14 15:44:00,044 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.DataXceiver(228): Receiving block blk_4643386764144918409_1009 src: /127.0.0.1:48990 dest: /127.0.0.1:48738
2010-11-14 15:44:00,044 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.FSDataset(1517): b=blk_4643386764144918409_1009, f=null
2010-11-14 15:44:00,044 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.FSDataset(1313): writeTo metafile is /var/users/jgray/hbase/trunk/target/test-data/5173adaa-a370-457d-8293-8f059f70bd31/dfs/data/data1/blocksBeingWritten/blk_4643386764144918409_1009.meta of size 0
2010-11-14 15:44:00,045 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:00,045 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:0 size:339 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:00,045 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 318 seqno 0 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:00,046 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded an ack: Replies for seqno 0 are SUCCESS
2010-11-14 15:44:00,046 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are SUCCESS
2010-11-14 15:44:00,046 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:31,551 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:-1 size:25 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:31,551 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 4 seqno -1 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:31,551 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(636): Changing block file offset of block blk_4643386764144918409_1009 from 310 to 0 meta file offset to 7
2010-11-14 15:44:31,552 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(422): Receiving empty packet for block blk_4643386764144918409_1009
2010-11-14 15:44:31,552 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded an ack: Replies for seqno -1 are SUCCESS
2010-11-14 15:44:31,552 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno -1 are
2010-11-14 15:44:31,552 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:31,867 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:1 size:804 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:31,868 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 783 seqno 1 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded an ack: Replies for seqno 1 are SUCCESS
2010-11-14 15:44:31,868 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are FAILED
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 waiting for local datanode to finish write.
2010-11-14 15:44:31,868 WARN  [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProcessor exception  for block blk_4643386764144918409_1009java.io.IOException: Bad response 1 for block blk_4643386764144918409_1009 from datanode 127.0.0.1:48738
2010-11-14 15:44:31,868 WARN  [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream(2616): Error Recovery for block blk_4643386764144918409_1009 bad datanode[0] 127.0.0.1:48738
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_4643386764144918409_1009 java.io.IOException: Connection reset by peer
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(844): PacketResponder blk_4643386764144918409_1009 0 : Thread is interrupted.
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(907): PacketResponder 0 for block blk_4643386764144918409_1009 terminating
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.DataXceiver(377): writeBlock blk_4643386764144918409_1009 received exception java.io.IOException: Connection reset by peer
2010-11-14 15:45:32,574 INFO  [IPC Server handler 8 on 40196] namenode.INodeFileUnderConstruction(212): BLOCK* blk_4643386764144918409_1009 recovery started, primary=127.0.0.1:48738
2010-11-14 15:45:34,183 INFO  [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1632): NameNode calls recoverBlock(block=blk_4643386764144918409_1009, targets=[127.0.0.1:48738])
2010-11-14 15:45:34,183 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1409): block=blk_4643386764144918409_1009
2010-11-14 15:45:34,184 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1419): getBlockMetaDataInfo successful block=blk_4643386764144918409_1009 length 771 genstamp 1009
2010-11-14 15:45:34,185 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1563): block=blk_4643386764144918409_1009, (length=771), syncList=[block:blk_4643386764144918409_1009 node:127.0.0.1:48738], closeFile=true
2010-11-14 15:45:34,186 INFO  [org.apache.hadoop.hdfs.server.datanode.DataNode$1@26c94114] datanode.DataNode(1450): oldblock=blk_4643386764144918409_1009(length=771), newblock=blk_4643386764144918409_1014(length=771), datanode=127.0.0.1:48738
2010-11-14 15:45:34,188 INFO  [IPC Server handler 1 on 40196] namenode.FSNamesystem(3149): BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:48738 is added to blk_4643386764144918409_1009 size 771
2010-11-14 15:45:34,189 INFO  [IPC Server handler 2 on 40196] namenode.FSNamesystem(1970): commitBlockSynchronization(lastblock=blk_4643386764144918409_1009, newgenerationstamp=1014, newlength=771, newtargets=[127.0.0.1:48738], closeFile=true, deleteBlock=false)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All of the recovery stuff happens after the abort.  The snippet around the exception/abort from this log is this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2010-11-14 15:44:31,867 DEBUG [PRI IPC Server handler 3 on 58498] hdfs.DFSClient$DFSOutputStream(3154): DFSClient writeChunk allocating new packet seqno=1, src=/user/jgray/.logs/dev692.sf2p.facebook.com
,58498,1289778232413/10.17.83.207%3A58498.1289778233005, packetSize=65557, chunksPerPacket=127, bytesCurBlock=0
2010-11-14 15:44:31,867 DEBUG [RegionServer:0;dev692.sf2p.facebook.com,58498,1289778232413.logSyncer] hdfs.DFSClient$DFSOutputStream(3238): DFSClient flush() : saveOffset 512 bytesCurBlock 771 lastFlush
Offset 310
2010-11-14 15:44:31,867 DEBUG [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$
DFSOutputStream$DataStreamer(2429): DataStreamer block blk_4643386764144918409_1009 wrote packet seqno:1 size:804 offsetInBlock:0 lastPacketInBlock:false
2010-11-14 15:44:31,868 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(393): Receiving one packet for block blk_4643386764144918409_1009 of length 783 seqno 1
 offsetInBlock 0 lastPacketInBlock false
2010-11-14 15:44:31,868 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver$PacketResponder(737): PacketResponder 0 adding seqno 1 to ack queue.
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 for block blk_4643386764144918409_1009 responded a
n ack: Replies for seqno 1 are SUCCESS
2010-11-14 15:44:31,868 DEBUG [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies for seqno 0 are FAILED
2010-11-14 15:44:31,868 DEBUG [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 for block blk_4643386764144918409_1009 
waiting for local datanode to finish write.
2010-11-14 15:44:31,868 WARN  [ResponseProcessor for block blk_4643386764144918409_1009] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProcessor exception  for block bl
k_4643386764144918409_1009java.io.IOException: Bad response 1 for block blk_4643386764144918409_1009 from datanode 127.0.0.1:48738
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2542)

2010-11-14 15:44:31,868 WARN  [DataStreamer for file /user/jgray/.logs/dev692.sf2p.facebook.com,58498,1289778232413/10.17.83.207%3A58498.1289778233005 block blk_4643386764144918409_1009] hdfs.DFSClient$
DFSOutputStream(2616): Error Recovery for block blk_4643386764144918409_1009 bad datanode[0] 127.0.0.1:48738
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.BlockReceiver(565): Exception in receiveBlock for block blk_4643386764144918409_1009 java.io.IOExcept
ion: Connection reset by peer
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(844): PacketResponder blk_4643386764144918409_1009 0 : Thread is interrupt
ed.
2010-11-14 15:44:31,869 INFO  [PacketResponder 0 for Block blk_4643386764144918409_1009] datanode.BlockReceiver$PacketResponder(907): PacketResponder 0 for block blk_4643386764144918409_1009 terminating
2010-11-14 15:44:31,869 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@6a25b72a] datanode.DataXceiver(377): writeBlock blk_4643386764144918409_1009 received exception java.io.IOException: Con
nection reset by peer
2010-11-14 15:44:31,869 WARN  [RegionServer:0;dev692.sf2p.facebook.com,58498,1289778232413.logSyncer] hdfs.DFSClient$DFSOutputStream(3293): Error while syncing
java.io.IOException: All datanodes 127.0.0.1:48738 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2666)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1500(DFSClient.java:2157)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2356)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12931931" author="tlipcon" created="Mon, 15 Nov 2010 00:14:38 +0000"  >&lt;p&gt;Does branch append even pass its own unit tests? It looks like the write pipeline is seriously screwed up, I&apos;d be surprised if it did.&lt;/p&gt;</comment>
                            <comment id="12931955" author="stack" created="Mon, 15 Nov 2010 01:49:22 +0000"  >&lt;p&gt;Here is a run against an hadoop that is minus hdfs-724.  One thing I notice is that the good run reports:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-11-15 01:42:02,432 DEBUG [Master:0;pynchon-432.lan:51131] ipc.Client$Connection(469): IPC Client (47) connection to localhost/127.0.0.1:51122 from stack sending #148
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... i.e. user &apos;stack&apos; whereas the bad run has &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-11-14 23:38:57,672 DEBUG [IPC Server handler 3 on 64745] ipc.HBaseClient$Connection(487): IPC Client (47) connection to /172.16.198.219:64748 from an unknown user sending #148
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;.. &apos;unknown user&apos;.&lt;/p&gt;</comment>
                            <comment id="12931956" author="tlipcon" created="Mon, 15 Nov 2010 01:49:31 +0000"  >&lt;p&gt;There seems to be some incompatibility between &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-895&quot; title=&quot;Allow hflush/sync to occur in parallel with new writes to the file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-895&quot;&gt;&lt;del&gt;HDFS-895&lt;/del&gt;&lt;/a&gt;... seeing some similar issues on trunk, working on it.&lt;/p&gt;</comment>
                            <comment id="12931960" author="stack" created="Mon, 15 Nov 2010 02:19:51 +0000"  >&lt;p&gt;Good on you Todd.&lt;/p&gt;

&lt;p&gt;Ignore the user issue mentioned above.&lt;/p&gt;

&lt;p&gt;Lining up a good log and a bad log, here is a good log:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 8173 2010-11-15 01:42:29,781 DEBUG [PRI IPC Server handler 1 on 51098] hdfs.DFSClient$DFSOutputStream(3099): DFSClient writeChunk allocating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; packet seqno=1, src=/user/stack/.lo      gs/pynchon-432.lan,51098,1289785308076/192.168.1.69%3A51098.1289785308941, packetSize=65557, chunksPerPacket=127, bytesCurBlock=0
 8174 2010-11-15 01:42:29,782 DEBUG [RegionServer:0;pynchon-432.lan,51098,1289785308076.logSyncer] hdfs.DFSClient$DFSOutputStream(3170): DFSClient flush() : saveOffset 512 bytesCurB      lock 762 lastFlushOffset 301
 8175 2010-11-15 01:42:29,782 DEBUG [DataStreamer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; file /user/stack/.logs/pynchon-432.lan,51098,1289785308076/192.168.1.69%3A51098.1289785308941 block blk_8612924244108136372_100      7] hdfs.DFSClient$DFSOutputStream$DataStreamer(2376): DataStreamer block blk_8612924244108136372_1007 wrote packet seqno:1 size:795 offsetInBlock:0 lastPacketInBlock:&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
 8176 2010-11-15 01:42:29,782 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@1a83e35b] datanode.BlockReceiver(393): Receiving one packet &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_8612924244108136372      _1007 of length 774 seqno 1 offsetInBlock 0 lastPacketInBlock &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
 8177 2010-11-15 01:42:29,782 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@1a83e35b] datanode.BlockReceiver(635): Changing block file offset of block blk_86129242441081      36372_1007 from 301 to 0 meta file offset to 7
 8178 2010-11-15 01:42:29,783 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@1a83e35b] datanode.BlockReceiver$PacketResponder(736): PacketResponder 0 adding seqno 1 to ac      k queue.
 8179 2010-11-15 01:42:29,783 DEBUG [PacketResponder 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Block blk_8612924244108136372_1007] datanode.BlockReceiver$PacketResponder(806): PacketResponder 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_861292424      4108136372_1007 acking &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; packet 1
 8180 2010-11-15 01:42:29,783 DEBUG [ResponseProcessor &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_8612924244108136372_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2481): DFSClient Replies &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; seqno 1       are SUCCESS
 8181 2010-11-15 01:42:29,783 DEBUG [PRI IPC Server handler 1 on 51098] ipc.HBaseRPC$Server(574): Served: put queueTime= 0 procesingTime= 3
 8182 2010-11-15 01:42:29,783 DEBUG [PRI IPC Server handler 1 on 51098] ipc.HBaseServer$Responder(718): IPC Server Responder: responding to #157 from 192.168.1.69:51194
 8183 2010-11-15 01:42:29,783 DEBUG [PRI IPC Server handler 1 on 51098] ipc.HBaseServer$Responder(737): IPC Server Responder: responding to #157 from 192.168.1.69:51194 Wrote 8 &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;      s.
 8184 2010-11-15 01:42:29,783 DEBUG [IPC Client (47) connection to pynchon-432.lan/192.168.1.69:51098 from an unknown user] ipc.HBaseClient$Connection(524): IPC Client (47) connecti      on to pynchon-432.lan/192.168.1.69:51098 from an unknown user got value #157
 8185 2010-11-15 01:42:29,784 DEBUG [IPC Server handler 4 on 51095] ipc.HBaseRPC$Invoker(261): Call: put 4
 8186 2010-11-15 01:42:29,784 INFO  [IPC Server handler 4 on 51095] catalog.MetaEditor(59): Added region test,,1289785349688.03c89b2db74c8c3215c5fd46429fa2f0. to META
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is bad log&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 8118 2010-11-14 23:38:57,616 DEBUG [PRI IPC Server handler 8 on 64748] hdfs.DFSClient$DFSOutputStream(3154): DFSClient writeChunk allocating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; packet seqno=1, src=/user/stack/.lo      gs/172.16.198.219,64748,1289777896452/172.16.198.219%3A64748.1289777897337, packetSize=65557, chunksPerPacket=127, bytesCurBlock=0
 8119 2010-11-14 23:38:57,617 DEBUG [RegionServer:0;172.16.198.219,64748,1289777896452.logSyncer] hdfs.DFSClient$DFSOutputStream(3238): DFSClient flush() : saveOffset 512 bytesCurBl      ock 761 lastFlushOffset 300
 8120 2010-11-14 23:38:57,617 DEBUG [DataStreamer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; file /user/stack/.logs/172.16.198.219,64748,1289777896452/172.16.198.219%3A64748.1289777897337 block blk_-8256147503844598069_1      007] hdfs.DFSClient$DFSOutputStream$DataStreamer(2429): DataStreamer block blk_-8256147503844598069_1007 wrote packet seqno:1 size:794 offsetInBlock:0 lastPacketInBlock:&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
 8121 2010-11-14 23:38:57,617 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@753bc30a] datanode.BlockReceiver(393): Receiving one packet &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-825614750384459806      9_1007 of length 773 seqno 1 offsetInBlock 0 lastPacketInBlock &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
 8122 2010-11-14 23:38:57,617 DEBUG [org.apache.hadoop.hdfs.server.datanode.DataXceiver@753bc30a] datanode.BlockReceiver$PacketResponder(737): PacketResponder 0 adding seqno 1 to ac      k queue.
 8123 2010-11-14 23:38:57,618 DEBUG [PacketResponder 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Block blk_-8256147503844598069_1007] datanode.BlockReceiver$PacketResponder(891): PacketResponder 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147      503844598069_1007 responded an ack: Replies &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; seqno 1 are SUCCESS
 8124 2010-11-14 23:38:57,618 DEBUG [ResponseProcessor &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147503844598069_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2534): DFSClient Replies &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; seqno 0       are FAILED
 8125 2010-11-14 23:38:57,618 DEBUG [PacketResponder 0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Block blk_-8256147503844598069_1007] datanode.BlockReceiver$PacketResponder(789): PacketResponder 0 seqno = -2 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block b      lk_-8256147503844598069_1007 waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; local datanode to finish write.
 8126 2010-11-14 23:38:57,618 WARN  [ResponseProcessor &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147503844598069_1007] hdfs.DFSClient$DFSOutputStream$ResponseProcessor(2580): DFSOutputStream ResponseProce      ssor exception  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147503844598069_1007java.io.IOException: Bad response 1 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147503844598069_1007 from datanode 127.0.0.1:64740
 8127     at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2542)
 8128 
 8129 2010-11-14 23:38:57,619 WARN  [DataStreamer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; file /user/stack/.logs/172.16.198.219,64748,1289777896452/172.16.198.219%3A64748.1289777897337 block blk_-8256147503844598069_1      007] hdfs.DFSClient$DFSOutputStream(2616): Error Recovery &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147503844598069_1007 bad datanode[0] 127.0.0.1:64740
 8130 2010-11-14 23:38:57,619 INFO  [org.apache.hadoop.hdfs.server.datanode.DataXceiver@753bc30a] datanode.BlockReceiver(565): Exception in receiveBlock &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_-8256147503844      598069_1007 java.io.EOFException: &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; trying to read 794 bytes
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;In the bad, its writing 794 bytes but in good its writing 795 bytes.&lt;/p&gt;

&lt;p&gt;The good does a &apos; Changing block file offset of block blk_8612924244108136372_1007 from 301 to 0 meta file offset to 7&apos; before it does   PacketResponder 0 adding seqno 1 to ack queue.&apos;  and then getting an ack to report SUCCESS.   The &apos;bad&apos; doesn&apos;t do this its size seem to be smaller by one byte and after acking seqno 1, it then goes on to look for ack on seqno 0 and reports FAILED.&lt;/p&gt;

&lt;p&gt;I don&apos;t know what this means (smile).  Seems significant. &lt;/p&gt;</comment>
                            <comment id="12931964" author="ryanobjc" created="Mon, 15 Nov 2010 03:08:32 +0000"  >&lt;p&gt;I got TestHBaseTestingUtility to pass on a variant of branch-20-append w/o 724 and WITH 895. &lt;/p&gt;

&lt;p&gt;It is published here:&lt;br/&gt;
&lt;a href=&quot;https://github.com/ryanobjc/hadoop-common/tree/branch-0.20-append&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/ryanobjc/hadoop-common/tree/branch-0.20-append&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I also pushed the jars to maven repo @ people.apache.org/~rawson/repo&lt;/p&gt;

&lt;p&gt;and I committed changes to pom.xml that depend on these new versions.  &lt;/p&gt;

&lt;p&gt;So we&apos;ll see what hudson thinks.&lt;/p&gt;</comment>
                            <comment id="12931989" author="ghelmling" created="Mon, 15 Nov 2010 05:48:41 +0000"  >&lt;p&gt;@Stack: For &quot;stack&quot; vs. &quot;unknown user&quot;, that&apos;s probably not related...&lt;/p&gt;

&lt;p&gt;The first is from a Hadoop RPC call (ipc.Client), where the second is from an HBase RPC call (ipc.HBaseClient).  HBase RPC &lt;em&gt;always&lt;/em&gt; passes &quot;null&quot; for the user in the RPC header &amp;#8211; it&apos;s hard coded in HBaseClient.  We&apos;ve changed this in the secure HBase branch, of course, but it&apos;s always been this way in the mainline.  Master and RS should be using the process username for all DFS client interactions.&lt;/p&gt;</comment>
                            <comment id="12932019" author="stack" created="Mon, 15 Nov 2010 09:26:49 +0000"  >&lt;p&gt;OK.  Chatting w/ Todd and Ryan, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; is not in CDH.  Removing &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt;, all hbase tests pass (Both for me and Ryan).  &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; looks like critical fix but I&apos;m going to go ahead and RC without it.  There seems to be something up w/ the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; that is in the append branch.  While we&apos;re figuring it out, our first 0.90.0 RC can get an airing (Anyone can sink the RC if they disagree w/ above).   My guess is that there&apos;ll at least be an RC2 and we can get any fix in then.  &lt;/p&gt;

&lt;p&gt;I made an hadoop jar off the tip of branch-0.20-append that includes hdfs-895 but removes hdfs-724, signed it and put it up on a hand-built repo up on people.apache.org.  The jar is named 0.20.3-append-r1034938-plusHDFS895-minusHDFS724.&lt;/p&gt;

&lt;p&gt;(@Gary &amp;#8211; thanks for the stack vs unknown user tidbit... I kinda picked up on it later but your clarification helped)&lt;/p&gt;</comment>
                            <comment id="12932055" author="stack" created="Mon, 15 Nov 2010 13:41:08 +0000"  >&lt;p&gt;Oh, just to say that Todd thinks it might be interaction between hdfs-724 and hdfs-895.  He was going to try and look into it.&lt;/p&gt;</comment>
                            <comment id="12932088" author="tlipcon" created="Mon, 15 Nov 2010 15:46:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;Oh, just to say that Todd thinks it might be interaction between hdfs-724 and hdfs-895. He was going to try and look into it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I found a bug on trunk &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-895&quot; title=&quot;Allow hflush/sync to occur in parallel with new writes to the file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-895&quot;&gt;&lt;del&gt;HDFS-895&lt;/del&gt;&lt;/a&gt; that was due to an issue interacting with &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt;, but according to people working on 20-append, this bug is present even with &lt;b&gt;just&lt;/b&gt; 724 without 895. So I think it&apos;s a bad backport.&lt;/p&gt;</comment>
                            <comment id="12932166" author="jdcryans" created="Mon, 15 Nov 2010 19:43:52 +0000"  >&lt;p&gt;So in our 0.90.0 RC email, we say that people can use the 0.20-append branch... but won&apos;t it be incompatible since it contains &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; which bumps the data transfer protocol?&lt;/p&gt;</comment>
                            <comment id="12932188" author="tlipcon" created="Mon, 15 Nov 2010 20:45:13 +0000"  >&lt;p&gt;Ah, yes... but do you know of anyone who actually has run a cluster on 20-append? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Probably not worth rebuilding RC, we can just tell people &quot;just kidding&quot; for now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12932190" author="jdcryans" created="Mon, 15 Nov 2010 20:47:53 +0000"  >&lt;p&gt;Thanks Todd, just wanted to confirm my understanding of the situation.&lt;/p&gt;</comment>
                            <comment id="12932257" author="hairong" created="Mon, 15 Nov 2010 23:28:18 +0000"  >&lt;p&gt;I found out the problem. Looks that &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; missed a piece of code. PipelineAck#readFields does not serialize the Heartbeat ack correctly. I just checked our internal branch and I did correctly there. That&apos;s why our internal testing did not show this error.&lt;/p&gt;</comment>
                            <comment id="12932259" author="stack" created="Mon, 15 Nov 2010 23:35:43 +0000"  >&lt;p&gt;@Hairong Excellent.  If you post a patch over in hdfs-724, I can try it for you in hbase.  What about the bump in the transfer protocol version J-D identifies above?  Could the hdfs-724 for 0.20-append branch not bumped the protocol?  What you think?&lt;/p&gt;</comment>
                            <comment id="12932261" author="hairong" created="Mon, 15 Nov 2010 23:36:29 +0000"  >&lt;p&gt;I attached a patch that fixes the problem: &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12459664/hbAckReply.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12459664/hbAckReply.patch&lt;/a&gt;. Could anybody give it a try? If it fixes the problem, I will commit it to append 0.20.&lt;/p&gt;</comment>
                            <comment id="12932268" author="hairong" created="Mon, 15 Nov 2010 23:59:25 +0000"  >&lt;p&gt;For the version # bump problem, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-724&quot; title=&quot;Pipeline close hangs if one of the datanode is not responsive.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-724&quot;&gt;&lt;del&gt;HDFS-724&lt;/del&gt;&lt;/a&gt; is indeed an incompatible change. For append 0.20, it changes the semantics/syntax of heartbeat packets.&lt;/p&gt;</comment>
                            <comment id="12932273" author="stack" created="Tue, 16 Nov 2010 00:04:46 +0000"  >&lt;p&gt;@Hairong OK. I don&apos;t think it&apos;ll be end of the world asking fellas to restart their clusters going between versions of the hadoop jar pre-724 and post-724.&lt;/p&gt;</comment>
                            <comment id="12932300" author="stack" created="Tue, 16 Nov 2010 01:10:12 +0000"  >&lt;p&gt;Tests are still running (its looking good).  Not done yet.  Will report back later.  J-D and Todd, you Ok w/ version number going up when we have hdfs-724 in the hbase hacked hadoop jar again?&lt;/p&gt;</comment>
                            <comment id="12932315" author="jdcryans" created="Tue, 16 Nov 2010 02:18:37 +0000"  >&lt;p&gt;My only concern was that if we back out 724, then people won&apos;t be able to run on the tip of 0.20-append since the protocol won&apos;t be the same.&lt;/p&gt;</comment>
                            <comment id="12932979" author="stack" created="Wed, 17 Nov 2010 15:11:22 +0000"  >&lt;p&gt;Add this fix to second 0.90.0 release candidate (by updating bundled hadoop now it has fixed 724 and 895)&lt;/p&gt;</comment>
                            <comment id="12965093" author="stack" created="Tue, 30 Nov 2010 05:22:20 +0000"  >&lt;p&gt;Here is some pom fixup to point at a new hadoop made from tip of branch-0.20-append today.  Ryan added the signed and md5&apos;d jars to his personal repo.  I tested it.  At least the test that used fail now passes with Hairong&apos;s fixup on the hdfs-724 backport.&lt;/p&gt;</comment>
                            <comment id="12965094" author="stack" created="Tue, 30 Nov 2010 05:24:55 +0000"  >&lt;p&gt;Applied to branch and trunk.  Closing.&lt;/p&gt;</comment>
                            <comment id="15017413" author="lars_francke" created="Fri, 20 Nov 2015 12:43:02 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12464937" name="3234.txt" size="1079" author="stack" created="Tue, 30 Nov 2010 05:22:20 +0000"/>
                            <attachment id="12459577" name="org.apache.hadoop.hbase.TestHBaseTestingUtility-output.txt" size="2645342" author="stack" created="Mon, 15 Nov 2010 01:49:21 +0000"/>
                            <attachment id="12459554" name="org.apache.hadoop.hbase.TestHBaseTestingUtility-output.txt" size="2600727" author="stack" created="Sun, 14 Nov 2010 13:05:31 +0000"/>
                            <attachment id="12459555" name="org.apache.hadoop.hbase.TestHBaseTestingUtility.txt" size="3102" author="stack" created="Sun, 14 Nov 2010 13:05:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 14 Nov 2010 23:11:32 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26728</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hlcf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100720</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>