<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:52:50 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1357/HBASE-1357.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1357] If one sets the hbase.master to 0.0.0.0 non local regionservers can&apos;t find the master</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1357</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;(2:11:20 PM) posix4e: so i want to run a back master on each node&lt;br/&gt;
(2:11:29 PM) posix4e: and i have my hbase.master set to 0.0.0.0&lt;br/&gt;
(2:14:59 PM) posix4e: each master only gets the local regionserver connecting&lt;br/&gt;
(2:15:08 PM) posix4e: as it must be using that variable to know what to connect to&lt;br/&gt;
(2:15:32 PM) nitay: the RS don&apos;t use hbase.master* anymore&lt;br/&gt;
(2:15:36 PM) nitay: ohhh i think i know th eproblem&lt;br/&gt;
(2:15:44 PM) nitay: so the RS use ZK to get the master address&lt;br/&gt;
(2:15:49 PM) nitay: but the masters are writing 0.0.0.0 to it&lt;br/&gt;
(2:15:58 PM) nitay: b/c they write whatever was in their conf&lt;br/&gt;
(2:16:20 PM) posix4e: yea&lt;br/&gt;
(2:16:42 PM) nitay: can u do a zookeeper dump of that node to verify my thinking?&lt;br/&gt;
(2:16:55 PM) posix4e: yea&lt;br/&gt;
(2:17:12 PM) nitay: it should be /hbase/master, unless u&apos;ve changed the defaults&lt;br/&gt;
(2:17:59 PM) nitay: hmm s o ye this is a problem, we solved this in RS (allowing 0.0.0.0) by having master actually write RS&apos;s address to ZK when it gets contacted&lt;br/&gt;
(2:18:21 PM) nitay: so now we need to find a way to find out the &lt;em&gt;actual&lt;/em&gt; address the master has bound to&lt;br/&gt;
(2:19:47 PM) posix4e: is their a way to do that?&lt;br/&gt;
(2:20:16 PM) nitay: i dont know, good question&lt;br/&gt;
(2:20:18 PM) posix4e: or does it require code changes i.e. regionserver checking zk&lt;br/&gt;
(2:20:27 PM) nitay: did u verify the master address?&lt;br/&gt;
(2:20:48 PM) posix4e: one sec&lt;br/&gt;
(2:21:03 PM) nitay: its almost like we want ZK to be able to tell us what address we&apos;re using to talk to it&lt;br/&gt;
(2:21:20 PM) nitay: that assumes u dont have different NICs to talk to ZK vs. HBase&lt;br/&gt;
(2:21:59 PM) nitay: posix4e, u can&apos;t really use the RS as far as i can tell b/c the RS knows nothing about the master until the master address appears in ZK&lt;br/&gt;
(2:22:25 PM) posix4e: 0:0:0:0:0:0:0:0:60000&lt;br/&gt;
(2:22:40 PM) nitay: yep that&apos;s the magic&lt;br/&gt;
(2:22:45 PM) nitay: k thx for verifying&lt;br/&gt;
(2:22:54 PM) nitay: u want to open up a JIRA?&lt;br/&gt;
(2:22:57 PM) posix4e: but if i could tell hbase.site to just use my hostname:port it would work ok&lt;br/&gt;
(2:22:58 PM) posix4e: yea&lt;br/&gt;
(2:23:09 PM) posix4e: can i quote this conversation?&lt;br/&gt;
(2:23:18 PM) nitay: yes please do&lt;br/&gt;
(2:23:45 PM) nitay: also, to fix this here and now for u, u&apos;d essentially need to actually set hbase.master* to the ip/host u&apos;re using&lt;br/&gt;
(2:23:55 PM) nitay: and change it on each backup master to that guy&apos;s host/ip&lt;br/&gt;
(2:24:02 PM) nitay: i know, its a royal PITA&lt;br/&gt;
(2:24:59 PM) posix4e: yea&lt;br/&gt;
(2:25:03 PM) posix4e: no problem&lt;br/&gt;
(2:25:20 PM) nitay: but that should work till we find a better solution&lt;br/&gt;
(2:25:21 PM) posix4e: I am trying to think how a patch would work&lt;br/&gt;
(2:25:25 PM) posix4e: have a masters file?&lt;br/&gt;
(2:25:44 PM) nitay: yeah if u have any ideas please offer them&lt;br/&gt;
(2:25:46 PM) nitay: hmm interesting idea&lt;br/&gt;
(2:26:16 PM) nitay: and then do some local gethostbyname() type thing checking against masters file?&lt;br/&gt;
(2:26:26 PM) posix4e: yea&lt;br/&gt;
(2:28:23 PM) nitay: one thing to note is we&apos;ve talked about eventually getting to a place where any RS can be master&lt;br/&gt;
(2:28:30 PM) nitay: but i like your idea&lt;br/&gt;
(2:28:37 PM) nitay: post it on the JIRA&lt;br/&gt;
(2:30:24 PM) nitay: i gotta run, thanks for the info posix4e - very helpful, its great to hear from people actually using this stuff&lt;br/&gt;
(2:32:56 PM) posix4e: yep&lt;/p&gt;

&lt;p&gt;I also solved this by manually setting the hbase.master  on each host to point to the local hostname, which sucks.&lt;/p&gt;</description>
                <environment>&lt;p&gt;All&lt;/p&gt;</environment>
        <key id="12424199">HBASE-1357</key>
            <summary>If one sets the hbase.master to 0.0.0.0 non local regionservers can&apos;t find the master</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jdcryans">Jean-Daniel Cryans</assignee>
                                    <reporter username="posix4e">Alex Newman</reporter>
                        <labels>
                    </labels>
                <created>Wed, 29 Apr 2009 18:38:29 +0000</created>
                <updated>Thu, 2 May 2013 02:28:56 +0000</updated>
                            <resolved>Tue, 2 Jun 2009 17:54:25 +0000</resolved>
                                    <version>0.20.0</version>
                    <version>0.20.1</version>
                    <version>0.90.0</version>
                                    <fixVersion>0.20.0</fixVersion>
                                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12704849" author="nitay" created="Thu, 30 Apr 2009 23:41:30 +0000"  >&lt;p&gt;Moving out of 0.20 for now. In meeting we discussed:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Getting rid of hbase.master.hostname property completely. Detect master address using an inet address detection scheme similar to what Hadoop does.&lt;/li&gt;
	&lt;li&gt;The above breaks the default ZooKeeper case of a single server running on the master node. RegionServers currently find this ZooKeeper using the master address property. This default case should be replaced by having the zoo.cfg turn into a template file which can be created and rsync&apos;ed to RegionServer nodes by e.g. start-hbase.sh script.&lt;/li&gt;
	&lt;li&gt;Add a masters file, similar to regionservers, with pool of servers for master election.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12711840" author="stack" created="Thu, 21 May 2009 22:04:25 +0000"  >&lt;p&gt;Boys chatting on #hbase today figure we should do this &amp;#8211; just remove master address and do lookup of host and write that to ZK.  Bringing it back into 0.20.0.  Nitay says assign it to him.&lt;/p&gt;</comment>
                            <comment id="12711845" author="posix4e" created="Thu, 21 May 2009 22:27:46 +0000"  >&lt;p&gt;We use the same script which restarts all of the zookeeper , thrift ,&lt;br/&gt;
master , region and rsync nodes to rewrite the hbase master address.&lt;br/&gt;
It&apos;s hacky but it works. The problem is the zookeeper node. I think&lt;br/&gt;
the way you do that in zookeeper is with a sequence and a watch (I&lt;br/&gt;
think). I would code this up but I am in a car on my handy. If no one&lt;br/&gt;
has it fixed by Tuesday I will talk to Nitay about it.&lt;/p&gt;


&lt;p&gt;&amp;#8211; &lt;br/&gt;
Sent from my mobile device&lt;/p&gt;</comment>
                            <comment id="12711857" author="stack" created="Thu, 21 May 2009 23:22:10 +0000"  >&lt;p&gt;Chatting w/ Nitay, he recalled that reason this issue was punted to 0.21 was because that in distributed hbase &amp;#8211; not standalone nor pseudo-distributed &amp;#8211; then clients and regionservers need to know where the zookeeper quorum is.  This means edit of zoo.cfg WHEN YOU WANT TO RUN IN DISTRIBUTED MODE.  Chatting w/ Nitay, I thought we could continue hiding ZK from noobs by doing something like adding a new property in hbase-site.xml named zookeeper.quorum and in it we&apos;d list all quorum members and then in background we&apos;d write the zoo.cfg but Nitay just raised his eyebrow when i suggested this.&lt;/p&gt;

&lt;p&gt;So, I&apos;m with him now.  Lets not beat around the bush.  When doing distributed mode, then you need to edit the ZK config.  Will reinforce that ZK is cluster mediator.  I think its fine.  The two basic modes out of the box will just work w/o zoo.cfg edits.  What you think J-D?&lt;/p&gt;</comment>
                            <comment id="12712111" author="jdcryans" created="Fri, 22 May 2009 14:34:19 +0000"  >&lt;p&gt;Ok let&apos;s do it like that. One thing that we should do tho is to have a more detailed &quot;How to run distributed mode&quot; documentation.&lt;/p&gt;</comment>
                            <comment id="12713186" author="jdcryans" created="Tue, 26 May 2009 18:44:55 +0000"  >&lt;p&gt;Nitay agreed to let me work on the problem since it&apos;s related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1445&quot; title=&quot;Add the ability to start a master from any machine&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1445&quot;&gt;&lt;del&gt;HBASE-1445&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="12713196" author="jdcryans" created="Tue, 26 May 2009 19:10:20 +0000"  >&lt;p&gt;Chatting with Nitay, we figured it would be best to just have a &quot;cluster mode&quot; configuration that tells HBase whether it&apos;s local or distributed instead of the master address since it won&apos;t be needed anymore.&lt;/p&gt;</comment>
                            <comment id="12714019" author="jdcryans" created="Thu, 28 May 2009 15:45:19 +0000"  >&lt;p&gt;First rough cut. There is no documentation tho the Getting Started will have to change a bit.&lt;/p&gt;

&lt;p&gt;I removed hbase.master.hostname and added a cluster mode. ZK complains and exits if mode is distributed but it&apos;s still managed by HBase like this:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2009-05-28 11:34:58,288 FATAL org.apache.hadoop.hbase.zookeeper.HQuorumPeer: Zookeeper should be managed only in a local cluster mode. Please edit conf/zoo.cfg and remove hbase.cluster.mode to set your ZK quorum addresses&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It passes test tho, and I tried PE in standalone, pseudo and fully distributed. All works.&lt;/p&gt;</comment>
                            <comment id="12714779" author="stack" created="Sat, 30 May 2009 22:01:39 +0000"  >&lt;p&gt;The choice of configuration name and setting is important since it&apos;ll be one of the first things presented a user.&lt;/p&gt;

&lt;p&gt;hbase.cluster.mode with &apos;local&apos; | &apos;distributed&apos; values seems a little awkward to me (and error-prone).&lt;/p&gt;

&lt;p&gt;What about hbase.cluster.distributed with a yes/no or true/false answer (I think Configuration will do right thing whatever you provide).&lt;/p&gt;

&lt;p&gt;If &apos;false&apos;, then hbase cluster is not distributed across a cluster of machines &amp;#8211; its standalone or pseudo-distributed.&lt;/p&gt;

&lt;p&gt;The description in hbase-default needs to be a little clearer.  Also, I&apos;m not too clear on what happens.  If distributed, it means user needs to edit zoo.cfg to point at quorum?  Where is the switch for whether or not hbase starts up the quorum?&lt;/p&gt;

&lt;p&gt;Otherwise, patch looks great.  I like --master option to PE.  I&apos;d also remove the commented out code in HConstants.&lt;/p&gt;


</comment>
                            <comment id="12715155" author="jdcryans" created="Mon, 1 Jun 2009 17:23:23 +0000"  >&lt;p&gt;In this patch I changed hbase.cluster.mode to hbase.cluster.distributed with options false|true. I also added new documentation in overview.html to explain the new steps for the fully-distributed setup.&lt;/p&gt;</comment>
                            <comment id="12715226" author="jdcryans" created="Mon, 1 Jun 2009 20:01:11 +0000"  >&lt;p&gt;Patch adds some clarification in the doc, adds hbase.master.dns.nameserver and hbase.master.dns.interface to specify which interface the master should use to communicate with which DNS (just like in hadoop).&lt;/p&gt;</comment>
                            <comment id="12715276" author="stack" created="Mon, 1 Jun 2009 21:39:37 +0000"  >&lt;p&gt;I still get this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-06-01 21:29:19,071 ERROR org.apache.hadoop.hbase.master.HMaster: Can not start master
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1090)
    at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1127)
Caused by: java.io.IOException: Could not read quorum servers from zoo.cfg
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.&amp;lt;init&amp;gt;(ZooKeeperWrapper.java:90)
    at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.&amp;lt;init&amp;gt;(ZooKeeperWrapper.java:78)
    at org.apache.hadoop.hbase.master.HMaster.&amp;lt;init&amp;gt;(HMaster.java:245)
    ... 6 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Its because I still have this in my zoo.cfg:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
server.0=${hbase.cluster.distributed}:2888:3888
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Its odd having a variable whose setttings are true/false in this position.&lt;/p&gt;

&lt;p&gt;It should be &apos;hbase.hostname&apos;, something we calculate for you when in local/pseudo mode putting in place hostname unless its overridden.&lt;/p&gt;

&lt;p&gt;Exception needs to be better.&lt;/p&gt;

&lt;p&gt;We can&apos;t put localhost here?   If hbase.cluster.distributed is true and this zoo.cfg is localhost then throw exception?  Would that work?&lt;/p&gt;</comment>
                            <comment id="12715553" author="jdcryans" created="Tue, 2 Jun 2009 15:29:33 +0000"  >&lt;p&gt;This latest version of the patch clears up hbase.cluster.distributed from zoo.cfg like Stack described. This will be thrown when the hbase.cluster.distributed is true and the value in zoo.cfg is localhost:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;localhost: starting zookeeper, logging to /home/jdcryans/svn/hbase/trunk/bin/../logs/hbase-jdcryans-zookeeper-jdcryans.mtl.out&lt;br/&gt;
localhost: java.io.IOException: The server in zoo.cfg cannot be set to localhost in a fully-distributed setup because it won&apos;t be reachable. See &quot;Getting Started&quot; for more information.&lt;br/&gt;
localhost: 	at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.parseConfig(HQuorumPeer.java:141)&lt;br/&gt;
localhost: 	at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.parseZooKeeperConfig(HQuorumPeer.java:82)&lt;br/&gt;
localhost: 	at org.apache.hadoop.hbase.zookeeper.HQuorumPeer.main(HQuorumPeer.java:58)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I tested master failover on 3 nodes doing hbase-daemons.sh start master then regionserver (which is kinda fun) and killed the first, then the second master. What I first saw is all regions getting reassigned (which was supposed to be fix) but this was because of the alls well messages:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2009-06-02 11:02:19,941 DEBUG org.apache.hadoop.hbase.master.HMaster: Started service threads&lt;br/&gt;
2009-06-02 11:02:19,942 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 9 on 62000: starting&lt;br/&gt;
2009-06-02 11:02:19,956 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {server}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; complete&lt;br/&gt;
2009-06-02 11:02:20,144 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.metaScanner scan of 54 row(s) of meta region &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {server}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; complete&lt;br/&gt;
2009-06-02 11:02:20,939 DEBUG org.apache.hadoop.hbase.master.ServerManager: Process all wells: address: 192.168.1.88:62020, startcode: 1243954293421, load: (requests=4, regions=19, usedHeap=27, maxHeap=963) openingCount: 0, nobalancingCount: 4&lt;br/&gt;
2009-06-02 11:02:20,945 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Wrote out of safe mode&lt;br/&gt;
2009-06-02 11:02:20,945 INFO org.apache.hadoop.hbase.master.RegionManager: exiting safe mode&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server is overloaded. Server load: 19 avg: 6.333333333333333, slop: 0.1&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 12 regions. mostLoadedRegions has 10 regions in it.&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0003047546,1242765850701&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0011562998,1241636821854&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0006799914,1242765888384&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0006349363,1242765888384&lt;br/&gt;
2009-06-02 11:02:20,954 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0001399792,1242840206758&lt;br/&gt;
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0000072704,1242764140942&lt;br/&gt;
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0008901015,1242765794486&lt;br/&gt;
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0002902745,1242765697441&lt;br/&gt;
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0000283600,1242764792449&lt;br/&gt;
2009-06-02 11:02:20,955 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region TestTable,0000612479,1242394901854&lt;br/&gt;
2009-06-02 11:02:20,955 INFO org.apache.hadoop.hbase.master.RegionManager: Skipped 0 region(s) that are in transition states&lt;br/&gt;
2009-06-02 11:02:21,164 DEBUG org.apache.hadoop.hbase.master.ServerManager: Process all wells: address: 192.168.1.87:62020, startcode: 1243954293575, load: (requests=57, regions=19, usedHeap=26, maxHeap=963) openingCount: 0, nobalancingCount: 4&lt;br/&gt;
2009-06-02 11:02:21,165 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server is overloaded. Server load: 19 avg: 12.666666666666666, slop: 0.1&lt;br/&gt;
2009-06-02 11:02:21,165 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 6 regions. mostLoadedRegions has 10 regions in it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s because the load is empty when adding a new region server so I added a check to instead use the load provided by the RS during the failover inspection. So it&apos;s fixed.&lt;/p&gt;</comment>
                            <comment id="12715582" author="stack" created="Tue, 2 Jun 2009 16:55:24 +0000"  >&lt;p&gt;+1.  Commit it.&lt;/p&gt;

&lt;p&gt;I tested it.  Makes sense.  Doc. will need rework but lets get all the other bits in there first before we do that.&lt;/p&gt;</comment>
                            <comment id="12715606" author="jdcryans" created="Tue, 2 Jun 2009 17:54:25 +0000"  >&lt;p&gt;Committed to trunk.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12426065">HBASE-1445</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="12426380">HBASE-1448</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12409276" name="hbase-1357-v1.patch" size="8196" author="jdcryans" created="Thu, 28 May 2009 15:45:19 +0000"/>
                            <attachment id="12409573" name="hbase-1357-v2.patch" size="12323" author="jdcryans" created="Mon, 1 Jun 2009 17:23:23 +0000"/>
                            <attachment id="12409590" name="hbase-1357-v3.patch" size="14804" author="jdcryans" created="Mon, 1 Jun 2009 20:01:10 +0000"/>
                            <attachment id="12409674" name="hbase-1357-v4.patch" size="18532" author="jdcryans" created="Tue, 2 Jun 2009 15:29:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 30 Apr 2009 23:41:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25712</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10342"><![CDATA[Incompatible change]]></customfieldvalue>
    <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 29 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hcrr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99331</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>hbase.master and hbase.master.hostname are now obsolete. hbase.cluster.distributed must be set at &amp;quot;true&amp;quot; to have a fully-distributed setup along with at least one configured ZK server which is not pointing at localhost.&lt;br/&gt;
Also, zoo.cfg must be in the classpath of every client.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>