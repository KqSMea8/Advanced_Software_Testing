<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:18:16 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-4365/HBASE-4365.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-4365] Add a decent heuristic for region size</title>
                <link>https://issues.apache.org/jira/browse/HBASE-4365</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;A few of us were brainstorming this morning about what the default region size should be. There were a few general points made:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;in some ways it&apos;s better to be too-large than too-small, since you can always split a table further, but you can&apos;t merge regions currently&lt;/li&gt;
	&lt;li&gt;with HFile v2 and multithreaded compactions there are fewer reasons to avoid very-large regions (10GB+)&lt;/li&gt;
	&lt;li&gt;for small tables you may want a small region size just so you can distribute load better across a cluster&lt;/li&gt;
	&lt;li&gt;for big tables, multi-GB is probably best&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12522490">HBASE-4365</key>
            <summary>Add a decent heuristic for region size</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="tlipcon">Todd Lipcon</reporter>
                        <labels>
                            <label>usability</label>
                    </labels>
                <created>Fri, 9 Sep 2011 23:56:56 +0000</created>
                <updated>Fri, 19 Oct 2012 19:01:30 +0000</updated>
                            <resolved>Fri, 24 Feb 2012 06:43:02 +0000</resolved>
                                    <version>0.92.1</version>
                    <version>0.94.0</version>
                                    <fixVersion>0.94.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                            <comments>
                            <comment id="13101668" author="tlipcon" created="Sat, 10 Sep 2011 00:06:31 +0000"  >&lt;p&gt;One idea that I&apos;d like to propose is the following. We would introduce a special value for max region size (eg 0 or -1) which configures HBase to use a heuristic instead of a constant. The heuristic would be:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;on region-open (and periodically, perhaps on flush or compaction) the RS scans META to find how many regions are in the table. Let&apos;s call this &lt;em&gt;R&lt;/em&gt;.&lt;/li&gt;
	&lt;li&gt;the region server also periodically scans ZK or asks the master to determine how many RS are in the cluster. Let&apos;s call this &lt;em&gt;S&lt;/em&gt;.&lt;/li&gt;
	&lt;li&gt;If &lt;em&gt;R&lt;/em&gt; &amp;lt; &lt;em&gt;S&lt;/em&gt; (ie there are fewer regions than servers), we use a max region size of perhaps 256MB. This is so that early in a table&apos;s lifetime, even if not pre-split, the table will split rapidly and be able to spread load across the cluster.&lt;/li&gt;
	&lt;li&gt;If &lt;em&gt;R&lt;/em&gt; &amp;lt; &lt;em&gt;5S&lt;/em&gt;, we use a max region size of perhaps 1GB.&lt;/li&gt;
	&lt;li&gt;If &lt;em&gt;R&lt;/em&gt; &amp;gt; &lt;em&gt;5S&lt;/em&gt;, we use a max region size of something rather large, like 10GB.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As an example, if we started putting a 1T table on a cluster with 20 nodes, we&apos;d quickly expand to 20 regions of 256M. The size would switch to 1G, and we&apos;d get another ~100 regions. The size would switch to 10G and we&apos;d get another 100 regions. So at the end, we&apos;d have around 200-250 regions, or about 10 per server.&lt;/p&gt;

&lt;p&gt;The specific numbers above aren&apos;t well thought out, but does this seem to be more &quot;foolproof&quot; than any predetermined default.&lt;/p&gt;</comment>
                            <comment id="13101716" author="yuzhihong@gmail.com" created="Sat, 10 Sep 2011 01:11:02 +0000"  >&lt;p&gt;There is some assumption for the above discussion.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;for small tables you may want a small region size just so you can distribute load better across a cluster&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Load balancer currently doesn&apos;t balance the regions for any single table. We should introduce a policy that does this.&lt;/p&gt;

&lt;p&gt;It seems that the proposal favors not pre-splitting tables. If so, we need some solid performance results to back the proposal.&lt;/p&gt;</comment>
                            <comment id="13101996" author="tlipcon" created="Sat, 10 Sep 2011 06:48:00 +0000"  >&lt;blockquote&gt;&lt;p&gt;Load balancer currently doesn&apos;t balance the regions for any single table. We should introduce a policy that does this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That seems orthogonal, to me. If you have a single table in the cluster, then you need at least as many regions as servers to make use of all of your servers.&lt;/p&gt;

&lt;p&gt;If you have many tables, then yes, a per-table balancing might be useful (in some cases), but it&apos;s the case regardless of whether we have a split size heuristic or manually set region size.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It seems that the proposal favors not pre-splitting tables. If so, we need some solid performance results to back the proposal.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Howso? I&apos;m suggesting that we retain the MAX_REGION_SIZE parameter, if you want to manually set it to some value, or set it to MAX_LONG and manually split. But, the default would be this heuristic, which would work well for many use cases.&lt;/p&gt;</comment>
                            <comment id="13102022" author="yuzhihong@gmail.com" created="Sat, 10 Sep 2011 09:33:47 +0000"  >&lt;p&gt;I understand the proposal provides better heuristic for determining region size.&lt;/p&gt;

&lt;p&gt;My comment about load balancer was assuming there&apos;re many tables in the cluster.&lt;/p&gt;

&lt;p&gt;My second comment originated from our practice of pre-splitting tables. It is possible that &lt;em&gt;R&lt;/em&gt; == &lt;em&gt;5S&lt;/em&gt; would be reached soon after the creation of the table for small-medium sized cluster.&lt;/p&gt;</comment>
                            <comment id="13102878" author="tlipcon" created="Mon, 12 Sep 2011 18:17:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;My comment about load balancer was assuming there&apos;re many tables in the cluster.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I guess I&apos;m confused how this is related to the region size heuristic. This is a general LB concern, but shouldn&apos;t be worse/better due to this heuristic, right?&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;My second comment originated from our practice of pre-splitting tables. It is possible that R == 5S would be reached soon after the creation of the table for small-medium sized cluster.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In that case, isn&apos;t it a good thing that we&apos;d automatically set the region size to be fairly large? ie if you&apos;ve pre-split to &lt;em&gt;5S&lt;/em&gt; regions, then you probably don&apos;t want it to keeps splitting faster on you.&lt;/p&gt;</comment>
                            <comment id="13103073" author="yuzhihong@gmail.com" created="Mon, 12 Sep 2011 21:26:21 +0000"  >&lt;p&gt;For pre-splitting tables, consider this scenario:&lt;br/&gt;
The pre-split regions didn&apos;t represent the actual distribution of row keys for the underlying table. Meaning, relatively low number of regions receive the writes initially.&lt;br/&gt;
Maybe splitting these regions relatively fast would achieve better performance.&lt;/p&gt;</comment>
                            <comment id="13103076" author="tlipcon" created="Mon, 12 Sep 2011 21:30:48 +0000"  >&lt;p&gt;Hmm, that would suggest a heuristic based not on number of regions, but based on total table size. However, it seems like a bit of an edge case.&lt;/p&gt;

&lt;p&gt;Perhaps we can make this a pluggable policy like so: allow max region size to be either a class name or an integer. If it&apos;s a class name, it refers to an implementation of some interface like &lt;tt&gt;MaxRegionSizeCalculator&lt;/tt&gt;. If it&apos;s an integer, it acts the same as today (fixed size). Then we could easily experiment with different heuristics.&lt;/p&gt;</comment>
                            <comment id="13103089" author="yuzhihong@gmail.com" created="Mon, 12 Sep 2011 21:42:54 +0000"  >&lt;p&gt;+1 on Todd&apos;s idea.&lt;br/&gt;
Minor comment: the pluggable policy would always represent a class name. We can devise a default policy, say ConstantMaxRegionSizePolicy, which returns the value of hbase.hregion.max.filesize&lt;/p&gt;</comment>
                            <comment id="13209220" author="stack" created="Thu, 16 Feb 2012 08:25:17 +0000"  >&lt;p&gt;Here is a first cut.&lt;/p&gt;

&lt;p&gt;It does not do the lookup of regions in a table across the cluster nor query zk to find out how many nodes are in the mix.  Its kinda hard to do this from a RegionSplitPolicy context.&lt;/p&gt;

&lt;p&gt;Instead, we count the number of regions that belong to a table on the current regionserver.  We then multiply the flushsize by this number and thats when we&apos;ll split.  If the multiplication produces a number &amp;gt; max filesize for a region, we&apos;ll take maxfilesize.&lt;/p&gt;

&lt;p&gt;If 1 region for a given table on a regionserver, we&apos;ll split on the first flush.&lt;/p&gt;

&lt;p&gt;If 5 regions from same table on a regionserver, we&apos;ll split at 5 * 128M and so on.&lt;/p&gt;

&lt;p&gt;We could have the size grow more aggressively by squaring the count of regions; that might make sense if cluster has lots of small tables &amp;#8211; in fact it might be better altogether.  What you all think?&lt;/p&gt;

&lt;p&gt;If agreeable, I should  make a new patch that makes this the default splitting policy.&lt;/p&gt;</comment>
                            <comment id="13209379" author="zhihyu@ebaysf.com" created="Thu, 16 Feb 2012 14:17:09 +0000"  >&lt;p&gt;Thanks for the patch.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; force = region.shouldForceSplit();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;shouldSplit() can return immediately if force is true, right ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        foundABigStore = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
+      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can break out of the for loop when foundABigStore becomes true.&lt;/p&gt;

&lt;p&gt;Validation in real cluster is appreciated.&lt;/p&gt;</comment>
                            <comment id="13209478" author="stack" created="Thu, 16 Feb 2012 16:27:13 +0000"  >&lt;p&gt;I can add the above changes (will fix the superclass from where I copied this stuff too) but I&apos;m more interested in feedback along the lines of whether folks think we should put this in as default split policy.  If so, will then spend time on it trying it on cluster, otherwise not.&lt;/p&gt;</comment>
                            <comment id="13209621" author="stack" created="Thu, 16 Feb 2012 19:11:12 +0000"  >&lt;p&gt;Chatting w/ J-D, probably less disruptive if we do square of the count of regions on a regionserver so we get to max size faster (then there&apos;ll be less regions created overall by this phenomeon).&lt;/p&gt;</comment>
                            <comment id="13211205" author="stack" created="Sun, 19 Feb 2012 04:41:47 +0000"  >&lt;p&gt;Making it so we consider this for 0.92.1 &amp;#8211; its usability.  Will try on cluster w/ square of the number of regions.&lt;/p&gt;</comment>
                            <comment id="13211619" author="lhofhansl" created="Mon, 20 Feb 2012 00:38:31 +0000"  >&lt;p&gt;Wouldn&apos;t we potentially do a lot of splitting when there are many regionservers?&lt;br/&gt;
(Maybe I am not grokking this fully)&lt;/p&gt;

&lt;p&gt;If we take the square of the of the number of regions, and say we have 10gb regions and flush size of 128mb, we&apos;d reach the 10gb after at 9 regions of the table on the same regionserver.&lt;br/&gt;
We were planning a region size of 5gb and flush size of 256mb, that would still be 5 regions.&lt;br/&gt;
(10gb/128mb ~ 78, 5gb/256mb ~ 19)&lt;/p&gt;</comment>
                            <comment id="13211620" author="lhofhansl" created="Mon, 20 Feb 2012 00:40:54 +0000"  >&lt;p&gt;Never use a calculator... 10gb/128mb = 80, 5bgb/256mb = 20.&lt;/p&gt;</comment>
                            <comment id="13212142" author="stack" created="Mon, 20 Feb 2012 22:16:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;Wouldn&apos;t we potentially do a lot of splitting when there are many regionservers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Each regionserver would split with the same growing reluctance.  Don&apos;t we want a bunch of splitting when lots of regionservers so they all get some amount of the incoming load promptly?&lt;/p&gt;

&lt;p&gt;This issue is about getting us to split fast at the start of a bulk load but then having the splitting fall off as more data made it in.&lt;/p&gt;

&lt;p&gt;I&apos;m thinking our default regionsize should be 10G.  I should add this to the this patch.&lt;/p&gt;

&lt;p&gt;I don&apos;t get what you are saying on the end Lars.  Is it good or bad that there are 5 regions on a regionserver before we get to the max size?  Balancer will cut in and move regions to other servers and they&apos;ll then split eagerly at first with rising reluctance.&lt;/p&gt;</comment>
                            <comment id="13212424" author="lhofhansl" created="Tue, 21 Feb 2012 07:32:06 +0000"  >&lt;p&gt;Was thinking that before each region server reaches 9 (or even just 5) regions for a table we&apos;d have a lot of regions.&lt;/p&gt;

&lt;p&gt;Say I have 10gb regionsize and 128mb flushsize and 100 regionservers.&lt;br/&gt;
If I understand correctly a regionserver would still split at a size &amp;lt; 10gb until there about 900 regions for the table (assuming somewhat even distribution).&lt;/p&gt;

&lt;p&gt;Maybe this is good?&lt;br/&gt;
I guess ideally we&apos;d get to about 100 regions and then just grow them unless they reach 10gb... Maybe even less regions if there&apos;re many tables.&lt;/p&gt;

&lt;p&gt;(As I said above I might not have grokked this correctly)&lt;/p&gt;</comment>
                            <comment id="13213389" author="stack" created="Wed, 22 Feb 2012 06:16:33 +0000"  >&lt;p&gt;If I understand correctly a regionserver would still split at a size &amp;lt; 10gb until there about 900 regions for the table (assuming somewhat even distribution).&lt;/p&gt;

&lt;p&gt;Well each split would take longer because the threshold will have grown closer to the 10GB, but yeah.  And I think this is what we want.  Doing to the power of 3 would make us rise to the 10GB faster.  We&apos;d split on first flush then at &lt;/p&gt;

&lt;p&gt;This is probably ok.  More regions means that we&apos;ll fan out regions over the cluster a little faster.  We&apos;ll have 9 regions for a table on each server which is probably too many still.  We could do to the power of 3 so we&apos;d split on first flush, then at 1G, 3.4G, 8.2G and then we&apos;d be at our 10G limit.&lt;/p&gt;</comment>
                            <comment id="13214072" author="stack" created="Wed, 22 Feb 2012 22:44:24 +0000"  >&lt;p&gt;Make it the square of the count of regions.&lt;/p&gt;

&lt;p&gt;Address also a problem found by j-d where I was getting region size from conf instead of from HTD.&lt;/p&gt;

&lt;p&gt;This patch works on trunk only.  Will need to do a version for 0.92.&lt;/p&gt;</comment>
                            <comment id="13214122" author="jdcryans" created="Thu, 23 Feb 2012 00:21:46 +0000"  >&lt;p&gt;The latest patch is looking good on my test cluster, will let the import finish before giving my +1 tho.&lt;/p&gt;</comment>
                            <comment id="13214129" author="tlipcon" created="Thu, 23 Feb 2012 00:31:32 +0000"  >&lt;p&gt;Got any comparison numbers for total import time, for say 100G load? Would be good to know that the new heuristic is definitely advantageous.&lt;/p&gt;</comment>
                            <comment id="13214134" author="jdcryans" created="Thu, 23 Feb 2012 00:41:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;Got any comparison numbers for total import time, for say 100G load?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not yet, but I can definitely see that it jumpstarts the import.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Would be good to know that the new heuristic is definitely advantageous.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is, I don&apos;t need numbers to tell you that.&lt;/p&gt;</comment>
                            <comment id="13214147" author="tlipcon" created="Thu, 23 Feb 2012 00:56:10 +0000"  >&lt;p&gt;OK &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13214226" author="jdcryans" created="Thu, 23 Feb 2012 02:34:19 +0000"  >&lt;p&gt;Alright I did a macrotest with 100GB. &lt;/p&gt;

&lt;p&gt;Configurations:&lt;br/&gt;
Good old 15 machines test cluster (1 master), 2xquad, 14GB given to HBase, 4x SATA.&lt;br/&gt;
The table is configured to flush at 256MB, split at 2GB.&lt;br/&gt;
40 clients that use a 12MB buffer, collocated on the RS.&lt;br/&gt;
Higher threshold for compactions.&lt;/p&gt;

&lt;p&gt;Without patch:&lt;br/&gt;
1558s&lt;/p&gt;

&lt;p&gt;With patch:&lt;br/&gt;
1457s&lt;/p&gt;

&lt;p&gt;1.07x improvement.&lt;/p&gt;

&lt;p&gt;Then what I saw is that once we&apos;ve split a few times and that the load got balanced, the performance is exactly the same. That&apos;s expected. Also it seems that my split-after-flush patch also goes into full effect.&lt;/p&gt;

&lt;p&gt;I&apos;m running another experiment right now uploading 1TB with flush set at 512MB and split at 20GB. I assume an even bigger difference. The reason to use 20GB is that with bigger data sets you need bigger regions, and starting such a load from scratch is currently horrible but this is what this jira is about.&lt;/p&gt;</comment>
                            <comment id="13214369" author="lhofhansl" created="Thu, 23 Feb 2012 06:29:02 +0000"  >&lt;p&gt;I like N^3. In the scenarios above it would lead to 3 and 4 region (resp) before we reach 10gb.&lt;br/&gt;
Could even be more radical and say: If we see 1 region on this regionserver we split at flushsize, if we see 2 or more we split at region size.&lt;br/&gt;
(this assumes that if a region server see 2 regions of the same table it&apos;s likely to be large)&lt;/p&gt;</comment>
                            <comment id="13214374" author="lhofhansl" created="Thu, 23 Feb 2012 06:34:27 +0000"  >&lt;p&gt;Also I wonder now whether it is time to separate the part of RegionSplitPolicy that decided when to split (shouldSplit(...)) from the part that decides where to split (getSplitPoint(...)).&lt;/p&gt;

&lt;p&gt;Thinking about &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5304&quot; title=&quot;Pluggable split key policy&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5304&quot;&gt;&lt;del&gt;HBASE-5304&lt;/del&gt;&lt;/a&gt;, where we want to control where a region split but don&apos;t care when it is split.&lt;/p&gt;</comment>
                            <comment id="13214378" author="zhihyu@ebaysf.com" created="Thu, 23 Feb 2012 06:38:57 +0000"  >&lt;p&gt;The above concern makes sense.&lt;br/&gt;
We&apos;d better make this change before branching 0.94&lt;/p&gt;</comment>
                            <comment id="13215080" author="jdcryans" created="Thu, 23 Feb 2012 21:39:15 +0000"  >&lt;p&gt;Conclusion for the 1TB upload:&lt;/p&gt;

&lt;p&gt;Flush size: 512MB&lt;br/&gt;
Split size: 20GB&lt;/p&gt;

&lt;p&gt;Without patch:&lt;br/&gt;
18012s&lt;/p&gt;

&lt;p&gt;With patch:&lt;br/&gt;
12505s&lt;/p&gt;

&lt;p&gt;It&apos;s 1.44x better, so a huge improvement. The difference here is due to the fact that it takes an awfully long time to split the first few regions without the patch. In the past I was starting the test with a smaller split size and then once I got a good distribution I was doing an online alter to set it to 20GB. Not anymore with this patch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Another observation: the upload in general is slowed down by &quot;too many store files&quot; blocking. I could trace this to compactions taking a long time to get rid of reference files (3.5GB taking more than 10 minutes) and during that time you can hit the block multiple times. We really ought to see how we can optimize the compactions, consider compacting those big files in many threads instead of only one, and enable referencing reference files to skip some compactions altogether.&lt;/p&gt;</comment>
                            <comment id="13215088" author="tlipcon" created="Thu, 23 Feb 2012 21:46:11 +0000"  >&lt;p&gt;Great results! Very cool.&lt;/p&gt;</comment>
                            <comment id="13215176" author="stack" created="Thu, 23 Feb 2012 23:08:58 +0000"  >&lt;p&gt;@Lars You want to put an upper bound on the number of regions?&lt;/p&gt;

&lt;p&gt;I think if we do power of three, we&apos;ll lose some of the benefit J-D sees above; we&apos;ll fan out the regions slower.&lt;/p&gt;

&lt;p&gt;Do you want to put an upper bound on the number of regions per regionserver for a table?  Say, three?  As in, when we get to three regions on a server, just scoot the split size up to the maximum.  So, given a power of two, we&apos;d split on first flush, then the next split would happen at (2*2*128M) 512M, then 9*128M=1.2G and thereafter we&apos;d split at the max, say 10G?&lt;/p&gt;

&lt;p&gt;Or should we just commit this for now and do more in another patch?&lt;/p&gt;</comment>
                            <comment id="13215189" author="lhofhansl" created="Thu, 23 Feb 2012 23:21:38 +0000"  >&lt;p&gt;@Stack: I trust J-D&apos;s test far more than my (untested) intuition &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
I do like the upper bound of 3, though.&lt;/p&gt;

&lt;p&gt;@J-D: Wow.&lt;br/&gt;
One question I had: Did you observe write blocking - due to the number of store files - more frequently than without the patch (because with the patch we tend to get more store-files).&lt;/p&gt;</comment>
                            <comment id="13215226" author="jdcryans" created="Fri, 24 Feb 2012 00:00:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;One question I had: Did you observe write blocking - due to the number of store files - more frequently than without the patch (because with the patch we tend to get more store-files).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I does happen a lot more in the beginning, growing out of the first few regions is really hard.&lt;/p&gt;</comment>
                            <comment id="13215227" author="jdcryans" created="Fri, 24 Feb 2012 00:01:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;One question I had: Did you observe write blocking - due to the number of store files - more frequently than without the patch (because with the patch we tend to get more store-files).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I does happen a lot more in the beginning, growing out of the first few regions is really hard.&lt;/p&gt;</comment>
                            <comment id="13215242" author="stack" created="Fri, 24 Feb 2012 00:15:09 +0000"  >&lt;p&gt;@Lars I think power of three will have us lose some of the fast fan out of regions.  I suggest we commit with power of two and 10G split boundary for this issue and look to other policies to improve on this basic win.&lt;/p&gt;</comment>
                            <comment id="13215336" author="lhofhansl" created="Fri, 24 Feb 2012 03:12:27 +0000"  >&lt;p&gt;@Stack: Yeah, power of three is probably too much.&lt;br/&gt;
In the last comment I was referring to the upper bound of 3 regions per regionserver that you suggested, at which point we set the split size to region size.&lt;/p&gt;

&lt;p&gt;@J-D: so in some cases we increase latency with this patch? Not saying it&apos;s a problem just curious.&lt;/p&gt;

&lt;p&gt;+1 on patch.&lt;/p&gt;</comment>
                            <comment id="13215342" author="jdcryans" created="Fri, 24 Feb 2012 03:30:37 +0000"  >&lt;p&gt;Ah sorry I misunderstood your question. I meant that without the patch there&apos;s a lot more blocking. With the patch there&apos;s more region so more compactions happening in parallel.&lt;/p&gt;</comment>
                            <comment id="13215360" author="lhofhansl" created="Fri, 24 Feb 2012 04:04:15 +0000"  >&lt;p&gt;Ah nice &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13215365" author="stack" created="Fri, 24 Feb 2012 04:24:28 +0000"  >&lt;p&gt;This version sets the default split policy to be the new one and ups the max file size to 10G from 1G.  This is what I&apos;ll commit unless objection.  It does square of the number of regions * flushsize.&lt;/p&gt;</comment>
                            <comment id="13215368" author="lhofhansl" created="Fri, 24 Feb 2012 04:31:39 +0000"  >&lt;p&gt;Wanna have KeyPrefixRegionSplitPolicy extend this new policy (rather than ConstantSizeRegionSplitPolicy)?&lt;/p&gt;</comment>
                            <comment id="13215383" author="stack" created="Fri, 24 Feb 2012 05:02:12 +0000"  >&lt;p&gt;Your wish is my command Lars.&lt;/p&gt;

&lt;p&gt;Also addressed Ted comments made earlier that I&apos;d forgotten to fix&lt;/p&gt;</comment>
                            <comment id="13215384" author="hadoopqa" created="Fri, 24 Feb 2012 05:04:27 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12515877/4365-v3.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12515877/4365-v3.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated -136 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 153 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1037//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1037//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1037//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1037//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1037//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1037//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13215386" author="zhihyu@ebaysf.com" created="Fri, 24 Feb 2012 05:09:38 +0000"  >&lt;p&gt;Minor comment:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-comment&quot;&gt;// If any of the stores are unable to split (eg they contain reference files)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Should read &apos;of the stores is unable&apos;&lt;/p&gt;

&lt;p&gt;Please also fix the failed test.&lt;/p&gt;</comment>
                            <comment id="13215390" author="hadoopqa" created="Fri, 24 Feb 2012 05:19:16 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12515883/4365-v4.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12515883/4365-v4.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated -136 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 153 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1038//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1038//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1038//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1038//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1038//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1038//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13215398" author="stack" created="Fri, 24 Feb 2012 05:37:59 +0000"  >&lt;p&gt;Fix test (wasn&apos;t accomodating of square of the number of regions) and address Ted comment.&lt;/p&gt;</comment>
                            <comment id="13215426" author="hadoopqa" created="Fri, 24 Feb 2012 06:30:30 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12515889/4365-v5.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12515889/4365-v5.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated -136 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 153 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.mapreduce.TestImportTsv&lt;br/&gt;
                  org.apache.hadoop.hbase.mapred.TestTableMapReduce&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1039//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1039//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1039//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1039//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/1039//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/1039//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13215427" author="lhofhansl" created="Fri, 24 Feb 2012 06:32:03 +0000"  >&lt;p&gt;+1 on V5&lt;/p&gt;</comment>
                            <comment id="13215440" author="stack" created="Fri, 24 Feb 2012 06:43:02 +0000"  >&lt;p&gt;Committed to trunk.  Thanks for reviews lads and testing j-d&lt;/p&gt;</comment>
                            <comment id="13215454" author="lhofhansl" created="Fri, 24 Feb 2012 07:13:14 +0000"  >&lt;p&gt;Thanks for also changing KeyPrefixRegionSplitPolicy Stack.&lt;br/&gt;
This is great, I&apos;ll deploy this to our test cluster next week.&lt;/p&gt;</comment>
                            <comment id="13215798" author="jdcryans" created="Fri, 24 Feb 2012 18:40:21 +0000"  >&lt;p&gt;FWIW running a 5TB upload took 18h.&lt;/p&gt;</comment>
                            <comment id="13215803" author="jdcryans" created="Fri, 24 Feb 2012 18:44:17 +0000"  >&lt;p&gt;Oh and no concurrent mode failures, as I don&apos;t use dumb configurations. Also my ZK timeout is set to 20s.&lt;/p&gt;</comment>
                            <comment id="13216326" author="hudson" created="Sat, 25 Feb 2012 06:01:28 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-security #122 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-security/122/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-security/122/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4365&quot; title=&quot;Add a decent heuristic for region size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4365&quot;&gt;&lt;del&gt;HBASE-4365&lt;/del&gt;&lt;/a&gt; Add a decent heuristic for region size (Revision 1293099)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
stack : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/HConstants.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13216340" author="hudson" created="Sat, 25 Feb 2012 06:08:55 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #2669 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2669/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2669/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4365&quot; title=&quot;Add a decent heuristic for region size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4365&quot;&gt;&lt;del&gt;HBASE-4365&lt;/del&gt;&lt;/a&gt; Add a decent heuristic for region size (Revision 1293099)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
stack : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/HConstants.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionSplitPolicy.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12542210">HBASE-5386</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12515658" name="4365-v2.txt" size="12147" author="stack" created="Wed, 22 Feb 2012 22:44:24 +0000"/>
                            <attachment id="12515877" name="4365-v3.txt" size="14659" author="stack" created="Fri, 24 Feb 2012 04:24:27 +0000"/>
                            <attachment id="12515883" name="4365-v4.txt" size="16132" author="stack" created="Fri, 24 Feb 2012 05:02:12 +0000"/>
                            <attachment id="12515889" name="4365-v5.txt" size="16206" author="stack" created="Fri, 24 Feb 2012 05:37:59 +0000"/>
                            <attachment id="12514774" name="4365.txt" size="12009" author="stack" created="Thu, 16 Feb 2012 08:25:16 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12522846">HBASE-4381</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 10 Sep 2011 01:11:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>33514</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 42 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08rrj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>49098</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Changes default splitting policy from ConstantSizeRegionSplitPolicy to IncreasingToUpperBoundRegionSplitPolicy.  Splits quickly initially slowing as the number of regions climbs.&lt;br/&gt;
&lt;br/&gt;
Split size is the number of regions that are on this server that all are of the same table, squared, times the region flush size OR the maximum region split size, whichever is smaller.  For example, if the flush size is 128M, then on first flush we will split which will make two regions that will split when their size is 2 * 2 * 128M = 512M.  If one of these regions splits, then there are three regions and now the split size is  3 * 3 * 128M =  1152M, and so on until we reach the configured maximum filesize and then from there on out, we&amp;#39;ll use that.&lt;br/&gt;
&lt;br/&gt;
Be warned, this new default could bring on lots of splits if you have many tables on your cluster.  Either go back to to the old split policy or up the lower bound configuration.&lt;br/&gt;
&lt;br/&gt;
This patch changes the default split size from 64M to 128M.  It makes the region eventual split size, hbase.hregion.max.filesize, 10G (It was 1G).</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>