<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:31:27 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5843/HBASE-5843.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5843] Improve HBase MTTR - Mean Time To Recover</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5843</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;A part of the approach is described here: &lt;a href=&quot;https://docs.google.com/document/d/1z03xRoZrIJmg7jsWuyKYl6zNournF_7ZHzdi0qz_B4c/edit&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1z03xRoZrIJmg7jsWuyKYl6zNournF_7ZHzdi0qz_B4c/edit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The ideal target is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;failure impact client applications only by an added delay to execute a query, whatever the failure.&lt;/li&gt;
	&lt;li&gt;this delay is always inferior to 1 second.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;We&apos;re not going to achieve that immediately...&lt;/p&gt;

&lt;p&gt;Priority will be given to the most frequent issues.&lt;br/&gt;
Short term:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;software crash&lt;/li&gt;
	&lt;li&gt;standard administrative tasks as stop/start of a cluster.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12551766">HBASE-5843</key>
            <summary>Improve HBase MTTR - Mean Time To Recover</summary>
                <type id="14" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Umbrella</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nkeywal">Nicolas Liochon</assignee>
                                    <reporter username="nkeywal">Nicolas Liochon</reporter>
                        <labels>
                    </labels>
                <created>Fri, 20 Apr 2012 10:32:51 +0000</created>
                <updated>Fri, 20 Nov 2015 11:52:13 +0000</updated>
                            <resolved>Tue, 15 Oct 2013 09:02:13 +0000</resolved>
                                    <version>0.95.2</version>
                                    <fixVersion>0.96.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>39</watches>
                                                                <comments>
                            <comment id="13397559" author="nkeywal" created="Wed, 20 Jun 2012 14:52:41 +0000"  >&lt;p&gt;Small status as of June.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Improvements identified&lt;br/&gt;
    Failure detection time: performed by ZK, with a timeout. With the default value, we needed 90 seconds before starting to act on a software or hardware issue.&lt;br/&gt;
    Recovery time - server side: split in two parts: reassigning the regions of a dead RS to a new RS, replaying the WAL. Must be as fast as possible.&lt;br/&gt;
    Recovery time - client side: errors should be transparent for the user code. On the client side, we must as well limit the time lost on errors to a minimum.&lt;br/&gt;
    Planned rolling restart: just make this as fast and less disruptive as possible&lt;br/&gt;
    Other possible changes. detailed below.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Status&lt;br/&gt;
    Failure detection time: software crash - done&lt;br/&gt;
    Done in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Failure detection time: hardware issue - not started&lt;br/&gt;
1) as much as possible, it should be handled by ZooKeeper and not HBase, see open Jira as &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-702&quot; title=&quot;GSoC 2010: Failure Detector Model&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-702&quot;&gt;ZOOKEEPER-702&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-922&quot; title=&quot;enable faster timeout of sessions in case of unexpected socket disconnect&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-922&quot;&gt;ZOOKEEPER-922&lt;/a&gt;, ...&lt;br/&gt;
2) we need to make easy for a monitoring tool to tag a RS or Master as dead. This way, specialized HW tools could point out dead RS. Jira to open.&lt;/p&gt;

&lt;p&gt;Recovery time - Server: in progress&lt;br/&gt;
1) bulk assignment: To be retested, there are many just-closed JIRA on this (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5998&quot; title=&quot;Bulk assignment: regionserver optimization by using a temporary cache for table descriptors when receveing an open regions request&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5998&quot;&gt;&lt;del&gt;HBASE-5998&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6109&quot; title=&quot;Improve RIT performances during assignment on large clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6109&quot;&gt;&lt;del&gt;HBASE-6109&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5970&quot; title=&quot;Improve the AssignmentManager#updateTimer and speed up handling opened event&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5970&quot;&gt;&lt;del&gt;HBASE-5970&lt;/del&gt;&lt;/a&gt;, ...). A lot of work by many people. There are still possible improvements (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6058&quot; title=&quot;Use ZK 3.4 API &amp;#39;multi&amp;#39; in bulk assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6058&quot;&gt;&lt;del&gt;HBASE-6058&lt;/del&gt;&lt;/a&gt;, ...)&lt;br/&gt;
2) Log replay: To be retested, there are many just-closed JIRA on this (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6134&quot; title=&quot;Improvement for split-worker to speed up distributed log splitting&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6134&quot;&gt;&lt;del&gt;HBASE-6134&lt;/del&gt;&lt;/a&gt;, ...).&lt;/p&gt;

&lt;p&gt;Recovery time - Client - done&lt;br/&gt;
1) The RS now returns the new RS to the client after a region move (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5992&quot; title=&quot;Generalization of region move implementation + manage draining servers in bulk assign&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5992&quot;&gt;&lt;del&gt;HBASE-5992&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5877&quot; title=&quot;When a query fails because the region has moved, let the regionserver return the new address to the client&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5877&quot;&gt;&lt;del&gt;HBASE-5877&lt;/del&gt;&lt;/a&gt;)&lt;br/&gt;
2) Client retries sooner on errors (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5924&quot; title=&quot;In the client code, don&amp;#39;t wait for all the requests to be executed before resubmitting a request in error.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5924&quot;&gt;&lt;del&gt;HBASE-5924&lt;/del&gt;&lt;/a&gt;).&lt;br/&gt;
3) In the future, it could be interesting to share the region location in ZK with the client. It&apos;s not reasonable today as it could lead to have too many connection to ZK. &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-1147&quot; title=&quot;Add support for local sessions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-1147&quot;&gt;&lt;del&gt;ZOOKEEPER-1147&lt;/del&gt;&lt;/a&gt; is an open JIRA on this.&lt;/p&gt;

&lt;p&gt;Planned rolling restart performances - in progress&lt;br/&gt;
Benefits from the modifications in the client mentioned above.&lt;br/&gt;
To do: analyze move performances to make it faster if possible.&lt;/p&gt;

&lt;p&gt;Other possible changes&lt;br/&gt;
    Restart the server immediately on software crash: done in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5939&quot; title=&quot;Add an autorestart option in the start scripts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5939&quot;&gt;&lt;del&gt;HBASE-5939&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
    Reuse the same assignment on software crash: not planned&lt;br/&gt;
    Use spare hardware to reuse the same assignment on hardware failure: not planned&lt;br/&gt;
    Multiple RS for the same region (excluded in the initial document: hbase architecture change previously discussed by Gary H./Andy P.): not planned&lt;/p&gt;
</comment>
                            <comment id="13397674" author="ram_krish" created="Wed, 20 Jun 2012 17:46:08 +0000"  >&lt;p&gt;@N&lt;br/&gt;
Just for update, I feel &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6060&quot; title=&quot;Regions&amp;#39;s in OPENING state from failed regionservers takes a long time to recover&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6060&quot;&gt;&lt;del&gt;HBASE-6060&lt;/del&gt;&lt;/a&gt;(under progress) may also be related to MTTR.&lt;/p&gt;</comment>
                            <comment id="13397876" author="apurtell" created="Wed, 20 Jun 2012 21:12:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt; are not in 0.94, can we port these back? Seems like pretty self contained changes.&lt;/p&gt;</comment>
                            <comment id="13398535" author="nkeywal" created="Thu, 21 Jun 2012 16:25:20 +0000"  >&lt;p&gt;@ram Thanks for pointing this one out. I will wait for the fix before redoing a perf check.&lt;br/&gt;
@andrew Yes, there should be no issue. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt; modifies the content of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt;, so the merged patch will be smaller. I will have a look.&lt;/p&gt;</comment>
                            <comment id="13409663" author="nkeywal" created="Mon, 9 Jul 2012 17:29:29 +0000"  >&lt;p&gt;Some tests results:&lt;/p&gt;

&lt;p&gt;I tested the following scenarios, on a local machine, a pseudo&lt;br/&gt;
distributed cluster with ZooKeeper and HBase writing in a ram drive,&lt;br/&gt;
no datanode nor namenode, with 2 region servers, and one empty table&lt;br/&gt;
with 10000 regions, 5K on each RS. Versions taken monday 2nd&lt;/p&gt;

&lt;p&gt;1) Clean stop of one RS; wait for all regions to become online again:&lt;br/&gt;
0.92: ~800 seconds&lt;br/&gt;
0.96: ~13 seconds&lt;/p&gt;

&lt;p&gt;=&amp;gt; Huge improvement, hopefully from stuff like &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5970&quot; title=&quot;Improve the AssignmentManager#updateTimer and speed up handling opened event&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5970&quot;&gt;&lt;del&gt;HBASE-5970&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6109&quot; title=&quot;Improve RIT performances during assignment on large clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6109&quot;&gt;&lt;del&gt;HBASE-6109&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;1.1) As above with 2Mb memory per server&lt;br/&gt;
Results as 1)&lt;/p&gt;

&lt;p&gt;=&amp;gt; Results don&apos;t depend on any GC stuff (memory reported is around 200 Mb)&lt;/p&gt;


&lt;p&gt;2) Kill -9 of a RS; wait for all regions to become online again:&lt;br/&gt;
0.92: 980s&lt;br/&gt;
0.96: ~13s&lt;/p&gt;

&lt;p&gt;=&amp;gt; The 180s gap comes from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt;. For master, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt; is not tested but should bring similar results.&lt;/p&gt;



&lt;p&gt;3) Start of the cluster after a clean stop; wait for all regions to&lt;br/&gt;
become online.&lt;br/&gt;
0.92: ~1020s&lt;br/&gt;
0.94: ~1023s (tested once only)&lt;br/&gt;
0.96: ~31s&lt;/p&gt;

&lt;p&gt;=&amp;gt; The benefit is visible at startup&lt;br/&gt;
=&amp;gt; This does not come from something implemented for 0.94&lt;/p&gt;



&lt;p&gt;4) As 3) But with HBase on a local HD&lt;br/&gt;
0.92: ~1044s (tested once only)&lt;br/&gt;
0.96: ~28s (tested once only)&lt;/p&gt;

&lt;p&gt;=&amp;gt; Similar results. Seems that HBase i/o was not and is not becoming the bottleneck.&lt;/p&gt;


&lt;p&gt;5) As 1) With 4RS instead of 2&lt;br/&gt;
0.92) 406s&lt;br/&gt;
0.96) 6s&lt;/p&gt;

&lt;p&gt;=&amp;gt; Twice faster in both cases. Scales with the number of RS with both versions on this minimalistic test.&lt;/p&gt;



&lt;p&gt;6) As 3) But with ZK on a local HD&lt;br/&gt;
Impossible to get something consistent here. Machine and test dependent.&lt;br/&gt;
The most credible result was similar to 2).&lt;br/&gt;
From ZK mailing list or &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-866&quot; title=&quot;Adding no disk persistence option in zookeeper.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-866&quot;&gt;ZOOKEEPER-866&lt;/a&gt; is seems that what we should expect.&lt;/p&gt;



&lt;p&gt;7) With 2 RS, Insert 20M simple puts; then kill -9 the second one. See how long it takes to have all the regions available.&lt;br/&gt;
0.92) 180s detection time+ then hangs twice out of 2 tests.&lt;br/&gt;
0.96) 14s (hangs once out of 3)&lt;/p&gt;

&lt;p&gt;=&amp;gt; There&apos;s a bug &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
=&amp;gt; Test to be changed to get a real difference when we need to replay the wal.&lt;/p&gt;</comment>
                            <comment id="13409666" author="nkeywal" created="Mon, 9 Jul 2012 17:33:49 +0000"  >&lt;p&gt;@andrew I had a look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt;, they have a small dependency to protobuf stuff that I had forgotten (they read the server name from zk), so it&apos;s not a pure git port.&lt;/p&gt;</comment>
                            <comment id="13418863" author="gchanan" created="Fri, 20 Jul 2012 01:30:12 +0000"  >&lt;p&gt;Looks great so far, nkeywal.&lt;/p&gt;

&lt;p&gt;Some questions:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2) Kill -9 of a RS; wait for all regions to become online again:&lt;br/&gt;
0.92: 980s&lt;br/&gt;
0.96: ~13s&lt;br/&gt;
=&amp;gt; The 180s gap comes from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt;. For master, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt; is not tested but should bring similar results.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m confused as to what the 180s gap refers to.  I see 980 (test 2) - 800 (test1) = 180, but that is against 0.92, which doesn&apos;t have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5970&quot; title=&quot;Improve the AssignmentManager#updateTimer and speed up handling opened event&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5970&quot;&gt;&lt;del&gt;HBASE-5970&lt;/del&gt;&lt;/a&gt;, right?  Could you clarify?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3) Start of the cluster after a clean stop; wait for all regions to&lt;br/&gt;
become online.&lt;br/&gt;
0.92: ~1020s&lt;br/&gt;
0.94: ~1023s (tested once only)&lt;br/&gt;
0.96: ~31s&lt;br/&gt;
=&amp;gt; The benefit is visible at startup&lt;br/&gt;
=&amp;gt; This does not come from something implemented for 0.94&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Awesome.. We think this is also due to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5970&quot; title=&quot;Improve the AssignmentManager#updateTimer and speed up handling opened event&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5970&quot;&gt;&lt;del&gt;HBASE-5970&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6109&quot; title=&quot;Improve RIT performances during assignment on large clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6109&quot;&gt;&lt;del&gt;HBASE-6109&lt;/del&gt;&lt;/a&gt;? (since I assume &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt; do not apply in this case).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;7) With 2 RS, Insert 20M simple puts; then kill -9 the second one. See how long it takes to have all the regions available.&lt;br/&gt;
0.92) 180s detection time+ then hangs twice out of 2 tests.&lt;br/&gt;
0.96) 14s (hangs once out of 3)&lt;br/&gt;
=&amp;gt; There&apos;s a bug &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Has a JIRA been filed?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Test to be changed to get a real difference when we need to replay the wal.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Could you clarify what you mean here?&lt;/p&gt;</comment>
                            <comment id="13418919" author="nkeywal" created="Fri, 20 Jul 2012 04:46:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m confused as to what the 180s gap refers to. I see 980 (test 2) - 800 (test1) = 180, but that is against 0.92, which doesn&apos;t have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5970&quot; title=&quot;Improve the AssignmentManager#updateTimer and speed up handling opened event&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5970&quot;&gt;&lt;del&gt;HBASE-5970&lt;/del&gt;&lt;/a&gt;, right? Could you clarify?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, it&apos;s because with a clean stop, the RS unregisters itself in ZK, so the recovery starts immediately. With a kill -9, the RS remains registered in ZK. So if you don&apos;t have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5926&quot; title=&quot;Delete the master znode after a master crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5926&quot;&gt;&lt;del&gt;HBASE-5926&lt;/del&gt;&lt;/a&gt;, you wait for the ZK timeout.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Awesome.. We think this is also due to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5970&quot; title=&quot;Improve the AssignmentManager#updateTimer and speed up handling opened event&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5970&quot;&gt;&lt;del&gt;HBASE-5970&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6109&quot; title=&quot;Improve RIT performances during assignment on large clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6109&quot;&gt;&lt;del&gt;HBASE-6109&lt;/del&gt;&lt;/a&gt;? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Has a JIRA been filed?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not yet. I&apos;m writing specific unit tests for this, I found issues that I have not yet fully analyzed, and I need to create the jiras. Also, may be my test was not good for this part: as I was doing the test without a datanode, it could be that the recovery was not working for this reason (I wonder if the sync works with the local file system for example).&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Test to be changed to get a real difference when we need to replay the wal.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Could you clarify what you mean here?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s does not last long enough, so I won&apos;t be able to see much difference even if there is one. So I need to redo the work with a real datanode, check that it recovers, then check that I measure something meaningful.&lt;br/&gt;
I will also redo the first tests with a DN to see if there is still a gap.&lt;/p&gt;

</comment>
                            <comment id="13427182" author="nkeywal" created="Thu, 2 Aug 2012 08:44:28 +0000"  >&lt;p&gt;Same tests as before, with the datanodes.&lt;/p&gt;

&lt;p&gt;1) Clean stop of one RS; wait for all regions to become online again:&lt;br/&gt;
Pseudo distributed without datanode:&lt;br/&gt;
0.92: ~800 seconds&lt;br/&gt;
0.96: ~13 seconds&lt;/p&gt;

&lt;p&gt;With two datanodes, on hadoop 1.0&lt;br/&gt;
0.92: ~460 seconds&lt;br/&gt;
0.96: ~12 seconds&lt;/p&gt;

&lt;p&gt;3) Start of the cluster after a clean stop; wait for all regions to&lt;br/&gt;
Pseudo distributed without datanode:&lt;br/&gt;
become online.&lt;br/&gt;
0.92: ~1020s&lt;br/&gt;
0.94: ~1023s (tested once only)&lt;br/&gt;
0.96: ~31s&lt;/p&gt;

&lt;p&gt;With two datanodes, on hadoop 1.0&lt;br/&gt;
0.92: ~640 seconds&lt;br/&gt;
0.96: ~35 seconds&lt;/p&gt;

&lt;p&gt;So it seems 0.92 is faster with the DN, but we still see a major improvement.&lt;/p&gt;</comment>
                            <comment id="13450625" author="nkeywal" created="Fri, 7 Sep 2012 13:53:28 +0000"  >&lt;p&gt;Some tests and analysis around Distributed Split&lt;/p&gt;

&lt;p&gt;Scenario with a single machine, killing only a region server&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;dfs.replication = 2&lt;/li&gt;
	&lt;li&gt;local HD. The test failed with the ramDrive.&lt;/li&gt;
	&lt;li&gt;Start with 2 DN and 2 RS. Create a table with 100 regions in the second one. The first holds meta &amp;amp; root.&lt;/li&gt;
	&lt;li&gt;Insert 10M rows, distributed on all regions. That creates 8 logs files of 60Mb each.&lt;/li&gt;
	&lt;li&gt;start 3 more DN &amp;amp; RS.&lt;/li&gt;
	&lt;li&gt;kill -9 the second RS (NOT the datanode).&lt;/li&gt;
	&lt;li&gt;Wait for the regions to be available again.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;0.94&lt;br/&gt;
~180s detection time (sometimes less, it&apos;s strange. But always more than 120s).&lt;br/&gt;
~25s split (two tasks of ~10s each per regionserver)&lt;br/&gt;
~30s assignment&lt;/p&gt;

&lt;p&gt;0.96&lt;br/&gt;
~0s detection time (because it&apos;s a kill -9, so we have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt;. A hw failure would bring the same result as 0.94).&lt;br/&gt;
~25s split (two tasks of ~10s each)&lt;br/&gt;
~30s assignment. Other tests seems to show that the actual time is taken on replaying the edits.&lt;/p&gt;

&lt;p&gt;=&amp;gt; No difference in time here. Except the detection time, that will not play a role in a HW failure.&lt;br/&gt;
=&amp;gt; A split task in done in 10s. If you have a reasonable cluster, you can expect this task to takes 10s in production. &lt;br/&gt;
=&amp;gt; Same should apply to replaying edits, if the regions are well redistributed among the other machines.&lt;br/&gt;
=&amp;gt; Still, the assignment/replaying could be faster. Analysis to do, and JIRA to create.&lt;/p&gt;

&lt;p&gt;As of today, if the regionserver crashes, we should have, in production:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;0.94: ~50s (30s detection + 10s split + 10s assignment)&lt;/li&gt;
	&lt;li&gt;0.96: ~20s (0s detection + 10s split + 10s assignment)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If it&apos;s a HW failure, we need to take into account that we&apos;ve lost a datanode as well.&lt;/p&gt;</comment>
                            <comment id="13450718" author="nkeywal" created="Fri, 7 Sep 2012 15:27:08 +0000"  >&lt;p&gt;Some tests and analysis around Distributed Split / Datanodes failures&lt;/p&gt;

&lt;p&gt;On a real cluster, 3 nodes.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;dfs.replication = 2&lt;/li&gt;
	&lt;li&gt;local HD. The test failed with the ramDrive.&lt;/li&gt;
	&lt;li&gt;Start with 2 DN and 2 RS. Create a table with 100 regions in the second one. The first holds meta &amp;amp; root.&lt;/li&gt;
	&lt;li&gt;Insert 1M or 10M rows, distributed on all regions. That creates 8 logs files of 60Mb each, on a single server.&lt;/li&gt;
	&lt;li&gt;Start another box with a DN and a RS. This box is empty (no regions, no blocks).&lt;/li&gt;
	&lt;li&gt;Unplug (physically) the box with the 100 regions and the 1 (for 1M puts) or 8 (for 10M puts) log files.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Durations are, in seconds. With HDFS 1.0.3 if not stated differently.&lt;/p&gt;

&lt;p&gt;1M puts on 0.94:&lt;br/&gt;
~180s detection time, sometimes around 150s&lt;br/&gt;
~130s split time (there is a single file to split. This is to be compared to the 10s per split above)&lt;br/&gt;
~180s assignment, included replaying edits. There could be some locks, as we&apos;re reassigning/replaying 50 regions per server.&lt;/p&gt;

&lt;p&gt;1M puts on 0.96 3 tests. One failure.&lt;br/&gt;
~180s detection time, sometimes around 150s&lt;br/&gt;
~180s split time. Once again a single file to split. It&apos;s unclear why it takes longer than 0.94&lt;br/&gt;
~180s assignment, as 0.94.&lt;/p&gt;

&lt;p&gt;Out of 3 tests, it failed once on 0.96. It didn&apos;t fail on 0.94.&lt;/p&gt;

&lt;p&gt;10M puts on 0.96 + HDFS branch 2 as of today&lt;br/&gt;
~180s detection time, sometimes around 150s&lt;br/&gt;
~11 minutes split. Basically it fails until HDFS nanemode marks the datanode as dead. It takes 7:30 minutes, so the split finishes after this.&lt;br/&gt;
~60s assignment? Tested only once.&lt;/p&gt;

&lt;p&gt;0M (zero) puts on 0.96 + HDFS branch 2 as of today&lt;br/&gt;
~180s detection time, sometimes around 150s&lt;br/&gt;
~0s split. &lt;br/&gt;
~3s assignment (This seems to say that the assignment time is spent in the edit replay.)&lt;/p&gt;

&lt;p&gt;10M puts on 0.96 + HDFS branch 2 + &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; full (read + write paths)&lt;br/&gt;
~180s detection time, sometimes around 150s&lt;br/&gt;
~150s split. This for a bad reasons: all tasks excepts one succeeds. The last one seems to connect to the dead server, and finishes after ~100s. Tested twice.&lt;br/&gt;
~50s assignment. Measured once.&lt;/p&gt;

&lt;p&gt;So:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The measures on assignments are fishy. But it seems to say that we are now spending our time in replaying edit. We could have issues linked to HDFS as well here: in the last two scenarios we&apos;re not going to the dead nodes when we replay/flush edits, so that could be the reason.&lt;/li&gt;
	&lt;li&gt;The split in 10s per 60Gb, on a single and slow HD. With a reasonable cluster, this should scale pretty well. We could improve things by using locality.&lt;/li&gt;
	&lt;li&gt;There will be datanodes errors if you don&apos;t have &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt;. And in this case it becomes complicated. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt;.&lt;/li&gt;
	&lt;li&gt;With &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt;, we&apos;re 500s faster. That&apos;s interesting.&lt;/li&gt;
	&lt;li&gt;Even with &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; there is still something to look at in how HDFS connects to the dead node. It seems the block is empty, so retried multiple times. There are multiple possible paths here.&lt;/li&gt;
	&lt;li&gt;We can expect, in production, server side point of view&lt;/li&gt;
	&lt;li&gt;30s detection time for hw failure, 0s for simpler case (kill -9, OOM, machine nicely rebooted, ...)&lt;/li&gt;
	&lt;li&gt;10s split (i.e: distributed along multiple region servers)&lt;/li&gt;
	&lt;li&gt;10s assignment (i.e. distributed as well).&lt;/li&gt;
	&lt;li&gt;Without HDFS effects here. See above.&lt;/li&gt;
	&lt;li&gt;This scenario is extreme, as we&apos;re loosing 50% of our data. Still, if you&apos;re loosing a regionserver with 300 regions, the split can go well if you&apos;re not lucky.&lt;/li&gt;
	&lt;li&gt;It means as well that the detection time dominates the other parameters when everything goes well.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Conclusion:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Link HDFS / HBase plays the critical role in this scenario. &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; is one of the keys.&lt;/li&gt;
	&lt;li&gt;The Distributed Split seems to work well in terms of performances.&lt;/li&gt;
	&lt;li&gt;Assignment itself seems ok. Replaying should be looked at (more in terms of lock than raw performances).&lt;/li&gt;
	&lt;li&gt;Detection time will become more an more important.&lt;/li&gt;
	&lt;li&gt;An improvement would be to reassign the region in parallel of the split, with:&lt;/li&gt;
	&lt;li&gt;continue to serve writes before the end of the split as well: the fact that we&apos;re splitting the logs does not mean we cannot write. There are real applications that could use this (may be open tsdb for example: whatever application that logs data: they just need to know where to write).&lt;/li&gt;
	&lt;li&gt;continue to server reads if there are timeranged with the max time stamp before the failure: There are many applications that don&apos;t need fresh data (i.e. less than 1 minute old).&lt;/li&gt;
	&lt;li&gt;With this, the downtime will be totally dominated by the detection time.&lt;/li&gt;
	&lt;li&gt;There are JIRAs around the detection time already (basically: improve ZK and open HBase to external monitoring systems).&lt;/li&gt;
	&lt;li&gt;There will be some work around the client part.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13451096" author="stack" created="Fri, 7 Sep 2012 22:43:30 +0000"  >&lt;p&gt;Nicolas, the above should be put on the dev list.  Its great stuff.  Too good to be buried out here as a comment on issue with such an unsexy subject.&lt;/p&gt;</comment>
                            <comment id="13451100" author="nkeywal" created="Fri, 7 Sep 2012 22:49:22 +0000"  >&lt;p&gt;Improve HBase MTTR ? Unsexy? Really?&lt;br/&gt;
More seriously: agreed. Will do.&lt;/p&gt;</comment>
                            <comment id="13453296" author="nkeywal" created="Tue, 11 Sep 2012 18:52:29 +0000"  >&lt;p&gt;The issues when reading even when &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; comes from something new in HDFS 2.0. It seems that the blocks being written are not anymore in the &apos;main&apos; block list, so the initial fix was not working for this block. It show that if you have the error with a single log file out of 8, you&apos;re still paying a huge price in terms of added delay...&lt;/p&gt;
</comment>
                            <comment id="13454911" author="nkeywal" created="Thu, 13 Sep 2012 14:11:35 +0000"  >&lt;p&gt;Test with meta:&lt;/p&gt;

&lt;p&gt;On a real cluster, 3 nodes.&lt;br/&gt;
    dfs.replication = 2&lt;br/&gt;
    local HD.&lt;br/&gt;
    Start with 2 DN and 2 RS. Create a table with 100 regions in the second one. The first holds meta &amp;amp; root.&lt;br/&gt;
    Start another box with a DN and a RS. This box is empty (no regions, no blocks).&lt;br/&gt;
    Unplug the box with meta &amp;amp; root.&lt;br/&gt;
    try to create a table&lt;/p&gt;

&lt;p&gt;=&amp;gt; Time taken is the recovery time of the bow holding meta. No bad surprise. It means as well that with the default zookeeper timeout you&apos;re loosing the cluster for 3 minutes if your meta regionserver dies. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6773&quot; title=&quot;Make the dfs replication factor configurable per table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6773&quot;&gt;&lt;del&gt;HBASE-6773&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6774&quot; title=&quot;Immediate assignment of regions that don&amp;#39;t have entries in HLog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6774&quot;&gt;HBASE-6774&lt;/a&gt; would help to increase meta failure resiliency.&lt;/p&gt;</comment>
                            <comment id="13459251" author="gchanan" created="Thu, 20 Sep 2012 00:33:11 +0000"  >&lt;p&gt;Again, great work Nicolas.  Some questions/comments:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It means as well that the detection time dominates the other parameters when everything goes well.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Detection time will become more an more important.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Interesting.  The sad part is we often find ourselves having to &lt;b&gt;increase&lt;/b&gt; the ZK timeout in order to deal with Juliet GC pauses.  Given that detection time dominates, perhaps we should put some effort into correcting that (multiple RS on a single box?)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Replaying should be looked at (more in terms of lock than raw performances).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Why do you say this with respect to locking?  Is the performance not as good as you would expect?  Or just haven&apos;t looked at it yet?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;An improvement would be to reassign the region in parallel of the split&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve wondered why we don&apos;t do this.  Do you see any implementation challenges with doing this?  Maybe I&apos;ll look into it.&lt;/p&gt;</comment>
                            <comment id="13459262" author="yuzhihong@gmail.com" created="Thu, 20 Sep 2012 00:58:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;multiple RS on a single box?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In case that box goes down, recovery would be more costly (compared to single region server running on the box), right ?&lt;/p&gt;</comment>
                            <comment id="13459278" author="gchanan" created="Thu, 20 Sep 2012 01:29:08 +0000"  >&lt;p&gt;I don&apos;t know.  You&apos;d have to split the logs and replay them, which should be similar performance whether there is 1 or 2 WALs?&lt;/p&gt;</comment>
                            <comment id="13459313" author="stack" created="Thu, 20 Sep 2012 04:08:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;...multiple RS on a single box?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Was reading today that VoltDB does offheap so can have small java heap and just run w/ defaults other than setting -Xmx and -Xms.  Could try a few RS per box.. w/ small heap each (RS needs to be put on a CPU diet but could look at that afterward).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;....An improvement would be to reassign the region in parallel of the split&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Elsewhere nkeywal puts up a suggested prescription where on crash we assign and then split logs (rather than other way round as we do now).  Nicolas suggests that the assigned regions could immediately start taking writes; we&apos;d throw an exception if a read attempt was made until after the split completed and the regions edits had been replayed: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6752&quot; title=&quot;On region server failure, serve writes and timeranged reads during the log split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6752&quot;&gt;HBASE-6752&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13459356" author="gchanan" created="Thu, 20 Sep 2012 05:20:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;Was reading today that VoltDB does offheap so can have small java heap and just run w/ defaults other than setting -Xmx and -Xms. Could try a few RS per box.. w/ small heap each (RS needs to be put on a CPU diet but could look at that afterward).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, CPU definitely need a diet.  Probably start with eliminating a bunch of threads.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Elsewhere nkeywal puts up a suggested prescription where on crash we assign and then split logs (rather than other way round as we do now). Nicolas suggests that the assigned regions could immediately start taking writes; we&apos;d throw an exception if a read attempt was made until after the split completed and the regions edits had been replayed: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6752&quot; title=&quot;On region server failure, serve writes and timeranged reads during the log split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6752&quot;&gt;HBASE-6752&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I think &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6752&quot; title=&quot;On region server failure, serve writes and timeranged reads during the log split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6752&quot;&gt;HBASE-6752&lt;/a&gt; is a great idea, but it doesn&apos;t address serving reads more quickly.  I&apos;m wondering if there is more we can do to address that.&lt;/p&gt;</comment>
                            <comment id="13459444" author="nkeywal" created="Thu, 20 Sep 2012 08:19:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;Interesting. The sad part is we often find ourselves having to increase the ZK timeout in order to deal with Juliet GC pauses. Given that detection time dominates, perhaps we should put some effort into correcting that (multiple RS on a single box?)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Imho, multiple RS on the same box would put us in a dead end: it increases the number of tcp connections, add workload on ZooKeeper, makes the balancer more complicated, ... We can also have operational issues (rolling upgrade, fixed tcp ports, ...).&lt;br/&gt;
The possible options I know are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;improving ZooKeeper to have an algorithm that takes variance into account: it&apos;s a common solution to have a good failure detection while minimizing wrong positive. 20 years ago, it saved TCP from dying by congestion. There is &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-702&quot; title=&quot;GSoC 2010: Failure Detector Model&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-702&quot;&gt;ZOOKEEPER-702&lt;/a&gt; about this. That&apos;s medium term (hopes...), but would be useful for HDFS HA also.&lt;/li&gt;
	&lt;li&gt;Using the new gc options available in JDK 1.7 (see &lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/tech/g1-intro-jsp-135488.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.oracle.com/technetwork/java/javase/tech/g1-intro-jsp-135488.html&lt;/a&gt;). That&apos;s short term, simple. Only issue, it has been tried a few month ago (by Andrew Purtell IIRC), and crashed the JVM. Still, it&apos;s something to look at, and may be we should raise the bugs to Oracle if we find some.&lt;/li&gt;
	&lt;li&gt;The offheap mentioned by Stack.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I don&apos;t think it&apos;s one or another, we&apos;re likely to need all of them &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. Still, knowing where we stand in regards of JDK 1.7 is important imho.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Yes, CPU definitely need a diet. Probably start with eliminating a bunch of threads.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s not directly MTTR, but I agree, we have far too many threads, and far too many thread pools. Not only it&apos;s bad for performance, it makes analysing the performances complicated.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Right, I think &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6752&quot; title=&quot;On region server failure, serve writes and timeranged reads during the log split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6752&quot;&gt;HBASE-6752&lt;/a&gt; is a great idea, but it doesn&apos;t address serving reads more quickly. I&apos;m wondering if there is more we can do to address that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There is &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6774&quot; title=&quot;Immediate assignment of regions that don&amp;#39;t have entries in HLog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6774&quot;&gt;HBASE-6774&lt;/a&gt; for the special case of &quot;empty hlog&quot; regions. It would be interesting to see how many regions are in this situation on different production clusters. There are so many ways to be in this situation... I would love to have a stat on &quot;at a given point of time, what&apos;s the proportion of the regions with a non empty memstore&quot;. And improving memstore flush policy would lead us to improvement here as well I think.&lt;br/&gt;
With &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6752&quot; title=&quot;On region server failure, serve writes and timeranged reads during the log split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6752&quot;&gt;HBASE-6752&lt;/a&gt; we serve as well timeranged reads (if they&apos;re lucky on the range).&lt;br/&gt;
But yep, we don&apos;t cover all cases. Ideas welcome &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Why do you say this with respect to locking? Is the performance not as good as you would expect? Or just haven&apos;t looked at it yet?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was expecting much better performances, but I haven&apos;t looked enough at it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;ve wondered why we don&apos;t do this. Do you see any implementation challenges with doing this? Maybe I&apos;ll look into it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Well, it&apos;s closed to the assignment part, so... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; But it would be great if you can have a look at this, because with all the discussions around assignment, it&apos;s important to take these new use cases into account as well..&lt;/p&gt;</comment>
                            <comment id="13476563" author="gchanan" created="Mon, 15 Oct 2012 23:20:49 +0000"  >&lt;p&gt;One other question, could you explain these numbers in more detail?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The split in 10s per 60Gb, on a single and slow HD. With a reasonable cluster, this should scale pretty well. We could improve things by using locality.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Should this be 10s per 60 Mb, which is the size of a log (&quot;creates 8 log files of 60 Mb each&quot;).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Insert 1M or 10M rows, distributed on all regions. That creates 8 logs files of 60Mb each, on a single server.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The number and size of log files is independent of the number of rows?  Is the size of a row in the 10M setup 1/10 the size of the row in the 1M setup?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We can expect, in production, server side point of view&lt;br/&gt;
30s detection time for hw failure, 0s for simpler case (kill -9, OOM, machine nicely rebooted, ...)&lt;br/&gt;
10s split (i.e: distributed along multiple region servers)&lt;br/&gt;
10s assignment (i.e. distributed as well).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For detection: In all the tests where we are not deleting the znode, either because on 0.94 or hw failure, the detection time is 180s.  Why is this listed as 30 sec?&lt;br/&gt;
For split: I think I understand this.  8 log files, 60 Mb each means 8 split tasks on 4 regionservers = 2 split tasks per RS.  This took ~25 seconds, so if we had enough RS for each RS to only do 1 split task, we&apos;d be done in about 10s.&lt;br/&gt;
For assignment: Not sure where this number if coming from.  I see ~30 assignment and we know this would go faster with more RS, but how fast?&lt;/p&gt;</comment>
                            <comment id="13476582" author="nkeywal" created="Mon, 15 Oct 2012 23:48:05 +0000"  >&lt;p&gt;Should this be 10s per 60 Mb, which is the size of a log (&quot;creates 8 log files of 60 Mb each&quot;).&lt;br/&gt;
Yes &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The number and size of log files is independent of the number of rows? Is the size of a row in the 10M setup 1/10 the size of the row in the 1M setup?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No / ~Yes. There&apos;s a maximum number of log files before a flush however. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;on 0.94 or hw failure, the detection time is 180s. Why is this listed as 30 sec?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s discussed very briefly in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5844&quot; title=&quot;Delete the region servers znode after a regions server crash&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5844&quot;&gt;&lt;del&gt;HBASE-5844&lt;/del&gt;&lt;/a&gt;: it was bumped recently from 60s to 180s to help newcomers. ZK default is 30s and is imho the best choice for someone requiring a reasonable mttr without too much risk.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For split: I think I understand this. 8 log files, 60 Mb each means 8 split tasks on 4 regionservers = 2 split tasks per RS. This took ~25 seconds, so if we had enough RS for each RS to only do 1 split task, we&apos;d be done in about 10s.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, exactly. It&apos;s a reasonable expectation imho, even if there will be other cases.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For assignment: Not sure where this number if coming from. I see ~30 assignment and we know this would go faster with more RS, but how fast?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The calculations seems to say that the time is spent in the replay (that&apos;s the initial results from 20/Jun/12 16:52 and the test with zero rows from 07/Sep/12 17:27). It would be useful to redo the tests, as many things changed recently on assignment. But if the results are ok, and if the cluster is big enough, the regions are distributed, we should expect only one replay. Again, it&apos;s average, not worse case.&lt;/p&gt;</comment>
                            <comment id="13476590" author="gchanan" created="Tue, 16 Oct 2012 00:05:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;No / ~Yes. There&apos;s a maximum number of log files before a flush however.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure what you mean here.  Are the rows the same or not?  Are there are just more flushes on the 10M case?&lt;/p&gt;

&lt;p&gt;Thanks for clarifying.&lt;/p&gt;</comment>
                            <comment id="13476830" author="nkeywal" created="Tue, 16 Oct 2012 07:50:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;Not sure what you mean here. Are the rows the same or not? Are there are just more flushes on the 10M case?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, it&apos;s exactly the same rows for 1M puts and 10M puts.&lt;/p&gt;</comment>
                            <comment id="13501838" author="nkeywal" created="Wed, 21 Nov 2012 10:35:07 +0000"  >&lt;p&gt;New scenario on datanode issue during a WAL write:&lt;/p&gt;

&lt;p&gt;Scenario: With Replication factor 2, Start 2 DN &amp;amp; 1 RS, do a first put. Start a new DN, unplug the second one. Do another put, measure the time of this second put.&lt;/p&gt;

&lt;p&gt;HBase trunk / HDFS 1.1: ~5 minutes&lt;br/&gt;
HBase trunk / HDFS 2 branch: ~40s seconds&lt;br/&gt;
HBase trunk / HDFS 2.0.2-alpha-rc3: ~40 seconds&lt;/p&gt;


&lt;p&gt;The time is HDFS 1.1 is spent in:&lt;br/&gt;
~66 seconds: wait for connection timeout (SocketTimeoutException: 66000 millis while waiting for the channel to be ready for read).&lt;br/&gt;
then, we have two imbricated retries loops:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;6 retries: Failed recovery attemp #0 from primary datanode x.y.z.w:11011 -&amp;gt; NoRouteToHostException&lt;/li&gt;
	&lt;li&gt;10 sub-retries: Retrying connect to server: deadbox/x.y.z.w:11021. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There are more or less 4 seconds between two sub-retries, so the total time it around:&lt;br/&gt;
66 + 6 * (~4 * 10) = ~300 seconds. That&apos;s our 5 minutes.&lt;/p&gt;

&lt;p&gt;If we change HDFS code to have &quot;RetryPolicies.TRY_ONCE_THEN_FAIL&quot; vs. the default &quot;RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)&quot;, the put succeeds in ~80 seconds.&lt;/p&gt;

&lt;p&gt;Conclusion:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;time with HDFS 2.x is in line with what we have for other scenarios (~40s), so it&apos;s acceptable today.&lt;/li&gt;
	&lt;li&gt;time with HDFS 1.x is much less satisfying (~5 minutes!), could be easily decreased to 80s with an HDFS modification.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Some points to think about:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Maybe we could decrease the timeout for WAL: we&apos;re usually writing much less data than for a memstore flush, so having more aggressive settings for the WAL makes sense. There is a (bad) side effect: we may have more false positive, and this could decrease the performances, and it will increase the workload when the cluster is globally instable. So on the long term it makes sense, but may be today is to early.&lt;/li&gt;
	&lt;li&gt;While the Namenode will consider the datanode as stale after 30s, we still continue trying. Again, it makes sense to lower the global workload, but it&apos;s a little bit boring... There could be optimizations if the datanode state was shared to DFSClients.&lt;/li&gt;
	&lt;li&gt;There are some cases that could be handled faster: ConnectionRefused means the box is there but the port is not open: no need to retry here. AndNoRouteToHostException could be considered as well as critical enough to stop trying. Again as well, this is trading global workload vs. reactivity.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13504937" author="nkeywal" created="Tue, 27 Nov 2012 20:46:32 +0000"  >&lt;p&gt;Some more tests: ther&lt;/p&gt;

&lt;p&gt;scenario: 2 RS. Create a table with 3000 regions.&lt;br/&gt;
Start a third RS. Kill -15 the second one: there are 1500 regions to reassign.&lt;/p&gt;

&lt;p&gt;I&apos;ve made multiple measures, there are all there:&lt;br/&gt;
It&apos;s in seconds.&lt;br/&gt;
0.96 trunk is patched with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7220&quot; title=&quot;Creating a table with 3000 regions on 2 nodes fails after 1 hour&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7220&quot;&gt;&lt;del&gt;HBASE-7220&lt;/del&gt;&lt;/a&gt; (no metrics at all).&lt;/p&gt;

&lt;p&gt;Creating a table with 3000 regions:&lt;br/&gt;
0.92: 261s; 260s&lt;br/&gt;
0.94.0: 260s; 260s&lt;br/&gt;
0.94.1: 261s; 260s&lt;br/&gt;
0.94 trunk: 292s; 281s; 282s;&lt;br/&gt;
0.96 trunk: 173s; 178s&lt;/p&gt;

&lt;p&gt;Reassign after the kill&lt;br/&gt;
0.92: 107s, 110s&lt;br/&gt;
0.94.0: 105s; 105s&lt;br/&gt;
0.94.1: 107s; 107s&lt;br/&gt;
0.94 trunk: 122s; 105s; 116s&lt;br/&gt;
0.96 trunk: 50s; 50s;&lt;/p&gt;

&lt;p&gt;Same test, but putting ZK on a separate node, 1Gb network.&lt;br/&gt;
Reassign after the kill:&lt;br/&gt;
0.96 trunk: 101s.&lt;/p&gt;


&lt;p&gt;I need to do more tests, but it seems to match the profiling I&apos;ve done; the time is now spent in ZK communications.&lt;/p&gt;</comment>
                            <comment id="13560164" author="tychang" created="Tue, 22 Jan 2013 23:27:11 +0000"  >&lt;p&gt;@ nkeywal What is the application bug(AB) mentioned in your design doc? Do you mean hbase bug? or hbase client application code bug? &lt;/p&gt;

&lt;p&gt;If it is hbase client application code bug, does that need stop/start region server to fix the issue? &lt;/p&gt;

&lt;p&gt;If it is hbase code bug, do you refer to hbase bug that cause region server einter some bad state like deadlock, and so on? I think that could benefit from restarting region server to fix the problem. &lt;/p&gt;
</comment>
                            <comment id="13560472" author="nkeywal" created="Wed, 23 Jan 2013 08:21:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;What is the application bug(AB) mentioned in your design doc? Do you mean hbase bug? or hbase client application code bug? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Mainly HBase, but it could be as well a coprocessor issue. HBase can be configured to stop the regionserver if a coprocessor sends unexpected exceptions, but it&apos;s quite easy to write buggy stuff, like a coprocessor that takes resources without freeing them. Here you may need to stop the region server.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;If it is hbase client application code bug, does that need stop/start region server to fix the issue? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For a pure client (i.e. a user of the hbase.client package), it would be an HBase bug imho: HBase/a regionserver should be resistant to any client behavior.&lt;br/&gt;
For a coprocessor, it&apos;s client code executed within the regionserver process. Thanks to Java, many coprocessors bugs will have a limited effect, but as said above there are some cases that cannot be handled simply.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If it is hbase code bug, do you refer to hbase bug that cause region server einter some bad state like deadlock, and so on? I think that could benefit from restarting region server to fix the problem. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes.&lt;/p&gt;</comment>
                            <comment id="13561471" author="tychang" created="Thu, 24 Jan 2013 07:31:26 +0000"  >&lt;p&gt;@nkeywal That is what I thought. Thanks for the clarification!!!&lt;/p&gt;

&lt;p&gt;Another follow up question:  how can you identify the AB problem ASAP? For example, do you conclude that there is a AB when a running hbase application read/write performance dramatically slow down?  But sometimes, it could be just a temporary issue and will recover after a while. Stop/start RS will just hurt the performance due to region movement even with the MTTR improvement here. Maybe simply just testing the performance for longer time before making conclusion? Will that work? I am trying to see if there is any other better ways to identify AB problem and use graceful_stop to help improve hbase cluster performance. &lt;/p&gt;

&lt;p&gt;Thanks. &lt;/p&gt;</comment>
                            <comment id="13561487" author="nkeywal" created="Thu, 24 Jan 2013 08:08:08 +0000"  >&lt;p&gt;Agreed, you need to be a kind of expert to make the difference between &apos;slow for the moment&apos; and &apos;broken because we have a bad internal state&apos;. This said, if the issue comes from a coprocessor bug, you just need to be an expert of this coprocessor.&lt;/p&gt;

&lt;p&gt;Likely, the number of threads/memory consumed/file descriptors/... could be considered as indicators that something is going wrong, but it could be already too late for a clean stop as well...&lt;/p&gt;</comment>
                            <comment id="13562253" author="tychang" created="Fri, 25 Jan 2013 01:27:02 +0000"  >&lt;p&gt;@nkeywal  Thanks! &lt;/p&gt;

&lt;p&gt;I have two more questions regarding your &quot;Improving failure detection&quot; section. &lt;/p&gt;

&lt;p&gt;1. For hardware failure detection, it depends on the zk session timeout value. You mentioned it is 30 sec, so mean time to detect is 15sec, which is reasonable short. But I think the default value nowdays is 180sec. (I am referring to zookeeper.session.timeout) I can see in our cluster that master does not detect the crashed machine until 3 minutes later when the ephemeral znode timeout. So it takes 3 minutes to detect hardware failure in default case. That seems pretty long and should be improved. &lt;/p&gt;

&lt;p&gt;2. For software bug leading to a dirty stop, you mentioned  to launch region server with a script. I think we can use daemontool to start region server, and in daemontool&apos;s callback module, we can do the clean up znode based on the exit error code. daemontool can also be configured to restart the RS immediately again if needed. That seems can simplify the process. What do you think? &lt;/p&gt;</comment>
                            <comment id="13562555" author="nkeywal" created="Fri, 25 Jan 2013 09:23:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;That seems pretty long and should be improved. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep, the default was set for new users that won&apos;t understand the impact. You should change it if you care about mttr.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That seems can simplify the process. What do you think? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s important to have the feature in the core, if not the feature is not tested hence does not work, or does not work for long. There is a jira to support a standard administration tool in the core (I can&apos;t find it, but it exists for sure).&lt;/p&gt;</comment>
                            <comment id="13795019" author="nkeywal" created="Tue, 15 Oct 2013 09:01:49 +0000"  >&lt;p&gt;Marking solved in 0.96.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MTTR has decreased from 10 minutes to less than one minute, ~30 seconds in many cases.&lt;/li&gt;
	&lt;li&gt;when a machine fails, the other machines in the cluster are still available.&lt;/li&gt;
	&lt;li&gt;detection time is zero when there is a crash.&lt;/li&gt;
	&lt;li&gt;log replay scales well, allowing a minimal replay time.&lt;/li&gt;
	&lt;li&gt;thanks to the new distributed wal replay, &quot;puts&quot; are not impacted by the recovery. Client applications can continue to stream their writes when there is a machine failure.&lt;/li&gt;
	&lt;li&gt;Some of the improvement were backported to 0.94 / HDFS 1.x, are now used in production and work as expected.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The remaining ideas mentioned in this umbrella ticket will be tracked independently. There is still room for improvement.&lt;/p&gt;</comment>
                            <comment id="13802229" author="stack" created="Tue, 22 Oct 2013 20:34:10 +0000"  >&lt;p&gt;I went over your old SU MTTR doc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt;  I like the consideration given to the general topic.  Do you think we should start up another effort to go to the next level w/ MTTR?  For example, you mention cold-standbys in that old doc.  There are also other efforts that would help such as the off-heap zk session pinging or getting distributed log replay so we can enable it as default, work to make read/writes during WAL replay faster (more slots), bringing regions online for writes immediately, more aggressive cleanup of outstanding WALs so less to split on crash, etc.&lt;/p&gt;

&lt;p&gt;Good on you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15015693" author="lars_francke" created="Fri, 20 Nov 2015 11:52:13 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12556729">HBASE-6060</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12545365">HADOOP-8144</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12596218">ZOOKEEPER-1495</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12554429">HBASE-5970</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12624557">HBASE-7386</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12445215">HBASE-2108</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12520603">HDFS-2296</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12473248">HBASE-2958</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12504853">HBASE-3809</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599111">HBASE-6401</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12558826">HBASE-6140</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12639658">HBASE-8216</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12598038">HBASE-6356</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12518226">HBASE-4177</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12624658">HBASE-7390</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12597472">HBASE-6328</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12624864">HBASE-7407</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12559584">HBASE-6175</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12458957">HBASE-2315</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12461037">HDFS-1075</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12461798">HDFS-1094</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599840">HDFS-3706</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12479384">ZOOKEEPER-922</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12601090">HBASE-6490</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12422389">HBASE-1316</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12455267">HBASE-2183</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12601413">HBASE-6508</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12639370">HDFS-4642</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12517954">ZOOKEEPER-1147</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599835">HDFS-3702</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12558546">HBASE-6134</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12596502">HBASE-6295</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599836">HDFS-3703</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12458994">ZOOKEEPER-702</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12411985">HBASE-1111</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12606932">HDFS-3912</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12611113">HBASE-6970</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12635155">HBASE-7989</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12597239">HBASE-6315</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12631762">HBASE-7815</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12606599">HBASE-6737</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12606597">HBASE-6736</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12609042">HBASE-6878</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12606604">HBASE-6738</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12598225">HBASE-6364</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12605960">HBASE-6713</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12618740">HBASE-7271</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12627933">HBASE-7590</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12630675">HBASE-7756</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12639425">HBASE-8204</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12554964">HBASE-5992</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12559157">HBASE-6156</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12553341">HBASE-5902</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12530597">HBASE-4755</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12612336">HBASE-7006</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12532186">HDFS-2576</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12596847">HBASE-6309</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12606879">HBASE-6751</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12606885">HBASE-6752</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12607449">HBASE-6772</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12607467">HBASE-6774</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12618349">HBASE-7246</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12623482">HBASE-7334</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12607465">HBASE-6773</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12623419">HBASE-7327</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599839">HDFS-3705</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12552157">HBASE-5859</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12556513">HBASE-6058</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12596276">HBASE-6290</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12608816">HBASE-6870</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12617472">HBASE-7213</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12551767">HBASE-5844</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12553730">HBASE-5924</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12553819">HBASE-5930</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599648">HBASE-6435</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12607626">HBASE-6783</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12618352">HBASE-7247</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12643830">HDFS-4721</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12552710">HBASE-5877</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12553765">HBASE-5926</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12553960">HBASE-5939</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12555112">HBASE-5998</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12558216">HBASE-6109</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12644597">HDFS-4754</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="12411985">HBASE-1111</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 20 Jun 2012 17:46:08 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>236573</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i035kn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>16336</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>