<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:56:59 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1861/HBASE-1861.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1861] Multi-Family support for bulk upload tools (HFileOutputFormat / loadtable.rb)</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1861</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Add multi-family support to bulk upload tools from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-48&quot; title=&quot;[hbase] Bulk load tools&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-48&quot;&gt;&lt;del&gt;HBASE-48&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12436435">HBASE-1861</key>
            <summary>Multi-Family support for bulk upload tools (HFileOutputFormat / loadtable.rb)</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nspiegelberg">Nicolas Spiegelberg</assignee>
                                    <reporter username="streamy">Jonathan Gray</reporter>
                        <labels>
                    </labels>
                <created>Wed, 23 Sep 2009 17:14:53 +0000</created>
                <updated>Fri, 20 Nov 2015 13:02:09 +0000</updated>
                            <resolved>Wed, 8 Dec 2010 17:19:45 +0000</resolved>
                                    <version>0.20.0</version>
                                    <fixVersion>0.92.0</fixVersion>
                                    <component>mapreduce</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>12</watches>
                                                                <comments>
                            <comment id="12792011" author="ikons" created="Thu, 17 Dec 2009 17:11:49 +0000"  >&lt;p&gt;Is anyone working on this?&lt;/p&gt;</comment>
                            <comment id="12792031" author="stack" created="Thu, 17 Dec 2009 18:01:23 +0000"  >&lt;p&gt;Not to my knowledge.   Thinking on it, this case is a little tougher than the single family case. &lt;/p&gt;

&lt;p&gt;1. In single family case, we just write single files and read the file metadata to create region (We extract from the file its start and end rows and use these conjuring the region description).  In the multiple family case, somehow you&apos;ll have to tie all files in a region together &amp;#8211; perhaps in metadata or with a file suffix or prefix.  I was thinking that you&apos;d keep a running tab on the size of the file in each family and then as soon as any one file went over the region maximum file size limit, you&apos;d rotate all files.&lt;br/&gt;
2. The loadtables.rb script would need to change to read across all files in a region to find the least first row and the maximum last row by looking at all file metadatas. &lt;/p&gt;

&lt;p&gt;If you want to discuss this issue more, put up some questions and I&apos;ll have a stab at them.  Thanks.&lt;/p&gt;</comment>
                            <comment id="12798375" author="ikons" created="Sat, 9 Jan 2010 16:30:12 +0000"  >&lt;p&gt;Hi again. One thing I noticed during bulk upload (of a single column family) is a bug in the following scenario (correct me if this is not the case): &lt;br/&gt;
I have a mapper that reads input and emmits KeyValue objects to be fed in the KeyValueSortReducer. The mapper emmits a number of KeyValue objects for each row. For the same rowid, the KeyValue objects have different columnids. &lt;br/&gt;
The problem is the following: when these KeyValue objects (that have the same rowid but different colids in the same column family) reach the reducer, the TreeSet used to sort KeyValues, keeps only the KeyValue that gets last (it replaces all entries with the last one that reaches the reducer), as the KeyValue.COMPARATOR compares only the rowid !!!!!&lt;br/&gt;
Can I use a different Comparator??? KeyValue objects of the same rowid must be sorted before writing them in the Hfile, or this does not matter???&lt;/p&gt;</comment>
                            <comment id="12895902" author="lars_francke" created="Fri, 6 Aug 2010 00:58:36 +0000"  >&lt;p&gt;I have taken a stab at it. This is what I did:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Currently once it is decided that a HFile becomes too large it is closed and a new one is open. This doesn&apos;t work anymore because there may still be KeyValues for the current row in other column families coming. So now I just set a flag that a HFile rotation is needed. On every write this flag is tested and when it is true and the row key changes I close all currently open HFiles
	&lt;ul&gt;
		&lt;li&gt;This gets slightly more complicated due to the fact that we only &lt;em&gt;close&lt;/em&gt; the HFiles but don&apos;t open new ones here because they may not be needed. So a check is still required on every write if we need to open a new HFile&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;As we later need to know which files belong together to a region I save them using the current task attempt id and a counter to guarantee their uniqueness&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The current tests all run with my changes which is a good sign.&lt;/p&gt;

&lt;p&gt;The second part is the loading of those files which seems to be more complicated and which could use some comments. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1923&quot; title=&quot;Bulk incremental load into an existing table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1923&quot;&gt;&lt;del&gt;HBASE-1923&lt;/del&gt;&lt;/a&gt; recently made this more complicated and I&apos;m not sure I fully understand. Basically these are the changes required:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;To create a new region we now have to look for the start- and endkey in all column families&lt;/li&gt;
	&lt;li&gt;We have to load all the column families HFiles for a single region, those might be different between regions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To make both steps easier I could write an additional metadata file during HFileOutputFormat which contains the start- and endkeys as well as all the column families that have HFiles for this region. This data is available during creation.&lt;/p&gt;

&lt;p&gt;So any input on how this would affect/be affected by the incremental stuff would be appreciated.&lt;/p&gt;</comment>
                            <comment id="12896082" author="lars_francke" created="Fri, 6 Aug 2010 16:48:39 +0000"  >&lt;p&gt;Okay after having talked with Lars George and looking over the incremental load stuff from Todd I&apos;ve got even more questions.&lt;br/&gt;
It seems as if - and we should really document this somewhere - there are now two distinct ways to bulk load stuff into HBase:&lt;/p&gt;

&lt;p&gt;loadtable.rb creates regions manually and just creates the metadata to be picked up by the metascanner. This seems like it is not very resource intensive (after the HFiles have been generated).&lt;/p&gt;

&lt;p&gt;And then there&apos;s the new completebulkload tool which shifts some of the load to HBase itself by (and please correct me if I understood this wrong) possibly splitting a lot of the existing regions and basically depending on HBase to put HFiles in appropriate places. This is a great solution for incremental loads as regions already exist. But is this a good solution performance/load wise for an empty table? My knowledge of HBase in this regard is still limited but I would have thought that the constant splitting would be pretty bad especially when starting with an empty table with no regions.&lt;/p&gt;

&lt;p&gt;I&apos;d love your input on how to solve this: multi column families only for empty tables supported by loadtable.rb or only for the incremental bulk load tool or for both?&lt;br/&gt;
This also includes the question if we should keep loadtable.rb if it is a better fit for &quot;cold imports&quot;.&lt;/p&gt;</comment>
                            <comment id="12908157" author="vidhyash" created="Fri, 10 Sep 2010 19:24:30 +0000"  >&lt;p&gt;Lars, &lt;br/&gt;
   Are you working on this issue: in particular, are you working on the completebulkload (incremental load) tool? I thought I will take a shot at it: Our group needs this tool to be up and working quite soon.. And Stack pointed me to this jira..&lt;/p&gt;

&lt;p&gt;Vidhya&lt;/p&gt;</comment>
                            <comment id="12908187" author="lars_francke" created="Fri, 10 Sep 2010 20:55:50 +0000"  >&lt;p&gt;Yes and yes but I wouldn&apos;t mind if you took over. Thank you! I&apos;ll attach a patch of what I have (it is unfinished and untested but perhaps it helps anyway) in a second.&lt;/p&gt;</comment>
                            <comment id="12918221" author="streamy" created="Tue, 5 Oct 2010 21:39:55 +0000"  >&lt;p&gt;Punting to 0.92&lt;/p&gt;</comment>
                            <comment id="12932794" author="nspiegelberg" created="Wed, 17 Nov 2010 03:09:48 +0000"  >&lt;p&gt;IRC chat with Todd about this issue.  I figured that areas I was confused about, others might be as well.  Highlights:&lt;/p&gt;

&lt;p&gt;1. Multi-family support is almost done, just need someone to verify (yay, me)&lt;br/&gt;
2. Remember that this code handles new table + import code&lt;br/&gt;
3. HLog rolling (for HFileOutputFormat) only makes sense in the new table case&lt;br/&gt;
3. splits are accounted for in LoadIncrementalHFiles,&lt;/p&gt;

&lt;p&gt;--------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:07pm&amp;#93;&lt;/span&gt; nspiegelberg: so, I&apos;m trying to understand why multiple CF doesn&apos;t work already&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:08pm&amp;#93;&lt;/span&gt; nspiegelberg: Also, since we&apos;re pre-splitting, I&apos;m trying to find all the code where bulk importing tries to split for you&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:08pm&amp;#93;&lt;/span&gt; tlipcon: i don&apos;t think it&apos;s that far off from working&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:08pm&amp;#93;&lt;/span&gt; tlipcon: just needs a little &quot;accounting&quot; to make sure that the hfiles all split at the same boundaries&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:08pm&amp;#93;&lt;/span&gt; tlipcon: I think it&apos;s just in LoadIncrementalHFiles&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:08pm&amp;#93;&lt;/span&gt; tlipcon: HFOF itself will split on max.region.size boundaries I think&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:09pm&amp;#93;&lt;/span&gt; nspiegelberg: from what I can tell, you&apos;re basically worried about when to roll HFiles and to make sure you don&apos;t roll them on the same Row&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:09pm&amp;#93;&lt;/span&gt; tlipcon: right&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:09pm&amp;#93;&lt;/span&gt; tlipcon: rolling HFiles is basically important when you underestimate the number of reducers you should be making&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:10pm&amp;#93;&lt;/span&gt; nspiegelberg: don&apos;t you do 1 reducer/region?&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:10pm&amp;#93;&lt;/span&gt; nspiegelberg: if there&apos;s no edits to a region, then that reducer is idle&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:11pm&amp;#93;&lt;/span&gt; tlipcon: you&apos;re only thinking about pre-created case&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:11pm&amp;#93;&lt;/span&gt; tlipcon: but HFOF also works for the new table case&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:11pm&amp;#93;&lt;/span&gt; tlipcon: and with reducer skew in that case, you&apos;d prefer one reducer to maybe make two regions&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:12pm&amp;#93;&lt;/span&gt; nspiegelberg: wouldn&apos;t that be accomplished by doing a split in LoadIncrementalHFiles?&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:12pm&amp;#93;&lt;/span&gt; tlipcon: yea, but that split is very slow&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:12pm&amp;#93;&lt;/span&gt; tlipcon: it&apos;s a physical split&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:12pm&amp;#93;&lt;/span&gt; tlipcon: it&apos;s only there to take care of the case where you&apos;ve got some splits in between running the MR and loading the hfiles&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:13pm&amp;#93;&lt;/span&gt; nspiegelberg: k.  it sounds like my life is simplified by pre-split regions&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:13pm&amp;#93;&lt;/span&gt; nspiegelberg: just need to not mess up split case&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:25pm&amp;#93;&lt;/span&gt; nspiegelberg: hey, I am still a little fuzzy on how you&apos;re handling the case where a split happens between configureIncrementalLoad() and bulkLoadHFile()&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:26pm&amp;#93;&lt;/span&gt; tlipcon: nspiegelberg: the completebulkload (LoadIncrementalHFiles) deals with it&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:26pm&amp;#93;&lt;/span&gt; tlipcon: it&apos;s ugly, it physically splits the hfile on the new boundary&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:26pm&amp;#93;&lt;/span&gt; tlipcon: and adds the new ones to a queue&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:28pm&amp;#93;&lt;/span&gt; nspiegelberg: so... if splitting doesn&apos;t happen until LoadIncrementalHFiles, why do you need to worry that you&apos;ve hit a new row when you roll HFiles?&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:29pm&amp;#93;&lt;/span&gt; nspiegelberg: it only makes sense to not roll HFiles until the next row when you want to use that as a split point&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:30pm&amp;#93;&lt;/span&gt; tlipcon: it&apos;s for new tables&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:30pm&amp;#93;&lt;/span&gt; tlipcon: for the actual incremental case, the rolling done by HFOF doesn&apos;t really buy you anything&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:30pm&amp;#93;&lt;/span&gt; tlipcon: except that it minimizes the amount of work&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:30pm&amp;#93;&lt;/span&gt; tlipcon: but for a new table, you might want 10 reducers, but maybe you have skew, so one of the reducers gets 5x as much data as the others&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:31pm&amp;#93;&lt;/span&gt; tlipcon: it should still make regions that fit your region size&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:31pm&amp;#93;&lt;/span&gt; nspiegelberg: well, in our case, we need to add a threshold to PutSortReducer so we don&apos;t try to put too many entries in an in-memory Map &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:31pm&amp;#93;&lt;/span&gt; nspiegelberg: so rolling the hfiles does make sense&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:35pm&amp;#93;&lt;/span&gt; tlipcon: recall that reducer only runs per row&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:35pm&amp;#93;&lt;/span&gt; tlipcon: it doesn&apos;t shove multiple rowsin the map&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:35pm&amp;#93;&lt;/span&gt; tlipcon: just multiple columns&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:36pm&amp;#93;&lt;/span&gt; nspiegelberg: the Iterable&amp;lt;Put&amp;gt; is not a stream?&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:36pm&amp;#93;&lt;/span&gt; &#8226; nspiegelberg is a MR n00b&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;6:36pm&amp;#93;&lt;/span&gt; tlipcon: it is streamed, but all those puts are for the same row&lt;/p&gt;</comment>
                            <comment id="12969086" author="hbasereviewboard" created="Wed, 8 Dec 2010 00:10:01 +0000"  >&lt;p&gt;Message from: &quot;Nicolas&quot; &amp;lt;nspiegelberg@facebook.com&amp;gt;&lt;/p&gt;

&lt;p&gt;-----------------------------------------------------------&lt;br/&gt;
This is an automatically generated e-mail. To reply, visit:&lt;br/&gt;
&lt;a href=&quot;http://review.cloudera.org/r/1272/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/1272/&lt;/a&gt;&lt;br/&gt;
-----------------------------------------------------------&lt;/p&gt;

&lt;p&gt;Review request for hbase.&lt;/p&gt;


&lt;p&gt;Summary&lt;br/&gt;
-------&lt;/p&gt;

&lt;p&gt;support writing to multiple column families for HFileOutputFormat.  also, added a max threshold for PutSortReducer because we had some pathological row cases.&lt;/p&gt;


&lt;p&gt;This addresses bug &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1861&quot; title=&quot;Multi-Family support for bulk upload tools (HFileOutputFormat / loadtable.rb)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1861&quot;&gt;&lt;del&gt;HBASE-1861&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
    &lt;a href=&quot;http://issues.apache.org/jira/browse/HBASE-1861&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/browse/HBASE-1861&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Diffs&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;  src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java 8ccdf4d &lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/mapreduce/PutSortReducer.java 5fb3e83 &lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java c5d56cc &lt;/p&gt;

&lt;p&gt;Diff: &lt;a href=&quot;http://review.cloudera.org/r/1272/diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/1272/diff&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Testing&lt;br/&gt;
-------&lt;/p&gt;

&lt;p&gt;mvn test -Dtest=ThestHFileOutputFormat&lt;br/&gt;
internal MR testing&lt;/p&gt;


&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Nicolas&lt;/p&gt;

</comment>
                            <comment id="12969118" author="hbasereviewboard" created="Wed, 8 Dec 2010 01:16:01 +0000"  >&lt;p&gt;Message from: stack@duboce.net&lt;/p&gt;

&lt;p&gt;-----------------------------------------------------------&lt;br/&gt;
This is an automatically generated e-mail. To reply, visit:&lt;br/&gt;
&lt;a href=&quot;http://review.cloudera.org/r/1272/#review2044&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/1272/#review2044&lt;/a&gt;&lt;br/&gt;
-----------------------------------------------------------&lt;/p&gt;

&lt;p&gt;Ship it!&lt;/p&gt;


&lt;p&gt;+1  Excellent.&lt;/p&gt;


&lt;p&gt;src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java&lt;br/&gt;
&amp;lt;&lt;a href=&quot;http://review.cloudera.org/r/1272/#comment6448&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/1272/#comment6448&lt;/a&gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;    Should this behavior be documented in method javadoc?&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;stack&lt;/li&gt;
&lt;/ul&gt;



</comment>
                            <comment id="12969123" author="hbasereviewboard" created="Wed, 8 Dec 2010 01:22:01 +0000"  >&lt;p&gt;Message from: &quot;Nicolas&quot; &amp;lt;nspiegelberg@facebook.com&amp;gt;&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;On 2010-12-07 17:13:55, stack wrote:&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;&amp;gt; src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java, line 93&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;&amp;gt; &amp;lt;&lt;a href=&quot;http://review.cloudera.org/r/1272/diff/1/?file=17977#file17977line93&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/1272/diff/1/?file=17977#file17977line93&lt;/a&gt;&amp;gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;&amp;gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;&amp;gt;     Should this behavior be documented in method javadoc?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;will do&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Nicolas&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;-----------------------------------------------------------&lt;br/&gt;
This is an automatically generated e-mail. To reply, visit:&lt;br/&gt;
&lt;a href=&quot;http://review.cloudera.org/r/1272/#review2044&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/1272/#review2044&lt;/a&gt;&lt;br/&gt;
-----------------------------------------------------------&lt;/p&gt;


</comment>
                            <comment id="12969379" author="stack" created="Wed, 8 Dec 2010 17:13:43 +0000"  >&lt;p&gt;@Nicolas Mother Hen is here.  You want to resolve this issue now you&apos;ve committed the patch?  Also, I did not see an addition made to CHANGES.txt on commit.  Did you mean to add one (On commit and resolve, we&apos;ll add an entry to the CHANGES.txt file that is under the hbase checkout dir).  Pardon me if I&apos;m telling you stuff you know already.  Good on you.&lt;/p&gt;</comment>
                            <comment id="12969381" author="nspiegelberg" created="Wed, 8 Dec 2010 17:16:40 +0000"  >&lt;p&gt;@stack : That stuff was already done for me on previous patches I committed and we&apos;re lazier about CHANGES.txt internally, so I&apos;ll get that corrected and remember to double-check next time &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12969383" author="stack" created="Wed, 8 Dec 2010 17:22:20 +0000"  >&lt;p&gt;@Nicolas nm on CHANGES.txt change.  I see it now.  Pardon Mother Hen for fussing.&lt;/p&gt;</comment>
                            <comment id="12997363" author="clehene" created="Mon, 21 Feb 2011 11:54:13 +0000"  >&lt;p&gt;How hard would it be to adapt this for 0.90 branch?&lt;/p&gt;</comment>
                            <comment id="12997633" author="nspiegelberg" created="Tue, 22 Feb 2011 03:26:00 +0000"  >&lt;p&gt;@Cosmin.  This should apply cleanly to 0.90.  I was just taking the path of least resistance until someone needed it in 0.90&lt;/p&gt;</comment>
                            <comment id="13015259" author="kntreadway" created="Mon, 4 Apr 2011 02:39:11 +0000"  >&lt;p&gt;I&apos;ve been testing this in 0.90.2. I found that some reduce tasks fail with a File Not Found exception if there are no keys in the input data which would fall into the region assigned to that reduce task. &lt;/p&gt;

&lt;p&gt;From what I can determine, it seems that an output directory is created in the write() method and expected to exist in the writeMetaData() method...if there are no keys to be written for that reduce task, the write method is never called, but writeMetaData is expecting the output directory to exist...thus the FnF exception.&lt;/p&gt;

&lt;p&gt;Simply checking if the file exists should fix the issue. I&apos;ve patched this on my end...should I post my fix here?&lt;/p&gt;</comment>
                            <comment id="13015517" author="tlipcon" created="Mon, 4 Apr 2011 17:56:49 +0000"  >&lt;p&gt;Hi Nichole. I think it&apos;s probably best to open a new JIRA for the bugfix since this one has been closed for a while.&lt;/p&gt;</comment>
                            <comment id="13019957" author="stack" created="Thu, 14 Apr 2011 18:15:00 +0000"  >&lt;p&gt;Nichole added &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3782&quot; title=&quot;Multi-Family support for bulk upload tools causes File Not Found Exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3782&quot;&gt;&lt;del&gt;HBASE-3782&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13030223" author="hudson" created="Sat, 7 May 2011 00:08:31 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #1909 (See &lt;a href=&quot;https://builds.apache.org/hudson/job/HBase-TRUNK/1909/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/hudson/job/HBase-TRUNK/1909/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3864&quot; title=&quot;Rename of hfile.min.blocksize.size in HBASE-2899 reverted in HBASE-1861&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3864&quot;&gt;&lt;del&gt;HBASE-3864&lt;/del&gt;&lt;/a&gt; Rename of hfile.min.blocksize.size in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2899&quot; title=&quot;hfile.min.blocksize.size ignored/documentation wrong&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2899&quot;&gt;&lt;del&gt;HBASE-2899&lt;/del&gt;&lt;/a&gt; reverted in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1861&quot; title=&quot;Multi-Family support for bulk upload tools (HFileOutputFormat / loadtable.rb)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1861&quot;&gt;&lt;del&gt;HBASE-1861&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15017996" author="lars_francke" created="Fri, 20 Nov 2015 13:02:09 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12504277">HBASE-3782</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12438532">HBASE-1923</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12380704">HBASE-48</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12454331" name="HBASE1861-incomplete.patch" size="23493" author="lars_francke" created="Fri, 10 Sep 2010 21:07:25 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 17 Dec 2009 17:11:49 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32293</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i05idz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>30076</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>