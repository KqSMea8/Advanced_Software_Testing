<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:23:13 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-4925/HBASE-4925.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-4925] Collect test cases for hadoop/hbase cluster</title>
                <link>https://issues.apache.org/jira/browse/HBASE-4925</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This entry is used to collect all the useful test cases to verify a hadoop/hbase cluster. This is to follow up on yesterday&apos;s hack day in Salesforce. Hopefully that the information would be very useful for the whole community.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12533403">HBASE-4925</key>
            <summary>Collect test cases for hadoop/hbase cluster</summary>
                <type id="13" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Brainstorming</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="thomaspan">Thomas Pan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Dec 2011 03:58:07 +0000</created>
                <updated>Sat, 11 Apr 2015 01:13:49 +0000</updated>
                            <resolved>Sat, 11 Apr 2015 01:13:49 +0000</resolved>
                                                                    <component>test</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="13160635" author="thomaspan" created="Thu, 1 Dec 2011 04:00:33 +0000"  >&lt;p&gt;Here is the list of fault injection test cases that we&apos;ve collected:&lt;br/&gt;
1. Kill -9 one region server and kill -9 the region server that serves the .META. table &lt;br/&gt;
2. While BES is writing data to HBase table, kill -9 the region server that holds the .META. table &lt;br/&gt;
3. Kill &lt;del&gt;9 the region server that serves the .META. table. Then, kill -9 the region server that serves the -ROOT&lt;/del&gt; table. &lt;span class=&quot;error&quot;&gt;&amp;#91;Thomas: Is it the case in our environment?&amp;#93;&lt;/span&gt; &lt;br/&gt;
4. A large number of region servers get killed. After restoration, there is no data loss. &lt;br/&gt;
5. No job impact while shifting from the primary HBase master to the secondary HBase master. &lt;br/&gt;
6. Shift from the primary HBase master to the secondary HBase master after multiple region servers fail. &lt;br/&gt;
7. Shift from the primary HBase master to the secondary HBase master after new region servers are added. &lt;br/&gt;
8. Repeatedly stop and restart the primary HBase master. There should be no major impact as the secondary HBase master kicks in automatically. &lt;br/&gt;
9. Shift from the primary HBase master to the secondary HBase master while a table is creating with 3600 regions. &lt;br/&gt;
10. Disable network access for the node running the region server that serves the .META. table &lt;br/&gt;
11. Disable network access for the node running the primary HBase master &lt;br/&gt;
12. Disable network access for the node running the secondary HBase master &lt;br/&gt;
13. Trigger short-lived network interruption for the node running the region server that serves the .META. table &lt;br/&gt;
14. Trigger short-lived network interruption for the node running the primary HBase master &lt;br/&gt;
15. Trigger short-lived network interruption for the node running the secondary HBase master &lt;br/&gt;
16. While BES is writing to a table heavily with high CPU usage in the cluster. &lt;br/&gt;
17. Restart one RS with high CPU usage in the cluster. &lt;br/&gt;
18. Offline data nodes with high CPU usage in the cluster. &lt;br/&gt;
19. While BES is writing to a table heavily with high memory usage in the cluster. &lt;br/&gt;
20. Restart one RS with high memory usage in the cluster. &lt;br/&gt;
21. Offline data nodes with high memory usage in the cluster. &lt;br/&gt;
22. With no load in the cluster, test failover of the primary NN to the secondary NN &lt;br/&gt;
23. With jobs running in the cluster, test failover of the primary NN to the secondary NN &lt;br/&gt;
24. Repeatedly stop and restart the primary NN to make sure that the NN failover works fine &lt;br/&gt;
25. Kill -9 the primary zookeeper. The failover to the second NN should be in time with no job failure. &lt;br/&gt;
26. Kill -9 the primary zookeeper and the primary NN, the cluster should quickly fail over to the secondary ZK and NN &lt;br/&gt;
27. Restart the node that holds the primary NN &lt;br/&gt;
28. Disable network access for the node running the primary NN &lt;br/&gt;
29. Trigger short-lived network interruption for the node running the primary NN &lt;br/&gt;
30. Disable network access for the node running the primary ZK &lt;br/&gt;
31. Trigger short-lived network interruption for the node running the primary ZK &lt;br/&gt;
32. Disable network access for the node running ZK in follower state &lt;br/&gt;
33. Trigger short-lived network interruption for the node running ZK in follower state &lt;br/&gt;
34. Offline multiple data nodes at once. Keep them offline for a while. &lt;br/&gt;
35. Offline multiple data nodes at once. Keep them offline for a while. Put them back at once. &lt;br/&gt;
37. Offline multiple data nodes at once. Put them back at once, instantly. &lt;br/&gt;
38. Offline a data node at once. Keep it offline for a while. &lt;br/&gt;
39. Offline a data node at once. Keep it offline for a while. Put it back at once. &lt;br/&gt;
40. Offline a data node at once. Put it back at once, instantly. &lt;br/&gt;
41. Hard disk failure in the primary NN triggers NN failover. &lt;br/&gt;
42. The directory dfs.data.dir on data node gets corrupted &lt;br/&gt;
43. Corrupted dfs.name.dir on the primary NN gets detected and triggers NN failover. &lt;br/&gt;
44. Corrupted dfs.name.dir on the secondary NN gets detected. &lt;br/&gt;
45. A data node runs out of disk space. &lt;br/&gt;
46. Under heavy IO on data nodes, BES writes to a table heavily. &lt;br/&gt;
47. Under heavy IO on data nodes, offline multiple data nodes.&lt;/p&gt;</comment>
                            <comment id="13161460" author="stack" created="Fri, 2 Dec 2011 07:06:05 +0000"  >&lt;p&gt;That is a pretty nice list Thomas. Thanks.  You have a means of running these scenarios now or you are waiting on framework in which you/we could write them?&lt;/p&gt;</comment>
                            <comment id="13161466" author="thomaspan" created="Fri, 2 Dec 2011 07:19:58 +0000"  >
&lt;p&gt;Once framework is available, we will write them. Regardless, we plan to carry out these tests to mainly achieve two goals:&lt;/p&gt;

&lt;p&gt;1. To build up the experiences on how to handle various outages before production launch, which we plan to share with the community once we have more details.&lt;br/&gt;
2. To reveal more issues in the code base so that the community could fix them. Last time, Todd found more HBase bugs while helping us recover from one big outage.&lt;/p&gt;</comment>
                            <comment id="13209115" author="sunnygao" created="Thu, 16 Feb 2012 05:12:09 +0000"  >&lt;p&gt;@ Thomas&lt;/p&gt;

&lt;p&gt;Your framework is available?  We finished part cases automation , but I find it is not stable. &lt;/p&gt;</comment>
                            <comment id="13209193" author="thomaspan" created="Thu, 16 Feb 2012 07:37:17 +0000"  >&lt;p&gt;gaojinchao, I am not the person working on the framework. Stack should know better. For stability, any cluster could go down, regardless. So, we need to set up the same cluster on different data centers. In case one is down, another is still running. The beauty of HBase is that the underlining data is still available. As far as the downtime is controllable, it should be fine.&lt;/p&gt;</comment>
                            <comment id="13210670" author="stack" created="Fri, 17 Feb 2012 23:36:10 +0000"  >&lt;p&gt;@Gaojinchao What did you finish?  You have an automation that runs some of Thomas&apos;s tests?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 2 Dec 2011 07:06:05 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>219131</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 43 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02b9b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11425</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>