<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:04:03 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2700/HBASE-2700.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2700] Handle master failover for regions in transition</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2700</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;To this point in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2692&quot; title=&quot;Master rewrite and cleanup for 0.90&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2692&quot;&gt;&lt;del&gt;HBASE-2692&lt;/del&gt;&lt;/a&gt; tasks we have moved everything for regions in transition into ZK, but we have not fully handled the master failover case.  This is to deal with that and to write tests for it.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12466501">HBASE-2700</key>
            <summary>Handle master failover for regions in transition</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12466475">HBASE-2692</parent>
                                    <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="streamy">Jonathan Gray</assignee>
                                    <reporter username="streamy">Jonathan Gray</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Jun 2010 21:46:05 +0000</created>
                <updated>Fri, 20 Nov 2015 12:40:56 +0000</updated>
                            <resolved>Tue, 19 Oct 2010 00:09:07 +0000</resolved>
                                                    <fixVersion>0.90.0</fixVersion>
                                    <component>master</component>
                    <component>Zookeeper</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12881673" author="streamy" created="Wed, 23 Jun 2010 12:54:18 +0000"  >&lt;p&gt;1. Master knows he is a failed-over master.&lt;br/&gt;
2. Determine if any RegionServers crashed by diffing ephemeral list with master-managed list.&lt;br/&gt;
   If so, process dead servers.&lt;br/&gt;
3. Get list of nodes in UNASSIGNED&lt;br/&gt;
   (CLOSING)    -&amp;gt; Wait.  This should enter CLOSED eventually.&lt;br/&gt;
					If timeout, deal with in same way we would deal with timeout w/o failover&lt;br/&gt;
   (CLOSED)		-&amp;gt; Generate a destination RS.  Send that RS an open message.&lt;br/&gt;
					The RS who gets the open message will only open if he can switch it from&lt;br/&gt;
					CLOSED to OPENING.  This ensures an open only occurs in one place.&lt;br/&gt;
   (OPENING)	-&amp;gt; Wait.  This should enter OPENED eventually.&lt;br/&gt;
					If timeout, deal with in same way we would deal with timeout w/o failover&lt;br/&gt;
   (OPENED)		-&amp;gt; Remove the znode, this region is not in transition any more.&lt;/p&gt;</comment>
                            <comment id="12881768" author="stack" created="Wed, 23 Jun 2010 17:21:24 +0000"  >&lt;p&gt;What is the list above Jon?   Whats context?  Thats what the patch you&apos;ll add here wilil do?&lt;/p&gt;</comment>
                            <comment id="12881780" author="stack" created="Wed, 23 Jun 2010 17:37:44 +0000"  >&lt;p&gt;Here is the pertinent paragraph from the BT table:&lt;/p&gt;

&lt;p&gt;&quot;When a master is started by the cluster management system, it needs to discover the current tablet assign- ments before it can change them. The master executes the following steps at startup. (1) The master grabs a unique master lock in Chubby, which prevents con- current master instantiations. (2) The master scans the servers directory in Chubby to find the live servers. (3) The master communicates with every live tablet server to discover what tablets are already assigned to each server. (4) The master scans the METADATA table to learn the set of tablets. Whenever this scan encounters a tablet that is not already assigned, the master adds the tablet to the set of unassigned tablets, which makes the tablet eligible for tablet assignment.&quot;&lt;/p&gt;

&lt;p&gt;Any reason, we are doing otherwise; e.g. Step 3, it asks the RS what its carrying rather than ask Chubby and Step 4, scans .META. to learn list of regions rather than scan Chubby?&lt;/p&gt;</comment>
                            <comment id="12882032" author="streamy" created="Thu, 24 Jun 2010 04:28:19 +0000"  >&lt;p&gt;Context of above is just a quick first go at what a master will do when it is a failed-over-to master.&lt;/p&gt;

&lt;p&gt;The ZK-based region transitions definitely break with the BT paper.  This direction came out of discussions about the best way to persist master state over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2485&quot; title=&quot;Persist Master in-memory state so on restart or failover, new instance can pick up where the old left off&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2485&quot;&gt;&lt;del&gt;HBASE-2485&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&apos;m generally very in favor of moving the master away from the basescanner/heartbeat way of doing things towards zk and rpc.  Assignment is untenable in the current style and master failover does not currently work.  This design makes this stuff a lot easier imo, not to mention faster.&lt;/p&gt;

&lt;p&gt;We can address specifics as patches come in, but in general is there opposition to the design of using zk for regions in transition and assignment rather than the basescanner and heartbeats?&lt;/p&gt;</comment>
                            <comment id="12882051" author="stack" created="Thu, 24 Jun 2010 05:33:35 +0000"  >&lt;p&gt;I&apos;m not sure zk-based assignment breaks w/ BT paper... how region assignment is done is not discussed (I think).  What does differ is the way that you are going about the onlining of a new server.  In BT, it asks regionservers what they are hosting and then diffs against content of .META.   Regions in transitions would be the diff between what an RS is hosting and the list in .META.?  An RPC to each server and then a quick scan of .META.?&lt;/p&gt;</comment>
                            <comment id="12882169" author="streamy" created="Thu, 24 Jun 2010 14:44:54 +0000"  >&lt;p&gt;During master failover, the most important thing is to determine whether there are regions which need actions to be taken by the master to get back to normal operation (for example, they were being moved for load balancing but master only issued the closeRegion not the openRegion yet).  Anything the master would need to cleanup immediately would be regions in transitions.  That information would be available immediately via ZK with this new design.  That seems superior to requiring each RS to tell you everything it holds and diff&apos;ing that against a full META scan which could take some time and is more complex.&lt;/p&gt;

&lt;p&gt;The other thing we&apos;d need to know is if any RS had failed at a time when no master was around to process its termination (by reassigning its regions to others).  For this, we propose a second list of online RS that is maintained by the master (in addition to the ephemeral node put up by each RS).  When a master actually processes a shutdown or onlining it would update its list.  A new master would diff these two lists of znodes to determine if anything changed during failover.&lt;/p&gt;

&lt;p&gt;With the zk-based assignment we would no longer need to do this metascan/rpc to each server.  To me that seems rather desirable and the zk approach is faster.  That recovery style described in BT also requires holding all region and assignment information in memory?  There has been some discussion around whether we want to go that way or not but for large clusters it could get significant.  Even if we do go that way I still think using the regions in transition out of zk is a better way to ensure cluster sanity when a failed over master starts up.&lt;/p&gt;</comment>
                            <comment id="12882245" author="stack" created="Thu, 24 Jun 2010 17:54:41 +0000"  >&lt;p&gt;The data in zk is proxy for actual state.  It&apos;ll likely have holes, misreporting whats actually out on the RS.  Going to the source to ask it what it actually has would seem less error-prone (less complex), no?&lt;/p&gt;

&lt;p&gt;Regards a second list of regionservers up in zk, could you not here also look at .META.?  It&apos;s server cell has hostnames.  Could compare this list to whats up in zk as live RSs and then w/ the difference, go poke around in filesystem to find logs to split?&lt;/p&gt;

&lt;p&gt;My mine concern is that there seems to have been no consideration of how the bible addresses this issue, if only a &quot;we looked at it and discarded it because of 1, 2, and 3&quot;.  The zk approach sounds like it could be &apos;faster&apos; but being a representation of state, there is the possibility that its representation may diverge from actual state.&lt;/p&gt;

&lt;p&gt;As to keeping all in memory in master, reading the BT paper, I&apos;m not sure if that is what is going on.  It would seem to indicate so in the sentence where it describes how master learns of dropped splits but as you say, for big clusters this  may prove untenable.&lt;/p&gt;
</comment>
                            <comment id="12882262" author="streamy" created="Thu, 24 Jun 2010 18:16:04 +0000"  >&lt;p&gt;In what situation does the data in ZK not have the actual state?  In order for a RS to, for example, open a region, it must transition a node in ZK from nothing, to OPENING, to OPENED; if this fails it does not open.  It seems to me that it is META which may not be up to date and META which can change without the proper notifications being sent.&lt;/p&gt;

&lt;p&gt;In style where we ask RS what they host and match that up against META, we then must do all edits of META on master side.  Otherwise there will always be race conditions between what master thinks is the state (via meta scan) and what the actual state is (via RS setting stuff in meta).  ZK allows us to ensure we never miss states and transitions.&lt;/p&gt;

&lt;p&gt;For second list of RS up in ZK, we could get this data in META but what about case where a RS died while something was getting assigned to it but it did not finish opening and died?  Whether this is a problem or not depends very much on who is the one who edits meta, whether we rely on meta to determine something is not assigned, etc...&lt;/p&gt;

&lt;p&gt;There has been consideration as to how this is handled in BT paper but I guess I just am of the mindset that the explicit, persistent message passing via ZK is a better direction than the meta scanning / per-rs check-in / heartbeating.  What happens if we have 1000 RS and 1M regions?  That&apos;s a significant amount of work to do.  What if a single RS happens to be in a 10 second GC pause?  What about race conditions between what is in META and what the RSs know about?  What if we see in META something is unassigned but the previous master asked an RS to open it?  That RS is in &quot;opening&quot; state but it is not yet assigned so would it come back with the list of assigned regions to that server?  This is super explicit via transitions in zk.&lt;/p&gt;

&lt;p&gt;As for all in memory, I think we can punt on this for a while.  The only thing pertinent to this discussion is that if holding it all in memory is possibly untenable, doesn&apos;t that mean that it&apos;s untenable to do master failover in this style (hold every RS and its R after asking it via RPC, and holding the META view of every R and the RS it is assigned to)?&lt;/p&gt;</comment>
                            <comment id="12882266" author="streamy" created="Thu, 24 Jun 2010 18:20:56 +0000"  >&lt;p&gt;My first point above I think is critical.  ZK is really not proxy for actual state.  It&apos;s not to say that it 100% mirrors state but it ensures enough state to know what you need to know, if that makes sense.&lt;/p&gt;

&lt;p&gt;For example, a M could die in the middle of load balancing.  Let&apos;s say that it sent out all the messages to RS to close regions, half of them actually finished closing and the master sent out opens to other RS for half of them.&lt;/p&gt;

&lt;p&gt;When master fails over, it figures out who is in transition.  It would see all the regions that were being balanced as either OPENING, OPENED, CLOSING, or CLOSED.  I see no possibility where any region that is not properly assigned to a server would be missing from this.&lt;/p&gt;

&lt;p&gt;We can then act upon those states.  If OPENED, well, we are done.  If we&apos;re responsible for meta updates, we would update meta and delete the node.  If OPENING, we would deal with it the same way we deal with a normal OPENING (possibly wait for some time and if nothing ever happens we try to assign elsewhere).  If CLOSING, same deal.  If CLOSED, that means the region was closed by the RS but that we are unsure whether the previous master actually did an assignment for it or not.  In this case, we generate a new assignment and assign it out to an RS.  That RS then needs to transition the CLOSED node to OPENING.  If the previous master actually had sent an open to someone, then they would also be attempting that transition from CLOSED to OPENING.  Only one will win and he will get the region.&lt;/p&gt;</comment>
                            <comment id="12882275" author="tlipcon" created="Thu, 24 Jun 2010 18:39:06 +0000"  >&lt;p&gt;Regarding keeping state in memory being untenable, what&apos;s the reasoning here? HDFS manages to keep all of the blocks in memory, and the number of blocks is WAY bigger than the number of regions. I&apos;m not saying I think we necessarily have to do it, but I don&apos;t think we should rule it out, either.&lt;/p&gt;</comment>
                            <comment id="12882280" author="streamy" created="Thu, 24 Jun 2010 18:49:39 +0000"  >&lt;p&gt;@Todd, just bringing up that it&apos;s &quot;possibly&quot; untenable.  I don&apos;t think we need to make this decision now and I&apos;m not for or against it at this point.  In case of doing failover in this style, you may have to hold 2X the assignment data (what meta says, and what each rs says).&lt;/p&gt;</comment>
                            <comment id="12882355" author="stack" created="Thu, 24 Jun 2010 21:57:40 +0000"  >&lt;p&gt;.bq In what situation does the data in ZK not have the actual state&lt;/p&gt;

&lt;p&gt;ZK is mediating the open/close of regions.  For the final word on what is actually happening on a RS, I&apos;d say asking the RS will be more reliable than asking ZK.&lt;/p&gt;

&lt;p&gt;.bq It seems to me that it is META which may not be up to date and META which can change without the proper notifications being sent. &lt;/p&gt;

&lt;p&gt;If meta doesn&apos;t have the list of regions and their locations, clients are not going to work.  Its a bug (I think we&apos;re clear on what zk is responsible for and where meta takes over).&lt;/p&gt;

&lt;p&gt;Regards your comment that Master then has to update meta rather than RS as you prefer, where do you get that from?  From the BT quote above: &quot;Whenever this scan encounters a tablet that is not already assigned, the master adds the tablet to the set of unassigned tablets, which makes the tablet eligible for tablet assignment.&quot;  Sounds like master just adds the region to the unassigned list.... which in our case is a queue up in zk.&lt;/p&gt;

&lt;p&gt;.bq ZK allows us to ensure we never miss states and transitions.&lt;/p&gt;

&lt;p&gt;I thought you said you can miss transitions in zk (Maybe you mean region transitions?).&lt;/p&gt;

&lt;p&gt;.bq For second list of RS up in ZK, we could get this data in META but what about case where a RS died while something was getting assigned to it but it did not finish opening and died? &lt;/p&gt;

&lt;p&gt;Then it&apos;ll have nothing on it and no mention in .META.... it never existed and it never took on data.  Wheres the prob?  The babysit process will restart it and hopefully second time around it&apos;ll have more luck.&lt;/p&gt;

&lt;p&gt;.bq ....persistent message passing via ZK is a better direction than the meta scanning / per-rs check-in / heartbeating. &lt;/p&gt;

&lt;p&gt;You overreach with the above.  I&apos;m talking about process master follows assuming master role.  I&apos;m not talking about heartbeating+messagepayload.  My suggestion that we consider what the BT paper does is about getting what the RS is carrying from the horse&apos;s mouth rather than from the mediator.... also, seems like we can simplify some by doing away w/ a second RS list up in ZK?&lt;/p&gt;

&lt;p&gt;.bq What if a single RS happens to be in a 10 second GC pause? &lt;/p&gt;

&lt;p&gt;Then RS will be slow to respond to master (or to zk).  Master can work on other RS reportings meantime.&lt;/p&gt;

&lt;p&gt;.bq What about race conditions between what is in META and what the RSs know about?&lt;/p&gt;

&lt;p&gt;What sort of race condiition are you thinking?  Master asks the RS for what it has.  RS could even volunteer opening/closing states if that&apos;d help (or master can just see zk for transitions &amp;#8211; might be good to have both to help understand state).  It reads .META. for list of regions and servers.  What do think could go amiss?&lt;/p&gt;

&lt;p&gt;.bq What if we see in META something is unassigned but the previous master asked an RS to open it?&lt;/p&gt;

&lt;p&gt;See above.  RS could report openings or master can check zk or both.&lt;/p&gt;

&lt;p&gt;On your second note, you have confidence that the representation of cluster state that is up in zk is always true.  I don&apos;t have the same confidence.&lt;/p&gt;











</comment>
                            <comment id="12882368" author="streamy" created="Thu, 24 Jun 2010 22:27:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;ZK is mediating the open/close of regions. For the final word on what is actually happening on a RS, I&apos;d say asking the RS will be more reliable than asking ZK.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you be more explicit here?  I see you have no confidence in ZK but going over the design multiple times I don&apos;t see where we would miss out on something by look at ZK.  Do you have any specific examples or situations where things could be out of sync or something could be missed?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If meta doesn&apos;t have the list of regions and their locations, clients are not going to work.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What I mean to say is, a master may not know that a region has been opened on an RS (has not received OPENED msg) but META would have that region assigned to the RS.  I need to think more about it but when I was thinking earlier I thought I had situation where a region would get lost if RS was one doing meta updates.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I thought you said you can miss transitions in zk (Maybe you mean region transitions?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can miss specific transitions but we&apos;ll always be able to recreate them because ZK will always contain states.  We could miss (in Master) the movement from OPENING to OPENED or CLOSING to CLOSED, but we will never be able to miss the OPENED state or CLOSED state because it is the master which does the next transition out of those.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For second list of RS up in ZK...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thinking more, I think it&apos;s fine to use meta scan to determine RS list rather than second list up in ZK.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You overreach with the above. I&apos;m talking about process master follows assuming master role. I&apos;m not talking about heartbeating+messagepayload. My suggestion that we consider what the BT paper does is about getting what the RS is carrying from the horse&apos;s mouth rather than from the mediator.... also, seems like we can simplify some by doing away w/ a second RS list up in ZK?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree we don&apos;t need second list.  But I think it&apos;s unnecessary to query every RS about what it hosts.  We will have knowledge of every potentially unassigned region in ZK which is the exact piece of information we&apos;re looking for.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;On your second note, you have confidence that the representation of cluster state that is up in zk is always true. I don&apos;t have the same confidence.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m interested in a scenario where region state up in zk is not sufficient or not representative.  Fair to question the design but we&apos;ve been mulling over bulletproofing region transitions for weeks and weeks and zk somehow not being representative seems like a deal breaker to the entire thing?&lt;/p&gt;</comment>
                            <comment id="12882384" author="stack" created="Thu, 24 Jun 2010 22:53:56 +0000"  >&lt;p&gt;We seem to be making progress on this thread.  Looks like we&apos;ve saved having to have a server list up in zk.&lt;/p&gt;

&lt;p&gt;.bq I&apos;m interested in a scenario where region state up in zk is not sufficient or not representative.&lt;/p&gt;

&lt;p&gt;I don&apos;t have specifics.  Past experience would have it that there&apos;ll likely be holes or transition modes we didn&apos;t consider.&lt;/p&gt;

&lt;p&gt;.bq I see you have no confidence in ZK....&lt;/p&gt;

&lt;p&gt;You have me wrong here.  Doing transitions via ZK has my total endorsement.  I look forward to the fat patches that move us there.&lt;/p&gt;</comment>
                            <comment id="12882387" author="streamy" created="Thu, 24 Jun 2010 22:59:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t have specifics. Past experience would have it that there&apos;ll likely be holes or transition modes we didn&apos;t consider.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Those would be implementation bugs, not design bugs.  In either case, whether we use zk just for transitions or also for failover, these bugs would need to be fixed.&lt;/p&gt;</comment>
                            <comment id="12904911" author="stack" created="Wed, 1 Sep 2010 05:12:25 +0000"  >&lt;p&gt;This should be done because of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2692&quot; title=&quot;Master rewrite and cleanup for 0.90&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2692&quot;&gt;&lt;del&gt;HBASE-2692&lt;/del&gt;&lt;/a&gt; commit.  Will leave it open till we add unit test to prove master failover works though RIT.&lt;/p&gt;</comment>
                            <comment id="12919101" author="hbasereviewboard" created="Thu, 7 Oct 2010 23:38:22 +0000"  >&lt;p&gt;Message from: &quot;Jonathan Gray&quot; &amp;lt;jgray@apache.org&amp;gt;&lt;/p&gt;

&lt;p&gt;-----------------------------------------------------------&lt;br/&gt;
This is an automatically generated e-mail. To reply, visit:&lt;br/&gt;
&lt;a href=&quot;http://review.cloudera.org/r/995/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/995/&lt;/a&gt;&lt;br/&gt;
-----------------------------------------------------------&lt;/p&gt;

&lt;p&gt;Review request for hbase and stack.&lt;/p&gt;


&lt;p&gt;Summary&lt;br/&gt;
-------&lt;/p&gt;

&lt;p&gt;First go at a unit test of master failover with regions in transition.&lt;/p&gt;

&lt;p&gt;Comment from the test method:&lt;/p&gt;

&lt;p&gt;  /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Complex test of master failover that tests as many permutations of the&lt;/li&gt;
	&lt;li&gt;different possible states that regions in transition could be in within ZK.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;This tests the proper handling of these states by the failed-over master&lt;/li&gt;
	&lt;li&gt;and includes a thorough testing of the timeout code as well.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;Starts with a single master and three regionservers.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;Creates two tables, enabledTable and disabledTable, each containing 5&lt;/li&gt;
	&lt;li&gt;regions.  The disabledTable is then disabled.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;After reaching steady-state, the master is killed.  We then mock several&lt;/li&gt;
	&lt;li&gt;states in ZK.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;After mocking them, we will startup a new master which should become the&lt;/li&gt;
	&lt;li&gt;active master and also detect that it is a failover.  The primary test&lt;/li&gt;
	&lt;li&gt;passing condition will be that all regions of the enabled table are&lt;/li&gt;
	&lt;li&gt;assigned and all the regions of the disabled table are not assigned.&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;The different scenarios to be tested are below:&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;b&amp;gt;ZK State:  OFFLINE&amp;lt;/b&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;A node can get into OFFLINE state if&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;An RS fails to open a region, so it reverts the state back to OFFLINE&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;The Master is assigning the region to a RS before it sends RPC&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;We will mock the scenarios&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Master has assigned an enabled region but RS failed so a region is&lt;/li&gt;
	&lt;li&gt;not assigned anywhere and is sitting in ZK as OFFLINE&amp;lt;/li&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;This seems to cover both cases?&amp;lt;/li&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;b&amp;gt;ZK State:  CLOSING&amp;lt;/b&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;A node can get into CLOSING state if&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;An RS has begun to close a region&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;We will mock the scenarios&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region was being closed but the RS died before finishing the close&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of enabled table was being closed but did not complete&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of disabled table was being closed but did not complete&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;b&amp;gt;ZK State:  CLOSED&amp;lt;/b&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;A node can get into CLOSED state if&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;An RS has completed closing a region but not acknowledged by master yet&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;We will mock the scenarios&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of a table that should be enabled was closed on an RS&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of a table that should be disabled was closed on an RS&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;b&amp;gt;ZK State:  OPENING&amp;lt;/b&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;A node can get into OPENING state if&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;An RS has begun to open a region&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;We will mock the scenarios&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;RS was opening a region of enabled table but never finishes&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;b&amp;gt;ZK State:  OPENED&amp;lt;/b&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;A node can get into OPENED state if&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;An RS has finished opening a region but not acknowledged by master yet&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;We will mock the scenarios&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of a table that should be enabled was opened on an RS&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of a table that should be disabled was opened on an RS&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of a table that should be enabled was opened by a now-dead RS&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of a table that should be disabled was opened by a now-dead RS&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;b&amp;gt;ZK State:  NONE&amp;lt;/b&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;A region could not have a transition node if&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;The server hosting the region died and no master processed it&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;p&amp;gt;We will mock the scenarios&amp;lt;/p&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of enabled table was on a dead RS that was not yet processed&lt;/li&gt;
	&lt;li&gt;&amp;lt;li&amp;gt;Region of disabled table was on a dead RS that was not yet processed&lt;/li&gt;
	&lt;li&gt;&amp;lt;/ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;@throws Exception&lt;br/&gt;
   */&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This addresses bug &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2700&quot; title=&quot;Handle master failover for regions in transition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2700&quot;&gt;&lt;del&gt;HBASE-2700&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
    &lt;a href=&quot;http://issues.apache.org/jira/browse/HBASE-2700&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/browse/HBASE-2700&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Diffs&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;  trunk/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java 1005264 &lt;br/&gt;
  trunk/src/main/java/org/apache/hadoop/hbase/master/HMaster.java 1005264 &lt;br/&gt;
  trunk/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java 1005264 &lt;br/&gt;
  trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/ZKAssign.java 1005264 &lt;br/&gt;
  trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java 1005264 &lt;br/&gt;
  trunk/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java 1005264 &lt;br/&gt;
  trunk/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java 1005264 &lt;/p&gt;

&lt;p&gt;Diff: &lt;a href=&quot;http://review.cloudera.org/r/995/diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.cloudera.org/r/995/diff&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Testing&lt;br/&gt;
-------&lt;/p&gt;

&lt;p&gt;running the unit test!&lt;/p&gt;


&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Jonathan&lt;/p&gt;

</comment>
                            <comment id="12922217" author="streamy" created="Mon, 18 Oct 2010 19:07:16 +0000"  >&lt;p&gt;Patch +1&apos;d by Stack on rb.  Thanks for review.&lt;/p&gt;

&lt;p&gt;This adds three unit tests of master failover.  The first is a basic test that we backup masters will take over.  The second is a test of master failover happening concurrently with regions in transition.  The third is a test of concurrent RIT as well as concurrent RS failure.  Tables/regions are used from both enabled and disabled tables as this is one of the trickier parts, ensuring disabled tables stay offline properly regardless of what state they were in during master failover.&lt;/p&gt;

&lt;p&gt;Tests are passing locally consistently.  Committing this patch, let&apos;s see what hudson says.&lt;/p&gt;</comment>
                            <comment id="12922373" author="streamy" created="Tue, 19 Oct 2010 00:09:07 +0000"  >&lt;p&gt;Committed.  Master failover unit tests all passing.&lt;/p&gt;</comment>
                            <comment id="15016837" author="lars_francke" created="Fri, 20 Nov 2015 12:40:56 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12457470" name="HBASE-2700-test-v6.patch" size="68742" author="streamy" created="Mon, 18 Oct 2010 19:07:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 23 Jun 2010 17:21:24 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32727</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hitj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100311</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>