<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:06:11 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2957/HBASE-2957.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2957] Release row lock when waiting for wal-sync</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2957</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Is there a reason to hold on to the row-lock while waiting for the WAL-sync to be completed by the logSyncer thread?&lt;/p&gt;

&lt;p&gt;I think data consistency will be guaranteed even if the following happens (a) the row lock is held while the row is updated in memory (b) the row lock is released after queuing the KV record for WAL-syncing (c) the log-sync system guarantees that the log records for any given row are synced in order (d) the HBase client only receives a success notification after the sync completes (no change from the current state)&lt;/p&gt;

&lt;p&gt;I think this should be a huge win. For my use case, and I am sure for others,  the handler thread spends the bulk of its row-lock critical section  time waiting for sync to complete.&lt;/p&gt;

&lt;p&gt;Even if the log-sync system cannot guarantee the orderly completion of sync records, the &quot;Don&apos;t hold row lock while waiting for sync&quot; option should be available to HBase clients on a per request basis.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12473203">HBASE-2957</key>
            <summary>Release row lock when waiting for wal-sync</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="8">Not A Problem</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="khemani">Prakash Khemani</reporter>
                        <labels>
                    </labels>
                <created>Fri, 3 Sep 2010 04:55:40 +0000</created>
                <updated>Wed, 16 Jul 2014 23:02:21 +0000</updated>
                            <resolved>Wed, 16 Jul 2014 23:02:21 +0000</resolved>
                                    <version>0.20.0</version>
                                                    <component>regionserver</component>
                    <component>wal</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12905798" author="khemani" created="Fri, 3 Sep 2010 05:43:26 +0000"  >&lt;p&gt;Actually, data consistency is not guaranteed if we return to the HBase client any value which has not yet been sync&apos;d to WAL. But for my use case, and I think for many others, it is OK.&lt;/p&gt;



</comment>
                            <comment id="12906312" author="dhruba" created="Sun, 5 Sep 2010 00:17:52 +0000"  >&lt;p&gt;I am seeing a typical use case of hbase where all the rows of&lt;br/&gt;
a table are not equally hot. A few rows are orders of magnitude&lt;br/&gt;
hotter than most other rows.&lt;/p&gt;

&lt;p&gt;Each get/put operation in hbase involes the following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 put operation                           get operation
 --------------------------------------------------------------
1. acquire the rowlock
2. append to hlog
3. update memstore                 read from memstore
4. release rowlock
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example, if the appliction workload consists of only increment operations on &lt;b&gt;one&lt;/b&gt; record, then&lt;br/&gt;
the entire workload is serialized and the throughout is purely dependent on the&lt;br/&gt;
speed of the append-hlog operation. The number of hlog.append calls is&lt;br/&gt;
precisely the same as the number of put calls. This can be slow, especially&lt;br/&gt;
because the append operation requires writing to three datanodes in hdfs.&lt;/p&gt;

&lt;p&gt;We can make the workload supertfast while keeping the same data consistency&lt;br/&gt;
guarantees if we can achieve some batching. For&lt;br/&gt;
each record, let&apos;s say that the memstore contains a version of the record that has been committed to&lt;br/&gt;
hlog and another version of the same record that is being updated in memory&lt;br/&gt;
but has not yet been committed to hlog. let&apos;s say that we refer to these two versions&lt;br/&gt;
of the record as &quot;memstore.inflight&quot; and &quot;memstore.committed&quot; versions.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 put operation                                        get operation
 ----------------------------------------------------------------------------------
1. acquire the rowlock
2. update memstore.inflight                   read memstores.committed
3. release rowlock
3. append to hlog
4. memstore.committed = memstore.inflight

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key to the above protocol is that the rowlock is released as soon&lt;br/&gt;
as memstore is updated. This means that multiple calls to put() for&lt;br/&gt;
the same record will be parallelized and would result in a fewer calls&lt;br/&gt;
to hlog.append.&lt;/p&gt;

&lt;p&gt;Do people think that this is feasible and beneficial? If so, I can delve deeper into the design and implementation of this performance improvement.&lt;/p&gt;</comment>
                            <comment id="12906549" author="stack" created="Mon, 6 Sep 2010 17:43:34 +0000"  >&lt;p&gt;First, are you fellas running with hdfs-895?  (See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2467&quot; title=&quot;Concurrent flushers in HLog sync using HDFS-895&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2467&quot;&gt;&lt;del&gt;HBASE-2467&lt;/del&gt;&lt;/a&gt;)  We have it deployed over here.   It makes a big difference.&lt;/p&gt;

&lt;p&gt;@Prakash So you are suggesting a patch that optionally allows returning to the client though WAL has not been appended.  A flag in the Put would be checked somewhere around here &amp;#8211; &lt;a href=&quot;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#1534&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#1534&lt;/a&gt; &amp;#8211; and if set, we&apos;d skip the inline WAL append and in its place we&apos;d just add the edit to a queue for adding the WAL out of bound with the current update?  I&apos;d guess you&apos;d want the flag on ICV too, somewhere around here &amp;#8211; &lt;a href=&quot;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#3046?&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#3046?&lt;/a&gt;  We&apos;d change our current &apos;writeToWAL&apos; flag so rather than being a binary, that it would allow a new queue-the-edit-for-WAL-addition option?&lt;/p&gt;</comment>
                            <comment id="12906558" author="stack" created="Mon, 6 Sep 2010 18:17:37 +0000"  >&lt;p&gt;@Dhruba How does memstore.inflight get flipped to be memstore.committed?  Safely?  There are some existing mechanisms that might be of help.  There is a read/write consistency class in hbase &lt;a href=&quot;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.html#1&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/regionserver/ReadWriteConsistencyControl.html#1&lt;/a&gt; that is used around memstore updates to help keep the reads and scans &apos;clean&apos;; i.e. prevents reading of partial updates on a row (There are some known caveats).  And then in KeyValue, there is a &apos;special&apos; extra timestamp used ensuring memstore consistency: &lt;a href=&quot;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/KeyValue.html#216&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/docs/r0.89.20100726/xref/org/apache/hadoop/hbase/KeyValue.html#216&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Would it be simpler adding an ICV batcher/catcher/reservoir/overspill that sat in front of an actual ICV call?   It would accumulate ICVs while WAL was busy syncing.  The ICV resevoir would be an optional facility; you&apos;d ask for it by setting a flag on the ICV call.  On return, the actual ICV would check the reservoir to see if any ICVs had been accumulating while it was off syncing.  If any, it&apos;d suck them all up and apply the ICVs in a batch.  We&apos;d need to add a new batch ICV call, one that added a block of ICVs to the WAL  and that then did the memstore updates playing the content of the batch one at a time.  We&apos;d return to the client as soon as we&apos;d added an ICV to the revervoir not waiting on WAL&lt;/p&gt;</comment>
                            <comment id="12906570" author="tlipcon" created="Mon, 6 Sep 2010 19:25:29 +0000"  >&lt;p&gt;Hm, I might have missed something in the discussion, but it seems Prakash&apos;s original suggestion should work. Right now we do for any edit:&lt;/p&gt;

&lt;p&gt;1. lock rows&lt;br/&gt;
2. write to hlog&lt;br/&gt;
3. sync hlog&lt;br/&gt;
4. beginMemstoreInsert&lt;br/&gt;
5. edit memstore&lt;br/&gt;
6. completeMemstoreInsert&lt;br/&gt;
7. release locks&lt;/p&gt;

&lt;p&gt;Instead, I think another valid order would be:&lt;/p&gt;

&lt;p&gt;1. lock rows&lt;br/&gt;
2. write to hlog&lt;br/&gt;
3. begin memstore insert&lt;br/&gt;
4. edit memstore&lt;br/&gt;
5. unlock rows&lt;br/&gt;
6. sync hlog&lt;br/&gt;
7. complete memstore insert&lt;/p&gt;

&lt;p&gt;The guarantee we have to provide is that we don&apos;t complete the memstore insert before sync, but assuming we hold that the same as today, it should be invisible to users but provide a speedup for all modifications except CAS. We&apos;ll have to be careful how this interacts with CAS, though.&lt;/p&gt;</comment>
                            <comment id="12906625" author="streamy" created="Tue, 7 Sep 2010 00:06:02 +0000"  >&lt;p&gt;I think Prakash and Dhruba are specifically thinking about ICVs rather than Puts.  The begin insert / complete insert stuff doesn&apos;t actually get used at all for ICVs, in this case the memstoreTS=0.  So there is no &quot;complete memstore insert&quot;.&lt;/p&gt;

&lt;p&gt;Maybe the special ICV group commit makes sense.  Otherwise we&apos;ll have to relax some constraints or change how ICV works.&lt;/p&gt;</comment>
                            <comment id="12906627" author="dhruba" created="Tue, 7 Sep 2010 00:27:37 +0000"  >&lt;p&gt;@Todd: in your proposal you unlock the row before the Hlog is synced. in the rare case then the Hlog.sync fails, that transaction will be lost but other readers would have already seen the new value because u released the rowlock.&lt;/p&gt;

&lt;p&gt;@Stack. @Jonathan: &quot; simpler adding an ICV batcher/catcher/reservoir/overspill that sat in front of an actual ICV call?&quot;&lt;br/&gt;
I agree and I think that proposal works when the use-case is only ICV. but do you want the solution generalized so that it works for a workload that does lots of &quot;puts&quot; into the same record? with the exiting code, all these put calls will get serialized via the rowlock and syncs to HLog does not get any batching.&lt;/p&gt;</comment>
                            <comment id="12906629" author="tlipcon" created="Tue, 7 Sep 2010 00:45:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;other readers would have already seen the new value because u released the rowlock&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually edits aren&apos;t visible until the &quot;complete memstore insert&quot; step - we do a sort of MVCC-ish here to prevent visibility until the sync is complete, much like what you were proposing above.&lt;/p&gt;</comment>
                            <comment id="12906634" author="streamy" created="Tue, 7 Sep 2010 01:26:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;but do you want the solution generalized so that it works for a workload that does lots of &quot;puts&quot; into the same record? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Absolutely.  I think the solution you are proposing (which to me looks to be the same as what Todd suggests) makes sense and we should do it.&lt;/p&gt;

&lt;p&gt;However, ICV has a completely different codepath and I think Prakash was specifically thinking about this use case.&lt;/p&gt;

&lt;p&gt;We should tackle both, maybe open a separate JIRA for ICV and make this one about a normal Put?&lt;/p&gt;</comment>
                            <comment id="12906645" author="ryanobjc" created="Tue, 7 Sep 2010 02:43:17 +0000"  >&lt;p&gt;why dont you shard your counters?  With the perf optimizations we did&lt;br/&gt;
last friday, you should easily be able to support 100m counters/day&lt;br/&gt;
per row, just shard row-wise and you are set for scaling.&lt;/p&gt;

&lt;p&gt;-ryan&lt;/p&gt;

</comment>
                            <comment id="12906648" author="streamy" created="Tue, 7 Sep 2010 02:55:51 +0000"  >&lt;p&gt;We are already sharding counters.  I don&apos;t think that precludes these optimizations.&lt;/p&gt;</comment>
                            <comment id="12906884" author="khemani" created="Tue, 7 Sep 2010 17:19:32 +0000"  >&lt;p&gt;Sorry, I was out and couldn&apos;t reply to this thread.&lt;/p&gt;

&lt;p&gt;I think a general solution that guarantees consistency for PUTs and ICVs and at the same time doesn&apos;t hold the row lock while updating hlog is possible.&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;

&lt;p&gt;Thinking aloud. First why do we want to hold the row lock around the log sync? Because we want the log sync to happen in causal ordering. Here is a scenario of what can go wrong if we release the row lock before the sync completes.&lt;br/&gt;
	1. client-1 does a put/icv on regionserver-1. releases the row lock before the sync.&lt;br/&gt;
	2. client-2 comes in and reads the new value. Based on this just read value, client-2 then does a put in regionserver-2.&lt;br/&gt;
	3. client-2 is able to do its sync on rs-2 before client-1&apos;s sync on rs-1 completes.&lt;br/&gt;
	4. rs-1 is brought down ungracefully. During recovery we will have client-2&apos;s update but not client-1&apos;s. And that violates the causal ordering of events.&lt;/p&gt;

&lt;p&gt;===&lt;br/&gt;
So we don&apos;t want anyone to read a value which has not already been synced. I think we can transfer the wait-for-sync to the reader instead of asking all writers to wait.&lt;/p&gt;

&lt;p&gt;A simple way to do that will be to attach a log-sync-number with every cell. When a cell is updated it will keep the next log-sync-number within itself. A get will not return until the current log-sync-number is at least as big as log-sync-number stored in the cell.&lt;/p&gt;

&lt;p&gt;An update can return immediately after queuing the sync. The &quot;wait-for-sync&quot; is transferred from the writer to the reader. If the reader comes in sufficiently late (which is likely) then there will be no wait-for-syncs in the system.&lt;/p&gt;

&lt;p&gt;===&lt;br/&gt;
Even in this scheme we will have to treat ICVs specially. Logically an ICV has a (a) GET the old value (b) PUT the new value (c) GET and return the new value&lt;/p&gt;

&lt;p&gt;There are 2 cases&lt;br/&gt;
(1) The ICV caller doesn&apos;t use the return value of the ICV. In this case the ICV need not wait for the earlier sync to complere. (In my use case this what happens predominantly)&lt;/p&gt;

&lt;p&gt;(2) The ICV caller uses the return value of the ICV call to make further updates. In this case the ICV has to wait for its sync to complete before it returns. While the ICV is waiting for the sync to complete it need not hold the row lock. (At least in my use case this is a very rare case)&lt;/p&gt;

&lt;p&gt;===&lt;br/&gt;
I think that it is true in general that while a GET is forced to wait for a sync to complete, there is no need to hold the row lock.&lt;/p&gt;

&lt;p&gt;===&lt;/p&gt;



</comment>
                            <comment id="12906889" author="tlipcon" created="Tue, 7 Sep 2010 17:31:04 +0000"  >&lt;blockquote&gt;
&lt;p&gt;A simple way to do that will be to attach a log-sync-number with every cell. When a cell is updated it will keep the next log-sync-number within itself. A get will not return until the current log-sync-number is at least as big as log-sync-number stored in the cell.&lt;/p&gt;

&lt;p&gt;An update can return immediately after queuing the sync. The &quot;wait-for-sync&quot; is transferred from the writer to the reader. If the reader comes in sufficiently late (which is likely) then there will be no wait-for-syncs in the system.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We actually already do this! Rather than using the log-sync number, the memstore has an internal timestamp for readability. When we scan a row, we record this number atomically and only return cell versions older than this timestamp. See the ReadWriteConsistencyControl class.&lt;/p&gt;

&lt;p&gt;The only change we have to make is that we unlock the row before we call sync and update the memstore TS, rather than after.&lt;/p&gt;</comment>
                            <comment id="12906923" author="ryanobjc" created="Tue, 7 Sep 2010 18:22:59 +0000"  >&lt;p&gt;would you use the delayed sync option?  It has the similar semantics, where we don&apos;t wait for the HDFS acks and continue with the memstore applications and then return to the client.  Data is only persisted when EITHER a different put without this option comes in, OR the time-based flush kicks in.  Our tests on a prior implementation of this indicated amazing speedups.&lt;/p&gt;

&lt;p&gt;Also todd&apos;s idea yesterday won&apos;t work - you can&apos;t put a long pole event (hlog sync) inside a memstore transaction sequence.  We already had something like that and It Just Did Not Work.  &lt;/p&gt;

&lt;p&gt;I think another option would be:&lt;/p&gt;

&lt;p&gt;Get latest Value for ICV&lt;br/&gt;
create next value&lt;br/&gt;
Write HLog &amp;amp; sync&lt;br/&gt;
Obtain row lock&lt;br/&gt;
begin memstore insert&lt;br/&gt;
insert memstore values&lt;br/&gt;
commit memstore &lt;br/&gt;
release row lock&lt;br/&gt;
return to client&lt;/p&gt;

&lt;p&gt;But this wouldn&apos;t work for ICVs either!  Multiple clients would create multiple next values, and that would be bad.&lt;/p&gt;

&lt;p&gt;If we had this potential implementation:&lt;/p&gt;

&lt;p&gt;Write HLog diff of ICV (ie: +1 to so and so)&lt;br/&gt;
Obtain row lock&lt;br/&gt;
get existing value&lt;br/&gt;
create new value&lt;br/&gt;
begin memstore insert&lt;br/&gt;
insert memstore values&lt;br/&gt;
commit memstore&lt;br/&gt;
release row lock&lt;br/&gt;
return to client&lt;/p&gt;

&lt;p&gt;I think you should seriously look into delayed sync.  You can run your sync thread as often as 100ms and it still improves performance. 100ms of lost data in the worst case isnt so bad... right?&lt;/p&gt;</comment>
                            <comment id="12906984" author="stack" created="Tue, 7 Sep 2010 21:40:09 +0000"  >&lt;p&gt;@Ryan &quot;...and It Just Did Not Work.&quot;&lt;/p&gt;

&lt;p&gt;Can you rehearse the probs seen for the sake of the new fellas (Prakash, etc).&lt;/p&gt;</comment>
                            <comment id="12907314" author="khemani" created="Wed, 8 Sep 2010 17:06:20 +0000"  >&lt;p&gt;Yes, delayed syncs should almost always work. The problem is that they don&apos;t guarantee consistency. What I am arguing is that we can get consistency and the delayed syncs like performance at the same time.&lt;/p&gt;

&lt;p&gt;I also discussed this offline with Jonathan a little bit. Let me try one more time.&lt;/p&gt;

&lt;p&gt;Today, by default, HBase operates in &quot;writers wait for sync&quot; mode. This is good because it guarantees both durability and consistency. It is bad because it can be slow.&lt;/p&gt;

&lt;p&gt;Deferred syncs neither guarantee durability nor consistency.&lt;/p&gt;

&lt;p&gt;By consistency I mean the following - if A is the cause of B, and if B is present in the logs then A must also be present in the logs.&lt;/p&gt;

&lt;p&gt;If we can have HBase operate in &quot;readers wait for sync&quot; mode then we don&apos;t guarantee durability but we still guarantee consistency. And the performance should be similar to that of deferred syncs.&lt;/p&gt;




</comment>
                            <comment id="12907322" author="streamy" created="Wed, 8 Sep 2010 17:19:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we can have HBase operate in &quot;readers wait for sync&quot; mode then we don&apos;t guarantee durability but we still guarantee consistency. And the performance should be similar to that of deferred syncs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And given that most workloads are write dominated, you would expect significantly more total throughput.&lt;/p&gt;</comment>
                            <comment id="12907357" author="ryanobjc" created="Wed, 8 Sep 2010 18:41:38 +0000"  >&lt;p&gt;Prakash, I think we must be talking about different things, because delayed syncs &lt;em&gt;do&lt;/em&gt; guarantee consistency. &lt;/p&gt;

&lt;p&gt;Some background on the code... HLog is a sequence of entries... while concurrent writes have indeterminate ordering, writes on the same row lock are effective serialized and thus there is a strong causal ordering between HLog entries and sequence of operations for writes under the same row lock.  If you look at the implementation of HLog, there is a sync block in the middle in append which causes all entries to have a serial sequence id and also protects the underlying file which is not multithread capable.&lt;/p&gt;

&lt;p&gt;The way that delayed syncs work is like so for ICVs:&lt;/p&gt;

&lt;p&gt;Begin Row Lock&lt;br/&gt;
Do Get&lt;br/&gt;
Make new Value from Old Value and increment Amount&lt;br/&gt;
Append to HLog&lt;br/&gt;
if (!delayed sync) Do HLog sync&lt;br/&gt;
Update Memstore&lt;br/&gt;
Release Row Lock&lt;/p&gt;

&lt;p&gt;The longest pole item in this sequence tends to be the hlog sync.  So make it optional.  The entries are now buffered in memory.  The next sync by either a client on a different table, or via the background thread timer will flush the entries to HDFS thus persisting* them.&lt;/p&gt;

&lt;p&gt;The ordering in the hlog will be the same with or without delayed sync, and performance is vastly improved at a small hit in durability.  For peacemeal ICVs, losing 100ms now and again should not be statistically significant in high volume cases.  The setting is on a per-table basis, thus you can choose your tradeoff at that level.&lt;/p&gt;</comment>
                            <comment id="12907380" author="khemani" created="Wed, 8 Sep 2010 19:45:55 +0000"  >
&lt;p&gt;I agree that the ordering on a given region server will be the same with or without delayed sync. But I am pretty sure that globally there will be inconsistencies.&lt;/p&gt;

&lt;p&gt;Say a value is updated on RS A. This value is not synced yet.&lt;/p&gt;

&lt;p&gt;The abovementioned unsynced value on RS A is read by someone and based on that value an update is made on another RS B. Say the update on RS B is synced.&lt;/p&gt;

&lt;p&gt;Now we have a window where B depends on A, B is in the logs but A isn&apos;t. In in this window if RS A dies and comes back up then we will have a situation where the update on RS B is present but update on RS A isn&apos;t.&lt;/p&gt;



</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 5 Sep 2010 00:17:52 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32852</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 15 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02flj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12128</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>