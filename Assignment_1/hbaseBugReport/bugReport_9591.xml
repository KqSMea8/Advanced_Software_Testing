<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:05:50 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-9591/HBASE-9591.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-9591] [replication] getting &quot;Current list of sinks is out of date&quot; all the time when a source is recovered</title>
                <link>https://issues.apache.org/jira/browse/HBASE-9591</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I tried killing a region server when the slave cluster was down, from that point on my log was filled with:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2013-09-20 00:31:03,942 INFO  [regionserver60020.replicationSource,1] org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager: Current list of sinks is out of date, updating
2013-09-20 00:31:04,226 INFO  [ReplicationExecutor-0.replicationSource,1-jdec2hbase0403-4,60020,1379636329634] org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager: Current list of sinks is out of date, updating
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first log line is from the normal source, the second is the recovered one. When we try to replicate, we call replicationSinkMgr.getReplicationSink() and if the list of machines was refreshed since the last time then we call chooseSinks() which in turn refreshes the list of sinks and resets our lastUpdateToPeers. The next source will notice the change, and will call chooseSinks() too. The first source is coming for another round, sees the list was refreshed, calls chooseSinks() again. It happens forever until the recovered queue is gone.&lt;/p&gt;

&lt;p&gt;We could have all the sources going to the same cluster share a thread-safe ReplicationSinkManager. We could also manage the same cluster separately for each source. Or even easier, if the list we get in chooseSinks() is the same we had before, consider it a noop.&lt;/p&gt;

&lt;p&gt;What do you think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gabriel.reid&quot; class=&quot;user-hover&quot; rel=&quot;gabriel.reid&quot;&gt;Gabriel Reid&lt;/a&gt;?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12669590">HBASE-9591</key>
            <summary>[replication] getting &quot;Current list of sinks is out of date&quot; all the time when a source is recovered</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jdcryans">Jean-Daniel Cryans</reporter>
                        <labels>
                    </labels>
                <created>Fri, 20 Sep 2013 00:50:02 +0000</created>
                <updated>Sun, 12 Oct 2014 04:26:03 +0000</updated>
                                            <version>0.96.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="13772582" author="lhofhansl" created="Fri, 20 Sep 2013 02:52:14 +0000"  >&lt;p&gt;Is this 0.96+ only, or a 0.94 issue as well?&lt;/p&gt;</comment>
                            <comment id="13772838" author="gabriel.reid" created="Fri, 20 Sep 2013 07:18:32 +0000"  >&lt;p&gt;I don&apos;t think I totally understand the situation. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I tried killing a region server when the slave cluster was down&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So just to be totally sure, a region server on the master cluster is being killed, while the whole slave cluster is down, is that correct? If that&apos;s the case, I&apos;m assuming that the list of region servers in the peer cluster would always remain empty, and wouldn&apos;t have any change events coming through ZK, and so the timestamp returned by ReplicationPeers#getTimestampOfLastChangeToPeer would stay the same. Of course, if that&apos;s all the case then it wouldn&apos;t lead to this situation, so I&apos;m definitely not understanding something.&lt;/p&gt;

&lt;p&gt;In any case, I think it does make sense to consider things a noop (and not update timestamps) if the list of sinks fetched in ReplicationSinkManager#chooseSinks is the same as the last time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; This shouldn&apos;t apply to 0.94, the ReplicationSinkManager is only 0.95+.&lt;/p&gt;</comment>
                            <comment id="13773432" author="jdcryans" created="Fri, 20 Sep 2013 21:11:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m assuming that the list of region servers in the peer cluster would always remain empty, and wouldn&apos;t have any change events coming through ZK, and so the timestamp returned by ReplicationPeers#getTimestampOfLastChangeToPeer would stay the same.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It was shut down and then it came back, but for this bug to trigger you only need 1 source to refresh the list to set the infinite-ish loop in motion, since that source will not change the refresh time for the other source.&lt;/p&gt;</comment>
                            <comment id="13774597" author="gabriel.reid" created="Mon, 23 Sep 2013 14:49:40 +0000"  >&lt;p&gt;Sorry for being so slow at looking at this.&lt;/p&gt;

&lt;p&gt;I just finally took a closer look, and now I&apos;m clear on what&apos;s going on. I think I&apos;m leaning towards managing each cluster fully separately (i.e. having a separate ReplicationPeers instance per peer cluster), but I&apos;m wondering what kind of impact that would have on resource usage. At first glance, it looks like it should be fine. I think that taking this approach will be better in terms of avoiding other variations of this bug in the future, which could be something that would happen if we do the &quot;noop if chooseSinks returns the same thing&quot; approach.&lt;/p&gt;

&lt;p&gt;On the other hand, the &quot;noop if chooseSinks returns the same thing&quot; approach will probably be quite a bit easier.&lt;/p&gt;

&lt;p&gt;Do you have a personal preference for the approach, or ideas on what would be &quot;best&quot;?&lt;/p&gt;</comment>
                            <comment id="13776759" author="jdcryans" created="Tue, 24 Sep 2013 21:16:00 +0000"  >&lt;blockquote&gt;&lt;p&gt;but I&apos;m wondering what kind of impact that would have on resource usage. At first glance, it looks like it should be fine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah it should not be too bad... unless we have a RS that recovers a ton of failed RS at the same time (but even then it might be dwarfed but the buffer we use to replicate). Seems cleaner too although more invasive given how we key using the peer id.&lt;/p&gt;</comment>
                            <comment id="14168520" author="enis" created="Sun, 12 Oct 2014 04:26:03 +0000"  >&lt;p&gt;Unscheduling from branch-1 for now. Feel free to bring back if there is a patch. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 20 Sep 2013 02:52:14 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>349522</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 9 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1o9mv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>349820</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>