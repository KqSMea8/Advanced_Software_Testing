<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:01:11 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15225/HBASE-15225.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15225] Connecting to HBase via newAPIHadoopRDD in PySpark gives  org.apache.hadoop.hbase.client.RetriesExhaustedException</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15225</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Unable to read HBase table into Spark with hbase security authentication set to kerberos. Seeing the following error. &lt;/p&gt;

&lt;p&gt;: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=31, exceptions:&lt;br/&gt;
Thu Feb 04 22:01:55 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:01:56 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:01:56 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:01:57 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:01:59 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:02:03 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:02:13 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:02:23 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:02:34 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:02:44 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:03:04 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:03:24 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:03:44 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:04:04 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:04:24 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:04:44 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
Thu Feb 04 22:05:04 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.IOException: Connection reset by peer&lt;br/&gt;
.&lt;br/&gt;
.&lt;br/&gt;
.&lt;br/&gt;
Thu Feb 04 22:09:46 CST 2016, org.apache.hadoop.hbase.client.RpcRetryingCaller@395327da, java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;/p&gt;

&lt;p&gt;at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:129)&lt;br/&gt;
at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:90)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:282)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:187)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ClientScanner.&amp;lt;init&amp;gt;(ClientScanner.java:182)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ClientScanner.&amp;lt;init&amp;gt;(ClientScanner.java:109)&lt;br/&gt;
at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:738)&lt;br/&gt;
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:178)&lt;br/&gt;
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:82)&lt;br/&gt;
at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:282)&lt;br/&gt;
at org.apache.hadoop.hbase.client.HTable.getRegionLocations(HTable.java:616)&lt;br/&gt;
at org.apache.hadoop.hbase.util.RegionSizeCalculator.&amp;lt;init&amp;gt;(RegionSizeCalculator.java:79)&lt;br/&gt;
at org.apache.hadoop.hbase.util.RegionSizeCalculator.&amp;lt;init&amp;gt;(RegionSizeCalculator.java:64)&lt;br/&gt;
at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:160)&lt;br/&gt;
at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:115)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)&lt;br/&gt;
at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)&lt;br/&gt;
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)&lt;br/&gt;
at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
at org.apache.spark.rdd.RDD.partitions(RDD.scala:237)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1277)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)&lt;br/&gt;
at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)&lt;br/&gt;
at org.apache.spark.rdd.RDD.take(RDD.scala:1272)&lt;br/&gt;
at org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:202)&lt;br/&gt;
at org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:530)&lt;br/&gt;
at org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)&lt;br/&gt;
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)&lt;br/&gt;
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)&lt;br/&gt;
at py4j.Gateway.invoke(Gateway.java:259)&lt;br/&gt;
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)&lt;br/&gt;
at py4j.commands.CallCommand.execute(CallCommand.java:79)&lt;br/&gt;
at py4j.GatewayConnection.run(GatewayConnection.java:207)&lt;br/&gt;
at java.lang.Thread.run(Thread.java:744)&lt;br/&gt;
Caused by: java.io.IOException: Call to d-767tfz1.target.com/10.66.241.13:60020 failed on local exception: java.io.EOFException&lt;br/&gt;
at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1484)&lt;br/&gt;
at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1456)&lt;br/&gt;
at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1656)&lt;br/&gt;
at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1714)&lt;br/&gt;
at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:29900)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:308)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:164)&lt;br/&gt;
at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:59)&lt;br/&gt;
at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)&lt;br/&gt;
... 42 more&lt;br/&gt;
Caused by: java.io.EOFException&lt;br/&gt;
at java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
at org.apache.hadoop.hbase.ipc.RpcClient$Connection.readResponse(RpcClient.java:1071)&lt;br/&gt;
at org.apache.hadoop.hbase.ipc.RpcClient$Connection.run(RpcClient.java:727)&lt;/p&gt;</description>
                <environment>&lt;p&gt;spark 1.6.0 , Hbase 0.98.4, kerberos,  hbase.rpc.protection set to authentication.&lt;/p&gt;</environment>
        <key id="12937235">HBASE-15225</key>
            <summary>Connecting to HBase via newAPIHadoopRDD in PySpark gives  org.apache.hadoop.hbase.client.RetriesExhaustedException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="88.sanjay">Sanjay Kumar</reporter>
                        <labels>
                    </labels>
                <created>Sat, 6 Feb 2016 05:20:28 +0000</created>
                <updated>Fri, 26 Feb 2016 10:44:39 +0000</updated>
                                            <version>0.98.4</version>
                                                    <component>mapreduce</component>
                    <component>spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="15135624" author="ted.m" created="Sat, 6 Feb 2016 05:44:45 +0000"  >&lt;p&gt;This should be marked as involved Jira.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=88.sanjay&quot; class=&quot;user-hover&quot; rel=&quot;88.sanjay&quot;&gt;Sanjay Kumar&lt;/a&gt; don&apos;t use newAPIHadoopRDD use the functions defined in the HBaseContext object which is defined in the hbaseSpark Module.&lt;/p&gt;

&lt;p&gt;That will take care of all you Spark to HBase connection issues.&lt;/p&gt;

&lt;p&gt;Documentation can be found here: &lt;a href=&quot;https://hbase.apache.org/book.html#spark&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hbase.apache.org/book.html#spark&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="15135626" author="ted.m" created="Sat, 6 Feb 2016 05:45:36 +0000"  >&lt;p&gt;Also if you are using PySpark then use the Spark SQL to HBase access pattern  also found in here &lt;a href=&quot;https://hbase.apache.org/book.html#_sparksql_dataframes&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hbase.apache.org/book.html#_sparksql_dataframes&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15135990" author="jinghe" created="Sat, 6 Feb 2016 20:24:36 +0000"  >&lt;p&gt;hbase-spark module is not in any of official released versions yet &amp;#8211; it is good to put it in the 1.x line.&lt;/p&gt;

&lt;p&gt;It worked fine for you with a non-Kerberos HBase? &lt;/p&gt;</comment>
                            <comment id="15135996" author="88.sanjay" created="Sat, 6 Feb 2016 20:42:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted.m&quot; class=&quot;user-hover&quot; rel=&quot;ted.m&quot;&gt;Theodore michael Malaska&lt;/a&gt; Was about to say the same thing as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryhe&quot; class=&quot;user-hover&quot; rel=&quot;jerryhe&quot;&gt;Jerry He&lt;/a&gt; said. i don&apos;t think abase-spark module is an option at this point. &lt;/p&gt;

&lt;p&gt;Yes it did work with a non-Kerberos HBase. &lt;/p&gt;</comment>
                            <comment id="15136000" author="88.sanjay" created="Sat, 6 Feb 2016 20:53:52 +0000"  >&lt;p&gt;From what I understand from the error. The RPC connection to one of the HBase region servers is getting timed out.  Is there a way for me to confirm that rpc authentication failing is actually the issue ?&lt;/p&gt;</comment>
                            <comment id="15136012" author="ted.m" created="Sat, 6 Feb 2016 21:18:02 +0000"  >&lt;p&gt;I have a version that works on kerberos clusters that my clients are using.  I&apos;m back from vacation this week and I will add it to hbase soon.&lt;/p&gt;

&lt;p&gt;Let me see if I can get it on my github.&lt;/p&gt;
</comment>
                            <comment id="15136013" author="ted.m" created="Sat, 6 Feb 2016 21:26:14 +0000"  >&lt;p&gt;OK I have updated the Github.  This has everything that is in HBase but with the scan kerberos fix plus it is back ported to CDH 5.5.  If you are not using CDH then just make the needed changes and fork the code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/tmalaska/SparkOnHBase&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/tmalaska/SparkOnHBase&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This week I will work with the HBase committers to get the kerberos fit checked in.&lt;/p&gt;

&lt;p&gt;Let me know if there is anything else you need.&lt;/p&gt;</comment>
                            <comment id="15168809" author="88.sanjay" created="Fri, 26 Feb 2016 10:44:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted.m&quot; class=&quot;user-hover&quot; rel=&quot;ted.m&quot;&gt;Theodore michael Malaska&lt;/a&gt; - We recently upgraded to Hortonworks 2.3.4 (HBase - 1.1.2 / spark 1.5.2). I built the code that you shared and tried the HBaseBulkGetExample. I see the error given below. Is it because of we have missed some configuration ? Have you seen something like this ? &lt;/p&gt;

&lt;p&gt;org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6744)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:628)&lt;br/&gt;
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:987)&lt;br/&gt;
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)&lt;br/&gt;
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)&lt;br/&gt;
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2147)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:422)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2145)&lt;/p&gt;

&lt;p&gt;	at org.apache.hadoop.ipc.Client.call(Client.java:1468)&lt;br/&gt;
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)&lt;br/&gt;
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)&lt;br/&gt;
	at com.sun.proxy.$Proxy26.getDelegationToken(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:909)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)&lt;br/&gt;
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)&lt;br/&gt;
	at com.sun.proxy.$Proxy27.getDelegationToken(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:1029)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1355)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:529)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:507)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2041)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$$anonfun$obtainTokensForNamenodes$1.apply(YarnSparkHadoopUtil.scala:126)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil$$anonfun$obtainTokensForNamenodes$1.apply(YarnSparkHadoopUtil.scala:123)&lt;br/&gt;
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:74)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil.obtainTokensForNamenodes(YarnSparkHadoopUtil.scala:123)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.getTokenRenewalInterval(Client.scala:500)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:533)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:633)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:123)&lt;br/&gt;
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56)&lt;br/&gt;
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)&lt;br/&gt;
	at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:523)&lt;br/&gt;
	at $line137.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$HBaseBulkGetExample$.main(&amp;lt;console&amp;gt;:94)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:83)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:88)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:90)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:92)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:94)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:96)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:98)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:100)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:102)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:104)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:106)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:108)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:110)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:112)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:114)&lt;br/&gt;
	at $line139.$read$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:116)&lt;br/&gt;
	at $line139.$read.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:118)&lt;br/&gt;
	at $line139.$read$.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:122)&lt;br/&gt;
	at $line139.$read$.&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at $line139.$eval$.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:7)&lt;br/&gt;
	at $line139.$eval$.&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at $line139.$eval.$print(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)&lt;br/&gt;
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)&lt;br/&gt;
	at org.apache.spark.repl.Main$.main(Main.scala:31)&lt;br/&gt;
	at org.apache.spark.repl.Main.main(Main.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
16/02/26 04:26:29 ERROR Utils: Uncaught exception in thread main&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
	at org.apache.spark.network.netty.NettyBlockTransferService.close(NettyBlockTransferService.scala:152)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1228)&lt;br/&gt;
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:100)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1749)&lt;br/&gt;
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)&lt;br/&gt;
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1748)&lt;br/&gt;
	at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:593)&lt;br/&gt;
	at $line137.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$HBaseBulkGetExample$.main(&amp;lt;console&amp;gt;:94)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:83)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:88)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:90)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:92)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:94)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:96)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:98)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:100)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:102)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:104)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:106)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:108)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:110)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:112)&lt;br/&gt;
	at $line139.$read$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:114)&lt;br/&gt;
	at $line139.$read$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:116)&lt;br/&gt;
	at $line139.$read.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:118)&lt;br/&gt;
	at $line139.$read$.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:122)&lt;br/&gt;
	at $line139.$read$.&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at $line139.$eval$.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:7)&lt;br/&gt;
	at $line139.$eval$.&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at $line139.$eval.$print(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)&lt;br/&gt;
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)&lt;br/&gt;
	at org.apache.spark.repl.Main$.main(Main.scala:31)&lt;br/&gt;
	at org.apache.spark.repl.Main.main(Main.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;/p&gt;
</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 6 Feb 2016 05:44:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            42 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2sidj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>