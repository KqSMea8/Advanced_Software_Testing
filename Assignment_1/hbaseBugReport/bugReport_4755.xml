<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:21:45 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-4755/HBASE-4755.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-4755] HBase based block placement in DFS</title>
                <link>https://issues.apache.org/jira/browse/HBASE-4755</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt; The feature as is only useful for HBase clusters that care about data locality on regionservers, but this feature can also enable a lot of nice features down the road.&lt;/p&gt;

&lt;p&gt;The basic idea is as follows: instead of letting HDFS determine where to replicate data (r=3) by place blocks on various regions, it is better to let HBase do so by providing hints to HDFS through the DFS client. That way instead of replicating data at a blocks level, we can replicate data at a per-region level (each region owned by a promary, a secondary and a tertiary regionserver). This is better for 2 things:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Can make region failover faster on clusters which benefit from data affinity&lt;/li&gt;
	&lt;li&gt;On large clusters with random block placement policy, this helps reduce the probability of data loss&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The algo is as follows:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Each region in META will have 3 columns which are the preferred regionservers for that region (primary, secondary and tertiary)&lt;/li&gt;
	&lt;li&gt;Preferred assignment can be controlled by a config knob&lt;/li&gt;
	&lt;li&gt;Upon cluster start, HMaster will enter a mapping from each region to 3 regionservers (random hash, could use current locality, etc)&lt;/li&gt;
	&lt;li&gt;The load balancer would assign out regions preferring region assignments to primary over secondary over tertiary over any other node&lt;/li&gt;
	&lt;li&gt;Periodically (say weekly, configurable) the HMaster would run a locality checked and make sure the map it has for region to regionservers is optimal.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Down the road, this can be enhanced to control region placement in the following cases:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Mixed hardware SKU where some regionservers can hold fewer regions&lt;/li&gt;
	&lt;li&gt;Load balancing across tables where we dont want multiple regions of a table to get assigned to the same regionservers&lt;/li&gt;
	&lt;li&gt;Multi-tenancy, where we can restrict the assignment of the regions of some table to a subset of regionservers, so an abusive app cannot take down the whole HBase cluster.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12530597">HBASE-4755</key>
            <summary>HBase based block placement in DFS</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="cgist">Christopher Gist</assignee>
                                    <reporter username="karthik.ranga">Karthik Ranganathan</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 Nov 2011 17:04:17 +0000</created>
                <updated>Mon, 14 Nov 2016 18:54:58 +0000</updated>
                                            <version>0.94.0</version>
                                                        <due></due>
                            <votes>1</votes>
                                    <watches>46</watches>
                                                                                                            <comments>
                            <comment id="13145711" author="tlipcon" created="Mon, 7 Nov 2011 19:04:13 +0000"  >&lt;p&gt;This will need appropriate HDFS-side support to provide location hints to the NN when allocating blocks. Is there an HDFS JIRA already filed for it?&lt;/p&gt;</comment>
                            <comment id="13145730" author="karthik.ranga" created="Mon, 7 Nov 2011 19:39:28 +0000"  >&lt;p&gt;Yes, totally. Not sure if there is a JIRA, but Pritam already has a patch for adding favored nodes on an addBlock call.&lt;/p&gt;

&lt;p&gt;Pritam - is there an HDFS JIRA for this?&lt;/p&gt;</comment>
                            <comment id="13147783" author="apurtell" created="Thu, 10 Nov 2011 15:45:27 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;We could also build upon this to locate read slaves (should that be separately pursued) on the secondary and tertiary node options.&lt;/p&gt;</comment>
                            <comment id="13147785" author="apurtell" created="Thu, 10 Nov 2011 15:46:39 +0000"  >&lt;p&gt;Proposing for 0.94.&lt;/p&gt;</comment>
                            <comment id="13560081" author="devaraj" created="Tue, 22 Jan 2013 21:55:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; is the hdfs jira that should be addressed. I have started digging in in this area with an aim to get this feature in hbase trunk (with the corresponding hdfs changes).&lt;/p&gt;</comment>
                            <comment id="13560158" author="stack" created="Tue, 22 Jan 2013 23:19:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt; Sweet!&lt;/p&gt;</comment>
                            <comment id="13579469" author="devaraj" created="Fri, 15 Feb 2013 20:00:15 +0000"  >&lt;p&gt;This is the first stab at this issue. The patch is a huge WIP patch (and has a lot of classes from the 0.89 branch). I think I am going to break this up into multiple jiras. &lt;/p&gt;

&lt;p&gt;1) In this patch, what I have done is only handled the createTable flow to honor region placements. It&apos;s really random assignment, but has the plumbing for updating meta with the region locations, etc. &lt;/p&gt;

&lt;p&gt;2) The next step is to have the creation of store files honor this region placement.&lt;/p&gt;

&lt;p&gt;3) The third step is to be able to run tools against meta to ensure the placement looks optimal.&lt;/p&gt;

&lt;p&gt;There could be more steps involved but the above are the high level ones for now, and probably each could be a subtask.&lt;/p&gt;</comment>
                            <comment id="13581646" author="enis" created="Tue, 19 Feb 2013 21:25:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;2) The next step is to have the creation of store files honor this region placement.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Is this patch useful without giving hints to DFSClient about block placement. I though we still don&apos;t have the pluming in hadoop yet.  &lt;/p&gt;</comment>
                            <comment id="13582000" author="devaraj" created="Wed, 20 Feb 2013 06:47:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is this patch useful without giving hints to DFSClient about block placement.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The stuff in Hadoop is tracked in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt;. I am thinking that we can use the reflection to figure out whether the underlying Hadoop supports the API to do with block placements or not.&lt;/p&gt;</comment>
                            <comment id="13587380" author="devaraj" created="Tue, 26 Feb 2013 18:43:23 +0000"  >&lt;p&gt;Added two subtasks for now - &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7932&quot; title=&quot;Do the necessary plumbing for the region locations in META table and send the info to the RegionServers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7932&quot;&gt;&lt;del&gt;HBASE-7932&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7942&quot; title=&quot;Make use of the plumbing in HBASE-7932 to provide location hints when region files are created&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7942&quot;&gt;&lt;del&gt;HBASE-7942&lt;/del&gt;&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="13588825" author="jmhsieh" created="Wed, 27 Feb 2013 21:56:36 +0000"  >&lt;p&gt;Some design level questions:&lt;/p&gt;

&lt;p&gt;I&apos;m curious about where the block location policy (the load balancer) lives or is intended to live.  Is it in HDFS or in HBase? (and why).  If hbase&apos;s region balancer and hdfs block balancer are in conflict who wins?  How should they work together? &lt;/p&gt;

&lt;p&gt;Are there alternatives you&apos;ve considered? &lt;/p&gt;

&lt;p&gt;What is the story for cases where node membership changes (DN&apos;s and RS&apos;s)?  (what if the hinted rs/dn is not there?)  &lt;/p&gt;

</comment>
                            <comment id="13588857" author="devaraj" created="Wed, 27 Feb 2013 22:26:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m curious about where the block location policy (the load balancer) lives or is intended to live. Is it in HDFS or in HBase? (and why). If hbase&apos;s region balancer and hdfs block balancer are in conflict who wins? How should they work together?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The block placement policy is there in HBase presently. This is because HBase specifies the location of the blocks on a per file basis (as per the current approach). The HDFS side treats these as hints.&lt;br/&gt;
The balancers run independently, and if the HDFS rebalancer damages the block placements, it&apos;d be repaired at the time compactions in hbase happen.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What is the story for cases where node membership changes (DN&apos;s and RS&apos;s)? (what if the hinted rs/dn is not there?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;HBase should be able to handle these cases (and the new patches will handle these). In addition, there would be a tool (that would be a subtask of this jira) that should be run periodically that would look at the block placements and update region maps (region -&amp;gt; favored nodes) in the meta table in HBase to keep the mapping more optimal in terms of locality of data.&lt;/p&gt;</comment>
                            <comment id="13588950" author="apurtell" created="Thu, 28 Feb 2013 00:14:00 +0000"  >&lt;p&gt;If block device type affinity hints are available, then this could be an additional factor for a future block placement algorithm. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2832&quot; title=&quot;Enable support for heterogeneous storages in HDFS - DN as a collection of storages&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2832&quot;&gt;&lt;del&gt;HDFS-2832&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6572&quot; title=&quot;Tiered HFile storage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6572&quot;&gt;HBASE-6572&lt;/a&gt; for relevant discussion. If device affinity hints might have a path component (e.g. xattr in the namespace that associates a path with an affinity hint), then this could be related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; also.&lt;/p&gt;</comment>
                            <comment id="13589095" author="jmhsieh" created="Thu, 28 Feb 2013 02:29:57 +0000"  >&lt;p&gt;I know work as already started here, but I want to make sure we consider multiple designs, have thought about where they may end up and the tradeoffs.&lt;/p&gt;

&lt;p&gt;I posted a &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576?focusedCommentId=13589059&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13589059&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;long post&lt;/a&gt;&lt;br/&gt;
 over on the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; jira, probably should have posted here instead. &lt;/p&gt;

&lt;p&gt;I&apos;m assuming that HBase provides the list of DN&apos;s, likely selected at region creation time?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The balancers run independently, and if the HDFS rebalancer damages the block placements, it&apos;d be repaired at the time compactions in hbase happen.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So they will potentially &quot;fight&quot; &amp;#8211; but we&apos;ll just have a perf penalty upon recovery.  Does the HDFS balancer by default run automatically (or is only triggered manually)?  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;HBase should be able to handle these cases (and the new patches will handle these). In addition, there would be a tool (that would be a subtask of this jira) that should be run periodically that would look at the block placements and update region maps (region -&amp;gt; favored nodes) in the meta table in HBase to keep the mapping more optimal in terms of locality of data.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, so so when we attempt to write when we compact or flush maybe we&apos;d check to see all N hdfs replica targets are alive?  Would the tool be the only process/thread that select data node targets?  Is this the only mechanism we have to force block replcias to different datanodes?&lt;/p&gt;

&lt;p&gt;My spidey sense feels that putting hdfs specific info in hbase means we need to go all in or push it hdfs stuff into hdfs an interact with a policy.&lt;/p&gt;</comment>
                            <comment id="13589112" author="jmhsieh" created="Thu, 28 Feb 2013 02:34:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;If block device type affinity hints are available, then this could be an additional factor for a future block placement algorithm. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;hm.. this could be really interesting for potentially forcing the hlog files to particular specialized device. &lt;/p&gt;

&lt;p&gt;It would be really cool to have different files from the same region on the same machine but on different drives.  This could reduce seek latencies by making them concurrent when dealing with multiple hfiles/colfams.  Or we could force compaction/flush writes to drives that aren&apos;t involved.  &lt;/p&gt;</comment>
                            <comment id="13589271" author="apurtell" created="Thu, 28 Feb 2013 07:02:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;This could reduce seek latencies by making them concurrent when dealing with multiple hfiles/colfams&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Or by servicing families with read-mostly access patterns from a low latency high-IOPS device in conjunction with &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13589278" author="devaraj" created="Thu, 28 Feb 2013 07:08:49 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; for spending time on the issue. I think I should write a spec up and we can continue the discussion on that spec as the base. Answers to some questions/comments:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m assuming that HBase provides the list of DN&apos;s, likely selected at region creation time?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes (in particular, I am currently only considering pre-split tables, and the createTable call in the master allocates the regions to particular datanodes). The regionservers would use this information (as the favored nodes) for creating any new file on the hdfs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So they will potentially &quot;fight&quot; &#8211; but we&apos;ll just have a perf penalty upon recovery. Does the HDFS balancer by default run automatically (or is only triggered manually)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They could fight, but the hdfs balancer could in theory be tweaked to not move blocks for certain paths. The hdfs balancer needs to be run manually.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Ok, so so when we attempt to write when we compact or flush maybe we&apos;d check to see all N hdfs replica targets are alive? Would the tool be the only process/thread that select data node targets? Is this the only mechanism we have to force block replcias to different datanodes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The tool would look at the regions, their locality information, and try to make sure the map from regions to favored nodes is optimal. It might reassign regions in this process (i.e., update the meta table with the new information, that would then be propagated to the regionservers). The regionservers, like before, would use this information (as the favored nodes) for creating any new file on the hdfs.&lt;/p&gt;</comment>
                            <comment id="13589576" author="jmhsieh" created="Thu, 28 Feb 2013 14:48:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think I should write a spec up and we can continue the discussion on that spec as the base.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1.  Especially since this feature touches multiple components (hdfs, region creation, table creation, region balancer).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yes (in particular, I am currently only considering pre-split tables, and the createTable call in the master allocates the regions to particular datanodes). The regionservers would use this information (as the favored nodes) for creating any new file on the hdfs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m assuming the first cut would use &quot;random&quot; assignment for 2nd and 3rd replicas?  &lt;/p&gt;

&lt;p&gt;Handling &quot;natural&quot; splits would be follow on jira? (a simple policy for splits is for the daughters to choose the same dns as the parent, something slightly smarter would have the parent as one of the replicas but pick 2 new nodes with higher priority).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;They could fight, but the hdfs balancer could in theory be tweaked to not move blocks for certain paths. The hdfs balancer needs to be run manually.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Make sense, but sounds like an argument for adding some persistent attribute state in hdfs. (or have hdfs consult hbase.. yuck).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The tool would look at the regions, their locality information, and try to make sure the map from regions to favored nodes is optimal. It might reassign regions in this process (i.e., update the meta table with the new information, that would then be propagated to the regionservers). The regionservers, like before, would use this information (as the favored nodes) for creating any new file on the hdfs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So this tool sounds like a new balancer that might fight with the built in hbase balancer.  This makes me want to make the hbase balancer external from the master. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13590316" author="devaraj" created="Fri, 1 Mar 2013 07:50:05 +0000"  >&lt;p&gt;Attaching the notes. Thanks all, for the active conversation on this topic. Let me know if you need more details on anything. I&apos;ll continue with the patches on the subtasks for now.&lt;/p&gt;</comment>
                            <comment id="13590886" author="devaraj" created="Fri, 1 Mar 2013 19:50:16 +0000"  >&lt;p&gt;The notes as a comment on the jira (for easy access &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/p&gt;

&lt;p&gt;At a high level this has a couple of parts to it:&lt;/p&gt;

&lt;p&gt;Creation of table flow (assuming pre-split table)&lt;br/&gt;
---------------------------------------------------------------------&lt;br/&gt;
0. HBase layer has policies based on which it decides where to place the region files. AssignmentDomain is defined when a new table is created (today it is all the nodes in the cluster).&lt;br/&gt;
1. The HMaster chooses the primary RS on a round-robin basis, and the secondary/tertiary RSs are chosen on different racks (best effort, and best effort to place secondary/tertiary on the same rack).&lt;br/&gt;
2. The meta table is updated with the information about the location mapping for the added regions.&lt;br/&gt;
3. The RegionServers are then asked to open the regions and they get the favored nodes information as well. The mapping information from regions to favorednodes is cached in the regionservers.&lt;br/&gt;
4. This information is then passed to the filesystem (&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt;) when the regionservers create new files on the filesystem. For now, a create API has been added in HDFS to accept a favorednodes list as an additional argument.&lt;/p&gt;

&lt;p&gt;Failure recovery&lt;br/&gt;
---------------------------------------------------------------------&lt;br/&gt;
When the primary RS dies, ideally, we should assign the regions on that RS to their respective secondaries (maybe whichever has less load or fewer primaries among the remaining two). At some point the maintenance tool should run and set the mapping in meta right (three RS locations, etc.)&lt;/p&gt;

&lt;p&gt;Maintenance of the metadata &amp;amp; region locations&lt;br/&gt;
---------------------------------------------------------------------&lt;br/&gt;
Over a period of time, nodes may fail, and/or hdfs-balancer may run that might potentially have a bad impact on the locality set up in above steps. &lt;/p&gt;

&lt;p&gt;Periodically, a tool would be run that would inspect the meta table, and see if the mapping is still optimal. The tool (as per the code in facebook&apos;s branch) takes a couple of options it can optimize for - maximum-locality, minimum-region-reassign, munkres algorithm for assigning secondary/tertiary RS for regions. There is a chore that periodically checks for updates in meta (based on timestamps) for the region locations and updates assignment-plans.&lt;br/&gt;
In 0.89-fb and in prior versions of HBase, the hbase balancer is run upon regionserver reporting heartbeats, and the balancer basically ensures that the assignment-plans that have been precomputed are met (and regions might get unassigned from their current regionservers, etc.).&lt;/p&gt;

&lt;p&gt;I think it makes sense to have the above tool be part of the locality-aware loadbalancer itself since the loadbalancer today runs asynchronously and it could do a lot more work without impacting heartbeat latencies, etc. It will also address the issue to do with the conflicts that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; raised in his previous comment. I&apos;ll look at this aspect some more.&lt;/p&gt;

&lt;p&gt;TODO:&lt;br/&gt;
Handle non pre-split tables&lt;/p&gt;</comment>
                            <comment id="13591025" author="jmhsieh" created="Fri, 1 Mar 2013 22:32:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt; thanks! This level of description is exactly what I was hoping for.&lt;/p&gt;

&lt;p&gt;Questions (some are just clarifying):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;An assignmentDomain is not what I&apos;ve been calling an affinity group over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; &amp;#8211; it is the set of possible DN&apos;s to assign to, yeah?  Are assignment domains persisted or just queried when creating tables (how does this info come from hdfs)?&lt;/li&gt;
	&lt;li&gt;Table creation was recently changed with the inclusion of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7365&quot; title=&quot;Safer table creation and deletion using .tmp dir&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7365&quot;&gt;&lt;del&gt;HBASE-7365&lt;/del&gt;&lt;/a&gt;.  Probably need figure out how to thread the AssignmentDomain stuff in, has interesting follow-on work for snapshot clones.  (snapshots should likely ignore on the first cut)&lt;/li&gt;
	&lt;li&gt;Step 3 &amp;#8211; this is done in assignment manager?&lt;/li&gt;
	&lt;li&gt;recovery done by assignment manager?&lt;/li&gt;
	&lt;li&gt;What does the maintenance tool interact with and need to see &amp;#8211; the new meta table cols, the master, the assignment domain?  is this maintenance tool the only place other than the creation time that changes the preferred dn&apos;s?  Should there be commands to manually override the dn choices?  What would the be roughly for surgery purposes?&lt;/li&gt;
	&lt;li&gt;The optimization policies are nice.&lt;/li&gt;
	&lt;li&gt;For natural splits &amp;#8211; for the first implementation &amp;#8211; what is the story? (no preferred dn&apos;s specified? copies parent&apos;s preferred DN&apos;s?)  If there are no dn&apos;s specified or if the specified dn&apos;s are invalid we &quot;fall back&quot; to the old randome policies, yeah?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13591101" author="devaraj" created="Fri, 1 Mar 2013 23:42:47 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; for the review. Responses to your comments/questions below:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;An assignmentDomain is not what I&apos;ve been calling an affinity group over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; &#8211; it is the set of possible DN&apos;s to assign to, yeah? Are assignment domains persisted or just queried when creating tables (how does this info come from hdfs)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, AssignmentDomain is a set of nodes you carve out based on some policies. Currently it is the whole cluster. I think work needs to be done to make it a useful abstraction for multi-tenancy or load-balancing, etc. and no they are not persisted as of now. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Table creation was recently changed with the inclusion of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7365&quot; title=&quot;Safer table creation and deletion using .tmp dir&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7365&quot;&gt;&lt;del&gt;HBASE-7365&lt;/del&gt;&lt;/a&gt;. Probably need figure out how to thread the AssignmentDomain stuff in, has interesting follow-on work for snapshot clones. (snapshots should likely ignore on the first cut)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think this matters in the short term. We are mostly talking about the real data in the context of AssignmentDomain as opposed to metadata (about file paths and so on).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Step 3 &#8211; this is done in assignment manager?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. An assign() method that takes a Map&amp;lt;HRegionInfo, List&amp;lt;ServerName&amp;gt;&amp;gt; has been introduced. The method internally assigns the region to the 0th element (the primary RS) in the list for every region in the map. This flow works well for (pre-split) tables created newly. I need to see how the flow of &quot;enableTable -&amp;gt; (after sometime)disableTable -&amp;gt; (after sometime)enableTable&quot; works in this context.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;recovery done by assignment manager?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, AM and SSH I&apos;d think. I haven&apos;t thought much about this part yet.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What does the maintenance tool interact with and need to see &#8211; the new meta table cols, the master, the assignment domain? is this maintenance tool the only place other than the creation time that changes the preferred dn&apos;s? Should there be commands to manually override the dn choices? What would the be roughly for surgery purposes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The tool would interact with the metatable cols and the AssignmentDomain (although in the first implementation, the assignmentdomain is the whole cluster). Yes this is the only place other than the table creation that would change the preferred DNs. Agree with manual overrides (that&apos;s what you meant by surgery?).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For natural splits &#8211; for the first implementation &#8211; what is the story? (no preferred dn&apos;s specified? copies parent&apos;s preferred DN&apos;s?) If there are no dn&apos;s specified or if the specified dn&apos;s are invalid we &quot;fall back&quot; to the old randome policies, yeah?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we can do better than random policies, and copying parent&apos;s preferred DNs seems fine as a start as well. I&apos;ll address this at some point.&lt;/p&gt;</comment>
                            <comment id="13591110" author="jmhsieh" created="Fri, 1 Mar 2013 23:51:06 +0000"  >&lt;p&gt;Good stuff. Thanks for bearing with me &amp;#8211; I like to understand things at this level to set my expectations before I dive into code reviews.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The tool would interact with the metatable cols and the AssignmentDomain (although in the first implementation, the assignmentdomain is the whole cluster). Yes this is the only place other than the table creation that would change the preferred DNs. Agree with manual overrides (that&apos;s what you meant by surgery?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;sorry about the bad english.  surgery == hbck-related things like fixups in case the tools or nodes make mistakes to unstuck a cluster.  &lt;/p&gt;

&lt;p&gt;I think it would be good to spell out the different planned pieces for the tools, placment policy framework/hooks, etc as subtasks.&lt;/p&gt;</comment>
                            <comment id="13591132" author="devaraj" created="Sat, 2 Mar 2013 00:12:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;sorry about the bad english. surgery == hbck-related things like fixups in case the tools or nodes make mistakes to unstuck a cluster.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;To me, one issue that might happen is that the assignmentdomain is small and when failures happen, you run out of nodes to choose from for assigning regions. Other than that, I don&apos;t think we should ever get to a situation where we end up in a situation where a cluster is stuck because of the placements. In the worst case, the cluster should operate as it does today (with region file blocks on random nodes). Do you have any other thought in mind?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think it would be good to spell out the different planned pieces for the tools, placment policy framework/hooks, etc as subtasks.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Okay will do.&lt;/p&gt;</comment>
                            <comment id="13592651" author="nkeywal" created="Mon, 4 Mar 2013 21:20:54 +0000"  >&lt;p&gt;I think it&apos;s quite good imho. I don&apos;t have much issues or warning, I think it&apos;s just gonna work.&lt;/p&gt;

&lt;p&gt;Some comments:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Creation of table flow (assuming pre-split table)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;All this is great imho. Taking into account the racks is quite important. AssignmentDomain seems good.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;2. The meta table is updated with the information about the location mapping for the added regions.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I understand this as &apos;meta holds the favored nodes information&apos;. It&apos;s fine for me.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Failure recovery&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The difficult point is to choose the third RS now: we&apos;ve got one missing. Some comments:&lt;br/&gt;
-&amp;gt; We now have 2 RS on the same rack. So the config will be primary &amp;amp; secondary on the same rask and tertiary on another (not ideal).&lt;br/&gt;
-&amp;gt; We can imagine a situation where the first RS will come back to life soon (rolling restart for example).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;TODO: Handle non pre-split tables&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;From a locality point of view, it&apos;s already an issue today: after the split, it seems better to move them to different servers, but we just created files locally. I would tend to think that it&apos;s the job a the next major compaction to clean all this.&lt;/p&gt;



&lt;p&gt;We may have a first step in which we just go to the same servers for WAL &amp;amp; newly created HFiles. Rationale:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;today the locality is achieved by triggering a major compaction, so this would remain ok.&lt;/li&gt;
	&lt;li&gt;from a WAL point of view, with a 100 nodes cluster and 8 files per WAL, you will not recover from a 5 nodes loss 5% of the time. If we go on the same servers we divide this probability by 8.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And a second step could be to use this in the balance.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if we lose a RS will have locality as well on the new node.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13594273" author="devaraj" created="Wed, 6 Mar 2013 02:57:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt;The difficult point is to choose the third RS now: we&apos;ve got one missing. Some comments:&lt;br/&gt;
-&amp;gt; We now have 2 RS on the same rack. So the config will be primary &amp;amp; secondary on the same rask and tertiary on another (not ideal).&lt;br/&gt;
-&amp;gt; We can imagine a situation where the first RS will come back to life soon (rolling restart for example).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm.. We should designate an RS in a different rack (all new store files would go to that node, and all existing data would eventually get to that node via compactions). For the rolling restart case, it should be fine since the meta assignments wouldn&apos;t change and when the primary comes back to life, the regions (probably currently assigned to the secondary) would be reassigned. But yeah, I see that the loadbalancer would probably have to be aware of the rolling restart situation so that it doesn&apos;t prematurely assume certain RSs are really &quot;down&quot; and take (wasteful) corrective actions.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We may have a first step in which we just go to the same servers for WAL &amp;amp; newly created HFiles. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm.. good point. Will tackle WALs as a subtask of this jira.&lt;/p&gt;

&lt;p&gt;The patch in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7932&quot; title=&quot;Do the necessary plumbing for the region locations in META table and send the info to the RegionServers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7932&quot;&gt;&lt;del&gt;HBASE-7932&lt;/del&gt;&lt;/a&gt; does a major part of the work for getting the location information to the meta table, and then send it down to the RS. I need to use the API (&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt;) in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7942&quot; title=&quot;Make use of the plumbing in HBASE-7932 to provide location hints when region files are created&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7942&quot;&gt;&lt;del&gt;HBASE-7942&lt;/del&gt;&lt;/a&gt; to really create the files on specific nodes. The balancer work would be separate subtask.&lt;/p&gt;</comment>
                            <comment id="13603947" author="devaraj" created="Fri, 15 Mar 2013 22:52:12 +0000"  >&lt;p&gt;I was discussing with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sanjay.radia&quot; class=&quot;user-hover&quot; rel=&quot;sanjay.radia&quot;&gt;Sanjay Radia&lt;/a&gt; on this topic since there is HDFS dependency.. He came up with another idea on the HDFS side (and he plans to implement it soon). Seemed good to me. When a RS failure happens, a random RS picks some region up (as usual). Now when the region is being served by that RS, HDFS also transparently replicates the associated blocks onto that. Eventually, the remote blocks become local. To start with, one simple API could be exposed by the HDFS - makeBlocksLocal(Path). When some client tries to access the region, the RS serves the region but also makes this API call for the HFile paths that the region is comprised of. The copying of the blocks happen in the background.&lt;/p&gt;

&lt;p&gt;The pros is that it is simple to think about, and doesn&apos;t intrude much into HBase. In the current approach (via &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt;), on a failure, the new RS would have all the blocks local. But it requires HBase to periodically go and make sure the locality maps (in the meta table) are optimal (since nodes go down, etc.), and maybe reassign regions based on degree of locality w.r.t. the datanodes, etc.&lt;br/&gt;
In this approach, we won&apos;t wait for a compaction to happen to rewrite the hfile data locally in the new RS.&lt;/p&gt;

&lt;p&gt;The cons is that when a failure happens, there might be significant network communication when the blocks of the accessed regions are getting localized into the new RSs for the regions (but maybe many of them are rack local to the new RSs). This will not be worse than what would happen today though.&lt;/p&gt;

&lt;p&gt;What do people think about the above?&lt;/p&gt;</comment>
                            <comment id="13603958" author="yuzhihong@gmail.com" created="Fri, 15 Mar 2013 23:00:40 +0000"  >&lt;p&gt;Interesting idea.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;one simple API could be exposed by the HDFS - makeBlocksLocal(Path)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Should the parameter for the new method be of type List&amp;lt;Path&amp;gt; or Path[] ? There may be more than one HFile involved with the region.&lt;br/&gt;
Does region server have to issue this call more than once to the Namenode ? Ideally there should be a call back informing the progress of data movement for the locality request.&lt;/p&gt;</comment>
                            <comment id="13603967" author="enis" created="Fri, 15 Mar 2013 23:12:43 +0000"  >&lt;p&gt;I like Sanjay&apos;s idea in that it is more generic, and can help other users apart from hbase. It can also be used further to get rid of distributed cache, which is a giant hack, and have true distributed cache with very high replication from within hdfs. &lt;br/&gt;
Assuming we have that, it is not clear to me how much we can gain by doing &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; on top of this. I think Devaraj lists the pros and cons quite neatly. &lt;br/&gt;
I would say, depending on the plan to implement it, we might as well push for it, and reevaluate the plans for &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="13603971" author="ghelmling" created="Fri, 15 Mar 2013 23:20:04 +0000"  >&lt;p&gt;Sanjay&apos;s idea seems like a useful capability in itself, but I don&apos;t think it solves the whole problem.  For certain use cases, being able to fail a region over to a host that already has data locality for the store files is going to be critical.  Adding the capability to pull blocks local on demand would shorten the period of high latency due to remote HDFS reads, but it doesn&apos;t eliminate it.  So I think that having &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; in place would still be extremely valuable.&lt;/p&gt;</comment>
                            <comment id="13603982" author="apurtell" created="Fri, 15 Mar 2013 23:31:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;For certain use cases, being able to fail a region over to a host that already has data locality for the store files is going to be critical&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Online serving cases in general will notice the increased latency of remote reads. Especially if local reads are short circuit, and maybe even on devices that can handle high read IOPS. It would be great if we can avoid remote reads as much as possible. &lt;/p&gt;</comment>
                            <comment id="13603990" author="devaraj" created="Fri, 15 Mar 2013 23:43:13 +0000"  >&lt;p&gt;Quick comments -&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt;, agree that the api should take List&amp;lt;Path&amp;gt; or Path[].. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ghelmling&quot; class=&quot;user-hover&quot; rel=&quot;ghelmling&quot;&gt;Gary Helmling&lt;/a&gt;, so I&apos;d say that Sanjay&apos;s idea would improve things without too much intrusion into HBase&apos;s internals. In the cases, where the RS failures are not very commonplace, it&apos;d benefit a lot..&lt;/p&gt;</comment>
                            <comment id="13603991" author="enis" created="Fri, 15 Mar 2013 23:43:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;Online serving cases in general will notice the increased latency of remote reads&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;On failover, we are paying at least for the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;replaying the log. Historically this has been the biggest problem, but should be getting better. And with Jeffrey&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7835&quot; title=&quot;Implementation of the log splitting without creating intermediate files &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7835&quot;&gt;&lt;del&gt;HBASE-7835&lt;/del&gt;&lt;/a&gt;, I would expect this becomes even better. We are not doing local reads to the log files here, and Sanjay&apos;s proposal does not help in this case. But the data is supposed to be one-two orders of magnitude smaller than in hfiles.&lt;/li&gt;
	&lt;li&gt;filling up the block cache: I think this is the biggest cost we are paying for online serve. HDFS improvements wont help us here.&lt;/li&gt;
	&lt;li&gt;remote reads to fill up the block cache: without &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt;, we will get eventual local reads. On failover, we can trigger all the blocks of all files to be copied local. I think we have to quantify how eventual this would be, and what we are paying for.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13603994" author="enis" created="Fri, 15 Mar 2013 23:47:42 +0000"  >&lt;p&gt;To be clear, I think we can do both of these. It is just that the more generic approach can be integrated quickly, and will benefit other hdfs clients, and might be a 80-20% situation. We can still pursue &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; in parallel if there is enough justification for the complexity it brings. I am just saying that it would be good to quantify the benefits, because managing block locations from HBase opens another can of worms. &lt;/p&gt;</comment>
                            <comment id="13604024" author="devaraj" created="Sat, 16 Mar 2013 00:10:33 +0000"  >&lt;p&gt;Sanjay&apos;s new jira. If &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4606&quot; title=&quot;HDFS API to move file replicas to caller&amp;#39;s location&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4606&quot;&gt;HDFS-4606&lt;/a&gt; is addressed, it&apos;d make sense to use it to get some immediate benefits. &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; is still relevant and it would definitely bring benefits, and the point I am trying to get my heads around is what is the cost/benefit of the two approaches.&lt;/p&gt;</comment>
                            <comment id="13604029" author="ghelmling" created="Sat, 16 Mar 2013 00:13:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;replaying the log. Historically this has been the biggest problem, but should be getting better. And with Jeffrey&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7835&quot; title=&quot;Implementation of the log splitting without creating intermediate files &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7835&quot;&gt;&lt;del&gt;HBASE-7835&lt;/del&gt;&lt;/a&gt;, I would expect this becomes even better. We are not doing local reads to the log files here, and Sanjay&apos;s proposal does not help in this case. But the data is supposed to be one-two orders of magnitude smaller than in hfiles.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This could also be handled by &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; if we allowed a WAL per region, an idea with it&apos;s own pros and cons that we probably shouldn&apos;t get into here.  So this is not intractable.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;filling up the block cache: I think this is the biggest cost we are paying for online serve. HDFS improvements wont help us here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This assumes a cachable working set, which is not always the case.  Of course the block cache will help mask the cost of remote reads when you have good cache affinity.&lt;/p&gt;

&lt;p&gt;I also agree that Sanjay&apos;s API is worth pursuing.  I think it was something that was actually proposed way back with the idea of in-process lucene indexing with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3529&quot; title=&quot;Add search to HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3529&quot;&gt;&lt;del&gt;HBASE-3529&lt;/del&gt;&lt;/a&gt;, but was unfortunately shot down.  I also agree that this would be an improvement over the current situation, but the time to pull blocks local can still be a painful cost, especially in the case of larger multi-node failures.&lt;/p&gt;

&lt;p&gt;I do agree that &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2576&quot; title=&quot;Namenode should have a favored nodes hint to enable clients to have control over block placement.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2576&quot;&gt;&lt;del&gt;HDFS-2576&lt;/del&gt;&lt;/a&gt; comes at a cost of complexity, especially in terms of interactions with the HDFS balancer and normal decommissioning or under-replication handling.  But I think closing that final gap for immediate locality will make HBase more attractive for certain scenarios.&lt;/p&gt;</comment>
                            <comment id="13688814" author="jiangbinglover" created="Thu, 20 Jun 2013 02:52:53 +0000"  >&lt;p&gt;if hdfs makes block movement because of DN crashes or other reasons, would hbase need to track the changes of these blocks? Or hdfs new feature can support movement based on their previous hints? &lt;/p&gt;</comment>
                            <comment id="13689754" author="devaraj" created="Thu, 20 Jun 2013 22:07:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jiangbinglover&quot; class=&quot;user-hover&quot; rel=&quot;jiangbinglover&quot;&gt;Bing Jiang&lt;/a&gt;, yes HBase would need to periodically refresh the mappings, and also when compactions happen, the data would be rewritten in the three current nodes. I need to implement the balancer in FavoredNodeLoadBalancer (balanceCluster method). I should have something shortly.&lt;/p&gt;</comment>
                            <comment id="14255597" author="xieliang007" created="Mon, 22 Dec 2014 09:56:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, if my understanding is correct, this JIRA could be set to resolved status?  i am just reading the related HDFS/HBase locality optimization jira and thinking to port a raw/simple stuff to our internal 0.94 branch now&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15664687" author="thiruvel" created="Mon, 14 Nov 2016 18:54:58 +0000"  >&lt;p&gt;The work to enhance favored nodes feature is continuing at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15531&quot; title=&quot;Favored Nodes Enhancements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15531&quot;&gt;HBASE-15531&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12603333">HBASE-6572</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12637298">HDFS-4606</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12532186">HDFS-2576</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12702753">HDFS-6133</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12715909">HDFS-6441</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12647583">HBASE-8549</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12661434">HBASE-9116</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is required by">
                                        <issuelink>
            <issuekey id="12551766">HBASE-5843</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12569594" name="4755-wip-1.patch" size="134086" author="devaraj" created="Fri, 15 Feb 2013 20:00:15 +0000"/>
                            <attachment id="12571553" name="hbase-4755-notes.txt" size="2710" author="devaraj" created="Fri, 1 Mar 2013 07:50:05 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12634002">HBASE-7932</subtask>
                            <subtask id="12634216">HBASE-7942</subtask>
                            <subtask id="12642576">HBASE-8344</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 7 Nov 2011 19:04:13 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>216335</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hs4f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>101818</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>