<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:58:35 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2053/HBASE-2053.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2053] Upper bound of outstanding WALs can be overrun</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2053</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Kevin Peterson up on hbase-user posted the following.  Of interest is the link on the end which is logs of WAL rolls and removals.  In once place we remove 70plus logs because the outstanding edits have moved passed the outstanding sequence numbers &amp;#8211; so our basic WAL removal mechanism is working &amp;#8211; but if you study the log, the tendency is steady climb in the number of logs.   HLog#cleanOldLogs needs to notice such an upward tendency and work more aggressively cleaning the old in this case.  Here is Kevin&apos;s note:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
n Tue, Dec 15, 2009 at 3:17 PM, Kevin Peterson &amp;lt;x@y.com&amp;gt; wrote:
This makes some sense now. I currently have 2200 regions across 3 tables. My
largest table accounts &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; about 1600 of those regions and is mostly active
at one end of the keyspace -- our key is based on date, but data only
roughly arrives in order. I also write to two secondary indexes, which have
no pattern to the key at all. One of these secondary tables has 488 regions
and the other has 96 regions.

We write about 10M items per day to the main table (articles). All of these
get written to one of the secondary indexes (article-ids). About a third get
written to the other secondary index. Total volume of data is about 10GB /
day written.

I think the key is as you say that the regions aren&apos;t filled enough to
flush. The articles table gets mostly written to near one end and I see
splits happening regularly. The index tables have no pattern so the 10
millions writes get scattered across the different regions. I&apos;ve looked more
closely at a log file (linked below), and &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; I forget about my main table
(which would tend to get flushed), and look only at the indexes, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; seems
to be what&apos;s happening:

1. Up to maxLogs HLogs, it doesn&apos;t &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; any flushes.
2. Once it gets above maxLogs, it will start flushing one region each time
it creates a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HLog.
3. If the first HLog had edits &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; say 50 regions, it will need to flush the
region with oldest edits 50 times before the HLog can be removed.

If N is the number of regions getting written to, but not getting enough
writes to flush on their own, then I think &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; converges to maxLogs + N
logs on average. If I think of maxLogs as &quot;number of logs to start flushing
regions at&quot; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; makes sense.

http:&lt;span class=&quot;code-comment&quot;&gt;//kdpeterson.net/paste/hbase-hadoop-regionserver-mi-prod-app35.ec2.biz360.com.log.2009-12-14
&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12443672">HBASE-2053</key>
            <summary>Upper bound of outstanding WALs can be overrun</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Thu, 17 Dec 2009 18:53:22 +0000</created>
                <updated>Fri, 20 Nov 2015 13:01:56 +0000</updated>
                            <resolved>Sun, 3 Jan 2010 04:09:10 +0000</resolved>
                                                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12794462" author="viper799" created="Thu, 24 Dec 2009 17:25:26 +0000"  >&lt;p&gt;I have a table that runs on 7 servers and it is 714 regions with a split size of 512mb so all quite large&lt;br/&gt;
I set the block size to 32MB so the index size is vary low &amp;lt; 1mb on each server so each server can handle a lot of regions.&lt;br/&gt;
I have test and one region server at one points could handle ~680 regions with out memory problems heap is 1gb.&lt;br/&gt;
I can do this because I do not need random access on this table form my apps so only full scans the table.&lt;/p&gt;

&lt;p&gt;The problem is coming from hosting ~120 regions on each server as edits come in the hlogs roll and flushes happen on the hot regions with the largest memstore the smaller memstores only get flushed when the hlog limit is reached and it starts flushing to help clear out the old logs. But this only happens once per hlog roll would could have 100 region edits or more outstanding edits in this hlog. &lt;/p&gt;

&lt;p&gt;I set my max logs to 24 and I see 100-120 log files a lot when data is getting added.&lt;/p&gt;

&lt;p&gt;I thank a simple fix for this would be to flushed the oldest outstanding seqnum and loop or queue flushes until it flushes all outstanding memstore edits for the oldest log file.&lt;br/&gt;
I am just not sure how that would work with the code for the next hlog roll if we are still flushing from the first one we do not want t block a hlog roll.&lt;/p&gt;

&lt;p&gt;My only problem with this is when I have a region fail for some reason the master may have to process 100+ logs to get going again&lt;br/&gt;
I know I read some where that we had plans on moving the recovering of logs from the master the the region servers so it could happen much more quickly.&lt;/p&gt;

&lt;p&gt;Also note we might have to start dealing with more hlogs hanging around with replication in the works.&lt;br/&gt;
So in the end this may not be a issue that we need to fix I do not know what the memory impact would be with 100&apos;s of logs &lt;/p&gt;</comment>
                            <comment id="12794527" author="stack" created="Fri, 25 Dec 2009 00:50:11 +0000"  >&lt;p&gt;Hey Billy.&lt;/p&gt;

&lt;p&gt;.bq I thank a simple fix for this would be to flushed the oldest outstanding seqnum and loop or queue flushes until it flushes all outstanding memstore edits for the oldest log file.&lt;/p&gt;

&lt;p&gt;The way it currently works is that if too many log files,  we&apos;ll find the region that has the oldest sequence number and schedule a flush on that.  This doesn&apos;t seem to be enough.  We need to do something like you suggest.... keep flushing the oldest till we pass out at least one WAL file. &lt;/p&gt;

&lt;p&gt;Can you post a regionserver log that has too many running WALs (if you have DEBUG enabled) so can study the issue?  Thanks.&lt;/p&gt;</comment>
                            <comment id="12794532" author="viper799" created="Fri, 25 Dec 2009 01:53:09 +0000"  >&lt;p&gt;Region server attached for a day&lt;br/&gt;
Might be some table flushes in there &lt;br/&gt;
From time to time I flush the whole table just to help clean out some logs in case of a failed region server.&lt;/p&gt;

&lt;p&gt;Hlog max = 24 and Hlog roll time = 5 mins&lt;/p&gt;</comment>
                            <comment id="12794533" author="viper799" created="Fri, 25 Dec 2009 01:55:18 +0000"  >&lt;p&gt;Hlog max = 32 and Hlog roll time = 5 mins&lt;/p&gt;</comment>
                            <comment id="12794973" author="stack" created="Tue, 29 Dec 2009 00:23:14 +0000"  >&lt;p&gt;Billy, looking at your log, it seems you have set hbase.regionserver.logroll.period to 5 minutes instead of the default hour, is that right?&lt;/p&gt;

&lt;p&gt;Whats happening when the log roll period is so short is that you&apos;ll get a log roll even if only one edit in it:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-12-22 00:01:38,102 INFO org.apache.hadoop.hbase.regionserver.HLog: Roll /hbase/.logs/server-2,60020,1261340217387/hlog.dat.1261461398009, entries=1, calcsize=0, filesize=303. New hlog /hbase/.logs/server-2,60020,1261340217387/hlog.dat.1261461698084
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="12794990" author="viper799" created="Tue, 29 Dec 2009 01:29:16 +0000"  >&lt;p&gt;I set to 5 mins to help lower the number of log files in down times when there is no imports happening.&lt;br/&gt;
The hlogs roll even with 0 edits in it and starts the flushing of memcache to get back under maxlogs&lt;br/&gt;
seams to help over time to reduce the log file count in slow import times.&lt;/p&gt;

&lt;p&gt;The log files that you see 70+ are all full thought so we still have a problem of building up logs &lt;br/&gt;
But I figured that when replication comes along the hbase.regionserver.logroll.period &lt;br/&gt;
would get reduced to 5 mins or less to help roll the logs so the logs could be process and sent to the other cluster.&lt;br/&gt;
So I figured I would try the lower roll time and see if I could find any problems now.&lt;/p&gt;</comment>
                            <comment id="12794994" author="stack" created="Tue, 29 Dec 2009 01:42:49 +0000"  >&lt;p&gt;Let me try and implement Billy&apos;s suggestion.  It looks doable.  I&apos;ll change rollWriter so it can return an array of Regions rather than a single Region.  Currently it&apos;ll just return one, the oldest but flushing the oldest might not be enough to let go of the oldest WAL.   Check sequenceids.  Return regions to flush until at least one WAL is let go.&lt;/p&gt;</comment>
                            <comment id="12795017" author="stack" created="Tue, 29 Dec 2009 06:17:31 +0000"  >&lt;p&gt;First cut at a patch.  All tests pass and it includes a unit test but need to test on cluster.&lt;/p&gt;

&lt;p&gt;This patch changes clearOldLogs so it returns an array of regions to flush instead of a single region.  Internally, if logs are &amp;gt; max logs, then we figure all regions whose memstores start out with edits &amp;lt; oldest WAL logs edit and return these for flushing.  Should make for removal of at least the oldest file on each invocation instead of what we had where we just flushed oldest memstore without regard for how it relates to outstanding WAL logs.&lt;/p&gt;</comment>
                            <comment id="12795548" author="stack" created="Thu, 31 Dec 2009 06:01:06 +0000"  >&lt;p&gt;Patch is helping &amp;#8211; it&apos;ll schedule more than one region for flushing on occasion &amp;#8211; but I&apos;m seeing that flushes are not keeping up.  I&apos;m logging their being added to the queue and am then dumping the queut but the flush thread is not waking up to clear the queue under load.  Investigating more.&lt;/p&gt;</comment>
                            <comment id="12795549" author="stack" created="Thu, 31 Dec 2009 06:01:35 +0000"  >&lt;p&gt;Working on this for 0.20.3.&lt;/p&gt;</comment>
                            <comment id="12795706" author="stack" created="Thu, 31 Dec 2009 23:37:39 +0000"  >&lt;p&gt;Added some logging.&lt;/p&gt;</comment>
                            <comment id="12795710" author="stack" created="Fri, 1 Jan 2010 00:21:50 +0000"  >&lt;p&gt;So, this is an interesting problem.  This patch is not enough.  It makes the situation better though so I think it should be committed to the branch and trunk.   It needs a +1.&lt;/p&gt;

&lt;p&gt;The patch makes it so we can queue more than one region flush if too many log files.  If too many log files, it&apos;ll make sure that we queue the flushing of regions enough to free up at least the oldest WAL file.  Previous we just flushed the oldest region though the oldest WAL could have more than just the oldest regions edits in it.&lt;/p&gt;

&lt;p&gt;This patch isn&apos;t enough though because flushing gets held up for long periods of time.&lt;/p&gt;

&lt;p&gt;One such reason is hdfs running slow so flush takes a long time.  Flushes are queued and then addressed oldest first.  Queue might accumulate many regions to flush just by way of normal operation.  Since its not a priority queue, the flush needed to free up the WAL may not happen for a while until all ahead of it in the queue have been cleared.&lt;/p&gt;

&lt;p&gt;A more serious one is the mechanism whereby we hold up flush on a region if we have too many store files.  The hold up is done for a single region but the wait on compaction is inline with the MemStoreFlusher#run thread so no other flushes can happen while we&apos;re waiting on store file count to shrink because of a compaction.  This is a bug.  I&apos;ll open an issue.&lt;/p&gt;

&lt;p&gt;It doesn&apos;t take much for us to accumulate many log files if the upload rate is high and flushing is taking a while or is  heldup.   Flushes can take 3-10 seconds on slow HDFS.  WALs can be rolling every 4-5 seconds in a PE upload.&lt;/p&gt;</comment>
                            <comment id="12795714" author="jdcryans" created="Fri, 1 Jan 2010 00:58:49 +0000"  >&lt;p&gt;+1 on patch.&lt;/p&gt;

&lt;p&gt;We get in this situation because our WALs are small and very limited in number so that we don&apos;t lose too much data and the Master can split fast.&lt;/p&gt;

&lt;p&gt;In 0.21, only the split speed factor remains. If we can get distributed splitting in (my fault if it&apos;s still not done), then we could probably set size and number much higher.&lt;/p&gt;</comment>
                            <comment id="12795742" author="viper799" created="Fri, 1 Jan 2010 10:03:27 +0000"  >&lt;p&gt;I agree with Jean-Daniel Cryans about size of hlog I would not care how big it was if we had distributed splitting &lt;br/&gt;
also now with the Hlog Build up solved we can somewhat now predict the max we have to recover at any point in time.&lt;/p&gt;</comment>
                            <comment id="12795946" author="stack" created="Sun, 3 Jan 2010 03:58:53 +0000"  >&lt;p&gt;Changing this to an improvement rather than bug because patch only improves the situation, it doesn&apos;t fix it.&lt;/p&gt;</comment>
                            <comment id="12795947" author="stack" created="Sun, 3 Jan 2010 04:09:10 +0000"  >&lt;p&gt;Applied branch and trunk.&lt;/p&gt;</comment>
                            <comment id="12795948" author="stack" created="Sun, 3 Jan 2010 04:09:52 +0000"  >&lt;p&gt;@j-d yes.  agreed.&lt;/p&gt;</comment>
                            <comment id="15017937" author="lars_francke" created="Fri, 20 Nov 2015 13:01:56 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12429233" name="2053-v2.patch" size="11132" author="stack" created="Thu, 31 Dec 2009 23:37:39 +0000"/>
                            <attachment id="12429057" name="2053.patch" size="11636" author="stack" created="Tue, 29 Dec 2009 06:17:31 +0000"/>
                            <attachment id="12428942" name="hbase-root-regionserver-server-2.log.2009-12-22.gz" size="373450" author="viper799" created="Fri, 25 Dec 2009 01:53:09 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 24 Dec 2009 17:25:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32386</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hga7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99900</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>