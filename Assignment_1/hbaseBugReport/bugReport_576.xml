<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:45:34 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-576/HBASE-576.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-576] Investigate IPC performance</title>
                <link>https://issues.apache.org/jira/browse/HBASE-576</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Turning off all file I/O, and running the PerformanceEvaluation test, of 1,048,576 sequential writes to HBase managed to achieve only 7,285 IPCs per second.&lt;/p&gt;

&lt;p&gt;Running PerformanceEvaluation sequential write test modified to do an abort instead of a commit, it was possible to do 68,337 operations per second. We are obviously spending a lot of time doing IPCs. &lt;/p&gt;

&lt;p&gt;We need to investigate to find the bottleneck. Marshalling and unmarshalling? Socket setup and teardown?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12393786">HBASE-576</key>
            <summary>Investigate IPC performance</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="jimk">Jim Kellerman</reporter>
                        <labels>
                    </labels>
                <created>Fri, 11 Apr 2008 21:11:35 +0000</created>
                <updated>Sun, 13 Sep 2009 22:26:25 +0000</updated>
                            <resolved>Wed, 8 Oct 2008 23:19:33 +0000</resolved>
                                    <version>0.1.0</version>
                    <version>0.1.1</version>
                    <version>0.1.2</version>
                    <version>0.2.0</version>
                                    <fixVersion>0.19.0</fixVersion>
                                    <component>IPC/RPC</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12588829" author="stack" created="Mon, 14 Apr 2008 22:34:00 +0000"  >&lt;p&gt;We should check out Dennis&apos;s patch: &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-3053&quot; title=&quot;New Server Framework for Hadoop RPC&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-3053&quot;&gt;&lt;del&gt;HADOOP-3053&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12589134" author="stack" created="Tue, 15 Apr 2008 16:04:33 +0000"  >&lt;p&gt;Dennis says patch does not improve throughput.  Suggests looking at &lt;a href=&quot;https://grizzly.dev.java.net/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://grizzly.dev.java.net/&lt;/a&gt;.  There is also &lt;a href=&quot;http://mina.apache.org/features.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mina.apache.org/features.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Lets first study what is taking the time &amp;#8211; serialization, introspection &amp;#8211; and try and speed up hadoop ipc.&lt;/p&gt;</comment>
                            <comment id="12636537" author="stack" created="Fri, 3 Oct 2008 05:48:44 +0000"  >&lt;p&gt;Looking at grizzly, the nio framework, and at our current RPC.&lt;/p&gt;

&lt;p&gt;Grizzly drop-in looks straight-forward enough; add to grizzly a protocol that can send and receive Writables.  Would be good to keep up things like the ping feature in current RPC and not change the error types, messages, and provocations that we&apos;ve come to know and love.   The bulk of the current RPC code which collects up method name and parameters into an Invoker Writable would be remain though recast some.&lt;/p&gt;

&lt;p&gt;Current RPC has single ipc Client instance per remote host (get client from cache using remote address and socketfactory hash).  Because only one Client per remote server, then its looking like request/response&apos;s will always run in series (though code would seem to support many Clients contending sending requests each sleeping till its response comes back).  In-series is fine for the usual case.  If multiple concurrent HTables in the one VM each trying to do its lookup on .META. say, the invocations run in series too.  Not the end of the world but could be better (Running tests to confirm).&lt;/p&gt;

&lt;p&gt;One thing I notice is that the one Connection per remote host is propagated up into TableServers in hbase client.  Need to undo it.&lt;/p&gt;</comment>
                            <comment id="12636538" author="stack" created="Fri, 3 Oct 2008 05:49:48 +0000"  >&lt;p&gt;Here&apos;s some changes to PE so can run multiple clients all up in the one VM; each client runs in its own thread.  Also added argument that takes how many rows to run per test.&lt;/p&gt;</comment>
                            <comment id="12636791" author="stack" created="Sat, 4 Oct 2008 06:45:36 +0000"  >&lt;p&gt;Above comment &apos;stack - 02/Oct/08 10:48 PM&apos; is incorrect.  In fact it&apos;s a Client per socketfactory rather than a Client per remote host.   A Client goes against many hosts.  For each, Client keeps a cache of Connections keyed by remote address (and then some).  An invocation on the remote host sends the request then sleeps till the response comes back.  During sending and receipt of response, the connection is devoted but otherwise, is available.  Unless the request or response large, socket is being &apos;multiplexed&apos; (if large, need to chunk request/response).&lt;/p&gt;

&lt;p&gt;Here&apos;s sample from a log with the RPC logging enabled plus some logging I added.  The log is of 4 threads running in single JVM each running PerformanceEvaluation test (need above patch applied).  The lone number after the name of the class is the thread name.   Threads were named 0, 1, 2, and 3.  The numbers at the end with the &apos;####&apos; in front are request ids.  See how they are interlaced.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
...
08/10/03 23:16:03 INFO ipc.HBaseClient: 3 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1330
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1327
08/10/03 23:16:03 DEBUG ipc.Client: 0 FINISHED WAITING ON ###1327
08/10/03 23:16:03 INFO ipc.HBaseClient: 0 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 80
08/10/03 23:16:03 INFO ipc.HBaseClient: 0 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1331
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1328
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1329
08/10/03 23:16:03 DEBUG ipc.Client: 1 FINISHED WAITING ON ###1328
08/10/03 23:16:03 INFO ipc.HBaseClient: 1 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.Client: 2 FINISHED WAITING ON ###1329
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 97
08/10/03 23:16:03 INFO ipc.HBaseClient: 2 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 98
08/10/03 23:16:03 INFO ipc.HBaseClient: 1 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 INFO ipc.HBaseClient: 2 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1332
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1333
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1330
08/10/03 23:16:03 DEBUG ipc.Client: 3 FINISHED WAITING ON ###1330
08/10/03 23:16:03 INFO ipc.HBaseClient: 3 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 99
08/10/03 23:16:03 INFO ipc.HBaseClient: 3 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1334
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1331
08/10/03 23:16:03 DEBUG ipc.Client: 0 FINISHED WAITING ON ###1331
08/10/03 23:16:03 INFO ipc.HBaseClient: 0 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 113
08/10/03 23:16:03 INFO ipc.HBaseClient: 0 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1335
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1332
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1333
08/10/03 23:16:03 DEBUG ipc.Client: 1 FINISHED WAITING ON ###1332
08/10/03 23:16:03 INFO ipc.HBaseClient: 1 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.Client: 2 FINISHED WAITING ON ###1333
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 178
08/10/03 23:16:03 INFO ipc.HBaseClient: 2 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
08/10/03 23:16:03 DEBUG ipc.HbaseRPC: Call: get 178
08/10/03 23:16:03 INFO ipc.HBaseClient: 1 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 INFO ipc.HBaseClient: 2 org.apache.hadoop.ipc.HBaseClient@da9ea4 start /208.76.44.139:60020
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1336
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user sending #####1337
08/10/03 23:16:03 DEBUG ipc.Client: IPC Client (47) connection to /208.76.44.139:60020 from an unknown user got value #####1334
08/10/03 23:16:03 DEBUG ipc.Client: 3 FINISHED WAITING ON ###1334
08/10/03 23:16:03 INFO ipc.HBaseClient: 3 org.apache.hadoop.ipc.HBaseClient@da9ea4 done
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hadoop RPC doesn&apos;t seem to have a &apos;big lock&apos; at its core, not unless the request or response large. Its pretty concurrent.&lt;/p&gt;

&lt;p&gt;Will look now at trying to hack in grizzly in the least intrusive manner just to see if grizzly is faster.&lt;/p&gt;</comment>
                            <comment id="12637337" author="stack" created="Tue, 7 Oct 2008 01:04:22 +0000"  >&lt;p&gt;Profiling, looks like multithreaded client is mostly waiting.  Retrying with clean everything so can get decent figures on throughput.&lt;/p&gt;

&lt;p&gt;One thing I noticed profiling is that ~5% of client-side CPU is spent doing convertions of keys into and out of bytes and booleans and longs going to HTableDescriptor (HTD isRootRegion/isMetaRegion became hotspot when we did hacks to make binary keys work).  At a minimum, the keys should be static finals rather than composed on each invocation and we should be putting native types into our HTD map.   They could even be static final ImmutableBytesWritables.&lt;/p&gt;

&lt;p&gt;Another thing to fix in client is sending the method name over as part of the Invocation.  We&apos;ve already made it so we don&apos;t send class name of parameter by subclassing hadoop RPC and jiggering ObjectWritable.  Wouldn&apos;t take much to do same for method name.  We spend a bunch of time sending and reading the method name; 10s of percents of CPU.&lt;/p&gt;</comment>
                            <comment id="12637343" author="jdcryans" created="Tue, 7 Oct 2008 01:24:49 +0000"  >&lt;p&gt;Patch that fixes some object creations in client that wasted 5% of the CPU.&lt;/p&gt;</comment>
                            <comment id="12637353" author="stack" created="Tue, 7 Oct 2008 03:26:17 +0000"  >&lt;p&gt;Thanks J-D. Patch looks good.  Pity couldn&apos;t be fixed better but yeah, would need migration script.  As is will save a bunch of churn.  Let me commit it.&lt;/p&gt;

&lt;p&gt;Looking at rpc, I see I broke it a while back; I removed the very reason we subclass RPC.  I replaced all our carefully planted HbaseObjectWritables with default ObjectWritables.  Means we&apos;re sending Strings instead of codes for our parameter names.&lt;/p&gt;

&lt;p&gt;So, did a test where a cluster had 1M rows loaded into 11 regions spread over 3 machines.  A single client could random-read at ~482/second.  Using above patch and running with 8 threads, was able to read at 1531/second.  Basic formula: throughput can be multiplied by # of threads up to maximum of number of cluster members: e.g. if 8 threads but only 3 servers, can only see 3X throughput improvement.  If 8 servers hosting regions, should see 8X.&lt;/p&gt;
</comment>
                            <comment id="12637380" author="stack" created="Tue, 7 Oct 2008 05:31:24 +0000"  >&lt;p&gt;Thanks for the patch J-D.  Went in w/ some slop.  I removed an unneeded import.&lt;/p&gt;

&lt;p&gt;Any chance of your making a patch to replace all instances of ObjectWritable in HbaseRPC with HbaseObjectWritable?  Its a silly regression on my part broken in the below commit:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
r679212 | stack | 2008-07-23 15:13:23 -0700 (Wed, 23 Jul 2008) | 1 line

HBASE-770 Update HBaseRPC to match hadoop 0.17 RPC
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I did some more timings. An instance with 3 threads did 936/second random reads which is about twice a single thread and about 2/3rds 8 threads.&lt;/p&gt;
</comment>
                            <comment id="12637562" author="stack" created="Tue, 7 Oct 2008 17:47:13 +0000"  >&lt;p&gt;Below numbers are for cluster of three regionservers w/ client running on master &amp;#8211; which was not running regionserver.&lt;/p&gt;

&lt;p&gt;482/s w/ 1 thread&lt;br/&gt;
936/s w/ 3 threads&lt;br/&gt;
1092/s w/ 4 threads&lt;br/&gt;
1531/s w/ 8 threads&lt;br/&gt;
1782/s w/ 16 threads&lt;/p&gt;

&lt;p&gt;My cluster is small.  That probably has something to do w/ why the less-than-linear progress.&lt;/p&gt;</comment>
                            <comment id="12638033" author="stack" created="Wed, 8 Oct 2008 19:10:54 +0000"  >&lt;p&gt;Patch that puts back HbaseObjectWritable and that sends codes for methodnames across the wire makes PE test 20% faster writing and reading over 0.18.1.  Patch coming soon.&lt;/p&gt;
</comment>
                            <comment id="12638121" author="stack" created="Wed, 8 Oct 2008 22:30:11 +0000"  >&lt;p&gt;Did a little informal test.  I put up 4 regionservers in my little cluster, loaded it w/ million rows then did random reading against the million rows from a remote client (different network).   I tried with different numbers of clients.  While the clients ran, I watched them in the profiler and I watched the requests/second up on the master node.  Here&apos;s a rough recording of what I saw up on the master requests/second.&lt;/p&gt;

&lt;p&gt;1 client - 230/s&lt;br/&gt;
4 clients - 630/s&lt;br/&gt;
16 clients - 1050/s&lt;br/&gt;
32 clients - 2460/s&lt;br/&gt;
64 clients - 2770/s&lt;br/&gt;
128 clients - 1150/s&lt;br/&gt;
256 clients - 1100/s&lt;/p&gt;

&lt;p&gt;For the 128 and 256 clients, I could most threads blocked in the client.   According to the profiler, when 4 or more clients, the RPC threads are spending all their time i/o on the net.  That 4 clients don&apos;t max the request/second would seem to say servers can easily carry more than one client request at a time (duh).  When the number of clients goes &amp;gt; 64, client looks like it starts to trip itself up spending bulk of time blocked.&lt;/p&gt;</comment>
                            <comment id="12638124" author="stack" created="Wed, 8 Oct 2008 22:42:37 +0000"  >&lt;p&gt;Doing same again running client on same net as cluster but running 3 regionservers only, I see that as the number of clients climb, the sustained throughput jumps around alot whereas its steady when number of clients are smaller.&lt;/p&gt;

&lt;p&gt;1 client - 450/s&lt;br/&gt;
3 clients - 930/s&lt;br/&gt;
9 clients - 1200/s&lt;/p&gt;</comment>
                            <comment id="12638127" author="stack" created="Wed, 8 Oct 2008 22:52:09 +0000"  >&lt;p&gt;Patch that passes codes rather than method names over RPC.  Also includes the PE patch and the restoration of HbaseObjectWritable.&lt;/p&gt;</comment>
                            <comment id="12638133" author="stack" created="Wed, 8 Oct 2008 23:19:33 +0000"  >&lt;p&gt;Resolving.  Was able to improve RPC some after investigation.  If we want to squeeze more juice out of RPC, we need to bring all of hadoop RPC local and start hacking on it or do our own from scatch.   Things to look into would be undoing reflection &amp;#8211; though this seems to be relatively inexpensive going by recent profilings and it makes extending RPC simple &amp;#8211; and we&apos;d look at adding non-blocking to the clientside and sending big data in chunks. The need for a new RPC seems less necessary given the above investigations having learned that current HRPC is concurrent to some degree on the clientside and that the serverside is already non-blocking, the reason I thought an nio framework like grizzly was worth consideration.&lt;/p&gt;</comment>
                            <comment id="12639598" author="stack" created="Tue, 14 Oct 2008 21:31:47 +0000"  >&lt;p&gt;I backported J-D&apos;s HTD patch.  Profiling 0.18 branch, checking if key is of root region is consuming loads of CPU.  This squashes that.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12391785" name="576-nomethodname.patch" size="18391" author="stack" created="Wed, 8 Oct 2008 22:52:09 +0000"/>
                            <attachment id="12391599" name="htd.patch" size="3083" author="jdcryans" created="Tue, 7 Oct 2008 01:24:49 +0000"/>
                            <attachment id="12391399" name="pe.patch" size="5759" author="stack" created="Fri, 3 Oct 2008 05:49:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 14 Apr 2008 22:34:00 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31765</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 10 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h85j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98583</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>