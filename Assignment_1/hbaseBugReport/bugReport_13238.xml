<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:41:10 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-13238/HBASE-13238.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-13238] Time out locks and abort if HDFS is wedged</title>
                <link>https://issues.apache.org/jira/browse/HBASE-13238</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This is a brainstorming issue on the topic of timing out locks and aborting rather than waiting infinitely. Perhaps even as a rule.&lt;/p&gt;

&lt;p&gt;We had a minor production incident where a region was unable to close after trying for 24 hours. The CloseRegionHandler was waiting for a write lock on the ReentrantReadWriteLock we take in HRegion#doClose. There were outstanding read locks. Three other threads were stuck in scanning, all blocked on the same DFSInputStream. Two were blocked in DFSInputStream#getFileLength, the third was waiting in epoll from SocketIOWithTimeout$SelectorPool#select with apparent infinite timeout from PacketReceiver#readChannelFully.&lt;/p&gt;

&lt;p&gt;This is similar to other issues we have seen before, in the context of the region wanting to finish a compaction before closing for a split, but can&apos;t due to some HDFS issue causing the reader to become extremely slow if not wedged. This has lead to what should be quick SplitTransactions causing availability problems of many minutes in length.&lt;/p&gt;

&lt;p&gt;The Hadoop version was 2.3 (specifically 2.3 CDH 5.0.1), and we are planning to upgrade, but &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; and I were discussing the issue in general and wonder if we should not be timing out locks such as the ReentrantReadWriteLock, and if so, abort the regionserver. In this case this would have caused recovery and reassignment of the region in question and we would not have had a prolonged availability problem. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12781940">HBASE-13238</key>
            <summary>Time out locks and abort if HDFS is wedged</summary>
                <type id="13" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Brainstorming</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Mar 2015 21:58:03 +0000</created>
                <updated>Sat, 13 Jun 2015 01:32:57 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="14361174" author="apurtell" created="Fri, 13 Mar 2015 22:02:38 +0000"  >&lt;p&gt;Of course, why we propose aborts is the DFSClient is not interruptible. There are several places where the DFSClient swallows interrupts.&lt;/p&gt;</comment>
                            <comment id="14361416" author="apache9" created="Sat, 14 Mar 2015 00:46:44 +0000"  >&lt;p&gt;SocketIOWithTimeout does not timeout? So why does it have a &apos;Timeout&apos; in its name...&lt;br/&gt;
I think this is an HDFS issue, all I/O operations should have a configurable bounded timeout, and in HBase we deal with these timeout exceptions.&lt;br/&gt;
Of course a timeout lock could help, but I think there are lots of places where we just use synchronized? &lt;br/&gt;
Thanks.&lt;/p&gt;</comment>
                            <comment id="14361448" author="apurtell" created="Sat, 14 Mar 2015 01:10:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;SocketIOWithTimeout does not timeout? So why does it have a &apos;Timeout&apos; in its name...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Beats me.&lt;/p&gt;

&lt;p&gt;FWIW, I dumped the stack of the regionserver several times over a period of hours and the thread in question had the same stack trace each time. &lt;/p&gt;

&lt;p&gt;Even if we get hung up at the HDFS level, it&apos;s our problem, we can&apos;t just have indefinite unavailability in some known circumstances.&lt;/p&gt;</comment>
                            <comment id="14361479" author="apache9" created="Sat, 14 Mar 2015 01:42:04 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Even if we get hung up at the HDFS level, it&apos;s our problem, we can&apos;t just have indefinite unavailability in some known circumstances.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree.&lt;br/&gt;
I know sometimes it is hard to make HDFS support us(&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5940&quot; title=&quot;HBase in-cluster backup based on the HDFS hardlink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5940&quot;&gt;&lt;del&gt;HBASE-5940&lt;/del&gt;&lt;/a&gt;, almost 3 years and no progress...), they have their own plan.&lt;br/&gt;
But suggest to keep the work around code only in critical places and strongly document why we do this. &lt;/p&gt;</comment>
                            <comment id="14361515" author="lhofhansl" created="Sat, 14 Mar 2015 02:16:40 +0000"  >&lt;p&gt;Ideal we fix this in HDFS, but in this case we actually had to get the region server killed, just so we can get &quot;unstuck&quot;. I think it&apos;s reasonable to abort a region server when a region cannot close for 10 minutes (or something).&lt;/p&gt;

&lt;p&gt;Do you still have the two stack traces &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;? Might be instructive to attach them here.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12457&quot; title=&quot;Regions in transition for a long time when CLOSE interleaves with a slow compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12457&quot;&gt;HBASE-12457&lt;/a&gt; might be related.&lt;/p&gt;</comment>
                            <comment id="14362130" author="apurtell" created="Sun, 15 Mar 2015 01:10:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you still have the two stack traces Andrew Purtell?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Here&apos;s the stacktrace of the hung thread as posted on an internal issue tracker but unfortunately I didn&apos;t save the complete stack dumps before I had to force reboot my laptop the other day:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Thread 102 (B.DefaultRpcServer.handler=19,queue=4,port=60020):
sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:335)
org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258)
org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:209)
org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:171)
org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102)
org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:173)
org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)
org.apache.hadoop.hdfs.RemoteBlockReader2.skip(RemoteBlockReader2.java:240)
org.apache.hadoop.hdfs.DFSInputStream.seek(DFSInputStream.java:1399)
org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:63)
org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1302)
org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1531)
org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1413)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A user wrote on on user@hbase with a closely related problem I think. I&apos;ve seen us get wedged in HDFS when closing regions through a variety of code paths. Let me see if I can dig up another stack trace from one of the SplitTransaction investigations.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/hbase-user/201503.mbox/%3CCADSBNAh5v9sxMBHPsqKt%3DyoP7Q-hjpHkbLaWL5_rBTot9wBg9w%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/hbase-user/201503.mbox/%3CCADSBNAh5v9sxMBHPsqKt%3DyoP7Q-hjpHkbLaWL5_rBTot9wBg9w%40mail.gmail.com%3E&lt;/a&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I think I found the thread that is stuck. Is restarting the server harmless in this state?&lt;/p&gt;

&lt;p&gt;&quot;RS_CLOSE_REGION-hdfs-ix03.se-ix.delta.prod,60020,1424687995350-1&quot; prio=10 tid=0x00007f75a0008000 nid=0x23ee in Object.wait() &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f757d30b000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: WAITING (on object monitor)&lt;br/&gt;
at java.lang.Object.wait(Native Method)&lt;br/&gt;
at java.lang.Object.wait(Object.java:503)&lt;br/&gt;
at org.apache.hadoop.hdfs.DFSOutputStream.waitAndQueueCurrentPacket(DFSOutputStream.java:1411)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;locked &amp;lt;0x00000007544573e8&amp;gt; (a java.util.LinkedList)&lt;br/&gt;
at org.apache.hadoop.hdfs.DFSOutputStream.writeChunk(DFSOutputStream.java:1479)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x0000000756780218&amp;gt; (a org.apache.hadoop.hdfs.DFSOutputStream)&lt;br/&gt;
at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:173)&lt;br/&gt;
at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:116)&lt;br/&gt;
at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:102)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x0000000756780218&amp;gt; (a org.apache.hadoop.hdfs.DFSOutputStream)&lt;br/&gt;
at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:54)&lt;br/&gt;
at java.io.DataOutputStream.write(DataOutputStream.java:107)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x00000007543ef268&amp;gt; (a&lt;br/&gt;
org.apache.hadoop.hdfs.client.HdfsDataOutputStream)&lt;br/&gt;
at java.io.FilterOutputStream.write(FilterOutputStream.java:97)&lt;br/&gt;
at org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.writeHeaderAndData(HFileBlock.java:1061)&lt;br/&gt;
at org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer.writeHeaderAndData(HFileBlock.java:1047)&lt;br/&gt;
at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.writeIntermediateBlock(HFileBlockIndex.java:952)&lt;br/&gt;
at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.writeIntermediateLevel(HFileBlockIndex.java:935)&lt;br/&gt;
at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter.writeIndexBlocks(HFileBlockIndex.java:844)&lt;br/&gt;
at org.apache.hadoop.hbase.io.hfile.HFileWriterV2.close(HFileWriterV2.java:403)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:1272)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:835)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x000000075d8b2110&amp;gt; (a java.lang.Object)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:746)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:2348)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1580)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1479)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:992)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:956)&lt;/li&gt;
	&lt;li&gt;locked &amp;lt;0x000000075d97b628&amp;gt; (a java.lang.Object)&lt;br/&gt;
at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:119)&lt;br/&gt;
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:175)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
at java.lang.Thread.run(Thread.java:745)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;</comment>
                            <comment id="14362330" author="deephacks" created="Sun, 15 Mar 2015 10:26:56 +0000"  >&lt;p&gt;Here are some logs from the region server around the time it got stuck. It complains about a bad block response from the datanode.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://pastebin.com/02rEEhqN&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://pastebin.com/02rEEhqN&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14562044" author="shrijeet" created="Thu, 28 May 2015 00:11:14 +0000"  >&lt;p&gt;Is this HDFS issue related &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7005&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HDFS-7005&lt;/a&gt; ? Looks like it is. &lt;/p&gt;</comment>
                            <comment id="14584373" author="apurtell" created="Sat, 13 Jun 2015 01:32:57 +0000"  >&lt;p&gt;Yes we are pretty sure the HDFS level cause is/was &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-7005&quot; title=&quot;DFS input streams do not timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-7005&quot;&gt;&lt;del&gt;HDFS-7005&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;I opened this to discuss aborting regionservers should HDFS get wedged for any reason (future bugs...) but upon reflection we are limited in what we can do. Aborting may not work, if a non daemon thread is stuck in HDFS code and is not interruptible. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 14 Mar 2015 00:46:44 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 26 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i26rfz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>