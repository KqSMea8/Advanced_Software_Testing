<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:00:08 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2234/HBASE-2234.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2234] Roll Hlog if any datanode in the write pipeline dies</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2234</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;HDFS does not replicate the last block of a file that is being written to. This means that is datanodes in the write pipeline die, then the data blocks in the transaction log would be experiencing reduced redundancy. It would be good if the region server can detect datanode-death in the write pipeline while writing to the transaction log and if this happens, close the current log an open a new one. This depends on &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-826&quot; title=&quot;Allow a mechanism for an application to detect that datanode(s)  have died in the write pipeline&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-826&quot;&gt;&lt;del&gt;HDFS-826&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12456602">HBASE-2234</key>
            <summary>Roll Hlog if any datanode in the write pipeline dies</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nspiegelberg">Nicolas Spiegelberg</assignee>
                                    <reporter username="dhruba">dhruba borthakur</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 Feb 2010 00:23:13 +0000</created>
                <updated>Fri, 20 Nov 2015 12:43:54 +0000</updated>
                            <resolved>Wed, 10 Mar 2010 15:45:26 +0000</resolved>
                                                    <fixVersion>0.90.0</fixVersion>
                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12835134" author="stack" created="Thu, 18 Feb 2010 06:40:56 +0000"  >&lt;p&gt;Adding into 0.20.4 and 0.21.  Also assigned it to myself.  Take it back if you want to do it Dhruba but I figure you have enough going on hdfs-side.&lt;/p&gt;</comment>
                            <comment id="12839110" author="nspiegelberg" created="Sat, 27 Feb 2010 01:15:15 +0000"  >&lt;p&gt;Here is a patch with associated unit test.  I was trying to figure out whether to test at the HLog or the HRegionServer level.  I wrote the same tests at both levels, but I submitted the HRegionServer one.  Let me know if you need the other.  Note that this also includes our group-commit code and syncFs() modifications, so it won&apos;t work straight off the trunk.&lt;/p&gt;</comment>
                            <comment id="12839123" author="nspiegelberg" created="Sat, 27 Feb 2010 01:51:12 +0000"  >&lt;p&gt;The patch that I supplied should merge properly if the &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1939&quot; title=&quot;HLog group commit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1939&quot;&gt;&lt;del&gt;HBASE-1939&lt;/del&gt;&lt;/a&gt;-0.24 patch is applied first.&lt;/p&gt;</comment>
                            <comment id="12839241" author="stack" created="Sat, 27 Feb 2010 12:04:36 +0000"  >&lt;p&gt;@ Nicolas &amp;#8211; Sweet.  Patch looks good.&lt;/p&gt;</comment>
                            <comment id="12839499" author="ryanobjc" created="Sun, 28 Feb 2010 21:03:38 +0000"  >&lt;p&gt;would it be necessary or possible to move this function to a static context:&lt;br/&gt;
+  private void rebindWriterFunc(SequenceFile.Writer writer) throws IOException {&lt;/p&gt;

&lt;p&gt;I guess since there is only one hlog instance per RS, maybe it&apos;s ok?&lt;/p&gt;</comment>
                            <comment id="12839522" author="nspiegelberg" created="Mon, 1 Mar 2010 01:29:31 +0000"  >&lt;p&gt;@ryan I could go a couple ways with rebindWriterFunc().  I either need the &quot;this&quot; pointer to set &quot;this.hdfs_out&quot;, in which case I don&apos;t need to explicitly pass in any params since it&apos;s always &quot;this.writer&quot;.  Or I could return an OutputStream and make this function static.  I&apos;m new to Java from an embedded C++ background; but to me it look like this is 6 of one, half-dozen of the other.  I have a couple internal comments about this diff I may need to apply as well, so I&apos;ll roll that change into my next diff.&lt;/p&gt;</comment>
                            <comment id="12841670" author="nspiegelberg" created="Fri, 5 Mar 2010 02:47:42 +0000"  >&lt;p&gt;Updates to address comments on from this jira &amp;amp; internal review.  Notable changes:&lt;br/&gt;
1. added checks to ensure clients with &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-826&quot; title=&quot;Allow a mechanism for an application to detect that datanode(s)  have died in the write pipeline&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-826&quot;&gt;&lt;del&gt;HDFS-826&lt;/del&gt;&lt;/a&gt; or append support would not be negatively affected&lt;br/&gt;
2. simplified rebindWriterFunc(). Only happens inside rollWriter()&lt;br/&gt;
3. asserts in TestLogRolling to discover the presence of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-826&quot; title=&quot;Allow a mechanism for an application to detect that datanode(s)  have died in the write pipeline&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-826&quot;&gt;&lt;del&gt;HDFS-826&lt;/del&gt;&lt;/a&gt; &amp;amp; append support.  You can easily change the asserts to LOG.info() depending upon your default HDFS jar distro.&lt;/p&gt;</comment>
                            <comment id="12841671" author="nspiegelberg" created="Fri, 5 Mar 2010 02:49:42 +0000"  >&lt;p&gt;...and then I clicked the wrong button on the Attachment License&lt;/p&gt;</comment>
                            <comment id="12841721" author="stack" created="Fri, 5 Mar 2010 06:29:29 +0000"  >&lt;p&gt;Assigning Nicolas since he&apos;s doing the work (Made N a contributor).&lt;/p&gt;

&lt;p&gt;@Nicolas, a few comments and solicitation of opinon.&lt;/p&gt;

&lt;p&gt;+ We need to update the hadoop we bundle.  We&apos;ll want to ship with hadoop 0.20.2.  It has at a minimum hdfs-127 fix.  We should probably apply hdfs-826 to the hadoop we ship too since its a client-side only change.  If we included hdfs-200, that&apos;d make it so this test you&apos;ve included actually gets exercised so we should apply it too?&lt;/p&gt;

&lt;p&gt;+ In fact, it looks like this test fails if 826 and 200 are not in place, is that right?  You probably don&apos;t want that.  Maybe skip out the test if they are not in place but don&apos;t fail I&apos;d say.&lt;/p&gt;

&lt;p&gt;+ Your test is great.&lt;/p&gt;

&lt;p&gt;+ FYI, we try not to reference log4j explicitly. &amp;#8211; i.e. the logger implementation &amp;#8211; but i think in this case you have no choice going by the commons dictum that the logger config. is outside of its scope (I was reading under &quot;Configuring the Underlying Logging System&quot; in &lt;a href=&quot;http://commons.apache.org/logging/apidocs/org/apache/commons/logging/package-summary.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://commons.apache.org/logging/apidocs/org/apache/commons/logging/package-summary.html&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;+ I like the comments you&apos;ve added to HLog.java&lt;/p&gt;

&lt;p&gt;+ The log message says hadoop-4379 if hdfs-200 is found... maybe add or change mention of hdfs-200&lt;/p&gt;

&lt;p&gt;Patch looks good otherwise.&lt;/p&gt;</comment>
                            <comment id="12841995" author="nspiegelberg" created="Fri, 5 Mar 2010 20:05:32 +0000"  >&lt;p&gt;Will correct comment/messaging issues.  The actual patch should work fine without append support, however I would lean towards applying &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-200&quot; title=&quot;In HDFS, sync() not yet guarantees data available to the new readers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-200&quot;&gt;&lt;del&gt;HDFS-200&lt;/del&gt;&lt;/a&gt; just to exercise this test.  In the unit test, we really just use append support for syncFs(), since it&apos;s a deterministic way to initialize the pipeline for reading &lt;span class=&quot;error&quot;&gt;&amp;#91;I&amp;#39;ll clarify in code comments&amp;#93;&lt;/span&gt;.  I&apos;ve experimented with modifying &quot;io.file.buffer.size&quot;, but it would take a lot more introspection and another nastiness to test it that way.  My biggest worry about &apos;skipping the test&apos; is that it&apos;s easy to accidentally replace the default with an un-patched JAR and not know about it since HBase has hundreds of tests.  We&apos;ve been regularly applying new HDFS client-side patches over here for append support, and I messed up once or twice when I was task switching between this issue and other testing.&lt;/p&gt;</comment>
                            <comment id="12842191" author="stack" created="Sat, 6 Mar 2010 05:31:19 +0000"  >&lt;p&gt;OK.  I agree with you.  Let me go build an hadoop 0.20.2 that has hdfs-826 and hdfs-200 applied.  Any others?  And then commit to 0.20 branch.&lt;/p&gt;</comment>
                            <comment id="12842193" author="stack" created="Sat, 6 Mar 2010 05:36:01 +0000"  >&lt;p&gt;@Nicolas And i think then this patch fine as is unless you want to add more comments (I can make my small suggested changes above on commit).&lt;/p&gt;</comment>
                            <comment id="12842343" author="nspiegelberg" created="Sat, 6 Mar 2010 23:30:08 +0000"  >&lt;p&gt;Go ahead and apply the existing patch with the comment changes.&lt;/p&gt;

&lt;p&gt;I spent a little bit of time yesterday trying to understand all the layers of buffering between the SequenceFile.Writer &amp;amp; actually having the pipeline opened and content sent to the datanodes.  I figured I&apos;d pass that information along since 0.20.2 currently does not support syncFs().  Without syncFs, the pipeline seems to be created every 64k, which is &apos;dfs.write.packet.size&apos;.  The stack trace, with associated buffering, that I was following:&lt;/p&gt;

&lt;p&gt;1. SequenceFile.Writer.append() &lt;br/&gt;
2. FSOutputSummer.write()              --&amp;gt; buffers to maxChunkSize. An HDFS chunk is the amount of data in between checksums. (default: 512bytes)&lt;br/&gt;
3. FSOutputSummer.flushBuffer() &lt;br/&gt;
4. FSOutputSummer. writeChecksumChunk() &lt;br/&gt;
5. DFSOutputStream.writeChunk()  --&amp;gt; buffers to currentPacket.maxChunk.  This is the maximum HDFS chunk count that can be place in a Packet.  Approx byte count is min(&quot;dfs.block.size&quot; (default:64MB), &quot;hbase.regionserver.hlog.blocksize&quot; (default:&quot;dfs.block.size&quot;), &quot;dfs.write.packet.size&quot; (default:64k))&lt;br/&gt;
5. DataStreamer.run() &amp;lt;-- creates the pipeline &lt;/p&gt;</comment>
                            <comment id="12842361" author="dhruba" created="Sun, 7 Mar 2010 04:49:37 +0000"  >&lt;p&gt;The DFSClient has a siliding window protocol to send data to datanodes. The default size of the window is 80 packets, each of size 64K.&lt;/p&gt;</comment>
                            <comment id="12843597" author="stack" created="Wed, 10 Mar 2010 15:44:53 +0000"  >&lt;p&gt;I committed this patch to branch.  All tests pass but the TestHeapSize.  I&apos;ll fix that in a sec (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2307&quot; title=&quot;hbase-2295 changed hregion size, testheapsize broke... fix it.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2307&quot;&gt;&lt;del&gt;HBASE-2307&lt;/del&gt;&lt;/a&gt;).  I opened hbase-2306 to forward-port the patch.  Its a little awkward at the moment since TRUNK has hadoop 0.21 in it but that might change soon making forward-port straight-forward.&lt;/p&gt;

&lt;p&gt;On the above buffering note above, dumb question, I presume syncFs forces dfsclient to flush all local data out to the datanode buffers?&lt;/p&gt;</comment>
                            <comment id="12843598" author="stack" created="Wed, 10 Mar 2010 15:45:26 +0000"  >&lt;p&gt;Resolving.  Thanks for the patch Nicolas.&lt;/p&gt;</comment>
                            <comment id="12843681" author="dhruba" created="Wed, 10 Mar 2010 18:27:07 +0000"  >&lt;p&gt;&amp;gt; I presume syncFs forces dfsclient to flush all local data out to the datanode buffers?&lt;/p&gt;

&lt;p&gt;That&apos;s right. It goes to the OS buffers on the datanodes.&lt;/p&gt;</comment>
                            <comment id="12849926" author="jdcryans" created="Thu, 25 Mar 2010 22:12:06 +0000"  >&lt;p&gt;I saw a minor issue with this patch, if HBase doesn&apos;t have hdfs-site.xml in its CP or isn&apos;t configured with the same dfs.replication, this call will not return the right value:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+                numCurrentReplicas &amp;lt; fs.getDefaultReplication()) {  
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example, I was testing &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2337&quot; title=&quot;log recovery: splitLog deletes old logs prematurely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2337&quot;&gt;&lt;del&gt;HBASE-2337&lt;/del&gt;&lt;/a&gt; on a single node and didn&apos;t put hadoop&apos;s conf dir in HBase&apos;s CP but did configure rep=1, I ended up rolling the logs for every edit:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
WARN org.apache.hadoop.hbase.regionserver.HLog: HDFS pipeline error detected. Found 1 replicas but expecting 3 replicas.  Requesting close of hlog.
WARN org.apache.hadoop.hbase.regionserver.HLog: HDFS pipeline error detected. Found 1 replicas but expecting 3 replicas.  Requesting close of hlog.
WARN org.apache.hadoop.hbase.regionserver.HLog: HDFS pipeline error detected. Found 1 replicas but expecting 3 replicas.  Requesting close of hlog.
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can see hordes of new users coming with the same issue. Is there a better way to ask HDFS about the default replication setting and store it?&lt;/p&gt;</comment>
                            <comment id="12850129" author="dhruba" created="Fri, 26 Mar 2010 13:15:20 +0000"  >&lt;p&gt;I agree with JD. Can we use &amp;lt;hlogpath&amp;gt;.getFiletatus().getReplication() instead of fs.getDefaltReplication()? This will will ensure that we look at the repl factor of the precise file we are interested in, rather than what the system-wide default value is.&lt;/p&gt;</comment>
                            <comment id="12850233" author="jdcryans" created="Fri, 26 Mar 2010 17:32:29 +0000"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2382&quot; title=&quot;Don&amp;#39;t rely on fs.getDefaultReplication() to roll HLogs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2382&quot;&gt;&lt;del&gt;HBASE-2382&lt;/del&gt;&lt;/a&gt; and assigned it to Nicolas so that he can take a look.&lt;/p&gt;</comment>
                            <comment id="12866901" author="stack" created="Wed, 12 May 2010 23:53:17 +0000"  >&lt;p&gt;Marking these as fixed against 0.21.0 rather than against 0.20.5.&lt;/p&gt;</comment>
                            <comment id="12867714" author="stack" created="Fri, 14 May 2010 22:38:42 +0000"  >&lt;p&gt;I forward-ported this patch to TRUNK (Some had been done as part of hbase-2544, the tests were ported forwarded as part of hbase-2551 part 1).&lt;/p&gt;</comment>
                            <comment id="15017652" author="lars_francke" created="Fri, 20 Nov 2015 12:43:54 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12443029">HDFS-826</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12439235">HBASE-1939</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12437961" name="HBASE-2234-20.4-1.patch" size="16175" author="nspiegelberg" created="Fri, 5 Mar 2010 02:49:42 +0000"/>
                            <attachment id="12437290" name="HBASE-2234-20.4.patch" size="15042" author="nspiegelberg" created="Sat, 27 Feb 2010 01:15:15 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 18 Feb 2010 06:40:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32472</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hgvb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99995</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>