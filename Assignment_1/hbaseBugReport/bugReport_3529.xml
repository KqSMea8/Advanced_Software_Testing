<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:10:58 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3529/HBASE-3529.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3529] Add search to HBase</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3529</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Using the Apache Lucene library we can add freetext search to HBase.  The advantages of this are:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;HBase is highly scalable and distributed&lt;/li&gt;
	&lt;li&gt;HBase is realtime&lt;/li&gt;
	&lt;li&gt;Lucene is a fast inverted index and will soon be realtime (see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2312&quot;&gt;LUCENE-2312&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;Lucene offers many types of queries not currently available in HBase (eg, AND, OR, NOT, phrase, etc)&lt;/li&gt;
	&lt;li&gt;It&apos;s easier to build scalable realtime systems on top of already architecturally sound, scalable realtime data system, eg, HBase.&lt;/li&gt;
	&lt;li&gt;Scaling realtime search will be as simple as scaling HBase.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Phase 1 - Indexing:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Integrate Lucene into HBase such that an index mirrors a given region.  This means cascading add, update, and deletes between a Lucene index and an HBase region (and vice versa).&lt;/li&gt;
	&lt;li&gt;Define meta-data to mark a region as indexed, and use a Solr schema to allow the user to define the fields and analyzers.&lt;/li&gt;
	&lt;li&gt;Integrate with the HLog to ensure that index recovery can occur properly (eg, on region server failure)&lt;/li&gt;
	&lt;li&gt;Mirror region splits with indexes (use Lucene&apos;s IndexSplitter?)&lt;/li&gt;
	&lt;li&gt;When a region is written to HDFS, also write the corresponding Lucene index to HDFS.&lt;/li&gt;
	&lt;li&gt;A row key will be the ID of a given Lucene document.  The Lucene docstore will explicitly not be used because the document/row data is stored in HBase.  We will need to solve what the best data structure for efficiently mapping a docid -&amp;gt; row key is.  It could be a docstore, field cache, column stride fields, or some other mechanism.&lt;/li&gt;
	&lt;li&gt;Write unit tests for the above&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Phase 2 - Queries:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Enable distributed Lucene queries&lt;/li&gt;
	&lt;li&gt;Regions that have Lucene indexes are inherently available and may be searched on, meaning there&apos;s no need for a separate search related system in Zookeeper.&lt;/li&gt;
	&lt;li&gt;Integrate search with HBase&apos;s RPC mechanis&lt;/li&gt;
&lt;/ul&gt;

</description>
                <environment></environment>
        <key id="12498553">HBASE-3529</key>
            <summary>Add search to HBase</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="7">Later</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jasonrutherglen">Jason Rutherglen</reporter>
                        <labels>
                    </labels>
                <created>Mon, 14 Feb 2011 17:15:24 +0000</created>
                <updated>Mon, 27 Jun 2016 12:46:13 +0000</updated>
                            <resolved>Tue, 12 Aug 2014 19:16:25 +0000</resolved>
                                    <version>0.90.0</version>
                                                        <due></due>
                            <votes>37</votes>
                                    <watches>105</watches>
                                                                <comments>
                            <comment id="12994384" author="stack" created="Mon, 14 Feb 2011 17:53:17 +0000"  >&lt;p&gt;@Jason Sounds excellent.  Could you do this up in a coprocessor?  &lt;a href=&quot;http://hbaseblog.com/2010/11/30/hbase-coprocessors/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbaseblog.com/2010/11/30/hbase-coprocessors/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12994395" author="jasonrutherglen" created="Mon, 14 Feb 2011 18:04:15 +0000"  >&lt;p&gt;Thanks.  Right the coprocessor is the key to sync&apos;ing HBase and Lucene.  This&apos;s where I&apos;ll probably start.&lt;/p&gt;</comment>
                            <comment id="12994406" author="jasonrutherglen" created="Mon, 14 Feb 2011 18:23:40 +0000"  >&lt;p&gt;Another issue brought up is the size of a region vs. the size of the Lucene index.  If the region is compressed the resultant Lucene index may in fact be a reasonable size.  Typically a maximum Lucene index size of 1 - 2 GB is optimal?  If the default region size 256 MB, and the data&apos;s been compressed by (what ratio?), then 256 MB could be ideal?&lt;/p&gt;</comment>
                            <comment id="12994800" author="jasonrutherglen" created="Tue, 15 Feb 2011 14:22:53 +0000"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2919&quot; title=&quot;IndexSplitter that divides by primary key term&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2919&quot;&gt;&lt;del&gt;LUCENE-2919&lt;/del&gt;&lt;/a&gt; to split indexes by the primary key, eg, the HBase keys.&lt;/p&gt;</comment>
                            <comment id="12994814" author="danharvey" created="Tue, 15 Feb 2011 14:47:23 +0000"  >&lt;p&gt;How would you deal with the data types / serialisation, would you assume the cell data is just UTF8 bytes to start with?&lt;/p&gt;</comment>
                            <comment id="12994820" author="jasonrutherglen" created="Tue, 15 Feb 2011 14:57:29 +0000"  >&lt;p&gt;I added a DocumentTransformer class that looks like this.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class DocumentTransformer {
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; Map&amp;lt;Term,Document&amp;gt; transform(Map&amp;lt;&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[], List&amp;lt;KeyValue&amp;gt;&amp;gt; familyMap) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; Term[] getIDTerms(Map&amp;lt;&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[], List&amp;lt;KeyValue&amp;gt;&amp;gt; familyMap) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The user can then define how they want to transform the underlying data to Lucene documents.  I&apos;m trying to find a JSON library to build the unit tests/demo app with.&lt;/p&gt;</comment>
                            <comment id="12994836" author="stack" created="Tue, 15 Feb 2011 15:18:58 +0000"  >&lt;p&gt;@Jason HBase ships with jersey-json (1.4).  See here for doc: &lt;a href=&quot;http://jackson.codehaus.org/Tutorial&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://jackson.codehaus.org/Tutorial&lt;/a&gt; (Should be easy enough updating jersey-json if needed).&lt;/p&gt;</comment>
                            <comment id="12995037" author="jasonrutherglen" created="Tue, 15 Feb 2011 22:38:26 +0000"  >&lt;p&gt;Stack, thanks for the pointer.&lt;/p&gt;</comment>
                            <comment id="12995331" author="jasonrutherglen" created="Wed, 16 Feb 2011 15:03:50 +0000"  >&lt;p&gt;There&apos;s one possible issue that&apos;s come to mind and that is the possible overhead associated with accessing the Lucene index if it&apos;s stored in HDFS.  Meaning, in Lucene today we have implementations such as NIOFSDirectory which uses NIO&apos;s positional read underneath, and it&apos;s made highly concurrent search apps much faster (as before we were sync&apos;ing per byte&lt;span class=&quot;error&quot;&gt;&amp;#91;1024&amp;#93;&lt;/span&gt; read call).  I&apos;m curious if HDFS has effectively implemented something similar to NIOFSDir underneath?  I see pread mentioned in HFile however I think it&apos;s referring to the HDFS specific implementation?&lt;/p&gt;</comment>
                            <comment id="12995398" author="stack" created="Wed, 16 Feb 2011 17:24:12 +0000"  >&lt;p&gt;@Jason Yeah, the pread is for hdfs.  Its going to be slow though because for EVERY pread invocation, HDFS sets up socket, loads new block, seeks to pread location, then returns bytes and closes sockets.  This is to be fixed but thats how it currently works.&lt;/p&gt;</comment>
                            <comment id="12996444" author="jasonrutherglen" created="Fri, 18 Feb 2011 15:03:17 +0000"  >&lt;p&gt;1) It may be more expedient for now to store the index in a dedicated directory, and save it to HDFS periodically.  However I&apos;m not sure &apos;when&apos; the loading into HDFS would occur, eg, if HBase is always writing to HDFS then there&apos;s no way to sync with that mechanism.  Perhaps it&apos;d need to be based on the iterative index size changes?  Ie, if the index has grown by 25% since the last save?&lt;/p&gt;

&lt;p&gt;2) I&apos;d like to design the recovery logic now.  It&apos;s simple to save the timestmap into Lucene, then on recovery get the max timestamp, and iterate from there over the HRegion for the remaining &apos;lost&apos; rows/documents.  What&apos;s the most efficient way to scan over &amp;gt; timestamp key values?&lt;/p&gt;

&lt;p&gt;3) We can create indexes for the entire HRegion or for the individual column families.  Perhaps this should be optional?  I wonder if there are dis/advantages from a user perspective?  If interleaving postings was efficient we could even design a system to enable parts of posting lists to be changed per column family, where duplicate docids would be written to intermediate in-memory indexes, and &apos;interleaved&apos; during posting iteration.&lt;/p&gt;</comment>
                            <comment id="12996461" author="yuzhihong@gmail.com" created="Fri, 18 Feb 2011 15:45:54 +0000"  >&lt;p&gt;From Scan.java:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;To only retrieve columns within a specific range of version timestamps,&lt;/li&gt;
	&lt;li&gt;execute 
{@link #setTimeRange(long, long) setTimeRange}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12997141" author="jasonrutherglen" created="Sun, 20 Feb 2011 14:12:08 +0000"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2930&quot; title=&quot;Store the last term in the terms dictionary&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2930&quot;&gt;&lt;del&gt;LUCENE-2930&lt;/del&gt;&lt;/a&gt; to store the last/max term of a field in the Lucene terms dictionary.  We can use this to more efficiently know the index&apos;s last commit point, and start indexing from there.  The alternative is to iterate the &lt;b&gt;entire&lt;/b&gt; terms dictionary, which for the unique timestamp, would be the length of the number of documents.&lt;/p&gt;</comment>
                            <comment id="12999528" author="jasonrutherglen" created="Fri, 25 Feb 2011 20:14:51 +0000"  >&lt;p&gt;Where is a good &apos;temp&apos; directory to place the Lucene indexes relative to other local HBase files?  &lt;/p&gt;</comment>
                            <comment id="12999533" author="ryanobjc" created="Fri, 25 Feb 2011 20:30:54 +0000"  >&lt;p&gt;there are no local hbase files. You&apos;ll have to come up with something yourself i guess?&lt;/p&gt;</comment>
                            <comment id="12999537" author="jasonrutherglen" created="Fri, 25 Feb 2011 20:33:52 +0000"  >&lt;p&gt;Maybe something relative to HDFS then?&lt;/p&gt;</comment>
                            <comment id="12999542" author="apurtell" created="Fri, 25 Feb 2011 20:36:29 +0000"  >&lt;p&gt;I mailed a comment back but it is not showing up fast enough.&lt;/p&gt;

&lt;p&gt;We have internally been discussing the addition of a Coprocessor framework API for reading and writing streams from/to the region data directory in HDFS.&lt;/p&gt;</comment>
                            <comment id="12999545" author="apurtell" created="Fri, 25 Feb 2011 20:39:21 +0000"  >&lt;p&gt;We have internally been discussing the addition of a Coprocessor framework API for reading and writing streams from/to the region data directory in HDFS.&lt;/p&gt;



</comment>
                            <comment id="12999546" author="ryanobjc" created="Fri, 25 Feb 2011 20:40:14 +0000"  >&lt;p&gt;it&apos;s going to be tricky, since with security some people may choose to run hdfs and hbase on different users. Futhermore most hadoop installs have multiple jbod-style disks, and places like /tmp won&apos;t have much room (my /tmp has &amp;lt; 2GB). If you can avoid local files as much as possible, I&apos;d try to do that.&lt;/p&gt;</comment>
                            <comment id="12999552" author="jasonrutherglen" created="Fri, 25 Feb 2011 20:48:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;We have internally been discussing the addition of a Coprocessor framework API for reading and writing streams from/to the region data directory in HDFS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This&apos;d be good, however for Lucene we&apos;ll need to directly access the local filesystem for performance reasons, eg, HDFS sounds like it&apos;s going to be slower than going direct (at the moment).  Because the indexes will be local, we&apos;ll need to periodically sync the local index to HDFS.  This isn&apos;t as difficult as it sounds, because we can save off a Lucene commit point and write the checkpoint&apos;s index files to HDFS, while letting other Lucene operations proceed.  I&apos;d say we can move to writing directly to HDFS when HBase no longer uses a heap based block store (and instead relies on the system IO cache).&lt;/p&gt;</comment>
                            <comment id="12999557" author="jasonrutherglen" created="Fri, 25 Feb 2011 20:53:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;it&apos;s going to be tricky, since with security some people may choose to run hdfs and hbase on different users. Futhermore most hadoop installs have multiple jbod-style disks, and places like /tmp won&apos;t have much room (my /tmp has &amp;lt; 2GB). If you can avoid local files as much as possible, I&apos;d try to do that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, /tmp probably isn&apos;t the best place.  The config and schema per table will be stored in HDFS, which means we can&apos;t store per server data there.  Maybe there&apos;s an HDFS API to introspect as to where it &lt;b&gt;would&lt;/b&gt; store files for a local FileSystem?&lt;/p&gt;</comment>
                            <comment id="12999559" author="apurtell" created="Fri, 25 Feb 2011 20:54:59 +0000"  >&lt;p&gt;Writing the indexes to HDFS is possible after &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2373&quot; title=&quot;Create a Codec to work with streaming and append-only filesystems&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2373&quot;&gt;&lt;del&gt;LUCENE-2373&lt;/del&gt;&lt;/a&gt;? We get direct reads from HDFS via &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; and the OS block cache can help there?&lt;/p&gt;</comment>
                            <comment id="12999562" author="ghelmling" created="Fri, 25 Feb 2011 20:58:22 +0000"  >&lt;p&gt;Yeah, as Ryan mentions, with security, writing to HDFS via a coprocessor extension will be easiest to enable.&lt;/p&gt;

&lt;p&gt;I wonder if that plus &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; (which allows reading directly from the local FS if the block exists on the local DN) would allow for good enough performance?  Of course, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; itself is tricky from a security perspective.&lt;/p&gt;

&lt;p&gt;If local disk writes are the only solution, then the best option may be to make the user plan for it and explicitly specify a Lucene index path in the coprocessor configuration.&lt;/p&gt;</comment>
                            <comment id="12999565" author="jasonrutherglen" created="Fri, 25 Feb 2011 21:02:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Writing the indexes to HDFS is possible after &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2373&quot; title=&quot;Create a Codec to work with streaming and append-only filesystems&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2373&quot;&gt;&lt;del&gt;LUCENE-2373&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, that&apos;s implemented in trunk as the append codecs. &lt;a href=&quot;https://hudson.apache.org/hudson/job/Lucene-trunk/javadoc//contrib-misc/org/apache/lucene/index/codecs/appending/AppendingCodec.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/Lucene-trunk/javadoc//contrib-misc/org/apache/lucene/index/codecs/appending/AppendingCodec.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We get direct reads from HDFS via &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; and the OS block cache can help there?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;BlockReaderLocal is sync&apos;d on each method, that&apos;s something we&apos;ve outgrown in Lucene a while back (and in it&apos;s place NIOFSDirectory is most used, with MMap second).  We&apos;d likely have a couple of options here, write to HDFS and &lt;span class=&quot;error&quot;&gt;&amp;#91;probably&amp;#93;&lt;/span&gt; slow queries to some extent, or write directly to a local directory and have the mechanical overhead of copying index files in/out of HDFS.&lt;/p&gt;</comment>
                            <comment id="12999579" author="jasonrutherglen" created="Fri, 25 Feb 2011 21:27:23 +0000"  >&lt;p&gt;Also, I&apos;m curious about how the HLog works, eg, it&apos;s archived into HDFS, is there a difference between what&apos;s archived and what&apos;s live (and would interleaving be necessary?).  The reason the HLog needs to be replayed &lt;span class=&quot;error&quot;&gt;&amp;#91;I think&amp;#93;&lt;/span&gt; is deletes need to be executed.  If we simply iterate/scan from a given timestamp, we&apos;d get the new rows however we&apos;d miss executing deletes.&lt;/p&gt;</comment>
                            <comment id="12999586" author="apurtell" created="Fri, 25 Feb 2011 21:37:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m curious about how the HLog works&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;See &lt;a href=&quot;http://www.larsgeorge.com/2010/01/hbase-architecture-101-write-ahead-log.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.larsgeorge.com/2010/01/hbase-architecture-101-write-ahead-log.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12999595" author="jasonrutherglen" created="Fri, 25 Feb 2011 22:00:45 +0000"  >&lt;p&gt;In the RegionObserver/Coprocessor I don&apos;t think there are methods to access the log replay (on server restart), is that something that&apos;s planned?  &lt;/p&gt;</comment>
                            <comment id="12999598" author="jasonrutherglen" created="Fri, 25 Feb 2011 22:10:04 +0000"  >&lt;p&gt;To answer the previous question there&apos;s this issue: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3257&quot; title=&quot;Coprocessors: Extend server side integration API to include HLog operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3257&quot;&gt;&lt;del&gt;HBASE-3257&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And on memstore flush, we&apos;ll do a Lucene index commit to ensure that when we replay the HLog, we won&apos;t need to access &lt;span class=&quot;error&quot;&gt;&amp;#91;potentially&amp;#93;&lt;/span&gt; out of date HLog entries.  We can store the checkpoint meta-data into the Lucene commit, which obviates the need to implement terms dict last term access. &lt;/p&gt;</comment>
                            <comment id="12999646" author="ekoontz" created="Fri, 25 Feb 2011 23:36:33 +0000"  >&lt;p&gt;As Jason pointed out on hbase-dev, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3257&quot; title=&quot;Coprocessors: Extend server side integration API to include HLog operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3257&quot;&gt;&lt;del&gt;HBASE-3257&lt;/del&gt;&lt;/a&gt; provides HLog-related functionality that could be used by coprocessors to add indexing information.&lt;/p&gt;</comment>
                            <comment id="12999676" author="jasonrutherglen" created="Sat, 26 Feb 2011 00:55:06 +0000"  >&lt;p&gt;Is &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12470743/HDFS-347-branch-20-append.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12470743/HDFS-347-branch-20-append.txt&lt;/a&gt; the patch applied to CDH?  If so, the readChunk method isn&apos;t implemented.  Is there a plan to implement that, perhaps with NIO positional read?  Implementing readChunk would make storing the indexes in HDFS entirely tenable.&lt;/p&gt;</comment>
                            <comment id="12999684" author="ryanobjc" created="Sat, 26 Feb 2011 01:03:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; is not in CDH nor in branch-20-append.&lt;/p&gt;

&lt;p&gt;As for a plan to implement it, perhaps you should?&lt;/p&gt;</comment>
                            <comment id="12999687" author="jasonrutherglen" created="Sat, 26 Feb 2011 01:09:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; is not in CDH nor in branch-20-append.&lt;/p&gt;

&lt;p&gt;As for a plan to implement it, perhaps you should?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Really?  Ah, I guess I misread this: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3186?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12997267#comment-12997267&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3186?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12997267#comment-12997267&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Sure, I can give a go at an NIO positional read version, it&apos;ll be a good learning experience.  Are there any caveats to be aware of?&lt;/p&gt;</comment>
                            <comment id="12999688" author="ryanobjc" created="Sat, 26 Feb 2011 01:11:58 +0000"  >&lt;p&gt;I do not know, the whole thing is pretty green field. There are a few different implementations of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;, and I haven&apos;t actually seen a credible attempt at really getting it into a shipping hadoop yet. The test patches are pretty great, but they are POC and won&apos;t actually be shipping (due to hadoop security).&lt;/p&gt;

&lt;p&gt;You can give it a shot, but be warned you might not get much for your troubles in terms of committed code.&lt;/p&gt;</comment>
                            <comment id="12999718" author="stack" created="Sat, 26 Feb 2011 04:52:42 +0000"  >&lt;p&gt;@Jason We could get hdfs-347 applied to branch-0.20-append.  Us HBasers are going to talk it up, that folks should apply it to their hadoop since the benefit is so great.  CDH will have something like an hdfs-347 but probably not till CDH4 (Todd talks of a version of hdfs-347 but one that will work w/ security &amp;#8211; see his patch up in hdfs-237 as opposed to the amended Dhruba patch posted by Ryan).  A hdfs-347 probably won&apos;t show in apache hadoop till 0.23/0.24 would be my guess.&lt;/p&gt;</comment>
                            <comment id="12999722" author="jasonrutherglen" created="Sat, 26 Feb 2011 05:53:50 +0000"  >&lt;p&gt;@Stack I didn&apos;t see any patches at &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-237&quot; title=&quot;Better handling of dfsadmin command when namenode is slow&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-237&quot;&gt;&lt;del&gt;HDFS-237&lt;/del&gt;&lt;/a&gt;.  I&apos;d be curious to learn what the security issues are, I guess they&apos;re articulated in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; as solvable by transferring file descriptors, though I&apos;m not sure why the user running the Hadoop Java process should not be accessing certain local files?  Also, maybe there are higher level synchronization issues to be aware of (eg, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-1605&quot; title=&quot;Convert DFSInputStream synchronized sections to a ReadWrite lock&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-1605&quot;&gt;HDFS-1605&lt;/a&gt;)?  I&apos;m sure much of this can be changed, though it may require a separate call &apos;path&apos; and classes to avoid any extraneous synchronization.  I do like this approach of making core changes to HDFS which&apos;ll benefit HBase and this issue, then also streamlines the Lucene integration (ie, there&apos;ll be no need for replicating the index back into HDFS from local disk), which&apos;ll reduce the aggregate complexity and testing.  &lt;/p&gt;</comment>
                            <comment id="12999727" author="stack" created="Sat, 26 Feb 2011 06:40:00 +0000"  >&lt;p&gt;@Jason Pardon me. The &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-237&quot; title=&quot;Better handling of dfsadmin command when namenode is slow&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-237&quot;&gt;&lt;del&gt;HDFS-237&lt;/del&gt;&lt;/a&gt; above is a mistype on my part.  I meant &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; (I was about to make jokes about your dyslexia but on review the affliction blew up in my face).  The hbase process can access local files as long as it gets the clearance via hdfs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;do like this approach of making core changes to HDFS which&apos;ll benefit HBase....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="12999789" author="jasonrutherglen" created="Sat, 26 Feb 2011 14:43:14 +0000"  >&lt;p&gt;It looks simple to change &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; (the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;-branch-20-append.txt patch) to read using positional reads, I&apos;m sure it&apos;s necessary as a block reader is instantiated per DFSInputStream? read(long position, byte[] buffer, int offset, int length) calls getBlockRange which is sync&apos;d.  Then the read method calls fetchBlockByteRange which calls BlockReader.newBlockReader, eg, the blockreader is per thread and isn&apos;t reused?  So the contention would be in getBlockRange?  Perhaps there&apos;s not an issue, or not much of one, if the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;-branch-20-append.txt patch (or something like it) is applied (using &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6311&quot; title=&quot;Add support for unix domain sockets to JNI libs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6311&quot;&gt;&lt;del&gt;HADOOP-6311&lt;/del&gt;&lt;/a&gt;)?  &lt;/p&gt;

&lt;p&gt;I guess the go ahead is to write a Lucene Directory that uses HDFS underneath, that gains concurrency by using DFSInputStream.read(long position, ...)?  Oh, the other issue would be all the overhead from simply loading a byte&lt;span class=&quot;error&quot;&gt;&amp;#91;1024&amp;#93;&lt;/span&gt; (eg, all the new object creation etc).  Hmm... That&apos;ll be a problem.  &lt;/p&gt;</comment>
                            <comment id="13000283" author="jasonrutherglen" created="Mon, 28 Feb 2011 13:55:13 +0000"  >&lt;p&gt;I started on the search part, which is nice as it can utilize HBase&apos;s Coprocessor RPC mechanism.  The design issue is if we need to store a unique &lt;span class=&quot;error&quot;&gt;&amp;#91;family, column, row, timestamp&amp;#93;&lt;/span&gt; per column/field into Lucene?  Or perhaps this only needs to be stored per column family?  This&apos;ll be used on iteration of the results from Lucene, which yields docids, we&apos;ll then lookup the values in the doc store, call Get for each doc, and add the Result to the search response.  I think this is how it should work?  &lt;/p&gt;</comment>
                            <comment id="13000398" author="stack" created="Mon, 28 Feb 2011 16:55:49 +0000"  >&lt;p&gt;You&apos;ll have to include row, column family, and qualifier at least if you are to get from lucene back to the the latest version of the cell, won&apos;t you?  If you want to index more than just the current version of a cell, you&apos;ll have to include the hbase timestamp in the lucene index.&lt;/p&gt;

&lt;p&gt;If your lucene indices are per column family, you could leave the column family out of the lucene document and it can be picked up from context; that would leave row, qualifier and timestamp in the lucene document.&lt;/p&gt;</comment>
                            <comment id="13000667" author="jasonrutherglen" created="Tue, 1 Mar 2011 02:07:18 +0000"  >&lt;p&gt;In regards to &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; and the issues around fast local file access.  I started reimplementing &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;, however I realized it&apos;ll be fruitless without an efficient &lt;span class=&quot;error&quot;&gt;&amp;#91;cached&amp;#93;&lt;/span&gt; way of finding the local file a given offset corresponds to.  Is there a way for the DFSClient to &quot;listen&quot; for changes to the DataNode and then keep a memory resident &apos;cache&apos; for the purpose of quickly accessing which local file(s) a given positional read + length corresponds to?&lt;/p&gt;</comment>
                            <comment id="13000693" author="stack" created="Tue, 1 Mar 2011 04:02:08 +0000"  >&lt;p&gt;@Jason Which offset are you talking off?  The storefile in hbase keeps offsets in a file index.  When we ask to read from a position in the hfile, dfsclient does a quick calc to figure which block and then relatively, the offset into the target block.  Are you talking of something more fine grained or something else?&lt;/p&gt;</comment>
                            <comment id="13000697" author="jasonrutherglen" created="Tue, 1 Mar 2011 04:20:25 +0000"  >&lt;p&gt;Sorry, I thought through the file access a little more.  I think we can use the block local reader as is, because Lucene reads the postings sequentially, we don&apos;t really need random file access (eg, the offset issue more or less goes away), we simply need to allow seek&apos;ing forward, and most postings will live inside of a single (64 - 128MB block).  The issue with this system is we may need to maintain an FSInputStream per thread per file because we probably don&apos;t want to open a new FSInputStream per query given the overhead or creating and destroying them?  Will this cause issues with the max file descriptors?&lt;/p&gt;</comment>
                            <comment id="13000706" author="stack" created="Tue, 1 Mar 2011 04:24:59 +0000"  >&lt;p&gt;@Jason Currently HBase keeps all files open all the time (Yeah, users have to up their ulimit if they have more than a smidgeon of data in hbase--requirement #4 or #5).&lt;/p&gt;</comment>
                            <comment id="13000721" author="jasonrutherglen" created="Tue, 1 Mar 2011 05:08:28 +0000"  >&lt;p&gt;Ah, going back to storing the &quot;row, qualifier and timestamp&quot; in a Lucene document/docstore, is that does require totally random reads.  I wonder if there&apos;s some efficient way to store row pointers in RAM (compression?) or a Hadooop data structure that can be used?  I think that storing this information in the Lucene field cache is going to cause OOMs.  It&apos;d be great if we could simply store a long that points to the exact row and column family we&apos;d like to reference, as that could easily be stored in RAM, and would possibly enable faster lookup?&lt;/p&gt;</comment>
                            <comment id="13000730" author="stack" created="Tue, 1 Mar 2011 05:38:56 +0000"  >&lt;p&gt;Are you thinking you could exploit hbase scan somehow?  If so, how you think it would work?&lt;/p&gt;

&lt;p&gt;Whats a lucene docid?  A long?  Or a double?  You could toBytes that and that&apos;d be the hbase row (HBase rows are byte arrays).  The column family could be one byte &amp;#8211; that&apos;d give you 256 maximum column family names.  Qualifier probably has to be lucene document field name.  You could try and keep these short. Timestamp is a long.  So thats two longs (docid + ts), one byte for cf, and say, 8 characters for field name.. thats about 25 bytes or so per lucene doc.  Will that cause you to run out of mem?&lt;/p&gt;</comment>
                            <comment id="13000866" author="jasonrutherglen" created="Tue, 1 Mar 2011 13:15:21 +0000"  >&lt;p&gt;@Stack Thanks for the analysis.  I forgot to mention that each subquery would also require it&apos;s own FSInputStream, which would be too many file descriptors.  The heap required for 25 bytes * 2 mil docs is 50MB, eg, that&apos;s too much?  &lt;/p&gt;

&lt;p&gt;I think we can go ahead with the positional read which&apos;d only require an FSInputStream per file, to be shared by all readers of that file (using FileChannel.read(ByteBuffer dst, long position) underneath.  Given the number of blocks per Lucene file will be &amp;lt; 10 and the blocks are of a fixed size, we can divide the (offset / blocksize) to efficiently obtain the block index?  I think it&apos;ll be efficient to translate a file offset into a local block file, eg, I&apos;m not sure why LocatedBlocks.findBlock uses a binary search because I&apos;m not familiar enough with HDFS. Then we&apos;d just need to cache the LocatedBlock(s), instead of looking them up from the DataNode on each small read byte&lt;span class=&quot;error&quot;&gt;&amp;#91;1024&amp;#93;&lt;/span&gt; call.&lt;/p&gt;

&lt;p&gt;In summary:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DFSClient.DFSInputStream.getBlockRange looks fast enough for many calls per second&lt;/li&gt;
	&lt;li&gt;locatedBlocks.findBlock uses a binary search for some reason, that&apos;ll be a bottleneck, why can&apos;t we divide the number the offset by the number of blocks.  Oh ok, that&apos;s because block sizes are variable.  I guess if the number of blocks is small the binary search will always be fast?  Or we can detect if the blocks are of the same size and divide to get the correct block?&lt;/li&gt;
	&lt;li&gt;DFSClient.DFSInputStream.fetchBlockByteRange is a hotspot because it calls chooseDataNode, whose return value &lt;span class=&quot;error&quot;&gt;&amp;#91;DNAddrPair&amp;#93;&lt;/span&gt; can be cached inside of LocatedBlock?&lt;/li&gt;
	&lt;li&gt;Later in fetchBlockByteRange we call DFSClient.createClientDatanodeProtocolProxy() and make a local RPC call, getBlockPathInfo.  I think the results of this &lt;span class=&quot;error&quot;&gt;&amp;#91;BlockPathInfo&amp;#93;&lt;/span&gt; can be cached into LocatedBlock as well?&lt;/li&gt;
	&lt;li&gt;Then instead of instantiating a new BlockReader object, we can call FileChannel.read(ByteBuffer b, long pos) directly?&lt;/li&gt;
	&lt;li&gt;With this solution in place we can safely store documents in the docstore without any worries, and in addition use the system that most efficient in Lucene today, all the while using the fewest file descriptors possible.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13001209" author="jasonrutherglen" created="Wed, 2 Mar 2011 00:28:32 +0000"  >&lt;p&gt;We&apos;ll want to keep a single ConcurrentMergeScheduler per HRegionServer (rather than per HRegion) even though there&apos;ll be an IndexWriter per HRegion (eg, the default is to have a CMS per IW, which could potentially generate too many threads).  I&apos;m wondering if there&apos;s a global attribute space to put the CMS so that it can be reused across HRegions?&lt;/p&gt;</comment>
                            <comment id="13001230" author="jasonrutherglen" created="Wed, 2 Mar 2011 01:20:59 +0000"  >&lt;p&gt;Here&apos;s a really simple first cut at converting &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; to use NIO positional read.  I&apos;m implementing this conservatively as I&apos;m honestly not entirely sure how HDFS works.  The TestFileLocalRead passes.  I don&apos;t know why we&apos;re closing the file descriptor after each read, I&apos;m going to start trying to remove that, and cache the FD (and other values) somewhere.&lt;/p&gt;</comment>
                            <comment id="13001269" author="ryanobjc" created="Wed, 2 Mar 2011 04:05:36 +0000"  >&lt;p&gt;can you submit this to the proper jira? This isn&apos;t hdfs &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13001277" author="jasonrutherglen" created="Wed, 2 Mar 2011 04:35:07 +0000"  >&lt;p&gt;@Ryan Sure, I just wanted to iterate here a little bit, and then test it out with the HDFSDirectory implementation, before submitting it to &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13001280" author="stack" created="Wed, 2 Mar 2011 04:57:06 +0000"  >&lt;p&gt;@Jason Do you need to hack on hdfs first?  Its critical to making the search work on hbase?&lt;/p&gt;</comment>
                            <comment id="13001285" author="jasonrutherglen" created="Wed, 2 Mar 2011 05:11:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you need to hack on hdfs first? Its critical to making the search work on hbase?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, HDFS as it is would make queries execute extremely slowly (because of random small reads), also I don&apos;t know how to implement the HDFSDirectory (the Lucene interface to the filesystem) without knowing how HDFS works.  In this case, we need to use NIO positional read underneath.  I think the patch shows NIO pos is doable and hopefully it&apos;ll be completed shortly, enough to implement HDFSDirectory and then run a performance comparison of HDFSDirectory vs. NIOFSDirectory.  Eg, we&apos;ll build identical indexes in both dirs, run the same queries and examine the difference in query speed.&lt;/p&gt;</comment>
                            <comment id="13001289" author="stack" created="Wed, 2 Mar 2011 05:28:41 +0000"  >&lt;p&gt;OK.&lt;/p&gt;

&lt;p&gt;Why niopositional read?  How is that different than the pread that is already in the dfsclient api?  You don&apos;t like going via the Block API?  Above you say in parens &apos;...(using FileChannel.read(ByteBuffer dst, long position)...&apos;  What if the data is not local, usually it is (&amp;gt; 99% of the time), but is not always; e.g. in time of failure or perhaps after a rebalance.  You going to get the FileChannel off the socket (thats the nio bit)?&lt;/p&gt;

&lt;p&gt;You do get the bit that hdfs-347 is a naughty hack as is.  A version that respects &apos;security&apos;, where the &apos;cleared&apos; fd is passed via unix domain sockets, for the dfsclient to use going direct is probably what&apos;ll go in sometime soon hopefully.&lt;/p&gt;

&lt;p&gt;You are messing down deep below hbase in dfs.  I&apos;m a little worried that you&apos;ll do a bunch of custom work that may work for your lucene directory implementation but that it will be so particular, it won&apos;t be accepted back into hdfs.&lt;/p&gt;</comment>
                            <comment id="13001292" author="yuzhihong@gmail.com" created="Wed, 2 Mar 2011 05:40:42 +0000"  >&lt;p&gt;In certain deployment, data node and region server are not on the same machine.&lt;br/&gt;
The above would pose performance issue.&lt;/p&gt;</comment>
                            <comment id="13001299" author="jasonrutherglen" created="Wed, 2 Mar 2011 06:05:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why niopositional read? How is that different than the pread that is already in the dfsclient api&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the goal of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; is it&apos;ll automatically switch between reading over the network and reading locally?  So the pread&apos;ll do one or the other?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You going to get the FileChannel off the socket (thats the nio bit)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s just for the local file.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What if the data is not local, usually it is (&amp;gt; 99% of the time), but is not always; e.g. in time of failure or perhaps after a rebalance. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we read off a socket I think there&apos;s going to be be a serious degradation in performance.  I think that&apos;s an invariant of search?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;A version that respects &apos;security&apos;, where the &apos;cleared&apos; fd is passed via unix domain sockets, for the dfsclient to use going direct is probably what&apos;ll go in sometime soon hopefully.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;ll be good!  I think this initial version (of HDFS modifications) is simply to get things going, as these other &lt;span class=&quot;error&quot;&gt;&amp;#91;HDFS&amp;#93;&lt;/span&gt; improvements are added we can use them and the DFSInputStream methods used by HDFSDirectory&apos;ll be the same?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You are messing down deep below hbase in dfs. I&apos;m a little worried that you&apos;ll do a bunch of custom work that may work for your lucene directory implementation but that it will be so particular, it won&apos;t be accepted back into hdfs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we need to pass the FD using Unix domain sockets then the HDFS work won&apos;t be useful.  If the UDS&apos;s enable positional read, then the &lt;span class=&quot;error&quot;&gt;&amp;#91;Lucene&amp;#93;&lt;/span&gt; HDFSDirectory will work well.  &lt;/p&gt;</comment>
                            <comment id="13002438" author="jasonrutherglen" created="Fri, 4 Mar 2011 04:08:05 +0000"  >&lt;p&gt;To get Solr distributed queries working across the searchable HBase cluster, we&apos;ll need &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-1431&quot; title=&quot;CommComponent abstracted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-1431&quot;&gt;&lt;del&gt;SOLR-1431&lt;/del&gt;&lt;/a&gt; completed.  Then in this issue, we&apos;ll implement the underlying data transfer protocol using HBase RPC (instead of HTTP).  &lt;/p&gt;</comment>
                            <comment id="13006931" author="jasonrutherglen" created="Tue, 15 Mar 2011 14:09:33 +0000"  >&lt;p&gt;Here&apos;s a first cut of this:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The default directory for the Lucene index is based on the region encoded name&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We&apos;re ignoring column family names for now, this should be configurable, however given we may integrate the Solr config system, there are no configuration settings as of yet.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Concurrent queries need to be tested, however they probably will not work because we need the underlying positional read enabled.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We&apos;re using the append codec because of HDFS&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The analyzer is hard coded, we should look at integrating the Solr schema system, however that is &lt;span class=&quot;error&quot;&gt;&amp;#91;currently&amp;#93;&lt;/span&gt; hardwired into the rest of the Solr config system.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;HBaseIndexSearcher uses the UID in the doc store to load the actual data from HBase&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;There are 2 unit tests, TestLuceneCoprocessor and TestHDFSDirectory&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;If HBase replication is turned on, we need to ensure each region&apos;s Lucene index has a unique path&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13007061" author="stack" created="Tue, 15 Mar 2011 18:14:31 +0000"  >&lt;p&gt;Patch looks great Jason.  Is it working?&lt;/p&gt;

&lt;p&gt;On license, its 2011, not 2010.&lt;/p&gt;

&lt;p&gt;What you need here?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-comment&quot;&gt;// sleep here is an ugly hack to allow region transitions to finish
&lt;/span&gt;+    &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(5000);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We should add an API for you that confirms region transitions for you rather than have you wait on a timer that may or may not work (On hudson, the apache build server, it is sure to fail though it may pass on all other platforms known to man).&lt;/p&gt;

&lt;p&gt;I love the very notion of an HBaseIndexSearcher.&lt;/p&gt;

&lt;p&gt;FYI, there is Bytes.equals in place of&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!Arrays.equals(r.getTableDesc().getName(), tableName)) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.. your choice. Just pointing it out....&lt;/p&gt;

&lt;p&gt;So, you think the package should be o.a.h.h.search?  Do you think this all should ship with hbase Jason?  By all means push back into hbase changes you need for your implementation but its looking big enough to be its own project?  What you reckon?&lt;/p&gt;

&lt;p&gt;Class comment missing from documenttransformer to explain what it does.  Its abstract.  Should it be an Interface?  (Has no functionality).&lt;/p&gt;

&lt;p&gt;Copyright missing from HDFSLockFactory.&lt;/p&gt;

&lt;p&gt;You are making HDFS locks.  Would it make more sense doing ephemeral locks in zk since zk is part of your toolkit when up on hbase?&lt;/p&gt;

&lt;p&gt;Whats going on here?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fileSystem.isDirectory(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Path(lockDir))) {&lt;span class=&quot;code-comment&quot;&gt;//lockDir.) {//isDirectory()) {&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;DefaultDocumentTransformer.java does non-standard license after the imports.  You do this in a few places.&lt;/p&gt;

&lt;p&gt;You probably should use Bytes.toStringBinary instead of +      String value = new String(kv.getValue());   The former does UTF-8 and it&apos;ll make binaries into printables if any present.&lt;/p&gt;

&lt;p&gt;ditto here: +          String rowStr = Bytes.toString(row);&lt;/p&gt;

&lt;p&gt;Class doc missing off HBaseIndexSearcher (or do you want to add package doc to boast about this amazing new utility?)&lt;/p&gt;

&lt;p&gt;What is this &apos;convert&apos; in HIS doing?  Cloning?&lt;/p&gt;

&lt;p&gt;Make the below use Logging instead of System.out?&lt;/p&gt;

&lt;p&gt;+    System.out.println(&quot;createOutput:&quot;+name);&lt;br/&gt;
+    return new HDFSIndexOutput(getPath(name));&lt;/p&gt;

&lt;p&gt;Have you done any perf testing on this stuff.  Is it going to be fast enough?  You hoping for most searches in in-memory.&lt;/p&gt;

&lt;p&gt;Whats appending codec?&lt;/p&gt;</comment>
                            <comment id="13007098" author="jasonrutherglen" created="Tue, 15 Mar 2011 19:01:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;Patch looks great Jason. Is it working?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Stack, thanks for your comments.  The test cases pass.  They&apos;re not very stressful yet.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;FYI, there is Bytes.equals in place of&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s copied from TestRegionObserverInterface.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You are making HDFS locks. Would it make more sense doing ephemeral locks in zk since zk is part of your toolkit when up on hbase?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s a good idea, however if HBase is enforcing the lock on a region, meaning the region can only exist on one server at a time, then the Lucene index locks are less important.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Have you done any perf testing on this stuff. Is it going to be fast enough? You hoping for most searches in in-memory.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can get the functionality working, restructuring Lucene or Solr as needed, assuming that positional reads in HDFS will be implemented (I have a separate HDFS patch that can be applied), then I&apos;ll start to benchmark.  The index doesn&apos;t need to be in heap space as the file local positional reads should rely on the system IO cache.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Whats appending codec?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Some Lucene segments files after being written seek back to the beginning of the file to write header information, the append codec only writes forward.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Class comment missing from documenttransformer to explain what it does. Its abstract. Should it be an Interface? (Has no functionality).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I will change it to an interface.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, you think the package should be o.a.h.h.search? Do you think this all should ship with hbase Jason? By all means push back into hbase changes you need for your implementation but its looking big enough to be its own project? What you reckon?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;ll be easier to write the code embedded in HBase, then if it works &lt;span class=&quot;error&quot;&gt;&amp;#91;well&amp;#93;&lt;/span&gt; we can decide?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What is this &apos;convert&apos; in HIS doing? Cloning?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s loading the actual data from HBase and returning it in a Lucene document.  While we can simply return the row + timestamp, loading the doc data is useful if we integrate Solr, because Solr needs a fully fleshed out document to perform for example, highlighting.&lt;/p&gt;

&lt;p&gt;I&apos;ll incorporate the rest of the code recommendations.  The next patch will &lt;span class=&quot;error&quot;&gt;&amp;#91;hopefully&amp;#93;&lt;/span&gt; have an RPC based search call, implement index splitting (eg, performing the same operation on the index as a region split), and have a test case for WAL based index restoring.&lt;/p&gt;</comment>
                            <comment id="13007105" author="tlipcon" created="Tue, 15 Mar 2011 19:06:12 +0000"  >&lt;p&gt;No matter how great this works, I don&apos;t think we should pull it into HBase proper. For me, &quot;contribs&quot; are an anti-pattern for various reasons we&apos;ve discussed before on the dev list.&lt;/p&gt;

&lt;p&gt;Now that we are a TLP, we could consider hosting subprojects for things like this - i.e with their own SVN trees and release cycles. But tying this release cycle to HBase core has a number of bad effects associated with it.&lt;/p&gt;

&lt;p&gt;Should we discuss on-list if there&apos;s disagreement?&lt;/p&gt;</comment>
                            <comment id="13007736" author="yuzhihong@gmail.com" created="Wed, 16 Mar 2011 23:25:21 +0000"  >&lt;p&gt;In DefaultDocumentTransformer, I think we should check whether row has changed:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (row == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
+          row = kv.getRow();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The following call should be made if row has changed:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
addFields(row, timestamp, doc, lucene);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13007761" author="apurtell" created="Thu, 17 Mar 2011 00:51:49 +0000"  >&lt;p&gt;@Todd Hosting subprojects sounds reasonable to me. We want to make a friendly home for cool new work but can also accommodate downstream packagers who don&apos;t want any kind of support implied.&lt;/p&gt;</comment>
                            <comment id="13007768" author="jasonrutherglen" created="Thu, 17 Mar 2011 01:09:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;In DefaultDocumentTransformer, I think we should check whether row has changed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s possible to modify multiple rows per postPut or postWALRestore?  Are the KeyValue(s) sorted by row, as we probably want to group row modifications together.  Also it seems that it&apos;s possible to only update a select few columns of a row?  So we may need to reload the entire row and index it again?&lt;/p&gt;</comment>
                            <comment id="13007780" author="stack" created="Thu, 17 Mar 2011 02:21:11 +0000"  >&lt;p&gt;@Andrew I&apos;m against taking on src/contribs given past experience where they tended to add friction to major core changes.  With hbase up in Apache git, I think its easier for projects that are not in our src tree to follow along (github makes it easy doc&apos;ing, etc., the related external project).  Discussion of the add-on up on hbase is grand (and encouraged I&apos;d say since it lets the rest of the hbase space know of the addition) but no src I&apos;d say.  Any changes to core an external project requires to work we should take on too (if good justification).&lt;/p&gt;</comment>
                            <comment id="13007782" author="apurtell" created="Thu, 17 Mar 2011 02:25:29 +0000"  >&lt;p&gt;@Stack I didn&apos;t say contrib, I said sub projects.&lt;/p&gt;
</comment>
                            <comment id="13007796" author="stack" created="Thu, 17 Mar 2011 03:54:44 +0000"  >&lt;p&gt;@Andrew Pardon me for my misread but I&apos;d be agin keeping up subprojects too because of the admin load.  We don&apos;t need it IMO.&lt;/p&gt;</comment>
                            <comment id="13008006" author="yuzhihong@gmail.com" created="Thu, 17 Mar 2011 17:07:12 +0000"  >&lt;p&gt;postWALRestore would pass one WALEdit which is for one row.&lt;br/&gt;
postPut is for one row as well.&lt;/p&gt;</comment>
                            <comment id="13016055" author="otis" created="Tue, 5 Apr 2011 18:07:06 +0000"  >&lt;p&gt;Jason, what is the current state of this work?  Does it work with the trunk?  Is there a list of issues/problems that need to be fixed before this can be called &quot;working&quot;? Thanks!&lt;/p&gt;</comment>
                            <comment id="13016079" author="jasonrutherglen" created="Tue, 5 Apr 2011 19:00:01 +0000"  >&lt;p&gt;@Otis The next step is to benchmark the query performance which may be degraded due to the random positional read performance of HDFS.  For this maybe we should use: &lt;a href=&quot;http://code.google.com/a/apache-extras.org/p/luceneutil/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/a/apache-extras.org/p/luceneutil/&lt;/a&gt;  Also, the blocking issues should &lt;span class=&quot;error&quot;&gt;&amp;#91;ideally&amp;#93;&lt;/span&gt; be resolved.  You can take a look at the Solr one &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-1431&quot; title=&quot;CommComponent abstracted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-1431&quot;&gt;&lt;del&gt;SOLR-1431&lt;/del&gt;&lt;/a&gt;, and commit it.&lt;/p&gt;</comment>
                            <comment id="13016085" author="otis" created="Tue, 5 Apr 2011 19:18:30 +0000"  >&lt;p&gt;Thanks Jason.  What&apos;s the Solr dependency about?  I thought your idea is to go with pure Lucene-level HBase + indexing integration, not Solr.  I do see you mention Solr&apos;s schema in the initial comments in this issue, but can&apos;t find any mentions of Solr in your patch.  Could you please clarify the approach?  Oh, and if the ML is a better medium, I can move my questions there.  Thanks.&lt;/p&gt;</comment>
                            <comment id="13016088" author="jasonrutherglen" created="Tue, 5 Apr 2011 19:29:13 +0000"  >&lt;p&gt;@Otis We can benchmark using Lucene in conjunction with &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;, of which I have a more streamlined version of that&apos;ll be available in Github.  Implementing Solr for benchmarking would create too much overhead.&lt;/p&gt;

&lt;p&gt;I think we may want to integrate with Solr &lt;span class=&quot;error&quot;&gt;&amp;#91;in the future&amp;#93;&lt;/span&gt; for out of the box distributed queries, facets, and also to make use of the schema.  I&apos;ll likely open additional Solr related issues when we get there.&lt;/p&gt;</comment>
                            <comment id="13017210" author="jasonrutherglen" created="Fri, 8 Apr 2011 00:00:51 +0000"  >&lt;p&gt;I placed the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; changes in a Github repository located at: &lt;a href=&quot;https://github.com/jasonrutherglen/HDFS-347-HBASE&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jasonrutherglen/HDFS-347-HBASE&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13017228" author="jasonrutherglen" created="Fri, 8 Apr 2011 01:27:19 +0000"  >&lt;p&gt;The HBase search related branch (at this point it&apos;s a branch, however it can/should be a coprocessor isolated Jar) is located at: &lt;a href=&quot;https://github.com/jasonrutherglen/HBase-Search&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jasonrutherglen/HBase-Search&lt;/a&gt;  The utility of it is to have all of the changes in one place, that will contain all of the integrated Lucene/HDFS changes.  This will allow easy benchmark and test execution.&lt;/p&gt;</comment>
                            <comment id="13018835" author="jasonrutherglen" created="Tue, 12 Apr 2011 13:43:19 +0000"  >&lt;p&gt;I&apos;m working on profiling and optimizing the HDFS random access, so that the Lucene HDFS queries are the same as native file system access using NIOFSDirectory.  &lt;/p&gt;

&lt;p&gt;I think one extremely direct approach is to set the max block size to something above all Lucene segments files (at runtime via the DFSClient.create method).  This will guarantee that there is only one underlying java.io.File per HDFS file, and so random access will avoid navigating block structures (which require expensive network calls, a binary search, and object creation overhead).&lt;/p&gt;</comment>
                            <comment id="13019879" author="jasonrutherglen" created="Thu, 14 Apr 2011 16:00:20 +0000"  >&lt;p&gt;Here are some basic benchmark numbers.  The code is more or less pushed to Github.  I need to verify it all works for a clean download of the various parts, of which there are 3, Lucene, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; Hadoop 0.20 append modified, and HBase with Search. &lt;/p&gt;

&lt;p&gt;The architecture is to write out a single block per Lucene file.  In this way we can simply obtain one underlying java.io.File directly from the DFSClient.  The file is then MMap&apos;ed using a modified version of the MMapDirectory called HDFSDirectory.&lt;/p&gt;

&lt;p&gt;The benchmark shows that storing Lucene indexes into HDFS and reading directly from HDFS is viable (as opposed to copying the files out of HDFS first to the local filesystem).&lt;/p&gt;

&lt;p&gt;Here are times in milliseconds, on the Wiki-EN corpus:&lt;/p&gt;

&lt;p&gt;lucene indexing duration: 50202&lt;br/&gt;
lucene query time #1: 11780&lt;br/&gt;
lucene query time #2: 6211&lt;br/&gt;
lucene query time #3: 6181&lt;/p&gt;

&lt;p&gt;hbase indexing duration: 70681&lt;br/&gt;
hbase query time #1: 8332&lt;br/&gt;
hbase query time #2: 6785&lt;br/&gt;
hbase query time #3: 6621&lt;/p&gt;

&lt;p&gt;As you can see, the indexing is a little bit slower when writing to HDFS.  However with the new changes going into Lucene (ie, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324&quot; title=&quot;Per thread DocumentsWriters that write their own private segments&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2324&quot;&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt;), a pause when flushing (due to HDFS overhead) will not slow down indexing.  So expect indexing parity soon.&lt;/p&gt;

&lt;p&gt;The main query times to look at are the #2 and #3, allowing for warmup of the system IO cache in #1.  HBase queries are somewhat slower because each new DFSInputStream created must contact the DataNode.  We can optimize this however I think for now we&apos;re good.&lt;/p&gt;

&lt;p&gt;Here are the queries being run (50 times per round), they are non-trivial.&lt;/p&gt;

&lt;p&gt;&quot;states&quot;&lt;br/&gt;
&quot;unit*&quot;&lt;br/&gt;
&quot;uni*&quot;&lt;br/&gt;
&quot;u*d&quot;&lt;br/&gt;
&quot;un*d&quot;&lt;br/&gt;
&quot;united~0.75&quot;&lt;br/&gt;
&quot;united~0.6&quot;&lt;br/&gt;
&quot;unit~0.7&quot;&lt;br/&gt;
&quot;unit~0.5&quot;, // 2&lt;br/&gt;
&quot;doctitle:/.&lt;b&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Uu&amp;#93;&lt;/span&gt;nited.&lt;/b&gt;/&quot;&lt;br/&gt;
&quot;united OR states&quot;&lt;br/&gt;
&quot;united AND states&quot;&lt;br/&gt;
&quot;nebraska AND states&quot;&lt;br/&gt;
&quot;\&quot;united states\&quot;&quot;&lt;br/&gt;
&quot;\&quot;united states\&quot;~3&quot;&lt;/p&gt;</comment>
                            <comment id="13020892" author="jasonrutherglen" created="Mon, 18 Apr 2011 02:33:41 +0000"  >&lt;p&gt;I updated the HBase search branch at Github and created complete instructions for how to execute the benchmark. This should also help with examining the code. The HBASE-SEARCH project contains 10,000 bz2 compressed wiki-en documents which account for 100 MB of the download. The slightly modified Lucene libraries are located in the lib/ directory (so that you do not need to download the entire Lucene branch source). &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jasonrutherglen/HBASE-SEARCH/blob/trunk/BENCHMARK.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jasonrutherglen/HBASE-SEARCH/blob/trunk/BENCHMARK.txt&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;The Lucene vs. HBase Search indexing and search times will be located in the file: &lt;br/&gt;
target/surefire-reports/org.apache.hadoop.hbase.search.TestSearchBenchmark-output.txt &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Benchmark Execution Instructions

Create a directory for the HBase Lucene installation.  Then run the following:

git clone git://github.com/jasonrutherglen/HDFS-347-HBASE.git HDFS-347-HBASE
cd HDFS-347-HBASE
ant mvn-install
cd ..

git clone git://github.com/jasonrutherglen/HBASE-SEARCH.git HBASE-SEARCH
cd HBASE-SEARCH
cd lib
./install-libs.sh
cd ..
cd wiki-en
tar -jxvf 10000.bz2
cd ..
mvn test -Dtest=TestSearchBenchmark
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Feel free to let me know if there are problems or if you have questions.&lt;/p&gt;</comment>
                            <comment id="13032015" author="jasonrutherglen" created="Wed, 11 May 2011 20:01:57 +0000"  >&lt;p&gt;I think the next round of benchmarking could involve showing that we need to directly access the underlying block file in order to not lose performance when running Lucene on HDFS.  This is somewhat as per the comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347?focusedCommentId=13013719&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13013719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HDFS-347?focusedCommentId=13013719&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13013719&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The next thing we wanted to look at was random I/O. There is a lot&lt;br/&gt;
more overhead on the datanode for this particular use case so this&lt;br/&gt;
could be a place where direct access could really excel&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can test using &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-941&quot; title=&quot;Datanode xceiver protocol should allow reuse of a connection&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-941&quot;&gt;&lt;del&gt;HDFS-941&lt;/del&gt;&lt;/a&gt; vs. direct block file access using MMap (by obtaining the local file path and the unix domain sockets).  I think then we&apos;ll show that for the Lucene case, we&apos;re on the right track by using direct file access.&lt;/p&gt;
</comment>
                            <comment id="13032196" author="jasonrutherglen" created="Thu, 12 May 2011 00:14:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-941&quot; title=&quot;Datanode xceiver protocol should allow reuse of a connection&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-941&quot;&gt;&lt;del&gt;HDFS-941&lt;/del&gt;&lt;/a&gt; isn&apos;t applying to trunk, and we&apos;ll need a semi-unique build of the HDFSDirectory and benchmarking code updated to Hadoop trunk (as opposed to Hadoop 0.20-append).  Given Unix Domain Sockets &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6311&quot; title=&quot;Add support for unix domain sockets to JNI libs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6311&quot;&gt;&lt;del&gt;HADOOP-6311&lt;/del&gt;&lt;/a&gt; is for trunk (rather than 0.20-append) we may want to wait for a version of HBase that runs on Hadoop trunk, (eg, the current direct file access works fine, Unix Domain Sockets is only for security, not speed).  Then we can put off benchmarking &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-941&quot; title=&quot;Datanode xceiver protocol should allow reuse of a connection&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-941&quot;&gt;&lt;del&gt;HDFS-941&lt;/del&gt;&lt;/a&gt; as well.  &lt;/p&gt;</comment>
                            <comment id="13033281" author="jasonrutherglen" created="Fri, 13 May 2011 20:55:08 +0000"  >&lt;p&gt;I updated the Lucene version to the latest from trunk which includes the new asynchronous flushing of the RAM buffer.  As expected, this has put the indexing creation using HDFS in line with Lucene (because the overhead from the DataNode does not delay further indexing).  Also it looks like the query times are in fact nearly the same as well.  &lt;/p&gt;

&lt;p&gt;Lucene indexing duration: 57858 ms&lt;br/&gt;
Lucene query time #1: 14208 ms&lt;br/&gt;
Lucene query time #2: 7024 ms&lt;br/&gt;
Lucene query time #3: 6902 ms&lt;/p&gt;

&lt;p&gt;HBase indexing duration: 50631 ms&lt;br/&gt;
HBase query time #1: 8625 ms&lt;br/&gt;
HBase query time #2: 7081 ms&lt;br/&gt;
HBase query time #3: 7139 ms&lt;/p&gt;</comment>
                            <comment id="13033288" author="stack" created="Fri, 13 May 2011 21:03:19 +0000"  >&lt;p&gt;Nice!&lt;/p&gt;</comment>
                            <comment id="13033289" author="tlipcon" created="Fri, 13 May 2011 21:03:59 +0000"  >&lt;p&gt;Awesome stuff. These query times above are using the hacky (non-secure non-checksummed) implementation of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;Apologies for my laziness of not looking through the code, but would you provide a one-paragraph description of how a user would interact with this? As I am understanding it, this is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;User defines some special property on a column family that they want to be searchable
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;this property would include a solr schema which specifies analyzers and fields&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;User inserts data using normal HBase APIs&lt;/li&gt;
	&lt;li&gt;User can now perform an arbitrary lucene search over the table, resulting in completely up-to-date results? (ie spans both memstore and flushed data)?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Is that right?&lt;/p&gt;</comment>
                            <comment id="13033311" author="jasonrutherglen" created="Fri, 13 May 2011 21:25:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;Awesome stuff. These query times above are using the hacky (non-secure non-checksummed) implementation of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s hackier than that.  It&apos;s basically obtaining the java.io.File directly from the FSInputStream.  However it&apos;s a good baseline to benchmark against things like &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6311&quot; title=&quot;Add support for unix domain sockets to JNI libs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6311&quot;&gt;&lt;del&gt;HADOOP-6311&lt;/del&gt;&lt;/a&gt; + &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;.  Those need to wait for HBase that works with Hadoop 0.22/trunk anyways?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;User defines some special property on a column family that they want to be searchable, this property would include a solr schema which specifies analyzers and fields&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Currently there&apos;s a DocumentTransformer class which needs to be implemented to transform column-family edits into a Lucene document.  That could use the Solr schema for example or any other separate system to tokenize the byte[]s into a Document.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;User can now perform an arbitrary lucene search over the table, resulting in completely up-to-date results? (ie spans both memstore and flushed data)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think for now we need to offer an external commit on the index, as Lucene only has near realtime search (eg, small segments will be written out, which will overwhelm HDFS).  &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2312&quot;&gt;LUCENE-2312&lt;/a&gt; will implement realtime search (eg, searching on the RAM buffer as it&apos;s being built).  The recent &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3092&quot; title=&quot;NRTCachingDirectory, to buffer small segments in a RAMDir&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-3092&quot;&gt;&lt;del&gt;LUCENE-3092&lt;/del&gt;&lt;/a&gt; could be used in the meantime to build segments in RAM, and only flush to HDFS when it&apos;s too RAM consuming, then we would not need to force the user to &apos;commit&apos; the index.&lt;/p&gt;

&lt;p&gt;To answer the question, yes, though today the indexing performance will not be as good as when &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2312&quot;&gt;LUCENE-2312&lt;/a&gt; is implemented or the user will need to &apos;commit&apos; the index to search on the latest data.&lt;/p&gt;

&lt;p&gt;Getting all of Solr work work with this system is fairly doable.  Each Solr core would map to a region.  Things like replication would be disabled.  The config files would be stored in HDFS (instead of the local filesystem).  For distributed queries, we need &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-1431&quot; title=&quot;CommComponent abstracted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-1431&quot;&gt;&lt;del&gt;SOLR-1431&lt;/del&gt;&lt;/a&gt;, and then to implement distributed networking using HBase RPC instead of Solr&apos;s HTTP RPC.  There are other smaller internal things that&apos;d need to change in Solr.  I think HBase RPC is aware of where regions live etc so I don&apos;t think we need to worry about putting failover logic into the distributed search code?&lt;/p&gt;

&lt;p&gt;I&apos;m going to post additional benchmarks shortly, eg, for 100,000 and 1 mil documents.&lt;/p&gt;</comment>
                            <comment id="13039516" author="jasonrutherglen" created="Thu, 26 May 2011 05:33:51 +0000"  >&lt;p&gt;In regards to checksums, I think we can verify/checksum the Lucene index&lt;br/&gt;
files only once when HDFSDirectory is created. We cannot checksum per file&lt;br/&gt;
open as the overhead would be too much. I think there&apos;ll need to be a hook&lt;br/&gt;
added to run the checksum via the HDFS client?&lt;/p&gt;

&lt;p&gt;The other issue is ensuring data locality as otherwise Lucene queries will&lt;br/&gt;
be unusable due to the inherent random access pattern. I think for this&lt;br/&gt;
we&apos;ll need to add something to the NameNode? Perhaps it would be a custom&lt;br/&gt;
placement policy, where if a given file is part of the Lucene index and&lt;br/&gt;
not local, we ask the NameNode to make it local (thereby over replicating&lt;br/&gt;
the file). I think this&apos;ll be a separate Jira issue?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;User inserts data using normal HBase APIs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, even if we &lt;span class=&quot;error&quot;&gt;&amp;#91;possibly&amp;#93;&lt;/span&gt; support Solr, we&apos;d only be implementing a&lt;br/&gt;
subset of the Solr functionality. One of the things that would go unused&lt;br/&gt;
is the ability to update documents using Solr APIs (which we&apos;d turn off),&lt;br/&gt;
instead the data will only be updated via HBase. The Solr query APIs and&lt;br/&gt;
schema would be the main parts of Solr we&apos;d be using. This can be roughly&lt;br/&gt;
defined as making using of the request handlers and search components:&lt;br/&gt;
&lt;a href=&quot;http://wiki.apache.org/solr/SearchComponent&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/solr/SearchComponent&lt;/a&gt; which perhaps should be&lt;br/&gt;
modularized out of Solr anyways.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; User can now perform an arbitrary lucene search over the table,&lt;br/&gt;
resulting in completely up-to-date results? (ie spans both memstore and&lt;br/&gt;
flushed data)? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, that is correct.&lt;/p&gt;</comment>
                            <comment id="13039882" author="jasonrutherglen" created="Thu, 26 May 2011 19:41:54 +0000"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2004&quot; title=&quot;Enable replicating and pinning files to a data node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2004&quot;&gt;&lt;del&gt;HDFS-2004&lt;/del&gt;&lt;/a&gt; to implement pinning HDFS files (in this case the Lucene index files) to the local DataNode.  I think this is necessary functionality for HBase search because all index files need to be local (we&apos;re MMap&apos;ing).  I think the common use case is a region server goes down, when the new one is brought up, files will likely not be local?&lt;/p&gt;</comment>
                            <comment id="13040465" author="jasonrutherglen" created="Fri, 27 May 2011 22:01:29 +0000"  >&lt;p&gt;In discussing with J-D (thanks!), we can place logic in the Lucene&lt;br/&gt;
Coprocessor preOpen method to find out if any of the blocks of the Lucene&lt;br/&gt;
files in HDFS are not local (by asking the NameNode), then we can:&lt;/p&gt;

&lt;p&gt;1) Rewrite, partially optimize, or fully optimize the index, thereby&lt;br/&gt;
rewriting the index files which causes them to &apos;go local&apos;.&lt;/p&gt;

&lt;p&gt;2) Extend the default placement policy and balancer to skip &apos;balancing&apos;&lt;br/&gt;
Lucene files, because we want them to stay local.&lt;/p&gt;

&lt;p&gt;3) Use &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2004&quot; title=&quot;Enable replicating and pinning files to a data node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2004&quot;&gt;&lt;del&gt;HDFS-2004&lt;/del&gt;&lt;/a&gt; to manually move non-local blocks to the local DataNode.&lt;/p&gt;

&lt;p&gt;Where #3 is more complex and will likely be much more time consuming.&lt;/p&gt;

&lt;p&gt;This functionality is important as it could currently be considered the only&lt;br/&gt;
&apos;blocker&apos; on putting HBase search into a test/production environment.&lt;/p&gt;</comment>
                            <comment id="13042913" author="jasonrutherglen" created="Thu, 2 Jun 2011 17:39:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-1431&quot; title=&quot;CommComponent abstracted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-1431&quot;&gt;&lt;del&gt;SOLR-1431&lt;/del&gt;&lt;/a&gt; is updated to trunk. I&apos;m tempted to start trying to plug in&lt;br/&gt;
Solr. I think the way to do this is to use the HTable.coprocessorExec&lt;br/&gt;
method (for the distributed search), where the Solr shards are of the form&lt;br/&gt;
&apos;shards=start:hexstartkey,end:hexendkey&apos;. Then HBase will take care of the&lt;br/&gt;
rest from an RPC perspective. Eg, forwarding the request to the individual&lt;br/&gt;
HRegion&apos;s running the SolrCoprocessor.&lt;/p&gt;

&lt;p&gt;I think we&apos;ll use a single Solr schema per region, though we can add a&lt;br/&gt;
special delimiter in the field name to indicate that the prefix is the&lt;br/&gt;
column family, then the column name. Something like &apos;headers:subject&apos; may&lt;br/&gt;
work. The main caveat is that the fields marked stored in fact&lt;br/&gt;
will not be stored into Lucene (because they&apos;re in HBase).&lt;/p&gt;</comment>
                            <comment id="13045981" author="alexb" created="Wed, 8 Jun 2011 14:38:03 +0000"  >&lt;p&gt;There seem to be a bug in the changed HDFS code in git://github.com/jasonrutherglen/HDFS-347-HBASE.&lt;/p&gt;

&lt;p&gt;hdfs-347-hbase/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java&lt;br/&gt;
getFile():1508&lt;/p&gt;

&lt;p&gt;      if ((targetAddr.equals(localHost) ||&lt;br/&gt;
        targetAddr.getHostName().startsWith(&quot;localhost&quot;))) {&lt;/p&gt;

&lt;p&gt;instead, it should be:&lt;/p&gt;

&lt;p&gt;      if ((targetAddr.getAddress().equals(localHost) ||&lt;br/&gt;
        targetAddr.getHostName().startsWith(&quot;localhost&quot;))) {&lt;/p&gt;

&lt;p&gt;This causes TestLuceneCoprocessor to fail in case the machine&apos;s host resolves to smth other than localhost (&quot;alexpc&quot; in my case).&lt;/p&gt;


&lt;p&gt;P.S. this was found during HBase hackathon in Berlin, hi from there!&lt;/p&gt;</comment>
                            <comment id="13046016" author="alexb" created="Wed, 8 Jun 2011 15:37:18 +0000"  >&lt;p&gt;Another problem we faced: looks like there&apos;s an issue in TestLuceneCoprocessor tests life-cycle or smth else:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;the testSearchRPC test fails if we run &quot;mvn clean -Dtest=TestLuceneCoprocessor test&quot;, other 2 pass (it fails on first assert: expected 20, but found 10)&lt;/li&gt;
	&lt;li&gt;if I add @Ignore to other two tests, i.e. the maven command runs only testSearchRPC, it works well&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13046023" author="jasonrutherglen" created="Wed, 8 Jun 2011 15:46:26 +0000"  >&lt;p&gt;Hi Alex, I have new code I will commit to Github.  &lt;/p&gt;</comment>
                            <comment id="13046026" author="alexb" created="Wed, 8 Jun 2011 15:48:48 +0000"  >&lt;p&gt;Thank you! Berlin is waiting! (kidding, we are going to leave very soon)&lt;/p&gt;</comment>
                            <comment id="13046258" author="otis" created="Wed, 8 Jun 2011 22:12:12 +0000"  >&lt;p&gt;A few more comments/questions for Jason:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I see PKIndexSplitter usage for splitting the index when a region splits.  I see you split the index, open 2 IndexWriters for 2 new Lucene indices, but then either you are not adding documents to them, or I&apos;m not seeing it?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Are there issues around distributed search?  It looks like it wasn&apos;t in your github branch.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;What will happen when a region changes its location/regionserver for whatever reason?  I see &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-2004&quot; title=&quot;Enable replicating and pinning files to a data node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-2004&quot;&gt;&lt;del&gt;HDFS-2004&lt;/del&gt;&lt;/a&gt; got -1ed and you said without that search will be slow.  Do you have an alternative plan?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;What is the reason for storing those 2 extra row fields? (the UID one at the other one... I think it&apos;s called rowStr or something like that)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;What about storing the index in HBase itself? (a la Solandra, I suppose)  Would this be doable?  Would it make things simpler in the sense that any splitting or moving around, etc. may be handled by HBase and we wouldn&apos;t have to make sure the Lucene index always mirrors what&apos;s in a region and make sure it follows the region wherever it goes?  Lars&apos; idea/question, and I hope I didn&apos;t misunderstand or misrepresent his ideas.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13046267" author="jasonrutherglen" created="Wed, 8 Jun 2011 22:48:52 +0000"  >&lt;p&gt;Otis, I think many of your questions have been addressed in this issue, though indeed the comment trail is long at this point.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Do you have an alternative plan?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3529?focusedCommentId=13040465&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13040465&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3529?focusedCommentId=13040465&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13040465&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Are there issues around distributed search? It looks like it wasn&apos;t in your github branch&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3529?focusedCommentId=13042913&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13042913&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3529?focusedCommentId=13042913&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13042913&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What about storing the index in HBase itself?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that&apos;s a great idea to test, though in a different Jira issue.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;PKIndexSplitter&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2919&quot; title=&quot;IndexSplitter that divides by primary key term&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2919&quot;&gt;&lt;del&gt;LUCENE-2919&lt;/del&gt;&lt;/a&gt;.  Given it&apos;s not been committed I may need to bring it over into the HBase search source tree.&lt;/p&gt;</comment>
                            <comment id="13046274" author="otis" created="Wed, 8 Jun 2011 23:15:09 +0000"  >&lt;p&gt;Re &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3529?focusedCommentId=13042913&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13042913&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3529?focusedCommentId=13042913&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13042913&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Does that mean that in order to implement distributed search you&apos;ll immediately convert this to HBase+Solr instead of HBase+Lucene, so that you don&apos;t have to do Lucene-level distributed search?  If so, what about NRTness that will be lost until Solr gets NRT search?&lt;/p&gt;</comment>
                            <comment id="13046696" author="jasonrutherglen" created="Thu, 9 Jun 2011 17:43:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;Does that mean that in order to implement distributed search you&apos;ll immediately convert this to HBase+Solr instead of HBase+Lucene&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the distributed search capability has been removed from Lucene (I just sent an email to Lucene dev)?  We should add it back?  Hence the possible Solr integration.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If so, what about NRTness that will be lost until Solr gets NRT search?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There&apos;s a Solr issue to add this though one wouldn&apos;t want to implement NRT without &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3092&quot; title=&quot;NRTCachingDirectory, to buffer small segments in a RAMDir&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-3092&quot;&gt;&lt;del&gt;LUCENE-3092&lt;/del&gt;&lt;/a&gt; + &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-2565&quot; title=&quot;Prevent IW#close and cut over to IW#commit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-2565&quot;&gt;&lt;del&gt;SOLR-2565&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13050658" author="jasonrutherglen" created="Thu, 16 Jun 2011 19:12:33 +0000"  >&lt;p&gt;To implement distributed search with sort, we&apos;ll need to serialize the field values across the RPC channel.  This can be implemented by assuming the sort is by ord which yields BytesRef values, which are easy to sort.&lt;/p&gt;</comment>
                            <comment id="13062266" author="jasonrutherglen" created="Sat, 9 Jul 2011 00:26:49 +0000"  >&lt;p&gt;With some recent patches committed to Lucene, I can post a patch to HBase trunk that should work fine, that will only require the special &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; modification/build.  Perhaps it&apos;s possible to Maven in the custom &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; so that no external libraries need to manually downloaded.&lt;/p&gt;</comment>
                            <comment id="13062269" author="apurtell" created="Sat, 9 Jul 2011 00:34:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;Perhaps it&apos;s possible to Maven in the custom &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; so that no external libraries need to manually downloaded.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Post 0.92 we plan to modularize the Maven build already for pluggable RPC and security-variant code. We can also conditionally build coprocessors set in their own packages. In this case, something like &lt;tt&gt;-D &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;&lt;/tt&gt; enables build of it, and pulls down a suitably patched Hadoop core jar?&lt;/p&gt;</comment>
                            <comment id="13062270" author="jasonrutherglen" created="Sat, 9 Jul 2011 00:39:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;We can also conditionally build coprocessors set in their own packages&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, that sounds interesting.  Currently I&apos;m pretending like search will be a part of HBase core.  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  If there is another directory to place it in, eg, a coprocessor or contrib directory, I will place it there.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In this case, something like &lt;tt&gt;-D &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;&lt;/tt&gt; enables build of it, and pulls down a suitably patched Hadoop core jar?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I have no idea how to post the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt;-LUCENE version to a Maven repo and get that working.  I can however probably figure it out.  &lt;/p&gt;

&lt;p&gt;I like the idea of posting a patch, putting things on Github seems quite remote, even to me, and I admit to preferring the simplicity of SVN on this currently one man project.&lt;/p&gt;</comment>
                            <comment id="13062273" author="apurtell" created="Sat, 9 Jul 2011 00:50:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;Currently I&apos;m pretending like search will be a part of HBase core.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Like security, I think there will be enough interest for this that &quot;core but conditional&quot; makes a lot of sense.&lt;/p&gt;</comment>
                            <comment id="13062286" author="jasonrutherglen" created="Sat, 9 Jul 2011 01:34:37 +0000"  >&lt;p&gt;What&apos;s the best way to set custom attributes on the Coprocessor?  Eg, I want to tell the Lucene Coprocessor where to look for a configuration file in HDFS.&lt;/p&gt;</comment>
                            <comment id="13062292" author="apurtell" created="Sat, 9 Jul 2011 02:01:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;What&apos;s the best way to set custom attributes on the Coprocessor? Eg, I want to tell the Lucene Coprocessor where to look for a configuration file in HDFS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4048&quot; title=&quot;[Coprocessors] Support configuration of coprocessor at load time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4048&quot;&gt;&lt;del&gt;HBASE-4048&lt;/del&gt;&lt;/a&gt; and HBase-3810. 3810 is still pending.&lt;/p&gt;</comment>
                            <comment id="13062302" author="jasonrutherglen" created="Sat, 9 Jul 2011 03:03:40 +0000"  >&lt;p&gt;I opened a trivial issue &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3296&quot; title=&quot;Enable passing a config into PKIndexSplitter&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-3296&quot;&gt;&lt;del&gt;LUCENE-3296&lt;/del&gt;&lt;/a&gt; so that the custom IW config can be passed in.&lt;/p&gt;</comment>
                            <comment id="13062318" author="jasonrutherglen" created="Sat, 9 Jul 2011 04:31:24 +0000"  >&lt;p&gt;I&apos;m signing up to &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; for the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; Maven hosting.&lt;/p&gt;

&lt;p&gt;1. &lt;a href=&quot;http://nexus.sonatype.org/oss-repository-hosting.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://nexus.sonatype.org/oss-repository-hosting.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13066551" author="jasonrutherglen" created="Sat, 16 Jul 2011 23:11:47 +0000"  >&lt;p&gt;In reviewing the &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; modification I&apos;d made, the only part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-347&quot; title=&quot;DFS read performance suboptimal when client co-located on nodes with data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-347&quot;&gt;&lt;del&gt;HDFS-347&lt;/del&gt;&lt;/a&gt; that&apos;s needed is obtaining the local file path from the DataNode via RPC.  I will generate a patch that implements this, and submit it to HDFS against the append-0.20 branch.  It would be nice to have a more generic introspective metadata&apos;ish API for HDFS that would encompass this (so that it&apos;s not so specific to only local data files).&lt;/p&gt;</comment>
                            <comment id="13066581" author="jasonrutherglen" created="Sun, 17 Jul 2011 03:35:46 +0000"  >&lt;p&gt;Here&apos;s a clean update to HDFS that enables obtaining the local file a block corresponds to.  I need to place this build in a Maven repository for the actual HBase Search patch.&lt;/p&gt;</comment>
                            <comment id="13113170" author="ekoontz" created="Fri, 23 Sep 2011 05:48:14 +0000"  >&lt;p&gt;&quot;What&apos;s the best way to set custom attributes on the Coprocessor? Eg, I want to tell the Lucene Coprocessor where to look for a configuration file in HDFS.&quot;&lt;/p&gt;

&lt;p&gt;&quot;See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4048&quot; title=&quot;[Coprocessors] Support configuration of coprocessor at load time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4048&quot;&gt;&lt;del&gt;HBASE-4048&lt;/del&gt;&lt;/a&gt; and HBase-3810. 3810 is still pending.&quot;&lt;/p&gt;</comment>
                            <comment id="13262494" author="martinalig" created="Thu, 26 Apr 2012 09:29:22 +0000"  >&lt;p&gt;@Json: Are you still working on this issue?&lt;/p&gt;</comment>
                            <comment id="13727536" author="linwukang" created="Fri, 2 Aug 2013 10:18:03 +0000"  >&lt;p&gt;I think the most difficut part to integrate Solr into hbase is How to maintain consistency between solr and hbase.&lt;/p&gt;</comment>
                            <comment id="14094537" author="apurtell" created="Tue, 12 Aug 2014 19:16:25 +0000"  >&lt;p&gt;No pressing use case for embedding search. NGDATA&apos;s hbase-indexer (&lt;a href=&quot;https://github.com/NGDATA/hbase-indexer&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/NGDATA/hbase-indexer&lt;/a&gt;) is an interesting option that masquerades as a replication endpoint and feeds incoming updates into Solr.&lt;/p&gt;</comment>
                            <comment id="15350934" author="dzmitry.lahoda" created="Mon, 27 Jun 2016 12:46:13 +0000"  >&lt;p&gt;I am seeking to replace oracle db + elastic search in data intensive legal ediscovery application. Hbase seems suit, and internal integration with lucene would be interesting option. I know about external integrations of hbase with solr/elastcisearch integrations, but these have own problems as I understand (need to data storage and orchestration of cluster instead of one).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12498609">LUCENE-2919</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12513319">LUCENE-3296</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12480542">HBASE-3257</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12508853">SOLR-2563</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12437924">HADOOP-6311</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12404278">HBASE-883</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12504334">HBASE-3786</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12512357">HBASE-4048</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12410156">HDFS-347</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12455118">HDFS-941</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12435659">SOLR-1431</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12508922">SOLR-2565</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12504862">HBASE-3810</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12473678" name="HBASE-3529.patch" size="42186" author="jasonrutherglen" created="Tue, 15 Mar 2011 14:09:33 +0000"/>
                            <attachment id="12486753" name="HDFS-APPEND-0.20-LOCAL-FILE.patch" size="8383" author="jasonrutherglen" created="Sun, 17 Jul 2011 03:35:46 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 14 Feb 2011 17:53:17 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>13106</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            24 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i00rxb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2456</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>