<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:26:13 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-11747/HBASE-11747.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-11747] ClusterStatus is too bulky </title>
                <link>https://issues.apache.org/jira/browse/HBASE-11747</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Following exception on 0.98 with 1M regions on cluster with 160 region servers&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Caused by: java.io.IOException: Call to regionserverhost:port failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1482)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1454)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1654)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1712)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.getClusterStatus(MasterProtos.java:42555)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$5.getClusterStatus(HConnectionManager.java:2132)
	at org.apache.hadoop.hbase.client.HBaseAdmin$16.call(HBaseAdmin.java:2166)
	at org.apache.hadoop.hbase.client.HBaseAdmin$16.call(HBaseAdmin.java:2162)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	... 43 more
Caused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
	at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12734137">HBASE-11747</key>
            <summary>ClusterStatus is too bulky </summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12714102">HBASE-11165</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="virag">Virag Kothari</reporter>
                        <labels>
                    </labels>
                <created>Thu, 14 Aug 2014 18:44:58 +0000</created>
                <updated>Thu, 20 Aug 2015 18:07:03 +0000</updated>
                                                                                <due></due>
                            <votes>1</votes>
                                    <watches>16</watches>
                                                                <comments>
                            <comment id="14097370" author="virag" created="Thu, 14 Aug 2014 18:49:16 +0000"  >&lt;p&gt;This exception will be thrown if message size is more than 64MB. With 1M regions (only open and no data) on 160 servers, the size is around 100Mb. &lt;br/&gt;
For now, did a workaround by setting the CodedInputStream.setSizeLimit() to a very high value. &lt;br/&gt;
 Do we need thinner API&apos;s? I assume RegionLoad is quite heavy.&lt;/p&gt;</comment>
                            <comment id="14097374" author="virag" created="Thu, 14 Aug 2014 18:51:51 +0000"  >&lt;p&gt;Attached is the full exception trace&lt;/p&gt;</comment>
                            <comment id="14097387" author="apurtell" created="Thu, 14 Aug 2014 19:01:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do we need thinner API&apos;s? I assume RegionLoad is quite heavy.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, but we have to be careful in 0.98 not to change APIs in a breaking way. I think increasing the message size limit to work around the problem is fine given that consideration. How often do you plan to call HBaseAdmin#getClusterStatus?&lt;/p&gt;</comment>
                            <comment id="14097392" author="apurtell" created="Thu, 14 Aug 2014 19:04:38 +0000"  >&lt;p&gt;We can add &lt;b&gt;new&lt;/b&gt; APIs. I wonder if it would be workable to introduce a streaming status API where the client uses a cursor to iterate over the master&apos;s picture of the cluster. Might be tricky wherever regions have migrated or servers have come and gone. The master would have to provide either a consistent snapshot of state or track changes since the client opened the cursor and mix in change deltas with iteration results.&lt;/p&gt;</comment>
                            <comment id="14097506" author="eclark" created="Thu, 14 Aug 2014 19:42:03 +0000"  >&lt;p&gt;We should look at using smaller region names as well.  There&apos;s no need to send the whole region name across.&lt;/p&gt;</comment>
                            <comment id="14101323" author="stack" created="Mon, 18 Aug 2014 21:18:20 +0000"  >&lt;p&gt;Good one. Every RS sending 100MB of &apos;status&apos; to the master every second or so is just obnoxious, especially so when much of this info is being duplicated no our metrics &apos;channel&apos;.  Thanks for bringing this one up Virag. We need a bit of fixup in here.&lt;/p&gt;</comment>
                            <comment id="14514224" author="devl.development" created="Mon, 27 Apr 2015 14:43:40 +0000"  >&lt;p&gt;Is there any progress on this, or a workaround we can make use of? The comments above by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; state setting: CodedInputStream.setSizeLimit() where can we do this, is it possible to do this in the application code? Or is it possible to set any other config param for example, will replication.source.size.capacity help with a workaround until a fix is implemented? Thanks&lt;/p&gt;</comment>
                            <comment id="14570287" author="apurtell" created="Wed, 3 Jun 2015 05:07:41 +0000"  >&lt;p&gt;One option is to use CodedInputStream#setSizeLimit in the client to effectively disable this check by setting it to Integer.MAX.&lt;/p&gt;</comment>
                            <comment id="14610877" author="mantonov" created="Wed, 1 Jul 2015 19:30:40 +0000"  >&lt;p&gt;Wondering about next steps/directions here. Do we want to just bump CodeInputStream#limit to higher numbers and see if that addresses problem at hands (I think it should), or do we want to optimize protocol? Ive seen 3 options here - 1)streaming instead of single message 2) decouple region/RS load info from cluster status itself 3) try to make data pieces themselves more compact, region names etc.&lt;/p&gt;</comment>
                            <comment id="14610905" author="stack" created="Wed, 1 Jul 2015 19:50:02 +0000"  >&lt;p&gt;Lets up CIS#limit for sure.&lt;/p&gt;

&lt;p&gt;In new JIRA optimize protocol. There are a few already if you search &apos;hbase clusterstatus&apos;. I like #2 and #3 from your list. For #2, was looking at exporting jmx so say the Master could read cluster metrics instead of getting metrics recast and served on the heartbeat. Was looking at &lt;a href=&quot;https://jolokia.org/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://jolokia.org/&lt;/a&gt; Seems more sensible than JMX federation (Seems like its possible to hook up as src for D3 graphing). Do we poll rather than have the stuff pushed? What happens in a big cluster?&lt;/p&gt;

&lt;p&gt;Good on you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14610932" author="thiruvel" created="Wed, 1 Jul 2015 20:10:55 +0000"  >&lt;p&gt;We are also exploring the option of compressing the status and sending from the server.&lt;/p&gt;</comment>
                            <comment id="14611051" author="mantonov" created="Wed, 1 Jul 2015 21:57:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For #2, was looking at exporting jmx so say the Master could read cluster metrics instead of getting metrics recast and served on the heartbeat&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Did you mean rpc, not jmx? I briefly looked at where it&apos;s actually used, and unless I&apos;m missing something, we don&apos;t really use it in any heardbeats. Cluster status is used:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for subscribers on (multicast) publishing (that&apos;s the only push as far as I can tell?)&lt;/li&gt;
	&lt;li&gt;in separate MasterRpcServices#GetClusterStatus rpc call and accordingly in Admin interface wrapping it (which is in the log posted in the jora)&lt;/li&gt;
	&lt;li&gt;in REST messages&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For regular heartbeats we just use MRS#regionServerReport rpc call, which only pushes to master RS server name/load (including region load). So as far as I can tell, those are already mostly decoupled. So I think the options (aside bumping the size of message) drift to something like &quot;check if monolithic cluster status is looking too big (over defined limit) on server side, and return it with empty load in this case, setting some flag indicating that message is partially constructed to not fail as transport level, and that client should use separate call to request server/region load for the list of RSs it&apos;s interested to know about?&quot;&lt;/p&gt;

&lt;p&gt;In other words, I guess I see 2 basic options:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;bump the size of message in this jira (trivial patch)&lt;/li&gt;
	&lt;li&gt;leave current ClusterStatus format as is for compatibility, but add handling to return empty LiveServerInfo list if it&apos;s coming up too big, add new rpc call to retrieve list of LiveServerInfo for a list (range?) of region servers. Here&apos;s where RS groups would be handy. What do you think?&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;Seems like its possible to hook up as src for D3 graphing&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hmm, that&apos;s something different, drawing metrics in the UI?&lt;/p&gt;</comment>
                            <comment id="14611054" author="mantonov" created="Wed, 1 Jul 2015 21:58:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thiruvel&quot; class=&quot;user-hover&quot; rel=&quot;thiruvel&quot;&gt;Thiruvel Thirumoolan&lt;/a&gt; &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We are also exploring the option of compressing the status and sending from the server.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could you please describe you case little more? You&apos;re facing this error, or just trying to optimize the traffic or something else? Would be interested to know the size of cluster/ # of regions you&apos;re serving.&lt;/p&gt;</comment>
                            <comment id="14611125" author="stack" created="Wed, 1 Jul 2015 22:53:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;Did you mean rpc, not jmx?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;JMX. Idea is to help shrink ClusterStatus by moving metrics out.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m missing something, we don&apos;t really use it in any heardbeats&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right (I thought we did but it is just a sub-element, the ServerLoad, that is passed on the heartbeat &amp;#8211; pardon me).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;check if monolithic cluster status is looking too big (over defined limit) on server side, and return it with empty load in this case, setting some flag indicating that message is partially constructed to not fail as transport level, and that client should use separate call to request server/region load for the list of RSs it&apos;s interested to know about?&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Rather than have a protocol that cuts in only when we are too big, could we not slim ClusterStatus so vitals only and always require client use a separate call for detail (or go to metrics system if it is counts, etc., that it is interested in)&lt;/p&gt;

&lt;p&gt;I like your suggestion of adding a new call for doing new &quot;protocol&quot; That&apos;d be best.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Hmm, that&apos;s something different, drawing metrics in the UI?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. Pardon my conflation. Will restrain myself in future.&lt;/p&gt;




</comment>
                            <comment id="14611159" author="mantonov" created="Wed, 1 Jul 2015 23:17:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;JMX. Idea is to help shrink ClusterStatus by moving metrics out.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hmm. JMX isn&apos;t a transport for messages, is it? I think I&apos;m missing something here.. I thought only of RPC messaging overhaul here. Could you describe JMX approach?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Rather than have a protocol that cuts in only when we are too big, could we not slim ClusterStatus so vitals only and always require client use a separate call for detail (or go to metrics system if it is counts, etc., that it is interested in). I like your suggestion of adding a new call for doing new &quot;protocol&quot; That&apos;d be best.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So you think, just modify ClusterStatus proto server side wiring, so we just never include load info in the message (we can avoid completely removing this field to maintain wire compatibility?), and add new rpc method? That&apos;s what i&apos;m thinking now too. Question - how would this new RPC overlap with metrics functionality? &lt;/p&gt;

&lt;p&gt;Let me walk thru users of ClusterStatus and see which of them actually use load info and for what (balancer, what else).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yes. Pardon my conflation. Will restrain myself in future.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Oh, I just meant, is there more aspects of this problem than what I see now, which should be considered while deciding of what approach to take.&lt;/p&gt;


</comment>
                            <comment id="14611166" author="mantonov" created="Wed, 1 Jul 2015 23:22:29 +0000"  >&lt;p&gt;(also curious to hear more opinions?)&lt;/p&gt;</comment>
                            <comment id="14611189" author="stack" created="Wed, 1 Jul 2015 23:41:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;JMX isn&apos;t a transport for messages, is it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No. Generally JMX is for management. HBase uses it to publish server attributes and metrics. HBase also puts up a JMX Bean Server so you can query the beans over the net. This mechanism uses java&apos;s crazy RMI which is mostly unusable by systems other than java and even then, has a ping-pong random port mechanism that requires open port ranges. The nice thing about the &lt;a href=&quot;https://jolokia.org/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://jolokia.org/&lt;/a&gt; is that it REST/JSON-ifies our JMX making it more palatable to more systems.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could you describe JMX approach?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ClusterStatus is made of various attributes including ServerLoad for every node in the cluster. ServerLoad is not actually server load. Rather, it is a dumping ground for all and sundry including server attributes, configuration, and metrics. Redoing ServerLoad so it is just load vitals would be a nice to have so we don&apos;t flood the master once a second as all report in with fat messages on their heartbeats. Server metrics are also available published out of our metrics system. Metrics are published variously &amp;#8211; as text in a servlet and as jmx beans available on each server (jmx is on a period IIRC, servlet is poll). That we are dumping out our metrics on a period via JMX and that we then go and collect them all again to put on a heartbeat is silly. Would be nice to refactor. If ServerLoad is slimmed, then it would help here given we do one up for each server and insert in ClusterStatus.&lt;/p&gt;

&lt;p&gt;That was high-level what I was thinking. Separate issue I&apos;d say, a background consideration when addressing this one.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Question - how would this new RPC overlap with metrics functionality?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Was thinking they&apos;d be distinct. If you want metrics, use our metrics system; we are publishing our metrics per server anyways.&lt;/p&gt;</comment>
                            <comment id="14705451" author="apurtell" created="Thu, 20 Aug 2015 18:07:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do we want to just bump CodeInputStream#limit to higher numbers and see if that addresses problem at hands&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I did this on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13825&quot; title=&quot;Use ProtobufUtil#mergeFrom and ProtobufUtil#mergeDelimitedFrom in place of builder methods of same name&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13825&quot;&gt;&lt;del&gt;HBASE-13825&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12834590">HBASE-13825</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12834590">HBASE-13825</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12661765" name="exceptiontrace" size="5948" author="virag" created="Thu, 14 Aug 2014 18:51:51 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 14 Aug 2014 19:01:42 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>412164</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 17 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1ywm7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>412153</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>