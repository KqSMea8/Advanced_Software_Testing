<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:50:19 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1050/HBASE-1050.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1050] Allow regions to split around scanners</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1050</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We have a number of scanners iterating over a table that also sees a lot of constant write activity. If the scans are too frequent we will suppress splitting. At a lull then a number of splits happen all at once, occasionally overwhelming DFS and causing file corruption. &lt;/p&gt;

&lt;p&gt;I wonder how much work it would be to split regions around scanners. Rather than wait for scanner leases to expire, suspend/block the scanner, split the table, and then negotiate with the client to continue. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12410241">HBASE-1050</key>
            <summary>Allow regions to split around scanners</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Tue, 9 Dec 2008 02:21:59 +0000</created>
                <updated>Sun, 13 Sep 2009 22:24:16 +0000</updated>
                            <resolved>Mon, 24 Aug 2009 20:57:45 +0000</resolved>
                                                    <fixVersion>0.20.0</fixVersion>
                                    <component>Client</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12655135" author="apurtell" created="Wed, 10 Dec 2008 08:12:58 +0000"  >&lt;p&gt;Did a cold restart of the cluster today. Regions doubled as HBase came up. Seems a large number of splits were suppressed by near-constant scanner activity prior to restart. &lt;/p&gt;</comment>
                            <comment id="12655304" author="stack" created="Wed, 10 Dec 2008 18:39:02 +0000"  >&lt;p&gt;Others have reported this phenomeon on list.   Marking blocker against 0.20.0.  Gets in the way of our scaling.&lt;/p&gt;</comment>
                            <comment id="12655480" author="apurtell" created="Thu, 11 Dec 2008 03:02:51 +0000"  >&lt;p&gt;A workaround for now is to temporarily pause the scanner jobs and use the force compaction feature of the 0.19.0 master UI or HBaseAdmin API. Can be scripted/automated. &lt;/p&gt;</comment>
                            <comment id="12656384" author="apurtell" created="Sun, 14 Dec 2008 12:38:21 +0000"  >&lt;p&gt;I&apos;ve recently seen a failure case where this issue caused enough data to build up in the region that when the compaction/split finally happened, it pushed DFS over the edge and file errors occurred. It seems the failure happened after HBase thought the new mapfile for one of the splits was fully written, but DFS disagreed (a block was corrupted and lost before replication as best as I can tell), and later reality caught up when the region was reassigned during a rebalance. The region was lost, and thus the test table, which had 978 regions at the time. I was deliberately stressing HBase and thus DFS at the time with 100 concurrent Heritrix crawler threads. &lt;/p&gt;

&lt;p&gt;I&apos;ll try to take up this issue upon my return from China if it is not otherwise being actively worked on by then.&lt;/p&gt;</comment>
                            <comment id="12658208" author="bluelu" created="Fri, 19 Dec 2008 22:32:40 +0000"  >&lt;p&gt;I think this one is hitting me as well on a regulary basis (and might also be the cause of the hangups I&apos;m experiencing)&lt;/p&gt;

&lt;p&gt;I have a thread that is constantly adding new data to a table. This table is also scanned regularly (every 30 minutes, but the job currently takes longer) and parts of the rows are overwritten with new data, so lots of region splits could happen in the meantime. &lt;/p&gt;

&lt;p&gt;I also had bad blocks in my DFS yesterday, allthough dfs did report a healthy state. When I restarted hbase. hbase never came up (there was a missing block in the rootfile)&lt;/p&gt;</comment>
                            <comment id="12658211" author="stack" created="Fri, 19 Dec 2008 22:37:00 +0000"  >&lt;p&gt;See the last item on this page: &lt;a href=&quot;http://wiki.apache.org/hadoop/Hbase/Troubleshooting#5&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop/Hbase/Troubleshooting#5&lt;/a&gt;.  Have you that config. in place?&lt;/p&gt;</comment>
                            <comment id="12658237" author="apurtell" created="Sat, 20 Dec 2008 00:55:45 +0000"  >&lt;p&gt;I have the config mods in place and yet have been getting lethal DFS errors (corrupt or missing blocks in mapfile data or index files) once my tables get above 1000 regions or so and I restart after a clean shutdown. I have 23 HRS on 23 data nodes and a table with equiprobable inserts. I suspect after a night of heavy writing and a shutdown, more than 50% of regions want to split upon restart, so every HRS is suddenly running splits. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1062&quot; title=&quot;Compactions at (re)start on a large table can overwhelm DFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1062&quot;&gt;&lt;del&gt;HBASE-1062&lt;/del&gt;&lt;/a&gt; helps.&lt;/p&gt;

&lt;p&gt;Missing block in the root region sounds like something different though. Should not be much/any writing there. &lt;/p&gt;</comment>
                            <comment id="12658254" author="stack" created="Sat, 20 Dec 2008 02:29:19 +0000"  >&lt;p&gt;Should we fix for 0.19.0 Andrew?  Seems like basic obstacle to scaling?&lt;/p&gt;</comment>
                            <comment id="12658255" author="apurtell" created="Sat, 20 Dec 2008 02:48:32 +0000"  >&lt;p&gt;How difficult do you think it would be? I think this would touch the store file scanner, maybe memcache, and for the client something like NSREs during scanning would have to handled. &lt;/p&gt;</comment>
                            <comment id="12658333" author="stack" created="Sat, 20 Dec 2008 23:16:36 +0000"  >&lt;p&gt;I took a quick look.  It doesn&apos;t seem too bad.  Client already has notion of getting new scanners as it moves across regions.  Would just need to make it keep last row fetched and then set up new scanner on split region at the row that follows the last one fetched.  Batching of scanner gets makes it a little more awkward.  Server-side, would need to throw NSRE if we try to next on a closed region.  I&apos;ll take a deeper look monday.&lt;/p&gt;</comment>
                            <comment id="12658334" author="stack" created="Sat, 20 Dec 2008 23:17:34 +0000"  >&lt;p&gt;Moving into 0.19 so it at least gets detailed examination.&lt;/p&gt;</comment>
                            <comment id="12659120" author="apurtell" created="Wed, 24 Dec 2008 17:12:19 +0000"  >&lt;p&gt;Just as a data point, today as part of an experiment a cluster that was shut down cleanly with 637 regions and average load of 28 came back up and settled down finally at 929 regions and an average load of 41.&lt;/p&gt;</comment>
                            <comment id="12659703" author="apurtell" created="Tue, 30 Dec 2008 00:06:10 +0000"  >&lt;p&gt;Ran a new experiment. I have two largish tables &amp;#8211; urls and content. Running scanners over and over on content, with 300 second pause in between. No scanners running at all over urls. Started up a crawler pounding, lots of new data. I&apos;m seeing urls split, content is not. Stands to reason both should be. &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Later...&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;After restart content went from ~1100 regions to ~1300 regions. &lt;/p&gt;
</comment>
                            <comment id="12660861" author="apurtell" created="Mon, 5 Jan 2009 19:15:06 +0000"  >&lt;p&gt;A configuration change to dfs.datanode.handler.count from its default of 3 to something much larger has altered overall system behavior on my cluster sufficiently to stop the apparent split suppression by scanning mapreduce jobs that I was seeing. &lt;/p&gt;

&lt;p&gt;The core of this issue &amp;#8211; allowing splits around scanners &amp;#8211; is still an important consideration for scalability IMHO. &lt;/p&gt;

&lt;p&gt;Moving out of 0.19 and marking down to major. &lt;/p&gt;</comment>
                            <comment id="12704829" author="apurtell" created="Thu, 30 Apr 2009 22:48:01 +0000"  >&lt;p&gt;Not critical for 0.20, moved out.&lt;/p&gt;</comment>
                            <comment id="12746281" author="stack" created="Fri, 21 Aug 2009 22:10:13 +0000"  >&lt;p&gt;Does this work now Andrew?  Can we close this?&lt;/p&gt;</comment>
                            <comment id="12747065" author="apurtell" created="Mon, 24 Aug 2009 20:57:45 +0000"  >&lt;p&gt;This is better. I don&apos;t see the same behavior as before. Close and reopen if seen again.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 10 Dec 2008 18:39:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31968</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 17 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hb0v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99048</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>