<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:28:11 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5487/HBASE-5487.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5487] Generic framework for Master-coordinated tasks</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5487</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Need a framework to execute master-coordinated tasks in a fault-tolerant manner. &lt;/p&gt;

&lt;p&gt;Master-coordinated tasks such as online-scheme change and delete-range (deleting region(s) based on start/end key) can make use of this framework.&lt;/p&gt;

&lt;p&gt;The advantages of framework are&lt;br/&gt;
1. Eliminate repeated code in Master, ZooKeeper tracker and Region-server for master-coordinated tasks&lt;br/&gt;
2. Ability to abstract the common functions across Master -&amp;gt; ZK and RS -&amp;gt; ZK&lt;br/&gt;
3. Easy to plugin new master-coordinated tasks without adding code to core components&lt;/p&gt;</description>
                <environment></environment>
        <key id="12544478">HBASE-5487</key>
            <summary>Generic framework for Master-coordinated tasks</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="mubarakseyed">Mubarak Seyed</reporter>
                        <labels>
                    </labels>
                <created>Tue, 28 Feb 2012 20:37:00 +0000</created>
                <updated>Tue, 1 Sep 2015 17:51:58 +0000</updated>
                                            <version>0.94.0</version>
                                                    <component>master</component>
                    <component>regionserver</component>
                    <component>Zookeeper</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>46</watches>
                                                                                                            <comments>
                            <comment id="13218785" author="stack" created="Wed, 29 Feb 2012 00:59:48 +0000"  >&lt;p&gt;I took a look at FATE over in accumulo.  Its some nice generic primitives for running a suite of idempotent operations (even if operation only part completes, if its run again, it should clean up and continue).  There is a notion of locking on a table (so can stop it transiting I suppose; there are read/write locks), a stack for operations (ops are pushed and popped off the stack), operations can respond done, failed, or even w/ a new set of operations to do first (This basic can be used to step through a number of tasks one after the other).  All is persisted up in zk run by the master; if master dies, a new master can pick up the half-done task and finish it.  Clients can watch zk to see if task is done.  There ain&apos;t too much to the fate package; there is fate class itself, an admin, a &apos;store&apos; interface of which there is a zk implementation.  We should for sure take inspiration at least from the work already done.&lt;/p&gt;

&lt;p&gt;Here are the ops they do via fate:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CreateTable(c.user, tableName, timeType, options)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RenameTable(tableId, oldTableName, newTableName)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CloneTable(c.user, srcTableId, tableName, propertiesToSet, propertiesToExclude)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DeleteTable(tableId)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ChangeTableState(tableId, TableOperation.ONLINE)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ChangeTableState(tableId, TableOperation.OFFLINE)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TableRangeOp(MergeInfo.Operation.MERGE, tableId, startRow, endRow)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TableRangeOp(MergeInfo.Operation.DELETE, tableId, startRow, endRow)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BulkImport(tableId, dir, failDir, setTime)), autoCleanup);
          fate.seedTransaction(opid, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TraceRepo&amp;lt;Master&amp;gt;(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CompactRange(tableId, startRow, endRow)), autoCleanup);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;CompactRange is their term for merge.  It takes a key range span, figures the tablets involved and runs the compact/merge.  We want that and then something to do the remove or regions too?&lt;/p&gt;


</comment>
                            <comment id="13245545" author="kturner" created="Tue, 3 Apr 2012 17:59:34 +0000"  >&lt;p&gt;I am an Accumulo developer.  CompactRange is our operation to force a range of tablets(regions) to major compact all of their files into one file.  The TableRangeOp will merge a range of tablets into one tablet.  TableRangeOp can also delete a range of row from a table efficiently.  It inserts splits points at the rows you want to delete, drops the tablets, and then merges whats left.&lt;/p&gt;</comment>
                            <comment id="13245596" author="kturner" created="Tue, 3 Apr 2012 18:56:31 +0000"  >&lt;p&gt;The description of FATE given in this ticket is pretty good.  The following resources may provide a little more info.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://people.apache.org/~kturner/accumulo14_15.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~kturner/accumulo14_15.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/incubator-accumulo-dev/201202.mbox/%3CCAGUtCHpcHTDue-C_2RyDkZm0diW=Zojd7-BzCGsZQDTiDznZqg@mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/incubator-accumulo-dev/201202.mbox/%3CCAGUtCHpcHTDue-C_2RyDkZm0diW=Zojd7-BzCGsZQDTiDznZqg@mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13245672" author="enis" created="Tue, 3 Apr 2012 20:07:22 +0000"  >&lt;p&gt;Keith thanks a lot for the refs, we badly need something like FATE. &lt;br/&gt;
Mubarak, or anybody else plans to work on this?&lt;/p&gt;</comment>
                            <comment id="13245829" author="mubarakseyed" created="Tue, 3 Apr 2012 22:23:58 +0000"  >&lt;p&gt;@Enis&lt;br/&gt;
I am not working on this right now. Thanks.&lt;/p&gt;</comment>
                            <comment id="13250769" author="kturner" created="Tue, 10 Apr 2012 15:57:35 +0000"  >&lt;p&gt;Accumulo 1.3 cannot not survive running our random walk test w/ the agitator (a perl script that kills accumulo processes, it not as devious as Todd&apos;s gremlins).  &lt;/p&gt;

&lt;p&gt;  Random walk + Agitation + Accumulo 1.3 == foobar&lt;/p&gt;

&lt;p&gt;Attempting the above would leave Accumulo in an inconsistent state (like corrupted metadata table) or test clients would die with unexpected exceptions.&lt;/p&gt;

&lt;p&gt;My point is that while developing FATE it was nice to have Random Walk + Agitation to really beat up the FATE framework and the FATE table operations.  We also wrote some new random walk test for 1.4 that were even meaner.&lt;/p&gt;</comment>
                            <comment id="13250770" author="kturner" created="Tue, 10 Apr 2012 15:59:12 +0000"  >&lt;p&gt;To add context to my above comment, Accumulo 1.3 does not have FATE it was introduced in 1.4.&lt;/p&gt;</comment>
                            <comment id="13536252" author="ndimiduk" created="Wed, 19 Dec 2012 18:29:32 +0000"  >&lt;p&gt;I&apos;ll take a stab at this.&lt;/p&gt;</comment>
                            <comment id="13536271" author="apurtell" created="Wed, 19 Dec 2012 18:38:52 +0000"  >&lt;p&gt;Brownie points if coprocessors can use it too.&lt;/p&gt;</comment>
                            <comment id="13536282" author="jmhsieh" created="Wed, 19 Dec 2012 18:44:53 +0000"  >&lt;p&gt;Not sure how this had the noob tag --this sounds like a fairly major undertaking!&lt;/p&gt;</comment>
                            <comment id="13536446" author="ndimiduk" created="Wed, 19 Dec 2012 21:45:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; Any chance you could describe in more detail what you have in mind? Say, 250 words, as part of a dependent JIRA, perhaps? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; We shall see!&lt;/p&gt;</comment>
                            <comment id="13536463" author="apurtell" created="Wed, 19 Dec 2012 21:59:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any chance you could describe in more detail what you have in mind? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Coprocessor &quot;applications&quot; that are running a MasterObserver or MasterEndpoint are very likely going to want to take advantage of a generic framework for master-coordinated tasks, so whatever API there is for this, should be available via MasterServices I&apos;d say.&lt;/p&gt;</comment>
                            <comment id="13537561" author="ndimiduk" created="Fri, 21 Dec 2012 00:37:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://reviews.apache.org/r/8732/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/8732/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&apos;s an initial hack to introduce Fate. I&apos;ll start from here on implementing some Master operations. Bring on the comments &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13537574" author="yuzhihong@gmail.com" created="Fri, 21 Dec 2012 00:53:56 +0000"  >&lt;p&gt;@Nick:&lt;br/&gt;
Please specify hbase for the Groups. This way people would get notified when new comments are made.&lt;/p&gt;

&lt;p&gt;conf/accumulo-site.xml seems to be empty. Did you want to put anything there ?&lt;/p&gt;</comment>
                            <comment id="13537584" author="ndimiduk" created="Fri, 21 Dec 2012 01:00:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Please specify hbase for the Groups.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I intentionally omitted the HBase group because this is not a real patch ready for real review. I simply want feedback from the people directly interested here. It&apos;s really a hack to bold the accumulo code into HMaster; I would -1 it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;conf/accumulo-site.xml seems to be empty. Did you want to put anything there ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oops, that was a local symlink. It will run w/o the site.xml file, it just logs a warning. I&apos;ll address this in my next patch.&lt;/p&gt;</comment>
                            <comment id="13551519" author="enis" created="Fri, 11 Jan 2013 21:05:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt; I guess you won&apos;t be working on this for some time, but wanna chime in the latest status? &lt;/p&gt;

&lt;p&gt;Thinking about the use case, what we want is to ensure that client operations does outlive master  failover. Which is why in accumulo/fate, the state is kept in zk, and the master just provides execution. I think we can achieve the same thing if we add a WAL for master. Again, we have to break up the operation (like create table) into adempotent pieces, and sync the WAL before executing them. On master failover we just have to replay the WAL. Not sure which one would be simpler though.&lt;/p&gt;</comment>
                            <comment id="13551597" author="ndimiduk" created="Fri, 11 Jan 2013 22:28:59 +0000"  >&lt;p&gt;I cannot comment on a WAL implementation vs this FATE stuff. Before I left on holiday, I had an FateManager integrated into HMaster and mostly ported over CreateTable as a FATE repo. The next task is to extract a Fate client and push it out to the client HBaseAdmin. I can make a priority to clean up my pending code and post another review, particularly if someone else is chomping to pick up the FATE PoC.&lt;/p&gt;

&lt;p&gt;Please advise.&lt;/p&gt;</comment>
                            <comment id="13551719" author="enis" created="Sat, 12 Jan 2013 01:09:31 +0000"  >&lt;p&gt;One more benefit for going WAL can be that you can replicate master operations (if you want to) to remote cluster. I am checking the HLog infrastructure right now, it does not look too intrusive to add an hlog for the master. Of course there would be some work for log splitting (or lack thereof), replaying, rolling, and serializing the operation steps as WALEdits. &lt;/p&gt;</comment>
                            <comment id="13553518" author="stack" created="Tue, 15 Jan 2013 05:46:00 +0000"  >&lt;p&gt;FATE would violate one of our &quot;Invariants&quot;, no permanent data in zk: See &lt;a href=&quot;http://hbase.apache.org/book.html#developing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/book.html#developing&lt;/a&gt; 15.10.4.  We could change the &apos;invariant&apos; (smile) or we could do the Enis suggestion of a master WAL.  That seems like a nice idea.  How would we delete items from the WAL when done?  Write a &apos;done&apos; record?  Let a WAL go when all of its tasks are &apos;DONE&apos;?&lt;/p&gt;</comment>
                            <comment id="13554026" author="ndimiduk" created="Tue, 15 Jan 2013 17:26:50 +0000"  >&lt;p&gt;FATE&apos;s approach of serializing the steps along the way is easily replicated via WAL. Write a step to the WAL to initiate, then write the next step, and the next, &amp;amp;c as you go. When the whole thing is complete, write a completion record.&lt;/p&gt;

&lt;p&gt;The WAL approach means a client cannot initiate tasks though, correct?&lt;/p&gt;</comment>
                            <comment id="13554071" author="jesse_yates" created="Tue, 15 Jan 2013 18:13:59 +0000"  >&lt;p&gt;I think using the WAL is not the ideal mechanism; it makes master failover a very heavy-weight operation (requires WAL replay) and it makes it really hard to reason about what changes should be replicated to the target clusters and which shouldn&apos;t (maybe one cluster mirrors prod, but the other is stores 2x history). &lt;/p&gt;

&lt;p&gt;We already require ZK to be up and store an awful amount of state up there to the point where we are hurting if we don&apos;t have it (particularly replication).&lt;/p&gt;

&lt;p&gt;An idea that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ctrezzo&quot; class=&quot;user-hover&quot; rel=&quot;ctrezzo&quot;&gt;Chris Trezzo&lt;/a&gt; just proposed (and I really like) is to move this to a &apos;system level table&apos;. Since we don&apos;t lose data in HBase, we can be sure it survives master failover, but has approaching zero MTTR and maintains our current invariants. Only problem is the added complexity &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13554074" author="sershe" created="Tue, 15 Jan 2013 18:16:50 +0000"  >&lt;p&gt;I&apos;d question the transient-ZK invariant, it seems that if we got rid of it we could do away with lots of complexity (and maybe an entire class of things like META).&lt;br/&gt;
However, if we were to keep it I wonder if hybrid ZK-WAL approach makes sense.&lt;br/&gt;
ZK doesn&apos;t lose data I presume, so in most cases failover is ZK only. If ZK state is wiped, replay WAL.&lt;/p&gt;</comment>
                            <comment id="13554118" author="jmhsieh" created="Tue, 15 Jan 2013 18:42:31 +0000"  >&lt;p&gt;Why not write to a system table, (like META) using the existing wal?  This question came up for snapshot clone/restore recovery, and is a potential solution for maintaining table enable/disable state (which is in zk and violates the rule), and possibly for replication state (also in zk, violating the rule).&lt;/p&gt;</comment>
                            <comment id="13554153" author="devaraj" created="Tue, 15 Jan 2013 18:57:16 +0000"  >&lt;p&gt;Having a system table to store all state that we need to survive across master restarts makes sense to me. That would avoid having the master to rebuild state when it resumes operation (assuming that we don&apos;t store &apos;permanent&apos; state in ZK). But we&apos;d then have two mechanisms to store state - ZK and system-table. We&apos;d need to be careful about what state goes where and all that. &lt;br/&gt;
The other thing that comes to my mind is what&apos;d happen if ZK went down while a cluster is operational. Is that an invariant? Something that must never happen?&lt;/p&gt;</comment>
                            <comment id="13554161" author="sershe" created="Tue, 15 Jan 2013 19:03:30 +0000"  >&lt;p&gt;If we want ZK to always be running then system table is no more reliable than ZK (from the perspective of uptime).&lt;/p&gt;</comment>
                            <comment id="13554176" author="enis" created="Tue, 15 Jan 2013 19:13:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;FATE would violate one of our &quot;Invariants&quot;, no permanent data in zk:&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think it depends on the definition of permanent. FATE keeps the stack of operation states in zk, but when the whole thing finishes, no zk data is left. You can still wipe out zk, if you are in a clean state. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;FATE&apos;s approach of serializing the steps along the way is easily replicated via WAL. Write a step to the WAL to initiate, then write the next step, and the next, &amp;amp;c as you go. When the whole thing is complete, write a completion record.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I think using the WAL is not the ideal mechanism; it makes master failover a very heavy-weight operation&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not sure why it is so heavy weight. It requires to replay only the non-finished steps of the operation of undo the operation, so that we can clean state. As in fate, we can divide every operation (create table, disable, etc) as a sequence of steps, which are idempotent, and undo/redo&apos;able. Before the operation, and before every step master syncs the step to the WAL, after the step is done, master does another sync to record a DONE state for that step. On replay, it just buffers non-finished operations (ignores already finished ones), and undos/redos the steps not finished. &lt;br/&gt;
We can do size based and periodic log rolling, and for every non-finished operation keep the smallest seqNum. When operation is done, we can just release that seqId, so that we can use the same fshlog infrastructure. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;An idea that Chris Trezzo just proposed (and I really like) is to move this to a &apos;system level table&apos;. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I must have missed this. This will still be a WAL I guess, but kept as an hbase table. Considering megastore also keeps its WAL in bigtable, &lt;br/&gt;
I think it is doable.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Why not write to a system table, (like META) using the existing wal? This question came up for snapshot clone/restore recovery, and is a potential solution for maintaining table enable/disable state (which is in zk and violates the rule)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I never liked the table enabled/disabled state, and agreed that it is a violation, as well as a major cause of bugs. I guess, regardless of where we keep it (fshlog or system table), it is clear that we need a WAL. &lt;/p&gt;</comment>
                            <comment id="13554204" author="jmhsieh" created="Tue, 15 Jan 2013 19:33:09 +0000"  >&lt;p&gt;Another argument for having all this state in a system table/meta (and thus in HDFS) and not having permanent state in ZK is that if you are doing something like &quot;backup&quot; of hbase you can do it by just by copying data form HDFS.  Administatively you could use the future hdfs snapshot feature or just copy the files from one cluster to another and have a decent chance of recovering (likely still painful, but has a good chance of success).  Having an extra step of having to copy data from one ZK instance to another sounds more complicated and even more error prone.&lt;/p&gt;</comment>
                            <comment id="13554704" author="sershe" created="Wed, 16 Jan 2013 03:26:02 +0000"  >&lt;p&gt;My main point is that currently two external region states in ZK and a table, plus two complex internal states in server and master, are a root of some non-trivial part of all evil. Especially the nature of ZK state, that comes and goes. Imho we should remove one of them from being actively managed. &lt;br/&gt;
ZK has notifications and seems better suited for locking/atomic updates; w.r.t. availability it has no disadvantage since everything (e.g. locating the root) fails without ZK anyway, even if we do remove state machines from there.&lt;br/&gt;
System tables are more native to HBase and have built-in WAL, plus have advantages for recovery.&lt;/p&gt;

&lt;p&gt;Maybe instead of WAL we can use ZK as universal source of region state (w/o assorted transient nodes e.g. one node per region that is always there, or maybe two if we want to use lock with lease to unassign) and mirror it to system table that is only used for recovery like you describe, or when ZK state disappears? &lt;br/&gt;
Otherwise I think we should just use system table as universal source of region state and get rid of ZK region state.&lt;br/&gt;
With one source of truth master and server logic can probably be dumber.&lt;/p&gt;</comment>
                            <comment id="13556116" author="jmhsieh" created="Thu, 17 Jan 2013 12:23:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;My main point is that currently two external region states in ZK and a table, plus two complex internal states in server and master, are a root of some non-trivial part of all evil. Especially the nature of ZK state, that comes and goes. Imho we should remove one of them from being actively managed. &lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;ZK has notifications and seems better suited for locking/atomic updates; w.r.t. availability it has no disadvantage since everything (e.g. locating the root) fails without ZK anyway, even if we do remove state machines from there.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;System tables are more native to HBase and have built-in WAL, plus have advantages for recovery.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ZK notifications are useful when there are two way communications &amp;#8211; log splitting, region server initiated splits.  I do agree that opens and closes seems more complicated than necessary. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe instead of WAL we can use ZK as universal source of region state (w/o assorted transient nodes e.g. one node per region that is always there, or maybe two if we want to use lock with lease to unassign) and mirror it to system table that is only used for recovery like you describe, or when ZK state disappears? &lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Otherwise I think we should just use system table as universal source of region state and get rid of ZK region state.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;With one source of truth master and server logic can probably be dumber.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Simpler, not dumber. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;The 0.20/0.89 version of the master actually had most things in meta &amp;#8211; and there are definitely some trade-offs with that approach.  In the transition to the 0.90 style master we traded some pain points for new ones.  If we change this again we need to make sure we keep those previous ones in mind to not duplicate the worst of them again. &lt;/p&gt;

&lt;p&gt;Is there any chance we could get a high level design deck/doc that illustrates these processes currently and what looks like after we move to this proposed FATE-like mechanism? Also, what operations would eventually get ported to this mechanism?  I think discussion and an example at the design/rpc comms level would help a whole lot by grounding this conversion in reality and not require diving into the code.  Once we basically agree on design, code reviews would be easier because they&apos;d be focused on the implementation matching the design.&lt;/p&gt;
</comment>
                            <comment id="13556706" author="ndimiduk" created="Thu, 17 Jan 2013 23:03:49 +0000"  >&lt;p&gt;I&apos;ve pushed my FATE WIP to &lt;a href=&quot;https://github.com/ndimiduk/hbase/tree/5487-protobuf-repos&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;github&lt;/a&gt; so you guys can see what a fate repo might look like for us. I appear to have introduced a bug while porting over the logic, but the general ideas is there.&lt;/p&gt;</comment>
                            <comment id="13556708" author="ndimiduk" created="Thu, 17 Jan 2013 23:04:23 +0000"  >&lt;p&gt;Removing myself from assignment as my priorities have shifted.&lt;/p&gt;</comment>
                            <comment id="13556709" author="enis" created="Thu, 17 Jan 2013 23:04:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is there any chance we could get a high level design deck/doc that illustrates these processes currently and what looks like after we move to this proposed FATE-like mechanism? Also, what operations would eventually get ported to this mechanism? I think discussion and an example at the design/rpc comms level would help a whole lot by grounding this conversion in reality and not require diving into the code. Once we basically agree on design, code reviews would be easier because they&apos;d be focused on the implementation matching the design.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed that before doing anything, we should do a design doc, and agree on that. I think what we are discussing here is pre-design discussions. &lt;/p&gt;</comment>
                            <comment id="13556750" author="jmhsieh" created="Thu, 17 Jan 2013 23:36:40 +0000"  >&lt;p&gt;I don&apos;t mean to stymie progress here, but I want to understand how the semantics of this will work in comparison to what exists currently.  Even in these pre-design steps, and since nick has already done some preliminary work, a distilled example in pseudo code or just the rpc/storage interactions would be helpful.  Generally a little bit of effort there is easier than going through a few thousand lines of code...&lt;/p&gt;</comment>
                            <comment id="13559338" author="sershe" created="Tue, 22 Jan 2013 03:15:11 +0000"  >&lt;p&gt;I have some different priorities currently, but if I have time I will try to do short write-up on ZK + backup table based approach. Maybe towards the eow...&lt;/p&gt;</comment>
                            <comment id="13601950" author="sershe" created="Thu, 14 Mar 2013 01:48:14 +0000"  >&lt;p&gt;Just checking; is this issue still relevant? I am particularly interested in all-encompassing solution for region management that would allow you to read AM and not go insane (I was just reading/debugging 0.94 AM for some time, that&apos;s why I remembered). I have a sketch of an idea, should I write it up?&lt;/p&gt;</comment>
                            <comment id="13601991" author="enis" created="Thu, 14 Mar 2013 02:54:43 +0000"  >&lt;p&gt;Yes, please write is up. Although AM related things were not initially in the main focus. &lt;/p&gt;</comment>
                            <comment id="13606744" author="jmhsieh" created="Tue, 19 Mar 2013 20:28:38 +0000"  >&lt;p&gt;+1 Please do a write up &amp;#8211; the standard stuff &amp;#8211; what problems exist (op1 and op2 clashes, etc), the proposed fix, and how it would fix it.&lt;/p&gt;</comment>
                            <comment id="13607093" author="sershe" created="Wed, 20 Mar 2013 00:25:28 +0000"  >&lt;p&gt;Attaching v0 write-up for region management.&lt;/p&gt;</comment>
                            <comment id="13614410" author="sershe" created="Tue, 26 Mar 2013 18:36:18 +0000"  >&lt;p&gt;Any opinion? Thanks. I think the same model should be used for all tables too, but it&apos;s less necessary at this time...&lt;/p&gt;</comment>
                            <comment id="13614484" author="jmhsieh" created="Tue, 26 Mar 2013 19:56:15 +0000"  >&lt;p&gt;Thanks for writing this up.  I read the first two sections and haven&apos;t spent time reading the details of the others yet.  (already have a bunch of quetsions).&lt;/p&gt;

&lt;p&gt;So the only problem is that the assignment manager and ssh is difficult to reason about?  Is the assignment manager the only &quot;master coordinated&quot; task in scope? &lt;/p&gt;

&lt;p&gt;I think there are more problems than that that we should enumerate:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Instead of asserting it is not clear if table (+region) locks scale, let&apos;s find out.&lt;/li&gt;
	&lt;li&gt;Master operations and processes can clash and we should understand where we need concurrency control.  (I&apos;m working on a table &amp;#8211; here&apos;s an draft distilled version &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, there exists an overly detailed version that I&apos;ll share once i get it fixed)&lt;/li&gt;
	&lt;li&gt;Should there be a notion of queuing operations?  (locking, or an actual queue) Should these operations be generically logged so they can complete if a master goes down in the middle? (ex: master goes down during a &quot;move&quot; operation after the close but before the open on the new rs).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The &quot;design principles&quot; is actually more of a proposed design.  &lt;/p&gt;

&lt;p&gt;Design principles::region record&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;how do we deal with operations where we need &quot;locks&quot; on multiple region because we are reading or modifying multiple regions &amp;#8211; e.g. splits, merges, snapshots?  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; had suggested in another jira making a the meta row per table, or maybe part of the solution is using the multi-row single meta region transaction.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What are alternatives?  why this approach vs others? &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;  &lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0AiVCAt6zRttFdDRVaG56RnpQZlNNZUNsRVJsSXM3YlE&amp;amp;usp=sharing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/spreadsheet/ccc?key=0AiVCAt6zRttFdDRVaG56RnpQZlNNZUNsRVJsSXM3YlE&amp;amp;usp=sharing&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13614612" author="jxiang" created="Tue, 26 Mar 2013 22:12:13 +0000"  >&lt;p&gt;Where do you think the new information will be, META table?&lt;/p&gt;</comment>
                            <comment id="13615901" author="sershe" created="Wed, 27 Mar 2013 23:56:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is the assignment manager the only &quot;master coordinated&quot; task in scope?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Only for the current document version... tables could be added.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Instead of asserting it is not clear if table (+region) locks scale, let&apos;s find out.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hmm... that would require implementing region locks, and having a very large cluster. I am talking more about unacceptable blocking of user operations, and management of expiring locks in presense of real-life failures.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Master operations and processes can clash and we should understand where we need concurrency control. (I&apos;m working on a table &#8211; here&apos;s an draft distilled version &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, there exists an overly detailed version that I&apos;ll share once i get it fixed)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Comments below.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Should there be a notion of queuing operations? (locking, or an actual queue) Should these operations be generically logged so they can complete if a master goes down in the middle? (ex: master goes down during a &quot;move&quot; operation after the close but before the open on the new rs).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You mean like WAL for operations?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The &quot;design principles&quot; is actually more of a proposed design.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, sorry, wanted to split it into two sections but never did. Will rename.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;how do we deal with operations where we need &quot;locks&quot; on multiple region because we are reading or modifying multiple regions &#8211; e.g. splits, merges, snapshots? Matteo Bertozzi had suggested in another jira making a the meta row per table, or maybe part of the solution is using the multi-row single meta region transaction.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Depends on where we store it, but yeah these have to be transactional. Last section (very short &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) suggests using ZK, which already supports that.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;What are alternatives? why this approach vs others?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I can expand the doc... the implicitly mentioned existing alternatives are locks, which I would argue scale less and are harder to manage; or transaction approach that is currently used (although not unified), for example via transient transaction nodes.&lt;/p&gt;

&lt;p&gt;Actually, one alternative approach I saw used for such things is to simplify concurrency of operations/etc. with actor-like model, where master has logical cluster state and previously saved target state, and periodically (often) takes an epic lock, looks at them quickly, and based on what it is doing, outputs new target cluster state and a list of physical things to do Then it releases epic lock, and the new target state is saved, and operations performed.&lt;br/&gt;
That way all state-management code becomes simple, because it runs in one place with no concurrency, and recovery just has to compare real cluster state with destination state. &lt;br/&gt;
But this will require thinking about this differently. &lt;br/&gt;
Also usually that would mean RSes won&apos;t be able to initiate operations (like split) - they will have to go thru master (which I would argue is ok).&lt;br/&gt;
Also it&apos;s not clear whether this will become too much of a bottleneck.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Where do you think the new information will be, META table?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It seems to me that ZK would be better (see last section), but META is also an option.&lt;/p&gt;

&lt;p&gt;From the spreadsheet:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Enabling and disabling table operations should be blocked when  any of these simple region operations are in progress&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not clear why (logically).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;move&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Move is close and open, doesn&apos;t require consistency, right?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Regionserver Processes ... However, the individual operations must  maintain the table integrity property.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not clear what this means for snapshots.&lt;/p&gt;
</comment>
                            <comment id="13616129" author="jmhsieh" created="Thu, 28 Mar 2013 08:10:11 +0000"  >&lt;p&gt;To do a major overhaul, we need something stronger than &quot;the code is hard to read&quot;.  I agree that it is hard to follow (see: &lt;a href=&quot;http://people.apache.org/~jmhsieh/hbase/120905-hbase-assignment.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://people.apache.org/~jmhsieh/hbase/120905-hbase-assignment.pdf&lt;/a&gt;) but it seems to be basically working which is a pretty strong argument.  Let&apos;s compare and point out what is wrong/broken in the current implementation and how the new design won&apos;t have those problems.  &lt;/p&gt;

&lt;p&gt;The spreadsheet link is my first step to enumerating semantics and distilling the set of possible problems and things that are being guarded from races.  Any major-overhaul solution should make sure that these operations, when issued concurrently, interact according to a sane set of semantics in the face of failures.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Only for the current document version... tables could be added&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So I buy open/close as a region operation.  split/merge are multi region operations &amp;#8211; is there enough state to recover from a failure?&lt;/p&gt;

&lt;p&gt;So alter table is a region operation? Why isn&apos;t it in the state machine? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Hmm... that would require implementing region locks, and having a very large cluster. I am talking more about unacceptable blocking of user operations, and management of expiring locks in presense of real-life failures.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Implementing region locks is too far &amp;#8211; I&apos;m asking for some back of the napkin discussionb.  I think we need  some measurements how much throughput we can get in ZK or with a ZK-lock implementation and compare his with # rs of watchers * # of regions * number of ops..&lt;/p&gt;

&lt;p&gt;The current regions-in-transition (RIT) code basically assumes that an absent znode is either closed or opened.  RIT znodes are present when the region is in the inbetween states (opening, closing, &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You mean like WAL for operations?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, we could call it an &quot;intent&quot; log.  It would have info so that a promoted backup master can look in one place and complete an operation started by the downed original master.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;... Also usually that would mean RSes won&apos;t be able to initiate operations (like split) - they will have to go thru master (which I would argue is ok).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I know I&apos;ve suggested something like this before.  Currently the RS initiates a split, and does the region open/meta changes.  If there are errors, at some point the master side detects a timeout.  An alternative would have splits initiated RS on the rs but have the master do some kind of atomic changes to meta and region state for the 3 involved regions (parent, daughter a and daughter b).  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Depends on where we store it, but yeah these have to be transactional. Last section (very short ) suggests using ZK, which already supports that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We need to be careful about ZK &amp;#8211; since it is a network connection also, exceptions could be failures or timeouts (which succeed but wan&apos;t able to ack).  If we can describe the properties (durable vs erasable) and assumptions (if the wipeable ZK is source of truth, how do we make sure the version state is recoverable without time travel?)&lt;/p&gt;</comment>
                            <comment id="13616130" author="jmhsieh" created="Thu, 28 Mar 2013 08:12:42 +0000"  >&lt;p&gt;I&apos;ll deal with the spreadsheet comments related by putting int somewhere that comments can be easily dropped into.  &lt;/p&gt;</comment>
                            <comment id="13616885" author="enis" created="Fri, 29 Mar 2013 00:18:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yeah, we could call it an &quot;intent&quot; log. It would have info so that a promoted backup master can look in one place and complete an operation started by the downed original master.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This was what I proposed in an earlier comment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5487?focusedCommentId=13551519&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13551519&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-5487?focusedCommentId=13551519&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13551519&lt;/a&gt;&lt;br/&gt;
What we need is a transactional authoritative, fault tolerant, and durable source for ground truth about the cluster state, and execution state. Whether we can do it using ZK or a master WAL, or a system table (using an implicit WAL), we will have to figure it out. &lt;/p&gt;</comment>
                            <comment id="13616971" author="sershe" created="Fri, 29 Mar 2013 01:44:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; Will reply in details later/tomorrow, one clarification - the justification was not just &quot;code is hard to understand&quot;, but &quot;code is hard to reason about AND we want to expand it to support a bunch of features.&quot;. I think the whole idea of general framework, whatever it is, is to have some unified model of things and way of doing these manipulations and expanding their set, that is not patchwork.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; I think finding one is the least of our problems &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; How to use it to store/manage state is the question.&lt;/p&gt;</comment>
                            <comment id="13617600" author="sershe" created="Fri, 29 Mar 2013 18:39:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any major-overhaul solution should make sure that these operations, when issued concurrently, interact according to a sane set of semantics in the face of failures.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is another (although not orthogonal) question.&lt;br/&gt;
I am looking for a sane way to define and enforce arbitrary semantics first. Then sane semantics can be enforced on top of that &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
For example, in &quot;actor-ish&quot; model below would make it easy to write simple code; persistent state would make sure there&apos;s definite state at any time, and all crucial transitions are atomic, so semantics would be easy to enforce as long as the code can handle a failed transition/recovery. Locks also make this simple, although locks have other problems imho.&lt;br/&gt;
Although we can go both ways, if we define sane semantics it would be easy to see how convenient they are to implement in a particular model.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So I buy open/close as a region operation. split/merge are multi region operations &#8211; is there enough state to recover from a failure?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There should be. Can you elaborate?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So alter table is a region operation? Why isn&apos;t it in the state machine?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Alter table is currently the operation that involves region operation, namely open/close. Open-close are in the state machine &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; As for tables, I am not sure state machine is the best model for table state, there isn&apos;t that much going on with the table that is properly an exclusive state.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Implementing region locks is too far &#8211; I&apos;m asking for some back of the napkin discussionb.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If a server holds a lock for a region for time Tlock during each day, and number of regions is N probability of some region lock (or table read-only lock) being held at any given time is (1-(1-(Tlock/Tday))^N), if I am writing this correctly. For 5 seconds of locking per day per region, for 10000 regions (not unreasonable for a large table/cluster) we will be holding some lock about 44% of the time for region operations.&lt;br/&gt;
Calculating the probability of any lock being in recovery (server went down with a lock less than recovery time ago) can also be done, but numbers for some parameters (how often do servers go down?) will be very speculative...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think we need some measurements how much throughput we can get in ZK or with a ZK-lock implementation and compare his with # rs of watchers * # of regions * number of ops...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Will there be many watchers/ops? You only watch and do ops when you acquire the lock, so unless region operations are very frequent... &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The current regions-in-transition (RIT) code basically assumes that an absent znode is either closed or opened. RIT znodes are present when the region is in the inbetween states (opening, closing,&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think &quot;either closed or opened&quot; is good enough &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Also, RITs don&apos;t cover all scenarios and things like table ops don&apos;t use them at all.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I know I&apos;ve suggested something like this before. Currently the RS initiates a split, and does the region open/meta changes. If there are errors, at some point the master side detects a timeout. An alternative would have splits initiated RS on the rs but have the master do some kind of atomic changes to meta and region state for the 3 involved regions (parent, daughter a and daughter b).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, although in other models (locks, persistent state) that is not required. Also if meta is cache for clients and not source of truth meta changes can still be on the server; I assume by meta you mean global state, wherever that is?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We need to be careful about ZK &#8211; since it is a network connection also, exceptions could be failures or timeouts (which succeed but wan&apos;t able to ack). If we can describe the properties (durable vs erasable) and assumptions (if the wipeable ZK is source of truth, how do we make sure the version state is recoverable without time travel?)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The former applies to any distributed state; as for the latter - I was thinking of ZK+&quot;WAL&quot; if we intend to keep ZK wipeable.&lt;/p&gt;</comment>
                            <comment id="13706491" author="stack" created="Thu, 11 Jul 2013 23:58:51 +0000"  >&lt;p&gt;Was just looking at a test failure issue.   A move operation was failing because the region we were asking it to move had not fully opened yet so move just failed:&lt;/p&gt;

&lt;p&gt;2013-07-11 09:35:00,489 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;RpcServer.handler=4,port=55346&amp;#93;&lt;/span&gt; master.AssignmentManager(2277): Attempting to unassign ephemeral,,1373535299969.ab63d8b7c5339b4a61fdd70e8cb8993a. but it is already in transition (OPEN, force=false)&lt;/p&gt;

&lt;p&gt;Client needs means of asking what happened to its move.&lt;/p&gt;

&lt;p&gt;Client should also be able to say I want the move to succeed..... in the above case then, the move op should be retried.&lt;/p&gt;

&lt;p&gt;Need a queue of outstanding ops.  Need to be able to query it for where is my op? (like fedex where is my package)&lt;/p&gt;</comment>
                            <comment id="13706520" author="enis" created="Fri, 12 Jul 2013 00:38:53 +0000"  >&lt;p&gt;The requirements are pretty much clear. We need a persistent set of ops running in the cluster. Every op is defined as a stack of steps which are undo/redoable and adempotent. The client submits an op, and gets an request_id, with which it can query the state of the operation later. The ops and steps are handled by state machines + stacked execution. The state is persisted via zk or a WAL log for master or an HBase table (which is just a higher level log). &lt;/p&gt;</comment>
                            <comment id="13706528" author="stack" created="Fri, 12 Jul 2013 00:46:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;The requirements are pretty much clear...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Smile. Yeah.  I wonder what is &apos;next&apos;?&lt;/p&gt;</comment>
                            <comment id="13706532" author="enis" created="Fri, 12 Jul 2013 00:58:09 +0000"  >&lt;p&gt;Once dust is settled for 0.96, I think this is a good candidate for 0.98 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;</comment>
                            <comment id="13706542" author="sershe" created="Fri, 12 Jul 2013 01:08:30 +0000"  >&lt;p&gt;Do these requirements cover the test failure just mentioned... or any other that come from collision of multiple ops on the same region?&lt;/p&gt;</comment>
                            <comment id="13706616" author="stack" created="Fri, 12 Jul 2013 03:45:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; Agree&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; No.  I think the move &apos;failing&apos; is not too bad; it is something we can work on perhaps having two types of move... a move &quot;recommendation&quot; and then a &quot;required&quot; move.  The offense in my scenario above is that the move failed silently.&lt;/p&gt;</comment>
                            <comment id="13786870" author="sershe" created="Sat, 5 Oct 2013 02:07:29 +0000"  >&lt;p&gt;I would like to resurrect this issue (and rename it). It seems that every fix to assignmentmanager introduces 1-3 more maps or lists around it, makes it even more impossible to comprehend and may add more bugs.&lt;/p&gt;

&lt;p&gt;We have talked a little bit here, and agreed on 2 key points.&lt;br/&gt;
1) Region state should be managed in one permanent place with one state machine; no separate and/or transient state machines, no operation-based state machines.&lt;br/&gt;
2) New assignment manager should be easily testable by simulating sequences in events.&lt;br/&gt;
I think my doc above is still reasonably good approximation, but of course we might need to discuss flesh out the details.&lt;/p&gt;

&lt;p&gt;Ahem. I said new assignment manager, that is because I would like to rename this jira &quot;rewrite assignment manager&quot;.&lt;br/&gt;
Wdyt? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13786872" author="sershe" created="Sat, 5 Oct 2013 02:08:07 +0000"  >&lt;p&gt;*of events&lt;/p&gt;</comment>
                            <comment id="13787446" author="stack" created="Sun, 6 Oct 2013 01:23:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;We have talked a little bit here, and agreed on 2 key points.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What about Enis&apos;s &apos;requirements&apos;.  We agree on his list?  That makes 3 key points.  I think add a fourth point where we rehearse what is wrong w/ the current system (Jon&apos;s suggestion) as it will help ensure we don&apos;t repeat the mistakes of the past.&lt;/p&gt;

&lt;p&gt;Make a subtask &apos;New Assignment Manager&apos;?  Or make this a subtask of a new issue called &apos;New Assignment Manager&apos;.  An issue named so will be easier to find than this one.  Also, others are interested in this effort (@honghua and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xieliang007&quot; class=&quot;user-hover&quot; rel=&quot;xieliang007&quot;&gt;Liang Xie&lt;/a&gt;) and it&apos;ll catch their attention.&lt;/p&gt;

&lt;p&gt;I think it too big a change to be done for the pending 0.98.  Lets not rush it.  It could even land post hbase 1.0 if 0.98 is to become 1.0.&lt;/p&gt;

&lt;p&gt;On the design doc.,&lt;/p&gt;

&lt;p&gt;+ Doc., needs author and date.  I would expect a section situating the document &amp;#8211; context &amp;#8211; that at least referred to the current &apos;design&apos; &amp;#8211; se &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2485&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-2485&lt;/a&gt; (it has &apos;state&apos; machine that looks like this one)&lt;br/&gt;
+ The problem section is too short (state kept in multiple places and all have to agree...); need more full list so can be sure proposal addresses them all&lt;br/&gt;
+ How is the proposal different from what we currently have?  I see us tying regionstate to table state.  That is new.  But the rest, where we have a record and it is atomically changed looks like our RegionState in Master memory?  There is an increasing &apos;version&apos; which should help ensure a &apos;direction&apos; for change which should help.&lt;br/&gt;
+ A single atomically mutable record of the regionstate is well and good but how then to get the cluster to align w/ what this record says?  For example, table record says it is disabled.  It has 10k regions.  How do we get the regions to agree w/ the Table record which says it is disabled?  We can send the closes but how we sure the close happened on all 10k regions?&lt;br/&gt;
+ I don&apos;t get this bit &quot;This	record	is	the	only	source	of	truth	about	the	region,	and is	never	removed	while	the	&lt;br/&gt;
region	is relevant.This	simplifies	current	situation	where	ZK	state,	master	state	and	&lt;br/&gt;
META	state	can	all	conflict	in	various	special	ways...&quot;  Its fine having a source of truth but ain&apos;t the hard part bring the system along?  (meta edits, clients, etc.).&lt;/p&gt;

&lt;p&gt;Experience has zk as messy to reason with.  It is also an indirection having RS and M go to zk to do &apos;state&apos;.&lt;/p&gt;

&lt;p&gt;Thank sfor writing this up Sergey&lt;/p&gt;</comment>
                            <comment id="13787493" author="stack" created="Sun, 6 Oct 2013 05:00:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; What you think of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; doc?&lt;/p&gt;</comment>
                            <comment id="13788303" author="jxiang" created="Mon, 7 Oct 2013 16:58:04 +0000"  >&lt;p&gt;The doc is very good. I like the Problem and Design Principles sections. The region state machine can be enhanced.  We should have a single source of truth which is scalable and performs well. I think it could be the master (in memory).  All actions (including split/merge) should be started and managed by the master. &lt;/p&gt;</comment>
                            <comment id="13788317" author="nkeywal" created="Mon, 7 Oct 2013 17:23:17 +0000"  >&lt;p&gt;3 comments:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I wonder if we should use &quot;something&quot; that would allow us to test all the possible states. At least, we should make this really testable, without needing to set up a zk, a set of rs and so on.&lt;/li&gt;
	&lt;li&gt;We should question the master based architecture. How does it work for the MapR implementation for example? Why the assignment manager is not in the region server holding meta? This would save one distributed state for example.&lt;/li&gt;
	&lt;li&gt;I really really really ( &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) think that we need to put performances as a requirement for any implementation. For example, something like: on a cluster with 5 racks of 20 regionserver each, with 200 regions per RS,, the assignment will be completed in 1s if we lose one rack. I saw a reference to async ZK in the doc, it&apos;s great, because the performances are 10 times better.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks for writing the doc Sergey.&lt;/p&gt;
</comment>
                            <comment id="13788324" author="stack" created="Mon, 7 Oct 2013 17:29:55 +0000"  >&lt;p&gt;+ Agree that master should become a lib that any regionserver can run.&lt;br/&gt;
+ Agree testable but would like to point out that it is possible to put up our current AM in a standalone mode &amp;#8211; it just takes mockery (smile)&lt;br/&gt;
+ Agree on perf.  Helps MTTR.&lt;/p&gt;</comment>
                            <comment id="13788331" author="jxiang" created="Mon, 7 Oct 2013 17:35:34 +0000"  >&lt;p&gt;Agree that master should hold the meta region.  It should hold other system table regions as well.&lt;/p&gt;</comment>
                            <comment id="13788334" author="jesse_yates" created="Mon, 7 Oct 2013 17:38:44 +0000"  >&lt;p&gt;-1 that it should hold other system tables. We know META isn&apos;t going to span more than a region, but it would be completely reasonable for other system tables to be larger (i.e. statistics). Maybe worth considering a single-region flag for certain tables to identify that they can never be split and can support single region transactions.&lt;/p&gt;</comment>
                            <comment id="13788342" author="jxiang" created="Mon, 7 Oct 2013 17:46:30 +0000"  >&lt;p&gt;That&apos;s right.  For big system tables which are not required for the system to start/run, it can be assigned to somewhere else. The master doesn&apos;t have to hold all system table regions.&lt;/p&gt;</comment>
                            <comment id="13788415" author="devaraj" created="Mon, 7 Oct 2013 18:37:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;We should have a single source of truth which is scalable and performs well. I think it could be the master (in memory). All actions (including split/merge) should be started and managed by the master.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree with this. We have done this in other components - HDFS, MapReduce, Yarn, etc.... Depending on ZK for the state management brings complexity. I think we should use ZK for only ephemeral stuff and not for storing state there. In that regard, using ZK for discovering lost RSs is fine, but not for storing the region states. I like the idea of the Master WAL to take care of master crashes.&lt;/p&gt;</comment>
                            <comment id="13788518" author="sershe" created="Mon, 7 Oct 2013 20:16:59 +0000"  >&lt;p&gt;we still need  a reliable store (ZK, system table, or master WAL). It seems ZK is the most scalable and best suited for the task. In perfect world we would have ZK library that we could host and have a quorum of masters running Paxos/ZK. But we don&apos;t have that...&lt;/p&gt;</comment>
                            <comment id="13789470" author="eclark" created="Tue, 8 Oct 2013 18:19:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;we still need a reliable store&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;HBase is a reliable store.  We should be using it as such for current state.&lt;/p&gt;

&lt;p&gt;If we co-locate the master process with meta, then the master noticing state changes is as simples as loading a co-processor that hooks mutations.  It also means that when master wants to look up current state there&apos;s no rpc overhead.  Simply target the hregion. This allows us to reduce the number of copies of state.  No longer will we need a local hash map + what&apos;s in zk, + what&apos;s in meta.  &lt;/p&gt;

&lt;p&gt;I think Jimmy&apos;s correct we should use zk for ephemeral only.  Everything else should be in our systems.  &lt;/p&gt;</comment>
                            <comment id="13789493" author="sershe" created="Tue, 8 Oct 2013 18:32:23 +0000"  >&lt;p&gt;Please let&apos;s not use coprocessors for mainline functionality... also, if we store state in system table that is hosted by master, then we don&apos;t need ZK at all, we should get rid of it.&lt;br/&gt;
The only disadvantages from using ZK that I see are the absence getKeyBefore/After API (easy to fix by having ephemeral META table for clients to query), and having extra moving part. If we don&apos;t get rid of ZK we don&apos;t alleviate the latter so I think we should either use it for &quot;everything&quot; or not at all... I would prefer to use it for everything.&lt;br/&gt;
As far as I see, ZK is more reliable than HBase RS or master, has built-in replication with faster recovery, is probably more scalable than reading from single RS, and has better model for atomic state changes. Probably has better tolerance for stuff like network partitioning too. We could do master WAL and all that stuff but I don&apos;t see a compelling reason to do this when we have a bunch of Apache code that is already written to solve all of these problems. &lt;br/&gt;
What is the reason to not use ZK? What is the advantage of system table, or disadvantage of ZK?&lt;/p&gt;</comment>
                            <comment id="13789576" author="eclark" created="Tue, 8 Oct 2013 19:22:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;Please let&apos;s not use coprocessors for mainline functionality&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We already do.  I don&apos;t see anything wrong with making HBase more modular.  If there are pain points with using co-processors that cause you to say no, then we should fix those.  Not just ignore them.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;also, if we store state in system table that is hosted by master, then we don&apos;t need ZK at all, we should get rid of it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We don&apos;t have ephemeral node capability at all.  And we need it for the bootstrap problem.  It allows clients to point at a relatively small number of nodes to discover the whole cluster.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As far as I see, ZK is more reliable than HBase RS or master&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Our master is only complex because of our use of zk to hold and mutate state.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;has built-in replication with faster recovery&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With the meta/system wal I think we can be within an order of magnitude.&lt;/p&gt;</comment>
                            <comment id="13789598" author="sershe" created="Tue, 8 Oct 2013 19:45:37 +0000"  >&lt;p&gt;Wrt coprocs - that is bad imho, that is not the kind of modular that we want. Core parts of the system should depend on well-defined interfaces, not a generic extension points. Imho, the litmus test for coproc, as a plugin interface, is - can you run HBase without it? If yes, then it&apos;s ok to be a coproc (e.g. accesscontrol). Otherwise we should have proper interfaces that have some meaning to the caller.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Our master is only complex because of our use of zk to hold and mutate state.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That is not due to ZK as such, that is due to multi-state-machine reconciliation model and truth in multiple places that it requires.&lt;br/&gt;
System table can have exact same problem of state in the table + state in memory, question is how you split and manage state between them, storage substrate doesn&apos;t matter as much. If truth was in ZK and nowhere else that wouldn&apos;t be a problem, same way as with system table.&lt;br/&gt;
Also, by reliable I meant that ZK is multiple nodes with built-in master recovery by design, whereas with master you need at least HA, and still it&apos;s probably worse than ZK in case of failure.&lt;br/&gt;
There are also other things that I mentioned.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With the meta/system wal I think we can be within an order of magnitude.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So, why would we write a bunch of new code to get &quot;within an order of magnitude&quot;? I don&apos;t see an advantage, or ZK disadvantage that you mention compared to multiple advantages of ZK.&lt;br/&gt;
Esp. if we cannot totally get rid of it, so we&apos;ll have an extra service regardless.&lt;/p&gt;</comment>
                            <comment id="13789612" author="sershe" created="Tue, 8 Oct 2013 19:59:24 +0000"  >&lt;p&gt;Btw I agree that the main point is to get rid of the complexity you mention (and in the doc I only mention storage mechanism in ZK in one paragraph in the end), so the storage mechanism choice is almost orthogonal.&lt;br/&gt;
But as far as it is concerned, it seems an obvious choice to use ZK for me ATM. I may not know something about ZK (or system tables?), but so far the pattern is that meta recovery is a big deal even without bugs, and with ZK we barely ever have any problems. &lt;/p&gt;</comment>
                            <comment id="13789642" author="eclark" created="Tue, 8 Oct 2013 20:22:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;Wrt coprocs - that is bad imho, that is not the kind of modular that we want.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m not tied to it being a co-proc.  But it does illustrate the idea that it can be done by watching mutations as they come into the normal hregion call stack.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That is not due to ZK as such, that is due to multi-state-machine reconciliation model and truth in multiple places that it requires.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In part it&apos;s due to getting zk messages out of order, and getting them delayed. Those pains are due in no small part because zk&apos;s client is single threaded.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;System table can have exact same problem of state in the table + state in memory, question is how you split and manage state between them, storage substrate doesn&apos;t matter as much.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But you only have the one state if you have master inside of the region server hosting meta.  There&apos;s no need to have a map of assignment if meta is actually just a function call away.  Also  The same is not true at all if you want to put state into zk.  Then you need a local cache if you want to make this performant at all (That&apos;s how we got to the current state).  Putting state into zk necessitates a split brain problem.  There&apos;s what the master see and what the outside worlds sees.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, why would we write a bunch of new code to get &quot;within an order of magnitude&quot;?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That code is already there, and in use.  We fail over meta right now in 240ms.  I was commenting on what you were saying that zk fails over faster. And that&apos;s true but for meta we&apos;ve narrowed that gap significantly. So I don&apos;t think that ZK has that much of an advantage.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t see an advantage, or ZK disadvantage that you mention compared to multiple advantages of ZK&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We&apos;ve tried putting state into zk.  That failed.  I really don&apos;t want to put a whole bunch of new code into hbase that does almost exactly the same thing as we currently have.  It&apos;s going to fail.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;so the storage mechanism choice is almost orthogonal.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For me it&apos;s not just about the storage.  It&apos;s about co-locating storage with the master means that these split brain problems are much rarer.&lt;/p&gt;</comment>
                            <comment id="13789874" author="sershe" created="Wed, 9 Oct 2013 00:32:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;We&apos;ve tried putting state into zk.  That failed.  I really don&apos;t want to put a whole bunch of new code into hbase that does almost exactly the same thing as we currently have.  It&apos;s going to fail.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As I said I don&apos;t think that is true. Our problem is not state being in ZK; it is that the state is in multiple places in ZK itself for different parts of the same region&apos;s state, plus some state in master to reconcile these, plus some state that is not in ZK but only in master, plus also meta.&lt;br/&gt;
I.e. not the split between ZK and master but split logical state within both and between them.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Then you need a local cache if you want to make this performant at all (That&apos;s how we got to the current state).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Our current state is not local cache, it&apos;s a bunch of actual state...&lt;/p&gt;

&lt;p&gt;I am not yet sure how bad ZK-master split brain problem will be if ZK has entire truth, let me think about it.&lt;br/&gt;
When you say no split-brain inside master, do you mean master will host the meta and do all reads and writes to meta with no local intermediate state in memory?&lt;/p&gt;</comment>
                            <comment id="13789997" author="stack" created="Wed, 9 Oct 2013 03:49:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;Our problem is not state being in ZK; it is that the state is in multiple places in ZK itself for different parts of the same region&apos;s state, plus some state in master to reconcile these, plus some state that is not in ZK but only in master, plus also meta.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Master is the Actor.  Having it go across a network to get/set the &apos;state&apos; in a service that is non-transactional wasn&apos;t our smartest move.&lt;/p&gt;

&lt;p&gt;Regionservers currently report state via ZK.  Master reads it from ZK.  Would be better if RS just reported directly to RS.&lt;/p&gt;</comment>
                            <comment id="13790109" author="devaraj" created="Wed, 9 Oct 2013 07:11:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;we still need a reliable store (ZK, system table, or master WAL). It seems ZK is the most scalable and best suited for the task&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt;, not ZK, IMHO. Let&apos;s use one of our internal storages rather than external system for storing the region state. I am all for removing ZK altogether from HBase. One less distributed system to worry about. One less component to manage. We already have heartbeats from RSs to master, and region open/close RPCs from master to the RSs. I think we have enough communication already in place between the master and RSs to deal with region states.... We also have chores in the master that tries to take some actions based on assignment timeouts... &lt;/p&gt;

&lt;p&gt;Would this model work (conceptually). It&apos;s late night here; please pardon me if there are glaring issues &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Please bear with me &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;All region state manipulation operations are initiated by the master and they act upon the meta region. We have extra columns to store the state of the region etc in the meta table. The initial rows are created by the master and the state of the regions are UNASSIGNED. This is not new - we already do this but IIRC we don&apos;t store the state of the region. Some state transitions happen through method executions and some of those method executions are RPCs from the master to some regionserver. I think that the states would be more granular here (to prevent potential replay/repetitions of large operations). I am wondering whether it makes sense to update the meta table from the various regionservers on the region state changes or go via the master.. But maybe the master doesn&apos;t need to be a bottleneck if possible. A regionserver could first update the meta table, and then just notify the master that a certain transition was done; the master could initiate the next transition (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; comment about coprocessor can probably be made to apply in this context). Only when a state change is recorded in meta, the operation is considered successful.&lt;/p&gt;

&lt;p&gt;Also, there is a chore (probably enhance catalog-janitor) in the master that periodically goes over the meta table and restarts (along with some diagnostics; probing regionservers in question etc.) failed/stuck state transitions. This chore runs once as soon as the master is started and the meta region is assigned to take care of transitions that were started in the previous life of the master and which are now waiting for some action from the master. For example, if the state was OPENING for a certain region, and the master crashed, the master would send a openRegion RPC to the region assignee upon restart. The region assignee would have been recorded as a column in the row for the region by the previous master.&lt;/p&gt;

&lt;p&gt;I think we should also save the operations that was initiated by the client on the master (either in WAL or in some system table) so that the master doesn&apos;t lose track of those and can execute them in the face of crashes &amp;amp; restarts. For example, if the user had sent a &apos;split region&apos; operation and the master crashed.&lt;/p&gt;</comment>
                            <comment id="13790151" author="nkeywal" created="Wed, 9 Oct 2013 08:08:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;zk vs. non zk.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ZK is used in HDFS HA, no? So any way we have it in our architecture. Then using it for permanent data is another discussion (stuff like &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-1147&quot; title=&quot;Add support for local sessions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-1147&quot;&gt;&lt;del&gt;ZOOKEEPER-1147&lt;/del&gt;&lt;/a&gt; makes it interesting.&lt;br/&gt;
I would personally prefer to remove the master rather than adding functions to it. Saying that there are some specific threads in the region servers holding .meta. is acceptable imho. &lt;/p&gt;</comment>
                            <comment id="13790172" author="jmhsieh" created="Wed, 9 Oct 2013 09:10:24 +0000"  >&lt;p&gt;ZK is used for the selection of the primary nn (via the failure controllers) but I believe the journal nodes (that do the durable consensus logging) does not use ZK at all. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tlipcon&quot; class=&quot;user-hover&quot; rel=&quot;tlipcon&quot;&gt;Todd Lipcon&lt;/a&gt;or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=atm&quot; class=&quot;user-hover&quot; rel=&quot;atm&quot;&gt;Aaron T. Myers&lt;/a&gt; can confirm.&lt;/p&gt;</comment>
                            <comment id="13790433" author="devaraj" created="Wed, 9 Oct 2013 15:04:04 +0000"  >&lt;p&gt;Removing the separate master daemon is fine by me, Nicolas. However, we still need someone to do various operations (servicing user requests and other janitorial tasks). Long back we were discussing that a random region server (elected via zk) could perform the master role.&lt;/p&gt;</comment>
                            <comment id="13790442" author="jxiang" created="Wed, 9 Oct 2013 15:09:42 +0000"  >&lt;p&gt;I prefer not to use ZK since it&apos;s kind of the root cause of uncertainty: has the master/region server got/processed the event? has the znode hijacked since master/region server changes its mind?&lt;/p&gt;

&lt;p&gt;We should store the state in meta table which is cached in the memory. &lt;/p&gt;

&lt;p&gt;Whether to use coprocessor it is not a big concern to me.  If we don&apos;t use coprocessor, I prefer to use the master as the proxy to do all meta table updates. Otherwise, we need to listen to something for updates.&lt;/p&gt;

&lt;p&gt;We should not have another janitor/chore. If an action is failed, it must be because of something unrecoverable by itself, not because of a bug in our code.  It should stay failed until the issue is resolved.&lt;/p&gt;

&lt;p&gt;We need to have something like FATE in accumulo to queue/retry actions taking several steps like split/merge/move.&lt;/p&gt;

&lt;p&gt;It is a nice-to-have to keep a history of region state transition.&lt;/p&gt;</comment>
                            <comment id="13790451" author="nkeywal" created="Wed, 9 Oct 2013 15:17:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;However, we still need someone to do various operations (servicing user requests and other janitorial tasks). &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, I agree. Balance is a good example. Less say that I&apos;m more comfortable w/ something that lowers the role of the master than the opposite.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I prefer not to use ZK &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When you say this, Jimmy, do you mean &quot;no ZK in HBase at all&quot;, or &quot;No ZK for permanent data&quot;, or &quot;No ZK at all for assignment&quot;? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We should store the state in meta table which is cached in the memory. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m fine with that (if we can make it work &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/p&gt;</comment>
                            <comment id="13790460" author="jxiang" created="Wed, 9 Oct 2013 15:21:45 +0000"  >&lt;p&gt;Nicolas, I mean no ZK for assignment.&lt;/p&gt;</comment>
                            <comment id="13791007" author="sershe" created="Wed, 9 Oct 2013 23:40:23 +0000"  >&lt;p&gt;Big response to not-responded-to recent comments.&lt;br/&gt;
Let me update the doc, EOW-ish probably depending on the number of bugs surfacing &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;&lt;br/&gt;
Let&apos;s keep discussion and doc here and branch tasks out for rewrites.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;+ The problem section is too short (state kept in multiple places and all have to agree...); need more full list so can be sure proposal addresses them all&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What level of detail do you have in mind? It&apos;s not a bug fix, so I cannot really say &quot;merge races with snapshot&quot;, or something like that; that could also be arguably resolved by another 100k patch to existing AM &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;+ How is the proposal different from what we currently have? I see us tying regionstate to table state. That is new. But the rest, where we have a record and it is atomically changed looks like our RegionState in Master memory? There is an increasing &apos;version&apos; which should help ensure a &apos;direction&apos; for change which should help.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;See the design principles (and below discussion &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;). We are trying to avoid multiple flavors of split-brain state.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Its fine having a source of truth but ain&apos;t the hard part bring the system along? (meta edits, clients, etc.).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Experience has zk as messy to reason with. It is also an indirection having RS and M go to zk to do &apos;state&apos;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think ZK got a bad reputation not on its own merit, but on how we use it.&lt;br/&gt;
I can see that problems exist but IMHO advantages outweigh the disadvantages compared to system table.&lt;br/&gt;
Co-located system table, I am not so sure, but so far there&apos;s no even high-level design for this (for example - do all splits have to go thru master/system table now? how does it recover? etc.).&lt;br/&gt;
Perhaps we should abstract an async persistence mechanism sufficiently and then decide. Whether it would be ZK+notifications, or system table, or memory + wal, or colocated system table, or what.&lt;br/&gt;
The problem is that the usage inside master of that interface would depend on perf characteristics.&lt;br/&gt;
Anyway, we can work out the state transitions/concurrency/recovery without tying 100% to particular store.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;+ Agree that master should become a lib that any regionserver can run.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That sounds possible.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;At least, we should make this really testable, without needing to set up a zk, a set of rs and so on.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1, see my comment above. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I really really really ( ) think that we need to put performances as a requirement for any implementation. For example, something like: on a cluster with 5 racks of 20 regionserver each, with 200 regions per RS,, the assignment will be completed in 1s if we lose one rack. I saw a reference to async ZK in the doc, it&apos;s great, because the performances are 10 times better.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We can measure and improve, but I am not really sure about what exact numbers will be, at this stage (we don&apos;t even know what storage is).&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A regionserver could first update the meta table, and then just notify the master that a certain transition was done; the master could initiate the next transition (Elliott Clark comment about coprocessor can probably be made to apply in this context). Only when a state change is recorded in meta, the operation is considered successful.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Split, for example, requires several changes to meta. Will master be able to see them together from the hook? If master is collocated in the same RS with meta, it should be small overhead to have master RPC.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, there is a chore (probably enhance catalog-janitor) in the master that periodically goes over the meta table and restarts (along with some diagnostics; probing regionservers in question etc.) failed/stuck state transitions. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1 on that. Transition states can indicate the start ts, and master will know when they started.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think we should also save the operations that was initiated by the client on the master (either in WAL or in some system table) so that the master doesn&apos;t lose track of those and can execute them in the face of crashes &amp;amp; restarts. For example, if the user had sent a &apos;split region&apos; operation and the master crashed&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, &quot;disable table&quot; or &quot;move region&quot; are a good example. Probably we&apos;d need ZK/system table/WAL for ongoing logical operations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We should not have another janitor/chore. If an action is failed, it must be because of something unrecoverable by itself, not because of a bug in our code. It should stay failed until the issue is resolved.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think the failures meant are things like RS went away, is slow or buggy, so OPENING got stuck - someone needs to pick it up over timeout.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We need to have something like FATE in accumulo to queue/retry actions taking several steps like split/merge/move.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We basically need something that allows atomic state changes. HBase or ZK or mem+wal fit the bill &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                            <comment id="13791012" author="sershe" created="Wed, 9 Oct 2013 23:47:36 +0000"  >&lt;p&gt;btw, any input on actor model? &lt;br/&gt;
Things queue up operations/notifications (&quot;ops&quot;) for master; &quot;AM&quot; runs on timer or when queue is non-empty, having as inputs, cluster state (incl. ongoing internal actions it ordered before e.g. OPENING state for a region) plus new ops from queue, on a single thread; generates new actions (not physically doing anything e,g, talking to RS); the ops state and cluster state is persisted; then actions are executed on different threads (e.g. messages sent to RS-es, etc.), and &quot;AM&quot; runs again, or sleeps for some time if ops queue is empty.&lt;/p&gt;

&lt;p&gt;That is a different model, not sure if it scales for large clusters.&lt;/p&gt;</comment>
                            <comment id="13791516" author="fenghh" created="Thu, 10 Oct 2013 14:11:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Master is the Actor. Having it go across a network to get/set the &apos;state&apos; in a service that is non-transactional wasn&apos;t our smartest move.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Regionservers currently report state via ZK. Master reads it from ZK. Would be better if RS just reported directly to RS.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; Yes, this is exactly what I proposed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9726&quot; title=&quot;Proposal for a new master design for assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9726&quot;&gt;&lt;del&gt;HBASE-9726&lt;/del&gt;&lt;/a&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am wondering whether it makes sense to update the meta table from the various regionservers on the region state changes or go via the master.. But maybe the master doesn&apos;t need to be a bottleneck if possible. A regionserver could first update the meta table, and then just notify the master that a certain transition was done; the master could initiate the next transition&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt; It would be better to let master updates the meta table rather than let various regionservers do it. Master being the single actor and truth-maintainer can avoid many tricky bugs/problems. And for frequent state changes to the meta table, the regionserver serving the (state) meta table would be sooner the bottleneck than master which issues the update requests, so whether it doesn&apos;t matter the update requests are from the master or from various regionservers.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I prefer not to use ZK since it&apos;s kind of the root cause of uncertainty: has the master/region server got/processed the event? has the znode hijacked since master/region server changes its mind?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We should store the state in meta table which is cached in the memory.&lt;br/&gt;
Whether to use coprocessor it is not a big concern to me. If we don&apos;t use coprocessor, I prefer to use the master as the proxy to do all meta table updates. Otherwise, we need to listen to something for updates.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; Agree. IMO ZK alone is not the root cause of uncertainty, the current usage pattern of ZK is the root cause, the pattern that regionserver updates state in ZK and master listens to the ZK and updates states in its local memory accordingly exhibits too many tricky scenarios/bugs due to ZK watch is one-time(which can result in missed state transition) and the notification/process is asyncronous(which can lead to delayed/non-update-to-date state in master memory). And by replacing ZK with meta table, we also need to discard this &apos;RS updates - master listen&apos; pattern since meta table inherently lack listen-notify mechanism&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think ZK got a bad reputation not on its own merit, but on how we use it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I can see that problems exist but IMHO advantages outweigh the disadvantages compared to system table.&lt;br/&gt;
Co-located system table, I am not so sure, but so far there&apos;s no even high-level design for this (for example - do all splits have to go thru master/system table now? how does it recover? etc.).&lt;br/&gt;
Perhaps we should abstract an async persistence mechanism sufficiently and then decide. Whether it would be ZK+notifications, or system table, or memory + wal, or colocated system table, or what.&lt;br/&gt;
The problem is that the usage inside master of that interface would depend on perf characteristics.&lt;br/&gt;
Anyway, we can work out the state transitions/concurrency/recovery without tying 100% to particular store.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; Agree on &quot;ZK got a bad reputation not on its own merit, but on how we use it.&quot;, especially if you mean currently master relies on ZK watch/notification to maintain/update master&apos;s in-memory region state. IMO this is almost the biggest root cause of current assignment design. If we just uses ZK the same way as using meta table to storing states, it makes no that big difference to store the states in ZK or meta table, right(except using meta table can have much better performance for restart of a big cluster with large amount of regions)? But using ZK&apos;s update/listen pattern does make the difference.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;btw, any input on actor model? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Things queue up operations/notifications (&quot;ops&quot;) for master; &quot;AM&quot; runs on timer or when queue is non-empty, having as inputs, cluster state (incl. ongoing internal actions it ordered before e.g. OPENING state for a region) plus new ops from queue, on a single thread; generates new actions (not physically doing anything e,g, talking to RS); the ops state and cluster state is persisted; then actions are executed on different threads (e.g. messages sent to RS-es, etc.), and &quot;AM&quot; runs again, or sleeps for some time if ops queue is empty.&lt;br/&gt;
That is a different model, not sure if it scales for large clusters.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; &quot;operations/notifications&quot; means RS responses action progress to master? Master is the single point to update the state &quot;truth&quot;(to meta table) and RS doesn&apos;t know where the states are stored and doesn&apos;t access them directly, right? I think a communication/storage diagram can help a lot for an overall clear understanding here&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13791520" author="fenghh" created="Thu, 10 Oct 2013 14:16:56 +0000"  >&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9726&quot; title=&quot;Proposal for a new master design for assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9726&quot;&gt;&lt;del&gt;HBASE-9726&lt;/del&gt;&lt;/a&gt; is closed as duplicated with this one, I copied the proposal of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9726&quot; title=&quot;Proposal for a new master design for assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9726&quot;&gt;&lt;del&gt;HBASE-9726&lt;/del&gt;&lt;/a&gt; here for discussion/reference:&lt;/p&gt;

&lt;p&gt;Current assignment process (also split process) relies on ZK for the communication between master and regionserver. This pattern has two drawbacks: &lt;br/&gt;
  1. For cluster with big number of regions(say, 10K-100K regions), ZK becomes the bottleneck for cluster restart since the assignment/split status/progress is stored in ZK due to ZK&apos;s limited write throughput &lt;br/&gt;
  2. Since ZK&apos;s watch is one-time and the event notification/process is asynchronous, there is no guarantee for master(the watcher) to be notified of the up-to-date status/progress in time, thereby master relies on idempotence for its correctness, which makes the logic/code very hard to understand/maintain &lt;/p&gt;

&lt;p&gt;A new assignment design proposal is as below: &lt;br/&gt;
  1. Assignment/split status/progress is stored in a system table(say &apos;assignTable&apos;) as meta table rather than ZK to improve the write throughput, hence to improve the proformance of restart for cluster with large number of regions. &lt;br/&gt;
  2. The communication pattern for assignment/split is changed this way: master talks directly with regionserver(master issues assign request to regionserver, regionserver responses the assign progress to master) and records the status/progress of each assignment/split in the &apos;assignTable&apos;, in case of master failure, new active master reads the &apos;assignTable&apos; to rebuilds the knowledge of the ongoing assignmeng/split tasks and continues from that knowledge. (regionserver doesn&apos;t write to the &apos;assignTable&apos;) &lt;/p&gt;</comment>
                            <comment id="13793086" author="sershe" created="Fri, 11 Oct 2013 22:15:45 +0000"  >&lt;p&gt;I think it&apos;s the approach discussed above.&lt;/p&gt;

&lt;p&gt;I will update the doc on monday, I think I&apos;m sold on collocated system table. &lt;br/&gt;
Initially we can just run an RS that runs master library and only hosts &quot;hardcoded&quot; system regions as master.&lt;br/&gt;
Then probably any RS (with caveats) can host the master regions and act as master, so recovery can become much easier.&lt;/p&gt;</comment>
                            <comment id="13793208" author="atm" created="Sat, 12 Oct 2013 02:02:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;ZK is used for the selection of the primary nn (via the failure controllers) but I believe the journal nodes (that do the durable consensus logging) does not use ZK at all. Todd Lipcon or Aaron T. Myers can confirm.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I can confirm this. The QJM in the NN uses its own (heavily ZK-inspired) consensus protocol, but does not rely on ZK itself. The only thing HDFS currently uses ZK for is for the leader election of the active NN, as Jon says here.&lt;/p&gt;</comment>
                            <comment id="13793436" author="jxiang" created="Sat, 12 Oct 2013 18:34:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;, to the uncertainty due to ZK, I don&apos;t think it is because the way how we use it.  It is more because ZK doesn&apos;t support continuous events.  You have to set the watch again after each event callback.  The problem is that after an event is triggered, when we try to get the data, the data could be changed again so an event is missed that will cause state jump.&lt;/p&gt;

&lt;p&gt;Currently, we do have a region state machine.  However, the machine is not strict due to the ZK thing.  We could jump over some state, which make the state transition machine can&apos;t be strictly enforced.  If we go without ZK, we can have a strict state machine to follow. That will make things much predictable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt;, to the janitor, I think we don&apos;t need it.  Currently, we have a timeout monitor.  But it is disabled and will be removed soon I think.  Without the monitor, ITBLL with CM runs very well. With 0.96 tip, I tried to run ITBLL with CM with aggressive region moving, and it is perfectly fine. If a RS is gone, SSH should handle it properly and assign regions.  If there is a janitor, it will compete with SSH in this case, which probably does more harm than good.&lt;/p&gt;

&lt;p&gt;To make some RS to serve the role of master, besides we can have meta on it, we can have some (not all, of course, to make &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jesse_yates&quot; class=&quot;user-hover&quot; rel=&quot;jesse_yates&quot;&gt;Jesse Yates&lt;/a&gt; happy &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) system tables on it too. This way, we can support level region assignments, i.e. we can open some regions before the rest, if these regions can be assigned to the master RS, or we can open on this master RS at first, then move away later after system is fully started. This applies to some special regions only for sure.&lt;/p&gt;

&lt;p&gt;Now, we bundle two import modules (master + meta) in one RS. It is critical to make sure it has light load, not die too often (even better, not die at all). So I think we should move other regions out of the RS once it&apos;s promoted to be the master one.&lt;/p&gt;

&lt;p&gt;I think we should allow only a list of RS with good hardware to be master, if not all RS nodes have decent/same hardware.&lt;/p&gt;</comment>
                            <comment id="13793881" author="fenghh" created="Mon, 14 Oct 2013 02:17:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;to the uncertainty due to ZK, I don&apos;t think it is because the way how we use it. It is more because ZK doesn&apos;t support continuous events. You have to set the watch again after each event callback. The problem is that after an event is triggered, when we try to get the data, the data could be changed again so an event is missed that will cause state jump.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree. &apos;one-time watch&apos; and &apos;asynchronous event notification&apos; are the root cause of current AM problem ( I mentioned in above comment, you can find it&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ). And when I said &apos;because the way we use it&apos;, I meant we use ZK&apos;s watch/event mechanism: A process(RS) updates ZK, and B process(master) gets notified the update via watch event. If we use ZK just as a reliable storage, just the way of using meta table, it makes no difference we use meta table or ZK (except performance difference)&lt;br/&gt;
In the theme of using meta table, we adopt another communication pattern for tasks(assign/split/merge): master requests RS to do something(and master stores the task progress/state to meta table), RS responses master of its progress periodically, master changes the task progress in both memory and meta table... ---under this theme we can use ZK to replace meta table, and avoid previous state transition miss problem as well, since we don&apos;t use ZK&apos;s watch/event mechanism, just using it as a reliable storage. right?&lt;br/&gt;
Just clarify, I think we share the same understanding of this problem, you can check my above comments &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13794232" author="jxiang" created="Mon, 14 Oct 2013 15:57:54 +0000"  >&lt;p&gt;Good. I think we are on the same page. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;just using it as a reliable storage.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We probably won&apos;t use ZK as a pure storage. Meta table + cache is a good alternative.&lt;/p&gt;</comment>
                            <comment id="13794304" author="sershe" created="Mon, 14 Oct 2013 17:42:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; by janitor, I mean not timeout monitor, but something picking up timeouts of non-master ops like open.&lt;br/&gt;
It&apos;s a rare case and probably never happens in int tests, but there can be a case where RS is taking too long to open.&lt;/p&gt;</comment>
                            <comment id="13794318" author="jxiang" created="Mon, 14 Oct 2013 17:53:48 +0000"  >&lt;p&gt;I see.&lt;/p&gt;</comment>
                            <comment id="13794792" author="sershe" created="Tue, 15 Oct 2013 02:17:22 +0000"  >&lt;p&gt;Ok, it&apos;s harder than I thought, I don&apos;t think I will be done today... but I think I have a clear picture now that covers the above feedback, so I am trying to cover all the failover scenarios and state conflicts.&lt;/p&gt;</comment>
                            <comment id="13794862" author="ecn" created="Tue, 15 Oct 2013 04:12:30 +0000"  >&lt;p&gt;Accumulo does manage tablet (region) assignment tracking through the metadata table, and further, uses a distributed state machine to scale up a little beyond a single master node. I have been meaning to write it up, but I&apos;ve not had a chance.&lt;/p&gt;

&lt;p&gt;I&apos;ve not kept up with every HBase improvement, so I don&apos;t know if it is pertinent... the accumulo metadata table is typically spread out over 50 - 100% of the available tablet (region) servers.&lt;/p&gt;

&lt;p&gt;Still, the metadata table, and especially the root table(t), is subject to hot-spotting on large map/reduce jobs where hundreds (or thousands) of clients are learning tablet locations at the same time.  Block caching is important, but at some point massive numbers of simultaneous RPC requests to a single node cause delays, or even timeouts and failures.&lt;/p&gt;

&lt;p&gt;But using accumulo to store accumulo state has scaled well.&lt;/p&gt;

&lt;p&gt;Accumulo has 2 frameworks for master tasks:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;master general state processing: a table should be online, assignments are recorded and servers repeatedly informed&lt;/li&gt;
	&lt;li&gt;FATE processing, where multi-stage operations are saved, executed and progress is re-recorded&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The first is general maintenance: keeping the system running.  Tablets are assigned, unassigned and in-general balanced.&lt;/p&gt;

&lt;p&gt;The second allows for temporal deviance: tablets are taken offline for a merge, for example.  The step-by-step allocation of resources and state are walked, each step recording progress in zookeeper.&lt;/p&gt;
</comment>
                            <comment id="13794892" author="stack" created="Tue, 15 Oct 2013 05:18:55 +0000"  >&lt;p&gt;Thanks for the helpful input &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13794915" author="jmhsieh" created="Tue, 15 Oct 2013 05:52:49 +0000"  >&lt;p&gt;FYI, I&apos;ve been looking at our support cases, and have been thinking and writing up a clean slate design for a master redesign with the problems we&apos;ve faced in the field in mind. I focus a bit more on invariants necessary in the different states, state transitions with master interactions, extensibility of the model, and on the recovery strategy.  It basically takes a pessimistic view of the world and if I had to summarize its spirit, I&apos;d call it the &quot;hbck-all-the-time&quot;.master.&lt;/p&gt;

&lt;p&gt;It is currently durable storage agnostic but requires atomic CAS operations (single row or single znode should be sufficient). When I re-read this thread it could use either of the implementation details described here (zk vs meta, etc).  It sounds like being based in hbase is preferred so a little more thought is going in that direction.   I&apos;m working currently on examples of how to extend it for new features currently (like fast write recovery aka distributed log replay) and proving to myself that it would be immune from problems we&apos;ve encountered before like double assignments, conflicting concurrent operations  (especially during recovery), and regions stuck in transitions in the face of failures, hangs or juliet pauses.&lt;/p&gt;

&lt;p&gt;I read Sergey&apos;s doc after my first cut and while there are some similarities it deviates in other places.  (I definitely want more on the error recovery and error prevention mechanics). My hope is to share it sometime this later week so that folks can read, discuss and compare the different designs presented at the upcoming dev meeting.  Before any jirae are file for implementation the design should also consider things like upgrades, compatibility and performance.&lt;/p&gt;

&lt;p&gt;I&apos;m also hoping I&apos;ll have time to take a look at the accumulo master&apos;s design as well for the discussion.  &lt;/p&gt;</comment>
                            <comment id="13795680" author="sershe" created="Tue, 15 Oct 2013 21:40:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; I am writing out very detailed operation and failover descriptions right now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13795737" author="jmhsieh" created="Tue, 15 Oct 2013 22:27:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; Looking forward to it!  &lt;/p&gt;</comment>
                            <comment id="13796362" author="enis" created="Wed, 16 Oct 2013 02:59:33 +0000"  >&lt;p&gt;I also started a document some time ago, but never got to finish it to the level of details I would like. However, I think we can agree on the design goals section which I augmented from the discussion so far:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Robust implementation&lt;/li&gt;
	&lt;li&gt;Compressive test coverage by mocking server and region assignment states (unit testable without MiniCluster and CM stuff)&lt;/li&gt;
	&lt;li&gt;Bulk region operations&lt;/li&gt;
	&lt;li&gt;Region operations should be isolated from server operations (AM vs SSH, log splitting), and table operations (disabling / disabled table, schema changes, etc) and cluster shutdown. AM and SSH should NEVER know about table state (disable/disabling). Server liveness checks can only be done as an optimization (servers can fail after the check is done)&lt;/li&gt;
	&lt;li&gt;There should be one source of truth&lt;/li&gt;
	&lt;li&gt;Should be compatible with master failover, and concurrent region operations(split, RS failover, balancer, etc)&lt;/li&gt;
	&lt;li&gt;AM should guarantee that a region can be hosted by a single region server at any given time&lt;/li&gt;
	&lt;li&gt;AM should be understandable by simple human beings like myself&lt;/li&gt;
	&lt;li&gt;Actions for AM should be logged (possibly separately). We would like to be able to construct the history for the regions from logs or some persisted state.&lt;/li&gt;
	&lt;li&gt;Assignment should be performant and parallelizable. We should target handling millions of regions and thousands of servers. A single region assignment should complete under 1 sec. (1PB data with 1 GB regions  = 1M regions)&lt;/li&gt;
	&lt;li&gt;No master abort when a region&#8217;s state cannot be determined. This results in support cases where master cannot start, and without master things become even worse. We should &#8220;quarantine&#8221; the regions if needed absolutely.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13796374" author="sershe" created="Wed, 16 Oct 2013 03:37:12 +0000"  >&lt;p&gt;attaching preliminary version... still need to iron out all operation details, where step-by-step operation starts many things may be wrong, but it should be good until then.&lt;/p&gt;

&lt;p&gt;Sorry Mac craps out when printing this as PDF due to mix of page orientations, let me try to find some better format for final version...&lt;/p&gt;</comment>
                            <comment id="13796384" author="stack" created="Wed, 16 Oct 2013 03:52:59 +0000"  >&lt;p&gt;@enis List makes for pretty good set of requirements.  We used to talk 100k regions but folks are long past that now so we are behind the curve (Flurry are &amp;gt;250k IIRC) and we may want to tend away from a few large regions and more toward many small regions if we can get AM to perform (advantages: smaller compression runs, easier to free up WALs, etc)&lt;/p&gt;</comment>
                            <comment id="13796530" author="nkeywal" created="Wed, 16 Oct 2013 08:08:20 +0000"  >&lt;p&gt;+1 for Enis&apos; requirements list &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;br/&gt;
I tend to think that AM and meta should be collocated.&lt;/p&gt;</comment>
                            <comment id="13796855" author="ndimiduk" created="Wed, 16 Oct 2013 15:01:27 +0000"  >&lt;p&gt;I&apos;m also a fan of Enis&apos;s list, particularly &quot;AM should be understandable by simple human beings like myself.&quot;&lt;/p&gt;

&lt;p&gt;The observation I&apos;ll add here is that AM and meta don&apos;t necessarily need to be collocated. What is necessary is that AM maintain a strongly consistent view of the world, at least from what I understand about the current design. That requirement can be relaxed iff there&apos;s an explicitly distributed state management system. Such a system is probably composed out of idempotent operations over CRDTs.&lt;/p&gt;

&lt;p&gt;I also question the wisdom of moving away from ZK for management of active cluster state, primarily because in our current architecture, that component is completely out of band of data operations. Meaning, the activities which put stress on the configuration consensus bits are different from the operations that put stress on a data provider. (Yes, data activity results in region relocation, but that&apos;s a maintenance task, not direct involvement.) Moving to dependency on collocation unnecessarily conflates those two aspects of the system.&lt;/p&gt;

&lt;p&gt;If the issues with Zookeeper originate from implementation details, why not fix implementation rather than look to a new architecture? For instance, the CoreOS folk have a little something called &lt;a href=&quot;https://github.com/coreos/etcd#etcd&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;etcd&lt;/a&gt;. Raft specifically may not provide the correct kind of available consensus we need; the idea is to examine both the baby and the bathwater.&lt;/p&gt;</comment>
                            <comment id="13796905" author="nkeywal" created="Wed, 16 Oct 2013 15:38:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;AM and meta don&apos;t necessarily need to be collocated&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If there are separated, you double the failure probability, as you need both AM and .META. to work. Moreover, speaking to .meta. becomes a distributed problem, while its less the case when they are collocated (only less because of HDFS).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;moving away from ZK for management &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I believe we will need it to determine who is the AM lead. I don&apos;t really know about storing in zookeeper vs. meta. As Jimmy said using zookeeper to do rpc calls seems wrong however.&lt;/p&gt;

&lt;p&gt;I guess this can be decided later. For the requirements, I don&apos;t have anything to add to Enis&apos; list.&lt;/p&gt;</comment>
                            <comment id="13796982" author="ram_krish" created="Wed, 16 Oct 2013 16:58:13 +0000"  >&lt;p&gt;Started going through this document.  With my experience with AM definitely the number of states we have and the dependency on ZK callback makes things bit difficult to understand and track and the state of truth is spread across.&lt;br/&gt;
In the doc, for the create table scenario there are cases where the Create table failure on master abort will result in a table creation that has lesser number of regions actually specified by the clients in the split.&lt;br/&gt;
The master failover part is another critical area as how we collect the alive and dead RS list and the list of Regions that were partially in either opening/closing and splitting. It is this failiure condition where we end up in lot of hidden areas.  &lt;br/&gt;
Will read the document and share the ideas if any.  &lt;/p&gt;</comment>
                            <comment id="13796985" author="ram_krish" created="Wed, 16 Oct 2013 17:00:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5583&quot; title=&quot;Master restart on create table with splitkeys does not recreate table with all the splitkey regions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5583&quot;&gt;&lt;del&gt;HBASE-5583&lt;/del&gt;&lt;/a&gt; is one such JIRA that handles create table failure cases.  &lt;/p&gt;</comment>
                            <comment id="13797043" author="sershe" created="Wed, 16 Oct 2013 17:44:34 +0000"  >&lt;p&gt;I don&apos;t think it can happen on create. Until all regions are moved to Closed state after being created (atomically via multi-row tx), table won&apos;t leave Creating state. If there&apos;s failover all regions are erased and created from scratch. Create table is rare enough for that to work.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; Wrt req list, mostly agree, however:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Bulk region operations&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you please elaborate? Is it the same as modifying several regions&apos; state under multi-row lock?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Region operations should be isolated from &lt;span class=&quot;error&quot;&gt;&amp;#91;snip&amp;#93;&lt;/span&gt; table operations (disabling / disabled table, schema changes, etc) and cluster shutdown. AM &lt;span class=&quot;error&quot;&gt;&amp;#91;snip&amp;#93;&lt;/span&gt; should NEVER know about table state (disable/disabling). &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Strongly disagree with this. If we are doing bunch of balancing and user disables a table at the same time, we have to handle it.&lt;br/&gt;
If user tries to force-assign regions of a table that is halfway thru create, we have to handle this. &lt;br/&gt;
For alter, we need to reopen regions, which will have to work w/splits and merges (it&apos;s covered in my doc).&lt;br/&gt;
For what purpose do you want to isolate them?&lt;br/&gt;
AM should not know about details e.g. schema logic, but it should know about logistics.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;No master abort when a region&#8217;s state cannot be determined. This results in support cases where master cannot start, and without master things become even worse. We should &#8220;quarantine&#8221; the regions if needed absolutely.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That is dangerous. IIRC in my spec I only put master abort if somebody changes table state under master; but in general, if region is in unknown state it&apos;s better to make admin act, than to just silently &quot;disappear&quot; part of data - that can lead to wrong results.&lt;br/&gt;
Perhaps table needs to be quaranteened then.&lt;/p&gt;</comment>
                            <comment id="13797368" author="sershe" created="Wed, 16 Oct 2013 22:30:04 +0000"  >&lt;p&gt;One more update from discussion here:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;we currently have many operations that cannot be monitored other than by side effects (create table), or at all. We need good way for user to wait for operations. Given that we send request to master, and many operations can recover from master failure, we cannot use simple async API with request and async response (at least not on the lowest level - client library can hide master failover and provide that API). The lowest-level master API should involve some sort of persistent operation cookie, so that you could still wait for operation after failover.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13797380" author="stack" created="Wed, 16 Oct 2013 22:39:21 +0000"  >&lt;p&gt;Are we conflating functionality here (going by last comment above by Sergey)?  There is AM and then there is another facility that uses AM to run sequences of steps to achieve an end (e.g. enable table)?   Or is the notion that a revamped AM would do all?  The long-running (enable a table w/ 1M regions) and short-term (assign region)?  If it is to do both, I suggest we call the new facility GOD.&lt;/p&gt;</comment>
                            <comment id="13797395" author="sershe" created="Wed, 16 Oct 2013 23:02:58 +0000"  >&lt;p&gt;You&apos;d need some way to connect these &quot;end&quot; with what AM is doing, so AM will have to support operations attached to its actions even if there&apos;s separate operation management. &lt;br/&gt;
Moreover, you&apos;d find out that these are not &quot;steps&quot;, they are state goals.&lt;br/&gt;
For example, if you are disabling table, you want to close regions. So in case of separate operation manager, you might create tasks to close all regions. But what if some server fails? Now some of your regions are already closed. Separate operations to close region might fail now, but the goal is achieved. If I start disabling table and then kill all RS-es, the table is now disabled &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; But all operations would fail.&lt;br/&gt;
State goals fit much more naturally in AM than &quot;steps&quot;. I want to avoid steps as much as possible.&lt;/p&gt;

&lt;p&gt;Stateful (as in, having separate state) multi-step operations are also hard to coordinate. In the above example, during recovery, you don&apos;t want to reopen region if the table is disabling, but you don&apos;t know until it&apos;s actually disabled if the table disable is an external operation. &lt;/p&gt;</comment>
                            <comment id="13797409" author="enis" created="Wed, 16 Oct 2013 23:15:56 +0000"  >&lt;p&gt;I think as a mental exercise to validate the new design, we should think about the cases for the following issues opened recently so that we can ensure that these classes of problems are eliminated: &lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9724&quot; title=&quot;Failed region split is not handled correctly by AM&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9724&quot;&gt;&lt;del&gt;HBASE-9724&lt;/del&gt;&lt;/a&gt; Failed region split is not handled correctly by AM&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9721&quot; title=&quot;RegionServer should not accept regionOpen RPC intended for another(previous) server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9721&quot;&gt;&lt;del&gt;HBASE-9721&lt;/del&gt;&lt;/a&gt; meta assignment did not timeout&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9696&quot; title=&quot;Master recovery ignores online merge znode&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9696&quot;&gt;&lt;del&gt;HBASE-9696&lt;/del&gt;&lt;/a&gt; Master recovery ignores online merge znode&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9777&quot; title=&quot;Two consecutive RS crashes could lead to their SSH stepping on each other&amp;#39;s toes and cause master abort&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9777&quot;&gt;&lt;del&gt;HBASE-9777&lt;/del&gt;&lt;/a&gt; Two consecutive RS crashes could lead to their SSH stepping on each other&apos;s toes and cause master abort&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9773&quot; title=&quot;Master aborted when hbck asked the master to assign a region that was already online&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9773&quot;&gt;&lt;del&gt;HBASE-9773&lt;/del&gt;&lt;/a&gt; Master aborted when hbck asked the master to assign a region that was already online&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9525&quot; title=&quot;&amp;quot;Move&amp;quot; region right after a region split is dangerous&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9525&quot;&gt;&lt;del&gt;HBASE-9525&lt;/del&gt;&lt;/a&gt; &quot;Move&quot; region right after a region split is dangerous&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9514&quot; title=&quot;Prevent region from assigning before log splitting is done&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9514&quot;&gt;&lt;del&gt;HBASE-9514&lt;/del&gt;&lt;/a&gt; Prevent region from assigning before log splitting is done&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9480&quot; title=&quot;Regions are unexpectedly made offline in certain failure conditions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9480&quot;&gt;&lt;del&gt;HBASE-9480&lt;/del&gt;&lt;/a&gt; Regions are unexpectedly made offline in certain failure conditions&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9387&quot; title=&quot;Region could get lost during assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9387&quot;&gt;&lt;del&gt;HBASE-9387&lt;/del&gt;&lt;/a&gt; Region could get lost during assignment&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;Can you please elaborate? Is it the same as modifying several regions&apos; state under multi-row lock?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Bulk loading requirement is there, so that we do multiple operations in parallel, sending openRegions rpcs for multiple regions at the same time, and not doing one-by-one assignment. That is all. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That is dangerous. IIRC in my spec I only put master abort if somebody changes table state under master; but in general, if region is in unknown state it&apos;s better to make admin act, than to just silently &quot;disappear&quot; part of data - that can lead to wrong results.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Quaranteing the table or region is fine, but master should not be down because of this (for example, a region can fail to open and you would want to track how many times the region failed to open so that you can decide at some point that the region should be quarantened state (or failed open state). I think there was some issue the region bouncing from server to server indefinitely. &lt;/p&gt;

&lt;p&gt;For table operations intermixing with region operations, I&apos;ll have to read your updated doc. &lt;/p&gt;</comment>
                            <comment id="13797472" author="sershe" created="Thu, 17 Oct 2013 00:26:43 +0000"  >&lt;p&gt;Ok, I split the doc in half. That way it will be easier to read and manager.&lt;br/&gt;
Part 1 is ready (as a current version), and describes high level design, operation semantics and interaction (I think the latter might be interesting for &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;&lt;br/&gt;
It also tries to capture the requirement lists above and high-level implementation (whatever is agreed upon to some degree).&lt;br/&gt;
Please tell me if something is missing or wrong.&lt;/p&gt;

&lt;p&gt;Part 2 I will keep attaching updates. It covers the design of operations - state machines, exact steps, how client tracks it, how recovery works, etc. It will follow part 1.&lt;/p&gt;</comment>
                            <comment id="13797507" author="ecn" created="Thu, 17 Oct 2013 01:17:12 +0000"  >&lt;p&gt;I&apos;m sorry for asking such a basic question... could someone please comment: what does AM stands for?&lt;/p&gt;

&lt;p&gt;I did a quick search through the ticket and the attachments and it didn&apos;t pop out at me.&lt;/p&gt;</comment>
                            <comment id="13797511" author="sershe" created="Thu, 17 Oct 2013 01:21:01 +0000"  >&lt;p&gt;AssignmentManager, a class in HBase master. Often but not always, when talking about it people also imply bunch of auxiliary classes around it like ServerShutdownHandler, RegionClosed/OpenedHandler, ZKTable, etc. Which together implement region assignment in HBase&lt;/p&gt;</comment>
                            <comment id="13797538" author="fenghh" created="Thu, 17 Oct 2013 02:07:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;I also question the wisdom of moving away from ZK for management of active cluster state...If the issues with Zookeeper originate from implementation details, why not fix implementation rather than look to a new architecture?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Using system table rather than ZK to store state info is for better (cluster restart) performance for big cluster with such as 250K regions. Certainly if we change the way of using ZK ( let master be the single point to read/write ZK, not using ZK&apos;s watch/notify mechanism), no correctness/logic difference between using system table and using ZK&lt;/p&gt;</comment>
                            <comment id="13798467" author="sershe" created="Thu, 17 Oct 2013 21:40:54 +0000"  >&lt;p&gt;The doc hasn&apos;t been out for long; just clarifying - anyone interested in providing feedback for part 1? &lt;br/&gt;
It&apos;d be really nice to start working out implementation details in part 2 with some confidence, and/or writing code. Should I assume lack of interest or silent agreement to rewrite according to part 1?&lt;/p&gt;</comment>
                            <comment id="13798612" author="jmhsieh" created="Thu, 17 Oct 2013 23:41:35 +0000"  >&lt;p&gt;I&apos;m doing a pass, will provide feedback tomorrow.&lt;/p&gt;</comment>
                            <comment id="13799465" author="devaraj" created="Fri, 18 Oct 2013 20:09:29 +0000"  >&lt;p&gt;Quick comments:&lt;br/&gt;
1. &quot;Master knows of all external updates to the system store&quot; - Are there such updates happening without master&apos;s knowledge&lt;br/&gt;
2. I presume once the client is told an operation is accepted, it would be saved/queued somewhere so even if a different node picks up the master&apos;s duties, it can execute the operation. Related to that is that the master should be able to get back with the correct return code for the operation even in the case of fail-overs. Also, the &quot;master&quot; could have triggered some operations like shutdown handling that should be completed.&lt;br/&gt;
3. I think we should support asynchronous operations (submit an operation and check periodically or something). There is no guarantee when a certain operation will complete especially when the operation requires co-ordination with other nodes and/or the node is falling behind in executing operations. We shouldn&apos;t force the model to be synchronous (we do not want to hold up precious node resources which we will in synchronous mode).&lt;br/&gt;
4. Maybe, we should explicitly state handling cases where the &quot;master&quot; sends a region operation to a regionserver and the regionserver doesn&apos;t get back within some timeout, as one of the requirements. Fencing the regionserver etc are the possible actions when this happens.&lt;br/&gt;
5. Should we fail-fast on the client side in case of conflicts? For example, if a client issued &quot;drop table&quot; and this operation is in progress. Another client comes in and says &quot;create table&quot; with the same name. We should allow clients to read the store without going through the &quot;master&quot;.&lt;br/&gt;
6. Wondering whether we need to differentiate priorities/ordering/etc. for operations like &quot;move region&quot; initiated by the master/balancer versus initiated by the user. Who wins, etc. These operations are &quot;advanced&quot; and won&apos;t be commonplace but worth calling it out?&lt;/p&gt;</comment>
                            <comment id="13799576" author="jmhsieh" created="Fri, 18 Oct 2013 22:10:48 +0000"  >&lt;p&gt;Yesterday, I shared with sergey and some of the folks interested this a draft of the design I&apos;ve been working on (I&apos;ll call it the hbck-master) and a list of questions related to Sergey&apos;s design.  Since sergey&apos;s has got master5 in the name of the doc I&apos;ll refer to it as &quot;master5&quot;.  He&apos;s answered some question in email but we should do technical discussions out here.  We&apos;ll be working together to hash out holes in each others designs and potentially merge designs.  &lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I have a lot of questions.  I&apos;ll hit the big questions first. Also would i be possible to put a version of this up as gdoc so we can point out nits and places that need minor clarification?   (I have a marked up physical copy version of the doc, would be easier to provide feedback).&lt;/p&gt;

&lt;p&gt;Main Concerns:&lt;/p&gt;

&lt;p&gt;What is a failure and how do you react to failures? I think the master5 design needs to spend more effort  to considering failure and recovery cases. I claim there are 4 types of responses from a networked IO operation -  two states we normally deal with ack successful, ack failed (nack) and unknown due to timeout  that succeeded (timeout success) and unknown due to timeout that failed (timeout failed). We have historically missed the last two timeout cases or assumed timeout means failure nack. It seems that master5 makes the same assumptions. &lt;/p&gt;

&lt;p&gt;I&apos;m very concerned about what we need to do to invalidate information cached RS information at clients in the case of hang, and that will violate the isolation guarantees that we claim to provide.  I really want a slice in-depth failure handling case analysis including a client with cached rs assignments for move and something more complicated such as split or alter.&lt;/p&gt;

&lt;p&gt;I really want more invariant specified for the FSM states.  e.g. if a region is in state X, does it have a row in meta? does have data on the FS? is it open on another region? is it open on only one region? I think having 8 pages of tables at the back of the master5 doc can be more concise and precise which will help us get attempt to prove correctness.  &lt;/p&gt;

&lt;p&gt;Clarification questions:&lt;/p&gt;

&lt;p&gt;1)  State update coordination.  What is a &quot;state updates from the outside&quot;  Do RS&apos;s initiate splitting on their own?  Maybe a picture would help so we can figure out if it is similar or different from hbck-master&apos;s?&lt;/p&gt;

&lt;p&gt;2) Single point of truth.  What is this truth? what the user specficied actions?  what the rs&apos;s are reporting?  the last state we were confirmed to be at? hbck-master tries to define what single point of truth means by defining intended, current, and actual state data with durability properties on each kind. What do clients look at who modifies what? &lt;/p&gt;

&lt;p&gt;3) Table record: &quot;if regions is out of date, it should be closed and reopened&quot;. It is not clear in master5 how regionservers find out that they are out of date. Moreover, how do clients talking to those RS&apos;s with stale versions know they are going to the correct RS especially in the face of RS failures due to timeout?&lt;/p&gt;

&lt;p&gt;4) region record: transition states.  Shouldn&apos;t be defined as part of the region record? (This is really similar to hbck-masters current state and intended state. )&lt;/p&gt;

&lt;p&gt;5) Note on user operations: the forgetting thing is scary to me &amp;#8211; in your move split example, what happens if an RS reads state that is forgotten?  &lt;/p&gt;

&lt;p&gt;6) table state machine. how do we guarantee clients are not writing to against out of date region versions? (in hang situations, regions could be open on multple places &amp;#8211; the hung RS and the new RS the region was assigned to and successfully opened on)  &lt;/p&gt;

&lt;p&gt;7) region state machine.  Earlier draft hand splitting and merge cases.  Are they elided in master5 or are not present any more. How would this get extended handle jeffrey&apos;s distributed log replay/fast write recovery feature?  &lt;/p&gt;

&lt;p&gt;8) logical interactions:  sounds like master5 allows concurrent operations in specfiic regions and and specfiic table.  (e.g. it will allow moves and splits and merges on the same region).  hbck-master (though not fully documented) only allows certain region transitions when the table is enabled or if the table is disabled.  Are we sure we don&apos;t get into race conditions?  What happens if disable gets issued &amp;#8211; its possible for someone to reopens the region and for old clients to continue writing to it even though it is closed?&lt;/p&gt;

&lt;p&gt;nit. 9) &quot;in cursive&quot; mean in italics. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;10) The table operations section have tables which I believe are the actions between FSM states in the table or region fsms.  Is this correct?  Can the edges be labeled to describe which steps these transitions correspond to?&lt;/p&gt;

&lt;p&gt;Short doc:&lt;br/&gt;
nit: Design Constraints, code should: Have AM logic isolated from the persistent storage of state.  &lt;br/&gt;
// I think this should be &quot;abstracted&quot; so we can plug in different implementations of persistent storage of state.&lt;/p&gt;</comment>
                            <comment id="13799588" author="jmhsieh" created="Fri, 18 Oct 2013 22:18:53 +0000"  >&lt;p&gt;Long version of the &quot;hbck-master&quot; design.  Note that is design level only and its goal is to try to convince us about correctness. It is long, but you can get the gist of it reading the first 4 pages.  It gets more detailed about region state machines and transitions and failure handling after that (including some proof sketches, how extensions would work, things ignored thus far). I&apos;m fairly certain there are still bugs in that.&lt;/p&gt;

&lt;p&gt;There are some ideas explored more deeply in hbck-master and there are others explored more in master5.  I think there are also a few places where I need clarification to see if we are in the same place or different places.&lt;/p&gt;
</comment>
                            <comment id="13799619" author="sershe" created="Fri, 18 Oct 2013 22:46:58 +0000"  >&lt;p&gt;Answers lifted from email also (some fixes + one answer was modified due to clarification here &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What is a failure and how do you react to failures? I think the master5 design needs to spend more effort  to considering failure and recovery cases. I claim there are 4 types of responses from a networked IO operation -  two states we normally deal with ack successful, ack failed (nack) and unknown due to timeout  that succeeded (timeout success) and unknown due to timeout that failed (timeout failed). We have historically missed the last two cases and they aren&apos;t considered in the master5 design. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There are a few considerations. Let me examine if there are other cases than these.&lt;br/&gt;
I am assuming the collocated table, which should reduce such cases for state (probably, if collocated table cannot be written reliably, master must stop-the-world and fail over).&lt;br/&gt;
When RS contacts master to do state update, it errs on the side of caution - no state update, no open region (or split).&lt;br/&gt;
Thus, except for the case of multiple masters running, we can always assume RS didn&apos;t online the region if we don&apos;t know about it.&lt;br/&gt;
Then, for messages to RS, see &quot;Note on messages&quot;; they are idempotent so they can always be resent.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1) State update coordination.  What is a &quot;state updates from the outside&quot;  Do RS&apos;s initiate splitting on their own?  Maybe a picture would help so we can figure out if it is similar or different from hbck-master&apos;s?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, these are RS messages. They are mentioned in some operation descriptions in part 2 - opening-&amp;gt;opened, closing-&amp;gt;closed; splitting, etc.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;2) Single point of truth.  hbck-master tries to define what single point of truth means by defining intended, current, and actual state data with durability properties on each kind. What do clients look at who modifies what? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, don&apos;t understand the question. I mean single source of truth mainly about what is going on with the region; it is described in design considerations.&lt;br/&gt;
I like the idea of &quot;intended state&quot;, however without more detailed reading I am not sure how it works for multiple ops e.g. master recovering the region while the user intends to split it, so the split should be executed after it&apos;s opened.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;3) Table record: &quot;if regions is out of date, it should be closed and reopened&quot;. It is not clear in master5 how regionservers find out that they are out of date. Moreover, how do clients talking to those RS&apos;s with stale versions know they are going to the correct RS especially in the face of RS failures due to timeout?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;On alter (and startup if failed), master tries to reopen all regions that are out of date.&lt;br/&gt;
Regions that are not opened with either pick up the new version when they are opened, or (e.g. if they are now Opening with old version) master discovers they are out of date when they are transitioned to Opened by RS, and reopens them again.&lt;/p&gt;

&lt;p&gt;As for any case of alter on enabled table, there are no guarantees for clients.&lt;br/&gt;
To provide these w/o disable/enable (or logical equivalent of coordinating all close-s and open-s), one would need some form of version-time-travel, or waiting for versions, or both.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;4) region record: transition states.  This is really similar to hbck-masters current state and intended state.  Shouldn&apos;t be defined as part of the region record?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I mention somewhere that could be done. One thing is that if several paths are possible between states, it&apos;s useful to know which is taken.&lt;br/&gt;
But do note that I store user intent separately from what is currently going on, so they are not exactly similar as far as I see.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;5) Note on user operations: the forgetting thing is scary to me &amp;#8211; in your move split example, what happens if an RS reads state that is forgotten?  &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think my description of this might be too vague. State is not forgotten; previous intent is forgotten. I.e. if user does several operations in order that conflict (e.g. split and then merge), the first one will be canceled (safely &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;br/&gt;
Also, RS does not read state as a guideline to what needs to be done.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;6) table state machine. how do we guarantee clients are writing from the correct version in the in failures?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you please elaborate?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;7) region state machine.  Earlier draft hand splitting and merge cases.  Are they elided in master5 or are not present any more. How would this get extended handle jeffrey&apos;s distributed log replay/fast write recovery feature? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As I mention somewhere these could be separate states. I was kind of afraid of blowing up state machine too much, so I noticed that for split/merge you anyway store siblings/children, so you can recognize them and for most purposes different split-merge states are the same as Opened and Closed.&lt;/p&gt;

&lt;p&gt;I will add those back, it would make sense.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;8) logical interactions:  sounds like master5 allows concurrent region and table operations.  hbck-master (though not fully documented) only allows certain region transitions when the table is enabled or if the table is disabled.  Are we sure we don&apos;t get into race conditions?  What happens if disable gets issued &amp;#8211; its possible for someone to reopens the region and for old clients to continue writing to it even though it is closed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, parallelism is intended. You can never be sure you have no races but we should aim for it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;master5 is missing disabled/enabled check, that is a mistake.&lt;/p&gt;

&lt;p&gt;Part1 operation interactions already cover it:&lt;/p&gt;

&lt;p&gt;    table disable doesn&apos;t ack until all regions are closed (master5 is wrong &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;br/&gt;
    region opening cannot start if table is already disabling or disabled.&lt;br/&gt;
    if region is already opening when disable is issued, opening will be opportunistically canceled.&lt;br/&gt;
    if disable fails to cancel opening, or server opens it first in a race, region will be opened, and master will issue close immediately after state update. Given that region is not closed, disable is not complete.&lt;br/&gt;
    if opening (or closing) times out, master will fence off RS and mark region as closed. If there was some way of fencing region separately (ZK lease?) it would be possible to use that.&lt;/p&gt;

&lt;p&gt;In any case, until client checks table state before every write, there&apos;s no easy way to prevent writes on disabling table. Writes on disabled table will not be possible.&lt;/p&gt;


&lt;p&gt;On ensuring there&apos;s no double assignment due to RS hanging:&lt;br/&gt;
The intent is to fence the WAL for region server, the way we do now. One could also use other mechanism.&lt;br/&gt;
Perhaps I could specify it more clearly; I think the problem of making sure RS is dead is nearly orthogonal.&lt;br/&gt;
In my model, due to how opening region is committed to opened, we can only be unsure when the region is in Opened state (or similar states such as Splitting which are not present in my current version, but will be added).&lt;br/&gt;
In that case, in absence of normal transition, we cannot do literally anything with the region unless we are sufficiently sure that RS is sufficiently dead (e.g. cannot write).&lt;br/&gt;
So, while we ensure that RS is dead we don&apos;t reassign.&lt;br/&gt;
My document implies (but doesn&apos;t elaborate, I&apos;ll fix that) that master does direct Opened-&amp;gt;Closed direct transition only when that is true.&lt;br/&gt;
A state called &quot;MaybeOpened&quot; could be added. Let me add it...&lt;/p&gt;</comment>
                            <comment id="13799634" author="stack" created="Fri, 18 Oct 2013 23:03:05 +0000"  >&lt;p&gt;Suggest moving the out on the dev mailing list as per the bible quoted below.  Start a thread there?&lt;/p&gt;


&lt;p&gt;From &quot;Producing Open Source Software&quot;:&lt;/p&gt;

&lt;p&gt;&quot;Make sure the bug tracker doesn&apos;t turn into a discussion forum.&lt;br/&gt;
Although it is important to maintain a human presence in the bug&lt;br/&gt;
tracker, it is not fundamentally suited to real-time discussion. Think&lt;br/&gt;
of it rather as an archiver, a way to organize facts and references&lt;br/&gt;
to other discussions, primarily those that take place on mailing lists.&lt;/p&gt;

&lt;p&gt;&quot;There are two reasons to make this distinction. First, the bug&lt;br/&gt;
tracker is more cumbersome to use than the mailing lists (or than&lt;br/&gt;
real-time chat forums, for that matter). This is not because bug&lt;br/&gt;
trackers have bad user interface design, it&apos;s just that their interfaces&lt;br/&gt;
were designed for capturing and presenting discrete states, not&lt;br/&gt;
free-flowing discussions. Second, not everyone who should be&lt;br/&gt;
involved in discussing a given issue is necessarily watching the bug&lt;br/&gt;
tracker. Part of good issue management...is to make sure each issue&lt;br/&gt;
is brought to the right peoples&apos; attention, rather than requiring every&lt;br/&gt;
developer to monitor all issues. In the section called &#8220;No&lt;br/&gt;
Conversations in the Bug Tracker&#8221; in&lt;br/&gt;
Chapter 6, Communications, we&apos;ll look at ways to make sure people&lt;br/&gt;
don&apos;t accidentally siphon discussions out of appropriate forums&lt;br/&gt;
and into the bug tracker.&quot;&lt;/p&gt;

&lt;p&gt;Pg. 50 of &lt;a href=&quot;http://producingoss.com/en/producingoss.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://producingoss.com/en/producingoss.pdf&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13799660" author="sershe" created="Fri, 18 Oct 2013 23:43:38 +0000"  >&lt;p&gt;We need some convention for inline responses in the mailing list (or tell me if there&apos;s one) &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13799709" author="jeffreyz" created="Sat, 19 Oct 2013 00:41:11 +0000"  >&lt;p&gt;There are already two good write ups on the topic. Here is yet the another one: &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12609244/Is%20the%20FATE%20of%20Assignment%20Manager%20FATE.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12609244/Is%20the%20FATE%20of%20Assignment%20Manager%20FATE.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The motivations I post this small draft are that I think we need a systematic model for things like table operations, region assignment and others like in future so that we can design them in a unified way in order for people easy to follow, easy to add test capabilities and easy to reason about the result.&lt;/p&gt;

&lt;p&gt;I think FATE provides us those capabilities. We can view FATE as a design/system model rather than an cold Accumulo implementation(in the sense we can change the implementation to fit HBase cases). &lt;br/&gt;
Under this model, we can simplify region assignment and force feature implementer to code in such a way that a partial failed operation can be resumed/retied and leave executions to the framework.&lt;/p&gt;

&lt;p&gt;The draft is only two pages long(if you know FATE it should only be one page) and hope we don&apos;t drop FATE as an design option too quickly.&lt;/p&gt;</comment>
                            <comment id="13799901" author="jmhsieh" created="Sat, 19 Oct 2013 13:55:47 +0000"  >&lt;p&gt;Attached a slightly revised version that fixed some typos and added paragraph to core design.&lt;/p&gt;</comment>
                            <comment id="13799905" author="jmhsieh" created="Sat, 19 Oct 2013 14:07:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt;  acked. &lt;/p&gt;

&lt;p&gt;Let&apos;s post design docs here, and move discussions comparing them to the mailing list.  &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; Let&apos;s name threads prefixed with &lt;span class=&quot;error&quot;&gt;&amp;#91;hbase-5487&amp;#93;&lt;/span&gt; in subject, and maybe rename subject lines if we get into a more focused discussion that warrants it own thread (it one part gets long), and in general reply inline.  (I found this interesting &lt;a href=&quot;http://en.wikipedia.org/wiki/Posting_style&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Posting_style&lt;/a&gt;).  &lt;/p&gt;

&lt;p&gt;I&apos;ll start by copying and pasting unresolved parts of the response-reply above to the dev mailing list.&lt;/p&gt;</comment>
                            <comment id="13802558" author="sershe" created="Wed, 23 Oct 2013 03:31:54 +0000"  >&lt;p&gt;Updating the part 1 doc based on &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; feedback (the parts that are in the scope of part 1).&lt;br/&gt;
Main changes: added details about &quot;single source of truth&quot;/&quot;split-brain&quot;, as well as added some &quot;out of the scope&quot; stuff about RS fencing, HA, etc. with references to possible solutions.&lt;br/&gt;
Also some minor changes.&lt;br/&gt;
Let me try to update part 2 for tomorrow...&lt;/p&gt;</comment>
                            <comment id="13841811" author="sershe" created="Fri, 6 Dec 2013 22:33:34 +0000"  >&lt;p&gt;Ah well, I never got to part 2. Did you guys make progress on this? I may have time to resurrect this again soon.&lt;/p&gt;</comment>
                            <comment id="13846861" author="jmhsieh" created="Thu, 12 Dec 2013 22:57:57 +0000"  >&lt;p&gt;Matteo and Aleks bring up an interesting case that any new master design should handle.  &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10136&quot; title=&quot;the table-lock of TableEventHandler is released too early because reOpenAllRegions() is asynchronous&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10136&quot;&gt;HBASE-10136&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13849396" author="sershe" created="Mon, 16 Dec 2013 17:57:45 +0000"  >&lt;p&gt;That&apos;s an interesting one. Given that snapshots by default have no guarantees wrt consistent writes between regions (or do they), seems like snapshot should get the latest schema in case of concurrent alter. Is there any consideration (other the arguably implementation issues of not recovering from close-open) that would prevent that? For consistent snapshots presumably the schema can be snapshotted first, I am assuming they don&apos;t stop the world and just take seqId/mvcc/ts or something, so the newer values with new schema will just &quot;not exist&quot;.&lt;/p&gt;</comment>
                            <comment id="13849403" author="jmhsieh" created="Mon, 16 Dec 2013 18:04:07 +0000"  >&lt;p&gt;The problem isn&apos;t that you would get snapshots with inconsistent schemas if the two operations were issued concurrently.  It is that open is async and outside the table write lock which means  the snapshot would fail because the region may no have been open. &lt;/p&gt;

&lt;p&gt;This is a particular case where we would want the open routines to act synchronously with table alters and split daugher region opens (both open before table lock released and snapshot can happen).&lt;/p&gt;
</comment>
                            <comment id="13849452" author="sershe" created="Mon, 16 Dec 2013 18:46:26 +0000"  >&lt;p&gt;IMHO this, in case of opens, promotes not being fault tolerant. In large clusters you cannot get around servers failing and regions closing and reopening. Snapshot should just be able to ride over that. Splits are more interesting.&lt;br/&gt;
Esp. if snapshots are used more (MR over snapshots), it may be nonviable to prevent splits and other operations for the duration of every snapshot, alter, ...&lt;/p&gt;</comment>
                            <comment id="13849682" author="apurtell" created="Mon, 16 Dec 2013 20:18:18 +0000"  >&lt;p&gt;MR over snapshots is already a terrible idea from a security perspective. &lt;/p&gt;</comment>
                            <comment id="13849732" author="sershe" created="Mon, 16 Dec 2013 21:20:58 +0000"  >&lt;p&gt;Yet, it&apos;s a very good idea from perf perspective, and logical given that many large MR jobs don&apos;t need realtime data. Snapshots can still be secured, and table-level granularity is sufficient for most cases I&apos;d suspect.&lt;br/&gt;
Regardless, it was just an example here.&lt;br/&gt;
MR over snapshots can be discussed &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8369&quot; title=&quot;MapReduce over snapshot files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8369&quot;&gt;&lt;del&gt;HBASE-8369&lt;/del&gt;&lt;/a&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13849738" author="apurtell" created="Mon, 16 Dec 2013 21:28:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;Snapshots can still be secured&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is debatable, and that is my point for bringing it up here. All of the enterprise customers I interact with universally want more than table-level granularity, which is why we spent so much time on cell granularity features recently - all of which are totally defeated by MR over snapshots. &lt;/p&gt;

&lt;p&gt;Bringing up MR snapshots as technical justification for other arguments needs qualification that MR over snapshots itself may have limited applicability.&lt;/p&gt;</comment>
                            <comment id="13849798" author="sershe" created="Mon, 16 Dec 2013 22:23:11 +0000"  >&lt;p&gt;There are other justifications... my point is that having a lock over distributed, lengthy operations on tables, esp. with region-level component blocking table-level ops also, is the king of all epic locks, and can cause lots of problems, esp. in large clusters. Snapshot is just one example.&lt;/p&gt;</comment>
                            <comment id="13850594" author="jmhsieh" created="Tue, 17 Dec 2013 15:57:23 +0000"  >&lt;p&gt;Having snapshots succeed while splits, merges and alters can be handled with the open synchronization point.  Having snapshots succeed through failovers would require some major revamping.  We can file that issue &amp;#8211; roughly it would be coordinating based on region name instead of region server name. (non-trivial work).&lt;/p&gt;</comment>
                            <comment id="14725774" author="sershe" created="Tue, 1 Sep 2015 17:51:58 +0000"  >&lt;p&gt;Long live AssignmentManager! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12628385">HBASE-7629</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12603331">HBASE-6571</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12684137">HBASE-10136</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12624812">HBASE-7403</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12695314">HBASE-10544</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12672801">HBASE-9726</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12730571">HBASE-11608</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12624815">HBASE-7404</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12534488">HBASE-4991</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12696009">HBASE-10569</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12618471">HBASE-7254</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12544674">HBASE-5494</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12609793" name="Entity management in Master - part 1.pdf" size="140967" author="sershe" created="Wed, 23 Oct 2013 03:31:54 +0000"/>
                            <attachment id="12608844" name="Entity management in Master - part 1.pdf" size="128351" author="sershe" created="Thu, 17 Oct 2013 00:27:45 +0000"/>
                            <attachment id="12609244" name="Is the FATE of Assignment Manager FATE.pdf" size="230321" author="jeffreyz" created="Sat, 19 Oct 2013 00:41:11 +0000"/>
                            <attachment id="12574461" name="Region management in Master.pdf" size="137395" author="sershe" created="Wed, 20 Mar 2013 00:25:28 +0000"/>
                            <attachment id="12608646" name="Region management in Master5.docx" size="212905" author="sershe" created="Wed, 16 Oct 2013 03:37:12 +0000"/>
                            <attachment id="12609226" name="hbckMasterV2-long.pdf" size="559320" author="jmhsieh" created="Fri, 18 Oct 2013 22:18:53 +0000"/>
                            <attachment id="12609272" name="hbckMasterV2b-long.pdf" size="562173" author="jmhsieh" created="Sat, 19 Oct 2013 13:55:47 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12630796">HBASE-7767</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 29 Feb 2012 00:59:48 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>229715</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 15 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0htif:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>102043</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>