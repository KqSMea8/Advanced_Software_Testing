<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:26:38 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5313/HBASE-5313.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5313] Restructure hfiles layout for better compression</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5313</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;A HFile block contain a stream of key-values. Can we can organize these kvs on the disk in a better way so that we get much greater compression ratios?&lt;/p&gt;

&lt;p&gt;One option (thanks Prakash) is to store all the keys in the beginning of the block (let&apos;s call this the key-section) and then store all their corresponding values towards the end of the block. This will allow us to not-even decompress the values when we are scanning and skipping over rows in the block.&lt;/p&gt;

&lt;p&gt;Any other ideas? &lt;/p&gt;
</description>
                <environment></environment>
        <key id="12540684">HBASE-5313</key>
            <summary>Restructure hfiles layout for better compression</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="dhruba">dhruba borthakur</assignee>
                                    <reporter username="dhruba">dhruba borthakur</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 Feb 2012 06:39:19 +0000</created>
                <updated>Mon, 4 Jul 2016 13:52:41 +0000</updated>
                                                                            <component>io</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>32</watches>
                                                                <comments>
                            <comment id="13202127" author="dhruba" created="Tue, 7 Feb 2012 07:16:07 +0000"  >&lt;p&gt;One option listed above is to keep all the keys in the beginning of the block and all the values in the end of the block. The keys will still be delta-encoded. The values can be lzo-compressed.&lt;/p&gt;

&lt;p&gt;any other ideas out there?&lt;/p&gt;</comment>
                            <comment id="13203324" author="he yongqiang" created="Wed, 8 Feb 2012 07:00:38 +0000"  >&lt;p&gt;As discussed earlier, one thing we can try is to use something like hive&apos;s rcfile. The thing different from hive is hbase row&apos;s value is not a single type. If it turns out the columnar file format helps, we can employ nested columnar format for the value (like what dremel does.). There is one thread on Quora about dremel &lt;a href=&quot;http://www.quora.com/How-will-Googles-Dremel-change-future-Hadoop-releases&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.quora.com/How-will-Googles-Dremel-change-future-Hadoop-releases&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13203836" author="nspiegelberg" created="Wed, 8 Feb 2012 18:47:54 +0000"  >&lt;p&gt;Storing all keys together would just help on CPU, correct?  We wouldn&apos;t get any disk size savings or IO savings with the current approach.&lt;/p&gt;</comment>
                            <comment id="13203899" author="lhofhansl" created="Wed, 8 Feb 2012 19:50:26 +0000"  >&lt;p&gt;Presumably storing the keys together might lends itself for better compression.&lt;br/&gt;
Do we need to index values then? In that case we&apos;d use up more space. Or how would we find the value belonging to a key?&lt;br/&gt;
I suppose we could use the value length from the key, then know we have nth key and by using the value length of all 1 to n-1 keys to find the value.&lt;br/&gt;
Or store the lengths with the values and scan the keys and values in &quot;parallel&quot;.&lt;/p&gt;</comment>
                            <comment id="13203918" author="khemani" created="Wed, 8 Feb 2012 20:04:58 +0000"  >&lt;p&gt;The values can be kept compressed in memory. We can uncompress them on&lt;br/&gt;
demand when writing out the key-values during rpc or compactions.&lt;/p&gt;

&lt;p&gt;The key has to have a pointer to the values. The pointer can be implicit&lt;br/&gt;
and can be derived from value lengths if all the values are stored in the&lt;br/&gt;
same order as keys.&lt;/p&gt;

&lt;p&gt;The value pointer has to be explicit if the values are stored in a&lt;br/&gt;
different order than the keys. We might want to write out the values in a&lt;br/&gt;
different order if we want to do per column compression. While writing out&lt;br/&gt;
the HFileBlock the following can be done - group all the values by their&lt;br/&gt;
column identifier, independently compress and write out each group of&lt;br/&gt;
values, go back to the keys and update the value pointers.&lt;/p&gt;


&lt;p&gt;On 2/8/12 11:50 AM, &quot;Lars Hofhansl (Commented) (JIRA)&quot; &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;

</comment>
                            <comment id="13203935" author="he yongqiang" created="Wed, 8 Feb 2012 20:18:31 +0000"  >&lt;p&gt;&quot;I suppose we could use the value length from the key, then know we have nth key and by using the value length of all 1 to n-1 keys to find the value.&quot;&lt;br/&gt;
Yes. The value length is stored in the key header. The key header is cheap. And can always be decompressed without a big cpu cost.&lt;/p&gt;</comment>
                            <comment id="13204031" author="tlipcon" created="Wed, 8 Feb 2012 21:50:20 +0000"  >&lt;p&gt;I&apos;m curious what the expected compression gain would be. Has anyone tried &quot;rearranging&quot; an example of a production hfile block and recompressing to see the difference?&lt;/p&gt;

&lt;p&gt;My thinking is that typical LZ-based compression (eg snappy) uses a hashtable for common substring identification which is up to 16K entries or so. So I don&apos;t know that it would do a particularly better job with the common keys if they were all grouped at the front of the block - so long as the keyval pairs are less than a few hundred bytes apart, it should still find them OK.&lt;/p&gt;

&lt;p&gt;Of course the other gains (storing large values compressed in RAM for example) seem good.&lt;/p&gt;</comment>
                            <comment id="13205892" author="he yongqiang" created="Sat, 11 Feb 2012 00:01:05 +0000"  >&lt;p&gt;@Todd, with such a small block size and data also already sorted, i was also thinking it is will be very hard to optimize the space.&lt;/p&gt;

&lt;p&gt;So we did some experiments by modifying today&apos;s HFileWriter. It turns out it can still save a lot if we play more tricks.&lt;/p&gt;

&lt;p&gt;Here are test results (block size is 16KB):&lt;/p&gt;

&lt;p&gt;&lt;b&gt;42MB HFile, with Delta compression and with LZO compression&lt;/b&gt; (with default setting on Apache trunk)&lt;/p&gt;

&lt;p&gt;&lt;b&gt;30MB HFile, with Columnar, with Delta compression, and with LZO compression.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Inside one block, first put all row keys inside that block, and do delta compression, and then LZO compression. After row key, put all column family data in that block, and do Delta+LZO for it. And then similarly put column_qualifier. etc&lt;/p&gt;

&lt;p&gt;&lt;b&gt;24MB HFile, with Columnar, Sort value column, Sort column_qualifier column, and with LZO compression.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Inside one block, first put all row keys inside that block, and do delta compression, and then LZO compression. After row key, put all column family data in that block, and do Delta+LZO for it. And then put column_qualifier, sort it, and then do Delta+LZO. TS column and Code column are processed the same as column family. The value column is processed the same as column_qualifier. So it is the same as disk format for the 30MB HFile, except all data for &apos;column_qualifier&apos; and &apos;value&apos; are sorted separately.&lt;/p&gt;

&lt;p&gt;Out of 24MB file, 6MB is used to store row keys, 7MB is used to store column_qualifier, and 6MB is to store value.&lt;/p&gt;

&lt;p&gt;More ideas are welcome! &lt;/p&gt;</comment>
                            <comment id="13205895" author="stack" created="Sat, 11 Feb 2012 00:05:52 +0000"  >&lt;p&gt;How do I read the above?  Its same amount of kvs in each of the files?&lt;/p&gt;</comment>
                            <comment id="13205909" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 00:13:02 +0000"  >&lt;p&gt;@Yongqiang:&lt;br/&gt;
Thanks for sharing the results.&lt;br/&gt;
Can you also list the time it took writing the HFile for each of the three schemes ?&lt;/p&gt;

&lt;p&gt;If you can characterize the row keys and values, that would be nice too.&lt;/p&gt;</comment>
                            <comment id="13205921" author="dhruba" created="Sat, 11 Feb 2012 00:24:20 +0000"  >&lt;p&gt;The same amount of kvs in each file. total of 3 million kvs for this experiment. The blocksize is 16 KB.&lt;/p&gt;</comment>
                            <comment id="13205931" author="jesse_yates" created="Sat, 11 Feb 2012 00:43:35 +0000"  >&lt;p&gt;However, those compression numbers are pretty nice. I worry a little bit about having now an hfileV3, so soon on the heels of the last, leading to a proliferation of versions. My other concern is that the columnar storage doesn&apos;t make sense for all cases - Dremel is for a specific use case.&lt;/p&gt;

&lt;p&gt;That being said, I would love to see the ability to do Dremel in HBase. How about along with a new version/columnar data support comes the ability to select storage files on a per-table basis? That would enable some tables to be optimized for certain use cases, other tables for others, rather than having to use completely different clusters (continuing the multi-tenancy story).&lt;/p&gt;</comment>
                            <comment id="13205978" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 02:12:57 +0000"  >&lt;p&gt;There&apos;re only two weeks before we branch 0.94&lt;br/&gt;
I think HFile v3 would be in 0.96, containing this feature and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5347&quot; title=&quot;GC free memory management in Level-1 Block Cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5347&quot;&gt;&lt;del&gt;HBASE-5347&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13205987" author="gqchen" created="Sat, 11 Feb 2012 02:40:58 +0000"  >&lt;p&gt;Yongqiang, what is the delta encoding algorithm did you use? The default algorithm only do a simple encoding. Do we have results using prefix with fast diff algorithm for the current hfile v2? &lt;/p&gt;

&lt;p&gt;I suppose this is only for the on-disk representation. How do we plan to represent it in block cache?  &lt;/p&gt;

&lt;p&gt;Sent from my iPhone&lt;/p&gt;

</comment>
                            <comment id="13206018" author="lhofhansl" created="Sat, 11 Feb 2012 04:51:17 +0000"  >&lt;p&gt;I agree with Ted, this is 0.96 material.&lt;/p&gt;</comment>
                            <comment id="13206237" author="dhruba" created="Sat, 11 Feb 2012 19:34:11 +0000"  >&lt;p&gt;yq: can we get some numbers how the compression is if we just do columnar and delta-compression (no lzo). This will tell us if there is benefit in storing data columnar in cache.&lt;/p&gt;

&lt;p&gt;we still have to measure the overhead of a read/scan when data us stored in columnar fashion. Very early to say whether this is 0.96 or something further out.&lt;/p&gt;</comment>
                            <comment id="13207057" author="he yongqiang" created="Mon, 13 Feb 2012 18:42:33 +0000"  >&lt;p&gt;&amp;gt;&amp;gt;Can you also list the time it took writing the HFile for each of the three schemes ?&lt;br/&gt;
@Zhihong, we are still trying to explore more ideas here. Once we got a finalized plan, i will get the cpu/latency numbers. &lt;/p&gt;


&lt;p&gt;&amp;gt;&amp;gt;Yongqiang, what is the delta encoding algorithm did you use? The default algorithm only do a simple encoding. Do we have results using prefix with fast diff algorithm for the current hfile v2?&lt;/p&gt;

&lt;p&gt;@jerry, i tried all three delta. And Diff with HFileWriterV2 is producing smallest file in my test. &lt;/p&gt;



</comment>
                            <comment id="13207070" author="he yongqiang" created="Mon, 13 Feb 2012 18:56:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;However, those compression numbers are pretty nice. I worry a little bit about having now an hfileV3, so soon on the heels of the last, leading to a proliferation of versions. My other concern is that the columnar storage doesn&apos;t make sense for all cases - Dremel is for a specific use case.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That being said, I would love to see the ability to do Dremel in HBase. How about along with a new version/columnar data support comes the ability to select storage files on a per-table basis? That would enable some tables to be optimized for certain use cases, other tables for others, rather than having to use completely different clusters (continuing the multi-tenancy story).&lt;/p&gt;

&lt;p&gt;@Jesse Yates, Yeah. Agree here. One big thing we need to answer is how to integrate with current HFile implementation. We want to reuse code as much as possible. I guess a nested columnar structure like Dremel is what we finally want for HBase. But we first need to figure out a good story of how applications will use it.&lt;/p&gt;
</comment>
                            <comment id="13214032" author="he yongqiang" created="Wed, 22 Feb 2012 21:58:17 +0000"  >&lt;p&gt;As a first step, we will go ahead with a simple columnar layout implementation. And leave more advanced features (like nested column layout) in a follow up. &lt;/p&gt;
</comment>
                            <comment id="13222512" author="he yongqiang" created="Mon, 5 Mar 2012 19:02:46 +0000"  >&lt;p&gt;As part of working on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5313&quot; title=&quot;Restructure hfiles layout for better compression&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5313&quot;&gt;HBASE-5313&lt;/a&gt;, we first tried to write a HFileWriter/HFileReader to do it. After finishing some work, it seems this requires a lot of code refactoring in order to reuse existing code as much as possible.&lt;/p&gt;

&lt;p&gt;Then we find seems adding a new columnar encoder/decoder would be easy to do. opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5521&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-5521&lt;/a&gt; to do encoder/decoder specific compression work.&lt;/p&gt;</comment>
                            <comment id="13222550" author="mcorgan" created="Mon, 5 Mar 2012 19:43:26 +0000"  >&lt;p&gt;Just noticed this jira.  I&apos;ve been working on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4676&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-4676&lt;/a&gt;.  In this trie format all the values are concatenated at the end of the block.  I haven&apos;t done anything with compressing them because they are generally small in my use cases, but seems like it would eventually be a good option.  I would think that the compression savings would be similar to the on-disk compression savings, but the benefit is that you have access to scan the keys while the data part of the block is still compressed.&lt;/p&gt;</comment>
                            <comment id="13226351" author="dhruba" created="Fri, 9 Mar 2012 19:24:42 +0000"  >&lt;p&gt;I am guessing that initially, we keep this new &quot;columnar encoding&quot; completely isolated inside a HFileBlock. At table creation time, one can specify that the table be stored in columnar-encoded fashion.&lt;/p&gt;

&lt;p&gt;A HFile will have information in the FixedFileTrailer that specifies whether the data inside the hfile is in columnar-format. A single HFileBlock will have four &quot;column-entity&quot;: all the rowkeys will be laid out first, followed by all the cf, followed by all the &quot;column names&quot;, followed by the timestamps, followed by the memstoreTS, followed by all the values.&lt;/p&gt;

&lt;p&gt;If &apos;prefix-encoding&apos; is enabled, then each column-entity will be prefix encoded individually. If compression (lzo, gz, etc) is enabled, the entire HFileBlock will be compressed accordingly.&lt;/p&gt;

&lt;p&gt;Prefix-encoding will work well for the rowkey entity and the column-family entity. The column name entity may need to be sorted and then prefix encoded. The timestamp entity may need special kind of encoding. One option (suggested by a co-worker Chip Turner) is to take the first timestamp as the base and xor it with each of the following timestamps (thus, zeroing out the common bits) and then storing it. Another option is to find the minimum timestamp in the block and then store diffs from that minimum value. Yet another option is to make Jan-01-2012 as the hbase-epoch and store the difference from that number.&lt;/p&gt;</comment>
                            <comment id="13233244" author="hudson" created="Tue, 20 Mar 2012 06:11:02 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-security #143 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-security/143/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-security/143/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5521&quot; title=&quot;Move compression/decompression to an encoder specific encoding context&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5521&quot;&gt;&lt;del&gt;HBASE-5521&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; Move compression/decompression to an encoder specific encoding&lt;br/&gt;
context&lt;/p&gt;

&lt;p&gt;Author: Yongqiang He&lt;/p&gt;

&lt;p&gt;Summary:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5521&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-5521&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As part of working on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5313&quot; title=&quot;Restructure hfiles layout for better compression&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5313&quot;&gt;HBASE-5313&lt;/a&gt;, we want to add a new columnar encoder/decoder.&lt;br/&gt;
It makes sense to move compression to be part of encoder/decoder:&lt;br/&gt;
1) a scanner for a columnar encoded block can do lazy decompression to a&lt;br/&gt;
specific part of a key value object&lt;br/&gt;
2) avoid an extra bytes copy from encoder to hblock-writer.&lt;/p&gt;

&lt;p&gt;If there is no encoder specified for a writer, the HBlock.Writer will use a&lt;br/&gt;
default compression-context to do something very similar to today&apos;s code.&lt;/p&gt;

&lt;p&gt;Test Plan: existing unit tests verified by mbautin and tedyu. And no new test&lt;br/&gt;
added here since this code is just a preparation for columnar encoder. Will add&lt;br/&gt;
testcase later in that diff.&lt;/p&gt;

&lt;p&gt;Reviewers: dhruba, tedyu, sc, mbautin&lt;/p&gt;

&lt;p&gt;Reviewed By: mbautin&lt;/p&gt;

&lt;p&gt;Differential Revision: &lt;a href=&quot;https://reviews.facebook.net/D2097&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D2097&lt;/a&gt; (Revision 1302602)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
mbautin : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDecodingContext.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13245093" author="kannanm" created="Tue, 3 Apr 2012 09:40:39 +0000"  >&lt;p&gt;Yongqiang: Any updates on this effort/investigation? I noticed &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5674&quot; title=&quot;add support in HBase to overwrite hbase timestamp to a version number during major compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5674&quot;&gt;&lt;del&gt;HBASE-5674&lt;/del&gt;&lt;/a&gt; that you had created which is sort of going after a specific part (timestamps)... but was curious where things are with respect to this JIRA.&lt;/p&gt;</comment>
                            <comment id="13247497" author="he yongqiang" created="Thu, 5 Apr 2012 18:41:01 +0000"  >&lt;p&gt;Hi Kannan,&lt;/p&gt;

&lt;p&gt;We are still experimenting this. The initial results shows only less than one quarter off, which is kind of not big enough for us. The timestamp issue is a low hanging fruit, which can cut 8%. &lt;br/&gt;
We will post some diff asap, once after we finalize our experiments.&lt;/p&gt;</comment>
                            <comment id="13708886" author="jdcryans" created="Mon, 15 Jul 2013 19:54:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=he+yongqiang&quot; class=&quot;user-hover&quot; rel=&quot;he yongqiang&quot;&gt;He Yongqiang&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dhruba&quot; class=&quot;user-hover&quot; rel=&quot;dhruba&quot;&gt;dhruba borthakur&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mikhail&quot; class=&quot;user-hover&quot; rel=&quot;mikhail&quot;&gt;Mikhail Bautin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Guys, I need your help to understand what&apos;s going on with this jira. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5521&quot; title=&quot;Move compression/decompression to an encoder specific encoding context&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5521&quot;&gt;&lt;del&gt;HBASE-5521&lt;/del&gt;&lt;/a&gt; has been committed more than a year ago and nothing moved after that. Moreover, the code breaks encoding by making it not thread safe. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8732&quot; title=&quot;HFileBlockDefaultEncodingContext isn&amp;#39;t thread-safe but is used by all readers, breaks column encoding&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8732&quot;&gt;&lt;del&gt;HBASE-8732&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This makes me think that the code in 5521 was not seriously tested (maybe waiting on this jira to tie all the loose ends?) and since we are trying to release 0.96.0 soonish, I&apos;m currently in favor of reverting it.&lt;/p&gt;</comment>
                            <comment id="13708969" author="mikhail" created="Mon, 15 Jul 2013 20:57:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jdcryans&quot; class=&quot;user-hover&quot; rel=&quot;jdcryans&quot;&gt;Jean-Daniel Cryans&lt;/a&gt;: I&apos;m OK with reverting &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5521&quot; title=&quot;Move compression/decompression to an encoder specific encoding context&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5521&quot;&gt;&lt;del&gt;HBASE-5521&lt;/del&gt;&lt;/a&gt; because it does not look like &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5313&quot; title=&quot;Restructure hfiles layout for better compression&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5313&quot;&gt;HBASE-5313&lt;/a&gt; is moving forward.&lt;/p&gt;</comment>
                            <comment id="13710526" author="jdcryans" created="Wed, 17 Jul 2013 00:26:08 +0000"  >&lt;p&gt;After some more investigation, I don&apos;t think it will be easy to do. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mcorgan&quot; class=&quot;user-hover&quot; rel=&quot;mcorgan&quot;&gt;Matt Corgan&lt;/a&gt;&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7162&quot; title=&quot;Prefix Compression - Trie data block encoding; hbase-common and hbase-server changes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7162&quot;&gt;&lt;del&gt;HBASE-7162&lt;/del&gt;&lt;/a&gt; relies on that code too. So we have to make HFileBlockDefaultEncodingContext thread safe it seems.&lt;/p&gt;</comment>
                            <comment id="15361344" author="srobertjames" created="Mon, 4 Jul 2016 13:52:41 +0000"  >&lt;p&gt;This ticket seems to have been abandoned.  Why? The results posted by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=he+yongqiang&quot; class=&quot;user-hover&quot; rel=&quot;he yongqiang&quot;&gt;He Yongqiang&lt;/a&gt; show a lot of performance gain: half the disk usage.  Has it just been forgotten, or has a decision been made not to do this? Why?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 8 Feb 2012 07:00:38 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>226079</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            23 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02ewv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12017</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310230" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Tags</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Phoenix</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>