<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:27:17 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5387/HBASE-5387.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5387] Reuse compression streams in HFileBlock.Writer</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5387</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We need to to reuse compression streams in HFileBlock.Writer instead of allocating them every time. The motivation is that when using Java&apos;s built-in implementation of Gzip, we allocate a new GZIPOutputStream object and an associated native data structure every time we create a compression stream. The native data structure is only deallocated in the finalizer. This is one suspected cause of recent TestHFileBlock failures on Hadoop QA: &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12542215">HBASE-5387</key>
            <summary>Reuse compression streams in HFileBlock.Writer</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mikhail">Mikhail Bautin</assignee>
                                    <reporter username="mikhail">Mikhail Bautin</reporter>
                        <labels>
                    </labels>
                <created>Sat, 11 Feb 2012 02:45:52 +0000</created>
                <updated>Fri, 12 Oct 2012 05:35:00 +0000</updated>
                            <resolved>Mon, 13 Feb 2012 22:24:25 +0000</resolved>
                                    <version>0.94.0</version>
                                    <fixVersion>0.94.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="13206006" author="hadoopqa" created="Sat, 11 Feb 2012 03:59:11 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12514195/Fix-deflater-leak-2012-02-10_18_48_45.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12514195/Fix-deflater-leak-2012-02-10_18_48_45.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 4 new or modified tests.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated -136 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 157 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.mapred.TestTableMapReduce&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestImportTsv&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/943//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/943//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/943//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/943//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/943//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/943//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13206009" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 04:10:18 +0000"  >&lt;p&gt;Looks like TestHFileBlock passes this time. &lt;/p&gt;</comment>
                            <comment id="13206012" author="mikhail" created="Sat, 11 Feb 2012 04:29:57 +0000"  >&lt;p&gt;@Ted: I think this addresses the root cause of TestHFileBlock and TestForceCacheImportantBlocks failures. As you suspected, Hadoop QA was pointing to a real bug in HBase. However, I think we have had this issue for a while (even in HFile v1), and it just got exposed as I increased the volume of IO happening within a single unit test. I will add a ulimit setting to our internal test runs so that we catch memory leaks like this in the future.&lt;/p&gt;</comment>
                            <comment id="13206017" author="lhofhansl" created="Sat, 11 Feb 2012 04:50:26 +0000"  >&lt;p&gt;Patch looks good to me.&lt;br/&gt;
Can you mark the overriden methods in ResetableGZIPOutputStream, ReusableGzipOutputStream, and ReusableStreamGzipCodec with @Override?&lt;/p&gt;

&lt;p&gt;Also, I would probably move the static code to get the GZIP_HEADER into ResetableGZIPOutputStream, and put a few more comments around it (took me a bit figure out what this was doing).&lt;/p&gt;</comment>
                            <comment id="13206020" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 04:57:16 +0000"  >&lt;p&gt;I ran TestSplitTransactionOnCluster twice on MacBook and it passed.&lt;br/&gt;
I used script to check for hung test and found none.&lt;/p&gt;

&lt;p&gt;I think the patch, after slight revision, should be good to go.&lt;/p&gt;

&lt;p&gt;Thanks for the quick turnaround, Mikhail.&lt;/p&gt;

&lt;p&gt;I think we should try to push ReusableStreamGzipCodec upstream to Hadoop.&lt;/p&gt;</comment>
                            <comment id="13206026" author="stack" created="Sat, 11 Feb 2012 05:25:15 +0000"  >&lt;p&gt;Any reason for hardcoding of 32K for buffer size:&lt;/p&gt;

&lt;p&gt;+      ((Configurable)codec).getConf().setInt(&quot;io.file.buffer.size&quot;, 32 * 1024);&lt;/p&gt;

&lt;p&gt;Give this an initial reasonable size?&lt;/p&gt;

&lt;p&gt;+        compressedByteStream = new ByteArrayOutputStream();&lt;/p&gt;

&lt;p&gt;So, we&apos;ll keep around the largest thing we ever wrote into this ByteArrayOutputStream?  Should we resize it or something from time to time?  Or I suppose we can just wait till its a prob?&lt;/p&gt;

&lt;p&gt;Is the gzip stuff brittle?  The header can be bigger than 10bytes I suppose (spec allows extensions IIRC) but I suppose its safe because we presume java or underlying native compression.&lt;/p&gt;

&lt;p&gt;Good stuff Mikhail.  +1 on patch.&lt;/p&gt;</comment>
                            <comment id="13206030" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 05:45:11 +0000"  >&lt;p&gt;w.r.t. the second comment above, I see the following in doCompression() :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        compressedByteStream.reset();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;According to &lt;a href=&quot;http://docs.oracle.com/javase/6/docs/api/java/io/ByteArrayOutputStream.html#reset%28%29&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.oracle.com/javase/6/docs/api/java/io/ByteArrayOutputStream.html#reset%28%29&lt;/a&gt;, after the above call, &apos;all currently accumulated output in the output stream is discarded.&apos;&lt;br/&gt;
The output stream can be used again, reusing the already allocated buffer space.&lt;/p&gt;</comment>
                            <comment id="13206040" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 05:55:41 +0000"  >&lt;p&gt;See &lt;a href=&quot;http://crawler.archive.org/apidocs/org/archive/io/GzipHeader.html#MINIMAL_GZIP_HEADER_LENGTH&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://crawler.archive.org/apidocs/org/archive/io/GzipHeader.html#MINIMAL_GZIP_HEADER_LENGTH&lt;/a&gt; for where header length came from.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://kickjava.com/src/java/util/zip/GZIPOutputStream.java.htm&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://kickjava.com/src/java/util/zip/GZIPOutputStream.java.htm&lt;/a&gt;, line 109 gives us better idea about header length.&lt;/p&gt;</comment>
                            <comment id="13206062" author="phabricator@reviews.facebook.net" created="Sat, 11 Feb 2012 09:19:59 +0000"  >&lt;p&gt;mbautin requested code review of &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: tedyu, Liyin, dhruba, JIRA&lt;/p&gt;

&lt;p&gt;  We need to to reuse compression streams in HFileBlock.Writer instead of allocating them every time. The motivation is that when using Java&apos;s built-in implementation of Gzip, we allocate a new GZIPOutputStream object and an associated native data structure any time. This is one suspected cause of recent TestHFileBlock failures on Hadoop QA: &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;TEST PLAN&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Run unit tests&lt;/li&gt;
	&lt;li&gt;Create a GZIP-compressed CF with new code, load some data, shut down HBase, deploy old code, restart HBase, and scan the table&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/p&gt;</comment>
                            <comment id="13206072" author="hadoopqa" created="Sat, 11 Feb 2012 10:01:37 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12514205/D1719.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12514205/D1719.1.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated -136 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 157 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/944//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/944//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/944//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/944//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/944//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/944//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13206150" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 15:39:49 +0000"  >&lt;p&gt;I looked at &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt; and only found one version.&lt;br/&gt;
What changes were made after first round of reviews ?&lt;/p&gt;</comment>
                            <comment id="13206156" author="stack" created="Sat, 11 Feb 2012 15:47:59 +0000"  >&lt;p&gt;IIRC, the BAOS will keep the outline of the largest allocation that went through it &amp;#8211; reset doesn&apos;t put the BAOS backing buffer back to original size... I haven&apos;t looked at the src... maybe its better now (I wrote that crawler gzipping thing you cite above).&lt;/p&gt;</comment>
                            <comment id="13206157" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 16:01:39 +0000"  >&lt;p&gt;I went through &lt;a href=&quot;http://www.docjar.com/html/api/java/io/ByteArrayOutputStream.java.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.docjar.com/html/api/java/io/ByteArrayOutputStream.java.html&lt;/a&gt; and didn&apos;t find how to get buf.length&lt;br/&gt;
size() method just returns count.&lt;/p&gt;

&lt;p&gt;One approach is to query BAOS.size() toward the end of doCompression(). If certain threshold is exceeded, we recreate the BAOS.&lt;/p&gt;</comment>
                            <comment id="13206183" author="phabricator@reviews.facebook.net" created="Sat, 11 Feb 2012 17:20:57 +0000"  >&lt;p&gt;lhofhansl has commented on the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;  Nice patch... Only minor comments.&lt;/p&gt;

&lt;p&gt;INLINE COMMENTS&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:40 Move this to ResetableGZIPOutputStream?&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:75 Would be nice to mark all these with @Override&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13206271" author="stack" created="Sat, 11 Feb 2012 21:37:33 +0000"  >&lt;p&gt;@Ted Yeah, I took a look at BAOS.  Reset just sets count to zero as you see.  If a BAOS goes big and stays big, maybe its not so bad.  If there are a bunch of these though, it could become a prob.  We could do as you suggest as long as we do it before we call resize?  So we&apos;d have a conditional where if the size &amp;gt; N times the buffer, then instead of resusing, we go get a new BAOS?&lt;/p&gt;</comment>
                            <comment id="13206273" author="zhihyu@ebaysf.com" created="Sat, 11 Feb 2012 21:47:55 +0000"  >&lt;p&gt;Something like that.&lt;/p&gt;</comment>
                            <comment id="13206287" author="lhofhansl" created="Sat, 11 Feb 2012 22:25:36 +0000"  >&lt;p&gt;We could also do something heuristically. Get a random float and if it is &amp;lt; 0.001 (for example, every 1000&apos;s time on average) throw away the old BAOS and get a new one.&lt;br/&gt;
Can the BAOS really get &amp;gt; the blocksize?&lt;/p&gt;

&lt;p&gt;Also what happens when somebody (foolishly, but it does happen) sets the block size to 512mb, in that case we might want to free this BOAS every single time.&lt;/p&gt;</comment>
                            <comment id="13206300" author="mikhail" created="Sat, 11 Feb 2012 23:27:52 +0000"  >&lt;p&gt;The BAOS in question only exists during the lifetime of the writer, and will be deallocated when flush or compaction is complete.&lt;/p&gt;</comment>
                            <comment id="13206303" author="lhofhansl" created="Sat, 11 Feb 2012 23:40:18 +0000"  >&lt;p&gt;I see, so it seems that there is nothing to worry about. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13206304" author="phabricator@reviews.facebook.net" created="Sat, 11 Feb 2012 23:46:58 +0000"  >&lt;p&gt;mbautin has commented on the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;INLINE COMMENTS&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java:213 Done.&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:45 Done.&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:40 Done.&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:48 Done.&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:92 Done.&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java:75 Done.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13206306" author="phabricator@reviews.facebook.net" created="Sat, 11 Feb 2012 23:48:58 +0000"  >&lt;p&gt;mbautin updated the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: tedyu, Liyin, dhruba, JIRA&lt;/p&gt;

&lt;p&gt;  Addressing review comments.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/p&gt;</comment>
                            <comment id="13206313" author="lhofhansl" created="Sun, 12 Feb 2012 00:05:44 +0000"  >&lt;p&gt;@Mikhail: So we&apos;re still creating a new GzipCodec frequently (with every new HFileBlock object) albeit less frequently than before?&lt;/p&gt;</comment>
                            <comment id="13206325" author="mikhail" created="Sun, 12 Feb 2012 01:18:22 +0000"  >&lt;p&gt;@Lars: where are we creating a new GzipCodec frequently? We only instantiate ReusableStreamGzipCodec once in the following block:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;Compression.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    GZ(&lt;span class=&quot;code-quote&quot;&gt;&quot;gz&quot;&lt;/span&gt;) {
      &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;transient&lt;/span&gt; GzipCodec codec;

      @Override
      DefaultCodec getCodec(Configuration conf) {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (codec == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
          codec = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ReusableStreamGzipCodec();
          codec.setConf(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Configuration(conf));
        }

        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; codec;
      }
    },
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What we are creating less frequently than before is the compressing output stream (a subclass of GZIPOutputStream with the associated native data structure): once per HFile writer with the patch vs. for every HFile block previously.&lt;/p&gt;</comment>
                            <comment id="13206331" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 01:54:59 +0000"  >&lt;p&gt;mbautin updated the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: tedyu, Liyin, dhruba, JIRA&lt;/p&gt;

&lt;p&gt;  Breaking compression stream creation methods into multiple parts. Avoiding using &quot;finish-on-flush&quot; stream in a couple of places, and deprecating the currently used method that is always called with downstream buffer size of 0, at least within HBase code. The method is still there for backwards compatibility. Also, getting rid of hard-coded buffer size by delegating the native compressor case to superclass in ReusableStreamGzipCodec.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java&lt;/p&gt;</comment>
                            <comment id="13206335" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 02:22:58 +0000"  >&lt;p&gt;lhofhansl has accepted the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;  Nice patch Mikhail.&lt;br/&gt;
  Two more nits inline, can be done on commit.&lt;/p&gt;

&lt;p&gt;INLINE COMMENTS&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java:216 Make an HConstant for &quot;io.file.buffer.size&quot; and/or add it to hbase-defaults.xml?&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java:613 Since it confused many of us, maybe add a brief comment that is OK to keep the BAOS per Writer, because of the specific, limited lifetime of a writer object.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13206337" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 02:28:58 +0000"  >&lt;p&gt;tedyu has commented on the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;  Looks good.&lt;/p&gt;

&lt;p&gt;INLINE COMMENTS&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java:328 Should we document this additional call ?&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13206340" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 02:36:57 +0000"  >&lt;p&gt;mbautin has commented on the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;  Looking into a bug in the most recent version of the patch.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13206343" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 03:39:06 +0000"  >&lt;p&gt;mbautin updated the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: tedyu, Liyin, dhruba, JIRA, lhofhansl&lt;/p&gt;

&lt;p&gt;  Fixing the bug in the previous version of the patch (broke some tests in case of no compression). createCompressionStream(OutputStream, Compressor, int) remains the primary API, and the new package-private method only used from HFileBlock is createPlainCompressionStream.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/p&gt;</comment>
                            <comment id="13206344" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 03:42:57 +0000"  >&lt;p&gt;mbautin updated the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: tedyu, Liyin, dhruba, JIRA, lhofhansl&lt;/p&gt;

&lt;p&gt;  Forgot to actually use ReusableStreamGzipCodec.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/p&gt;</comment>
                            <comment id="13206350" author="zhihyu@ebaysf.com" created="Sun, 12 Feb 2012 05:23:42 +0000"  >&lt;p&gt;Latest patch from Mikhail.&lt;/p&gt;</comment>
                            <comment id="13206361" author="mikhail" created="Sun, 12 Feb 2012 07:14:55 +0000"  >&lt;p&gt;@Ted: thanks for attaching the patch. I will look into how to fix Phabricator to attach Jenkins-compatible patches.&lt;/p&gt;</comment>
                            <comment id="13206435" author="zhihyu@ebaysf.com" created="Sun, 12 Feb 2012 15:42:46 +0000"  >&lt;p&gt;The failure of Phabricator attaching patches might have something to do with the fact that Apache Jenkins has been down for roughly a whole day.&lt;/p&gt;</comment>
                            <comment id="13206654" author="hadoopqa" created="Mon, 13 Feb 2012 03:36:05 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12514262/Fix-deflater-leak-2012-02-12_00_37_27.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12514262/Fix-deflater-leak-2012-02-12_00_37_27.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 4 new or modified tests.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated -136 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 157 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.master.TestSplitLogManager&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;br/&gt;
                  org.apache.hadoop.hbase.mapred.TestTableMapReduce&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestClassLoading&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestImportTsv&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/946//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/946//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/946//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/946//artifact/trunk/patchprocess/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/946//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/946//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13206758" author="mikhail" created="Mon, 13 Feb 2012 08:53:51 +0000"  >&lt;p&gt;Those reported test failures seem unrelated. Is this OK to commit?&lt;/p&gt;</comment>
                            <comment id="13206916" author="zhihyu@ebaysf.com" created="Mon, 13 Feb 2012 15:00:34 +0000"  >&lt;p&gt;I ran the following tests with latest patch and they passed:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  578  mt -Dtest=TestClassLoading
  579  mt -Dtest=TestSplitLogManager
  581  mt -Dtest=TestHFileOutputFormat
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The latest patch should be good to go.&lt;/p&gt;</comment>
                            <comment id="13206985" author="lhofhansl" created="Mon, 13 Feb 2012 17:11:48 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13207018" author="ram_krish" created="Mon, 13 Feb 2012 18:02:46 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13207108" author="zhihyu@ebaysf.com" created="Mon, 13 Feb 2012 19:31:22 +0000"  >&lt;p&gt;@Mikhail:&lt;br/&gt;
Can you integrate the patch so that we have a green TRUNK build ?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13207111" author="mikhail" created="Mon, 13 Feb 2012 19:34:54 +0000"  >&lt;p&gt;@Ted: sure. I was rebasing and running a final local test run.&lt;/p&gt;</comment>
                            <comment id="13207124" author="zhihyu@ebaysf.com" created="Mon, 13 Feb 2012 19:39:58 +0000"  >&lt;p&gt;Thanks for being prudent.&lt;/p&gt;</comment>
                            <comment id="13207252" author="zhihyu@ebaysf.com" created="Mon, 13 Feb 2012 21:50:21 +0000"  >&lt;p&gt;Looks like patch has been integrated.&lt;/p&gt;

&lt;p&gt;Please announce integration on the JIRA in the future.&lt;/p&gt;</comment>
                            <comment id="13207273" author="hudson" created="Mon, 13 Feb 2012 22:20:57 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #2661 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2661/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2661/&lt;/a&gt;)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&lt;/p&gt;

&lt;p&gt;Summary: We need to to reuse compression streams in HFileBlock.Writer instead of&lt;br/&gt;
allocating them every time. The motivation is that when using Java&apos;s built-in&lt;br/&gt;
implementation of Gzip, we allocate a new GZIPOutputStream object and an&lt;br/&gt;
associated native data structure any time. This is one suspected cause of recent&lt;br/&gt;
TestHFileBlock failures on Hadoop QA:&lt;br/&gt;
&lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Run unit tests&lt;/li&gt;
	&lt;li&gt;Create a GZIP-compressed CF with new code, load some data, shut down HBase,&lt;br/&gt;
deploy old code, restart HBase, and scan the table&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Reviewers: tedyu, Liyin, dhruba, JIRA, lhofhansl&lt;/p&gt;

&lt;p&gt;Reviewed By: lhofhansl&lt;/p&gt;

&lt;p&gt;CC: tedyu, lhofhansl, mbautin&lt;/p&gt;

&lt;p&gt;Differential Revision: &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt; (Revision 1243667)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
mbautin : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13207324" author="hudson" created="Mon, 13 Feb 2012 23:15:37 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-security #110 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-security/110/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-security/110/&lt;/a&gt;)&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&lt;/p&gt;

&lt;p&gt;Summary: We need to to reuse compression streams in HFileBlock.Writer instead of&lt;br/&gt;
allocating them every time. The motivation is that when using Java&apos;s built-in&lt;br/&gt;
implementation of Gzip, we allocate a new GZIPOutputStream object and an&lt;br/&gt;
associated native data structure any time. This is one suspected cause of recent&lt;br/&gt;
TestHFileBlock failures on Hadoop QA:&lt;br/&gt;
&lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Test Plan:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Run unit tests&lt;/li&gt;
	&lt;li&gt;Create a GZIP-compressed CF with new code, load some data, shut down HBase,&lt;br/&gt;
deploy old code, restart HBase, and scan the table&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Reviewers: tedyu, Liyin, dhruba, JIRA, lhofhansl&lt;/p&gt;

&lt;p&gt;Reviewed By: lhofhansl&lt;/p&gt;

&lt;p&gt;CC: tedyu, lhofhansl, mbautin&lt;/p&gt;

&lt;p&gt;Differential Revision: &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt; (Revision 1243667)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
mbautin : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13207368" author="phabricator@reviews.facebook.net" created="Mon, 13 Feb 2012 23:56:57 +0000"  >&lt;p&gt;mbautin requested code review of &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;89-fb&amp;#93;&lt;/span&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: khemani, Liyin, Kannan, lhofhansl, tedyu, JIRA&lt;/p&gt;

&lt;p&gt;  Porting the trunk patch &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt; to 89-fb to make Gzip compression streams reusable when no Hadoop native libraries loaded. Also, making the Compression class and HFileBlock in 89-fb more similar to its trunk version. This touches the same code in HFileBlock with the data block encoding patch (&lt;a href=&quot;https://reviews.facebook.net/D1659&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1659&lt;/a&gt;), which will have to be rebased on this.&lt;/p&gt;

&lt;p&gt;TEST PLAN&lt;br/&gt;
  Run unit tests&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1725&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1725&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java&lt;/p&gt;

&lt;p&gt;MANAGE HERALD DIFFERENTIAL RULES&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/herald/view/differential/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/herald/view/differential/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;WHY DID I GET THIS EMAIL?&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/herald/transcript/3681/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/herald/transcript/3681/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Tip: use the X-Herald-Rules header to filter Herald messages in your client.&lt;/p&gt;</comment>
                            <comment id="13209171" author="phabricator@reviews.facebook.net" created="Thu, 16 Feb 2012 06:53:08 +0000"  >&lt;p&gt;mbautin has committed the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1719&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;COMMIT&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/rHBASE1243667&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/rHBASE1243667&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13209387" author="ram_krish" created="Thu, 16 Feb 2012 14:31:17 +0000"  >&lt;p&gt;We are facing the same problem in the 0.90 also when using GZIP. Below is a snapshot of the &apos;top&apos; command&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
13236 root      20   0 21.9g  21g  12m S    1 68.4 450:56.37 /opt/nn/jdk1.6.0_22
13236 root      20   0 21.9g  21g  12m S    1 68.4 450:56.71 /opt/nn/jdk1.6.0_22
13236 root      20   0 21.9g  21g  12m S    1 68.4 450:57.06 /opt/nn/jdk1.6.0_22
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Configured heap is 4G for RS.&lt;/p&gt;</comment>
                            <comment id="13213112" author="phabricator@reviews.facebook.net" created="Tue, 21 Feb 2012 23:38:46 +0000"  >&lt;p&gt;mbautin updated the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;89-fb&amp;#93;&lt;/span&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;br/&gt;
Reviewers: khemani, Liyin, Kannan, lhofhansl, tedyu, JIRA&lt;/p&gt;

&lt;p&gt;  Rebasing on recent changes in 89-fb (data block encoding).&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1725&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1725&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;AFFECTED FILES&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;br/&gt;
  src/main/java/org/apache/hadoop/hbase/io/hfile/ReusableStreamGzipCodec.java&lt;br/&gt;
  src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/p&gt;</comment>
                            <comment id="13213408" author="phabricator@reviews.facebook.net" created="Wed, 22 Feb 2012 07:04:48 +0000"  >&lt;p&gt;Karthik has commented on the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;89-fb&amp;#93;&lt;/span&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;  Looks good! I think I might end up using ReusableStreamGzipCodec also in the compressed RPC change.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1725&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1725&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13213409" author="phabricator@reviews.facebook.net" created="Wed, 22 Feb 2012 07:06:47 +0000"  >&lt;p&gt;Karthik has accepted the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;89-fb&amp;#93;&lt;/span&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1725&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1725&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;BRANCH&lt;br/&gt;
  fix_gzip5&lt;/p&gt;</comment>
                            <comment id="13213825" author="phabricator@reviews.facebook.net" created="Wed, 22 Feb 2012 18:35:48 +0000"  >&lt;p&gt;mbautin has committed the revision &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;jira&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5387&quot; title=&quot;Reuse compression streams in HFileBlock.Writer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5387&quot;&gt;&lt;del&gt;HBASE-5387&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;89-fb&amp;#93;&lt;/span&gt; Reuse compression streams in HFileBlock.Writer&quot;.&lt;/p&gt;

&lt;p&gt;REVISION DETAIL&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/D1725&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/D1725&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;COMMIT&lt;br/&gt;
  &lt;a href=&quot;https://reviews.facebook.net/rHBASEEIGHTNINEFBBRANCH1292434&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.facebook.net/rHBASEEIGHTNINEFBBRANCH1292434&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12514260" name="5387.txt" size="12441" author="zhihyu@ebaysf.com" created="Sun, 12 Feb 2012 05:23:42 +0000"/>
                            <attachment id="12514205" name="ASF.LICENSE.NOT.GRANTED--D1719.1.patch" size="10682" author="phabricator@reviews.facebook.net" created="Sat, 11 Feb 2012 09:20:00 +0000"/>
                            <attachment id="12514243" name="ASF.LICENSE.NOT.GRANTED--D1719.2.patch" size="12112" author="phabricator@reviews.facebook.net" created="Sat, 11 Feb 2012 23:48:58 +0000"/>
                            <attachment id="12514256" name="ASF.LICENSE.NOT.GRANTED--D1719.3.patch" size="15964" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 01:54:59 +0000"/>
                            <attachment id="12514257" name="ASF.LICENSE.NOT.GRANTED--D1719.4.patch" size="12173" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 03:39:08 +0000"/>
                            <attachment id="12514258" name="ASF.LICENSE.NOT.GRANTED--D1719.5.patch" size="12441" author="phabricator@reviews.facebook.net" created="Sun, 12 Feb 2012 03:42:59 +0000"/>
                            <attachment id="12514422" name="ASF.LICENSE.NOT.GRANTED--D1725.1.patch" size="28054" author="phabricator@reviews.facebook.net" created="Mon, 13 Feb 2012 23:56:58 +0000"/>
                            <attachment id="12515506" name="ASF.LICENSE.NOT.GRANTED--D1725.2.patch" size="15441" author="phabricator@reviews.facebook.net" created="Tue, 21 Feb 2012 23:38:47 +0000"/>
                            <attachment id="12514195" name="Fix-deflater-leak-2012-02-10_18_48_45.patch" size="11918" author="mikhail" created="Sat, 11 Feb 2012 02:49:08 +0000"/>
                            <attachment id="12514254" name="Fix-deflater-leak-2012-02-11_17_13_10.patch" size="13428" author="mikhail" created="Sun, 12 Feb 2012 01:13:41 +0000"/>
                            <attachment id="12514262" name="Fix-deflater-leak-2012-02-12_00_37_27.patch" size="13782" author="mikhail" created="Sun, 12 Feb 2012 08:37:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 11 Feb 2012 03:59:11 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>227502</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 43 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0166n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4771</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>