<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:35:16 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-12672/HBASE-12672.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-12672] Support optionally replicating a table synchronously</title>
                <link>https://issues.apache.org/jira/browse/HBASE-12672</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Sometimes a table may contain state in which it&apos;s crucial to know that replication occurred before continuing with the update. Though this would mean that you&apos;d block updates to this table if your secondary cluster was unavailable, that might be a tradeoff that a user would be willing to make. An example would be in support of SQL sequences. See this thread (&lt;a href=&quot;http://s.apache.org/fTP&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://s.apache.org/fTP&lt;/a&gt;) and &lt;a href=&quot;https://issues.apache.org/jira/browse/PHOENIX-1422&quot; title=&quot;Stateless Sequences&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PHOENIX-1422&quot;&gt;PHOENIX-1422&lt;/a&gt; for more discussion and context.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12760923">HBASE-12672</key>
            <summary>Support optionally replicating a table synchronously</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jamestaylor">James Taylor</reporter>
                        <labels>
                    </labels>
                <created>Thu, 11 Dec 2014 03:06:52 +0000</created>
                <updated>Fri, 12 Dec 2014 21:09:19 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                <comments>
                            <comment id="14242134" author="lhofhansl" created="Thu, 11 Dec 2014 04:38:03 +0000"  >&lt;p&gt;Frankly, I am not a fan of this.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Synchronous replication creates two points of failure. If the target cluster is down we&apos;ll render the primary unusable, as everything would be waiting replication to finish since it is synchronous. That is true if we make this by table, but it would be same, target is down, primary is no longer usable for this table. At best we can timeout on the replication and pass this timeout as an failed write error back to the client.&lt;/li&gt;
	&lt;li&gt;This would be a completely new code path. Right now we tail the WALs, that necessarily happens after the write has already finished. We&apos;d need to invent a new way for this (possibly through coprocessor hooks).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Some database do synchronous replication to alleviate read load. HBase can already scale by just adding more boxes (granted, though, read of a particular data item would be served by a single server only - except for region replicas).&lt;/p&gt;

&lt;p&gt;Now, maybe there is a place for this doing intra-datacenter replication to keep a copy of the data in separate cluster (maybe for map/reduce), ever there I&apos;d ask: Why does it have to synchronous?&lt;/p&gt;

&lt;p&gt;Everything in HBase is idempotent. Phoenix adds more of that &quot;database&apos;y stuff&quot;, which is awesome, but comes with a loss of idempotency - such as in the issue raised above. I think we&apos;d be better served finding other solutions (stateless sequences for example), that is easier to reason about and more resilient against failure.&lt;/p&gt;

&lt;p&gt;None of this means we can/should not do this.&lt;/p&gt;</comment>
                            <comment id="14242143" author="apurtell" created="Thu, 11 Dec 2014 04:47:40 +0000"  >&lt;p&gt;It would be fantastic to have this capability in HBase, but I won&apos;t expect a patch forthcoming on this JIRA right away, it&apos;s a hard problem. &lt;/p&gt;

&lt;p&gt;In a naive implementation, the source would wait for the sink to ack before returning control back to the client. Performance and scalability of the cluster would be bounded by what is most likely a WAN link. True, I&apos;m making an assumption, but I think it would hold for the majority of possible use cases. WAN links don&apos;t have the reliability of datacenter networks. Every blip on the WAN would be a period of unavailability in the source cluster. Downtime in the sink would mean downtime in the source too, which would defeat the probable goal of service resiliency behind the introduction of replication in the first place. For something workable (resilient and performant) look to examples like Megastore and Spanner that make consensus work over wide area networks. &lt;/p&gt;</comment>
                            <comment id="14243763" author="jamestaylor" created="Fri, 12 Dec 2014 06:07:33 +0000"  >&lt;p&gt;What is you could define the replication message protocol to have a higher level of service for some HBase tables? So instead of replicating synchronously, you&apos;d have guaranteed delivery. It&apos;d be more expensive, but a useful option under some circumstances.&lt;/p&gt;</comment>
                            <comment id="14243865" author="lhofhansl" created="Fri, 12 Dec 2014 08:31:33 +0000"  >&lt;p&gt;You mean an ordering between tables or events. Hmm, also tricky as they might be hosted by different machines.&lt;br/&gt;
We should do some brainstorming.&lt;/p&gt;</comment>
                            <comment id="14243882" author="jamestaylor" created="Fri, 12 Dec 2014 08:48:40 +0000"  >&lt;p&gt;No, I mean it could still be asynchronously applied on the secondary&lt;br/&gt;
cluster, but it wouldn&apos;t return back to the client until it made it to&lt;br/&gt;
the point of submitting the change to the secondary cluster in a&lt;br/&gt;
durable queue (but before the change is actually applied). That way,&lt;br/&gt;
the secondary cluster would necessarily have to be up (i.e. it&lt;br/&gt;
wouldn&apos;t be a second point of failure). Only the queue would have to&lt;br/&gt;
be writable to. What&apos;s the protocol now?&lt;/p&gt;
</comment>
                            <comment id="14244524" author="lhofhansl" created="Fri, 12 Dec 2014 17:57:50 +0000"  >&lt;p&gt;That how it works now: The queue is the WAL. Once the edit is in the WAL it will be made visible to reader and it will eventually be shipped to the secondary.&lt;/p&gt;

&lt;p&gt;Is what you mean:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;put it in the queue (but it&apos;s &lt;b&gt;not&lt;/b&gt; visible to any readers)&lt;/li&gt;
	&lt;li&gt;only once the edit is applied on the secondary we make it visible in the primary&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;?&lt;/p&gt;</comment>
                            <comment id="14244650" author="jamestaylor" created="Fri, 12 Dec 2014 19:28:25 +0000"  >&lt;p&gt;That&apos;s not what I meant, but it&apos;s a good idea. I was looking for a stronger guarantee of replication (only for these &quot;special&quot; tables) in that once the server returns to the client, the client knows that the secondary cluster will have that change (regardless of what happens to the primary cluster). In other words, if the primary cluster were to get hit by a meteor right after the client received the newly increment value, it&apos;d be guaranteed that the secondary cluster would &lt;b&gt;eventually&lt;/b&gt; see the value. And by &lt;b&gt;eventually&lt;/b&gt;, I don&apos;t mean when the primary cluster comes back up, but &lt;b&gt;eventually&lt;/b&gt; as in &quot;if the secondary cluster was switched over to we could drain any durable queues containing replicated values before starting to take traffic.&quot; It&apos;s not as strong a contract as synchronous replication, as we wouldn&apos;t need it to be synchronous. We just would need a guarantee that it&apos;ll be applied to the secondary cluster before the primary cluster returns back to the client.&lt;/p&gt;</comment>
                            <comment id="14244668" author="apurtell" created="Fri, 12 Dec 2014 19:39:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;only once the edit is applied on the secondary we make it visible in the primary&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How does that work when HBase clients, upon returning from a write, do and should expect to read their own writes immediately? &lt;/p&gt;</comment>
                            <comment id="14244674" author="apurtell" created="Fri, 12 Dec 2014 19:41:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;eventually as in &quot;if the secondary cluster was switched over to we could drain any durable queues containing replicated values before starting to take traffic.&quot; &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is an interesting idea and it would reasonably handle the case where source to sink communication over the WAN is reliable and relatively low latency but maybe the queue of edits for application at the sink site is backed up due to local issues. What it won&apos;t handle well is (IMO, a quite common case) where the WAN link is not reliable and not low latency, so service we can provide to clients of the special tables - and any activity that depends on the special table update - is bounded by the characteristics of the WAN link not the local in-datacenter networks.&lt;/p&gt;</comment>
                            <comment id="14244681" author="jamestaylor" created="Fri, 12 Dec 2014 19:47:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;any activity that depends on the special table update - is bounded by the characteristics of the WAN link not the local in-datacenter networks.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, that&apos;d be the tradeoff, so you&apos;d only want to do it in special circumstances for tables that need this behavior. In particular, it may be ok for sequences because it&apos;s incremented once for a configurable batch size, so the cost is amortized over the batch size.&lt;/p&gt;</comment>
                            <comment id="14244781" author="apurtell" created="Fri, 12 Dec 2014 21:09:19 +0000"  >&lt;p&gt;Thinking on this a bit more:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;only once the edit is applied on the secondary we make it visible in the primary&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;How does that work when HBase clients, upon returning from a write, do and should expect to read their own writes immediately?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It would be strange, but we could make clients aware of the special aspect of the table, thus advising them the nature of reality has changed, and let them (i.e. the HBase client library) build synchronous semantics on top: write something, then spin until the last written edit becomes visible, then return control back to the application. We&apos;d need to ensure the sink reliably acks edits and checks that the source acked the ack, and retransmit acks if not, for reliable signal delivery for making &quot;pending&quot; edits visible.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 11 Dec 2014 04:38:03 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i23b33:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>