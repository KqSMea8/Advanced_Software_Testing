<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:20:42 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-4633/HBASE-4633.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-4633] Potential memory leak in client RPC timeout mechanism</title>
                <link>https://issues.apache.org/jira/browse/HBASE-4633</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Relevant Jiras: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2937&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-2937&lt;/a&gt;,&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4003&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-4003&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We have been using the &apos;hbase.client.operation.timeout&apos; knob&lt;br/&gt;
introduced in 2937 for quite some time now. It helps us enforce SLA.&lt;br/&gt;
We have two HBase clusters and two HBase client clusters. One of them&lt;br/&gt;
is much busier than the other.&lt;/p&gt;

&lt;p&gt;We have seen a deterministic behavior of clients running in busy&lt;br/&gt;
cluster. Their (client&apos;s) memory footprint increases consistently&lt;br/&gt;
after they have been up for roughly 24 hours.&lt;br/&gt;
This memory footprint almost doubles from its usual value (usual case&lt;br/&gt;
== RPC timeout disabled). After much investigation nothing concrete&lt;br/&gt;
came out and we had to put a hack&lt;br/&gt;
which keep heap size in control even when RPC timeout is enabled. Also&lt;br/&gt;
note , the same behavior is not observed in &apos;not so busy&lt;br/&gt;
cluster.&lt;/p&gt;

&lt;p&gt;The patch is here : &lt;a href=&quot;https://gist.github.com/1288023&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/1288023&lt;/a&gt;&lt;/p&gt;</description>
                <environment>&lt;p&gt;HBase version: 0.90.3 + Patches , Hadoop version: CDH3u0&lt;/p&gt;</environment>
        <key id="12527865">HBASE-4633</key>
            <summary>Potential memory leak in client RPC timeout mechanism</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="5">Cannot Reproduce</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="shrijeet">Shrijeet Paliwal</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Oct 2011 01:58:22 +0000</created>
                <updated>Wed, 16 Nov 2016 21:02:50 +0000</updated>
                            <resolved>Wed, 16 Nov 2016 21:02:49 +0000</resolved>
                                    <version>0.90.3</version>
                                                    <component>Client</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                <comments>
                            <comment id="13131347" author="liyin" created="Thu, 20 Oct 2011 04:03:38 +0000"  >&lt;p&gt;I have also noticed some memory leak problems in HBase client.&lt;br/&gt;
Our symptom is that the memory footprint will increase as time. But the actual heap size of the client is not increasing.&lt;br/&gt;
The leak should come from non-heap memory.&lt;br/&gt;
But Not sure the leak comes from HBase Client jar itself or just our client code.&lt;br/&gt;
So I am very interested to know when you have keep the heap size in control, is the memory leaking solved ?&lt;/p&gt;</comment>
                            <comment id="13131677" author="shrijeet" created="Thu, 20 Oct 2011 15:01:12 +0000"  >&lt;p&gt;@Liyin, &lt;br/&gt;
Are you using RPC timeouts for client operations? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But Not sure the leak comes from HBase Client jar itself or just our client code. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In the absence of a concrete evidence that leak is indeed in HBase client jar, I have similar feeling. It could be in our application layer. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Our symptom is that the memory footprint will increase as time. But the actual heap size of the client is not increasing.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We observe the used memory using a collectd plugin &lt;a href=&quot;http://collectd.org/wiki/index.php/Plugin:Memory&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://collectd.org/wiki/index.php/Plugin:Memory&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So I am very interested to know when you have keep the heap size in control, is the memory leaking solved ?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We run with max and min memory set as -Xmx2&lt;/p&gt;
{X}G -Xms{X}
&lt;p&gt;G . And when &apos;leak&apos; happens the plugin shows the used memory touching 2X value, so it does seem heap size is increasing. Correct me here if I am mistaken. &lt;/p&gt;

&lt;p&gt;Let me know if you need more inputs. &lt;/p&gt;</comment>
                            <comment id="13131771" author="stack" created="Thu, 20 Oct 2011 17:04:09 +0000"  >&lt;p&gt;@Shrijeet I wonder if you ran the client with:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-XX:-HeapDumpOnOutOfMemoryError	Dump heap to file when java.lang.OutOfMemoryError is thrown. Manageable. (Introduced in 1.4.2 update 12, 5.0 update 7.)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... if the dump would reveal anything interesting.  I don&apos;t mind digging in heap if you get one (put it somewhere I can pull it).&lt;/p&gt;</comment>
                            <comment id="13131786" author="shrijeet" created="Thu, 20 Oct 2011 17:18:40 +0000"  >&lt;p&gt;@Stack&lt;br/&gt;
No we did not run with that flag. Also we never got to a point when application had to die cause of OOM. The reasons (I guess) are :&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;We have GC flags to do garbage collection as fast as possible.&lt;/li&gt;
	&lt;li&gt;The monitoring in place starts sending our alerts and we usually shoot the server in the head before it OOMs&lt;/li&gt;
	&lt;li&gt;The load balancer will kick in and start sending no work to application server realizing it is in bad state.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;As mentioned earlier I have found it hard to reproduce in dev environment, failing to simulate production like load. But I must try again when.&lt;/p&gt;</comment>
                            <comment id="13131791" author="stack" created="Thu, 20 Oct 2011 17:22:34 +0000"  >&lt;p&gt;Dang.&lt;/p&gt;</comment>
                            <comment id="13162475" author="shrijeet" created="Sun, 4 Dec 2011 20:48:10 +0000"  >&lt;p&gt;Recent updates: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;In my case the leak/memory-hold is not in HBase client. I could not find enough evidence to conclude that. What I did find is, our application holds one heavy object in memory. This object is shared between threads. Every N minutes the application creates a new instance of this class. Unless any thread is still holding on to an old instance, all old instances are GCed in time. Hence in theory at any time there should be only one active instance of heavy object.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Under heavy load and client operation RPC timeout enabled, some threads get stuck. This causes multiple instances of heavy object. In turn heap grows.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;After reading client code multiple times I can not gather why there will be a case when application thread will get stuck for several minutes. We have safe guards to clean up calls &apos;forcefully&apos; if they have been alive for more than rpc timeout interval. &lt;/p&gt;

&lt;p&gt;I had planned to update the title of Jira to reflect above finding but Gaojinchao observed something interesting at his end and so keeping title same for now. Gaojinchao&apos;s thread is here: &lt;a href=&quot;http://search-hadoop.com/m/teczL8KvcH&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://search-hadoop.com/m/teczL8KvcH&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13162536" author="sunnygao" created="Mon, 5 Dec 2011 00:43:43 +0000"  >&lt;p&gt;I attached the stack picture.&lt;/p&gt;</comment>
                            <comment id="13162538" author="sunnygao" created="Mon, 5 Dec 2011 00:51:50 +0000"  >&lt;p&gt;Hbase version is 0.90.4 + patch.&lt;br/&gt;
Cluseter number is 10&lt;br/&gt;
One HBase client process includes 50 threads, So the max threads connect to the RS is (50 * RS number).&lt;/p&gt;

&lt;p&gt;I have noticed some memory leak problems in my HBase client.&lt;br/&gt;
RES has increased to 27g&lt;br/&gt;
PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND&lt;br/&gt;
12676 root      20   0 30.8g  27g 5092 S    2 57.5 587:57.76 /opt/java/jre/bin/java -Djava.library.path=lib/.&lt;/p&gt;

&lt;p&gt;But I am not sure the leak comes from HBase Client jar itself or just our client code.&lt;/p&gt;

&lt;p&gt;This is some parameters of jvm.&lt;br/&gt;
:-Xms15g -Xmn12g -Xmx15g -XX:PermSize=64m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=65 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=1 -XX:+CMSParallelRemarkEnabled &lt;/p&gt;</comment>
                            <comment id="13162541" author="sunnygao" created="Mon, 5 Dec 2011 00:57:56 +0000"  >&lt;p&gt;This issue appears to be a configuration problem:&lt;br/&gt;
1. HBase client uses NIO(socket) API that uses the direct memory.&lt;br/&gt;
2. Default -XXMaxDirectMemorySize value is equal to -Xmx value, So if there doesn&apos;t have &quot;full gc&quot;, all direct memory can&apos;t reclaim. Unfortunately, using GC confiugre parameter of our client doesn&apos;t produce any &quot;full gc&lt;/p&gt;
</comment>
                            <comment id="13162596" author="sunnygao" created="Mon, 5 Dec 2011 04:26:17 +0000"  >&lt;p&gt;I have tested, Memory does not increase when specified MaxDirectMemorySize with moderate value.&lt;/p&gt;

&lt;p&gt;In my cluster, nearly one hours , trigger a full GC. look this logs:&lt;br/&gt;
10022.210: [Full GC (System) 10022.210: &lt;span class=&quot;error&quot;&gt;&amp;#91;Tenured: 577566K-&amp;gt;257349K(1048576K), 1.7515610 secs&amp;#93;&lt;/span&gt; 9651924K-&amp;gt;257349K(14260672K), &lt;span class=&quot;error&quot;&gt;&amp;#91;Perm : 19161K-&amp;gt;19161K(65536K)&amp;#93;&lt;/span&gt;, 1.7518350 secs] &lt;span class=&quot;error&quot;&gt;&amp;#91;Times: user=1.75 sys=0.00, real=1.75 secs&amp;#93;&lt;/span&gt; .........&lt;/p&gt;

&lt;p&gt;.........&lt;br/&gt;
13532.930: [GC 13532.931: &lt;span class=&quot;error&quot;&gt;&amp;#91;ParNew: 12801558K-&amp;gt;981626K(13212096K), 0.1414370 secs&amp;#93;&lt;/span&gt; 13111752K-&amp;gt;1291828K(14260672K), 0.1416880 secs] &lt;span class=&quot;error&quot;&gt;&amp;#91;Times: user=1.90 sys=0.01, real=0.14 secs&amp;#93;&lt;/span&gt;&lt;br/&gt;
13624.630: [Full GC (System) 13624.630: &lt;span class=&quot;error&quot;&gt;&amp;#91;Tenured: 310202K-&amp;gt;175378K(1048576K), 1.9529280 secs&amp;#93;&lt;/span&gt; 11581276K-&amp;gt;175378K(14260672K), &lt;span class=&quot;error&quot;&gt;&amp;#91;Perm : 19225K-&amp;gt;19225K(65536K)&amp;#93;&lt;/span&gt;, 1.9531660 secs] &lt;br/&gt;
           &lt;span class=&quot;error&quot;&gt;&amp;#91;Times: user=1.94 sys=0.00, real=1.96 secs&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; I monitored the memory. It is stable.&lt;/p&gt;

&lt;p&gt; 7543 root      20   0 16.9g  15g 9892 S    1 33.0   1258:59 java&lt;br/&gt;
 7543 root      20   0 16.9g  15g 9892 S    0 33.0   1258:59 java&lt;br/&gt;
 7543 root      20   0 16.9g  15g 9892 S    1 33.0   1258:59 java&lt;br/&gt;
 7543 root      20   0 16.9g  15g 9892 S    0 33.0   1258:59 java&lt;br/&gt;
 7543 root      20   0 16.9g  15g 9892 S    1 33.0   1258:59 java&lt;br/&gt;
 7543 root      20   0 16.9g  15g 9892 S    1 33.0   1259:00 java&lt;/p&gt;</comment>
                            <comment id="13162896" author="stack" created="Mon, 5 Dec 2011 17:44:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;Default -XXMaxDirectMemorySize value is equal to -Xmx value....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;According to this, &lt;a href=&quot;http://stackoverflow.com/questions/3773775/default-for-xxmaxdirectmemorysize&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/3773775/default-for-xxmaxdirectmemorysize&lt;/a&gt;, default max is 64M (which is still too big).   Should we set this to 1M as our default?&lt;/p&gt;
</comment>
                            <comment id="13162897" author="stack" created="Mon, 5 Dec 2011 17:46:49 +0000"  >&lt;p&gt;For completeness, here is Jonathan Payne&apos;s note on this phenomeon: &lt;a href=&quot;http://grokbase.com/t/hbase.apache.org/dev/2011/10/interesting-note-on-hbase-client-from-asynchbase-list-fwd-standard-hbase-client-asynchbase-client-netty-and-direct-memory-buffers/22e7lummyzqwfdgh35n6hxe7ubxm&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://grokbase.com/t/hbase.apache.org/dev/2011/10/interesting-note-on-hbase-client-from-asynchbase-list-fwd-standard-hbase-client-asynchbase-client-netty-and-direct-memory-buffers/22e7lummyzqwfdgh35n6hxe7ubxm&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13726222" author="asafm" created="Thu, 1 Aug 2013 08:37:17 +0000"  >&lt;p&gt;So eventually, this issue wasn&apos;t solved? We&apos;re experiencing this brutally in production with 0.94.7&lt;/p&gt;</comment>
                            <comment id="13728759" author="asafm" created="Sun, 4 Aug 2013 04:37:19 +0000"  >&lt;p&gt;Just adding the comments made in #4956 to this issue for who ever read this issue first:&lt;br/&gt;
In Jonathan stack-trace you can see that BufferedOutputStream is used. This constitute that the writes are made 8k at a time, thus the ThreadLocal buffers suggested here should be no more than 8k, thus can&apos;t possibly lead to the memory leak.&lt;/p&gt;</comment>
                            <comment id="13730323" author="stack" created="Tue, 6 Aug 2013 04:12:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=asafm&quot; class=&quot;user-hover&quot; rel=&quot;asafm&quot;&gt;Asaf Mesika&lt;/a&gt; What is going on?  You having mem issues?  The 8k buffer size trick ain&apos;t cutting it for you?&lt;/p&gt;</comment>
                            <comment id="13731633" author="asafm" created="Wed, 7 Aug 2013 04:44:17 +0000"  >&lt;p&gt;If running with -Xmx6GB our application running HBase Client, then we are witnessing process resident memory increasing to 10GB. When placing -X:MaxDirectMemorySize=2g, we&apos;re seeing WARN OutOfMemoryException, but no error since the client has 10 retries. Our next step is to run it with profiler to diagnose why so much direct memory is used. Want to make sure its HBaseClient code and not some other third party lib we&apos;re using (although we&apos;re pretty sure its the only one calling ByteBuffer.allocateDirect transitively).&lt;/p&gt;</comment>
                            <comment id="15671621" author="stack" created="Wed, 16 Nov 2016 21:02:50 +0000"  >&lt;p&gt;RPC is very different now. Resolving as cannot repro.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12533914">HBASE-4956</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12506072" name="HBaseclientstack.png" size="29875" author="sunnygao" created="Mon, 5 Dec 2011 00:43:43 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 20 Oct 2011 04:03:38 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>92132</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02bhz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11464</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>