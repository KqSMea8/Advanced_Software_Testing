<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:23:29 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-4956/HBASE-4956.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-4956] Control direct memory buffer consumption by HBaseClient</title>
                <link>https://issues.apache.org/jira/browse/HBASE-4956</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;As Jonathan explained here &lt;a href=&quot;https://groups.google.com/group/asynchbase/browse_thread/thread/c45bc7ba788b2357?pli=1&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://groups.google.com/group/asynchbase/browse_thread/thread/c45bc7ba788b2357?pli=1&lt;/a&gt; , standard hbase client inadvertently consumes large amount of direct memory.&lt;/p&gt;

&lt;p&gt;We should consider using netty for NIO-related tasks.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12533914">HBASE-4956</key>
            <summary>Control direct memory buffer consumption by HBaseClient</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bcopeland">Bob Copeland</assignee>
                                    <reporter username="yuzhihong@gmail.com">Ted Yu</reporter>
                        <labels>
                    </labels>
                <created>Mon, 5 Dec 2011 18:48:53 +0000</created>
                <updated>Thu, 24 Sep 2015 15:39:38 +0000</updated>
                            <resolved>Thu, 19 Jul 2012 21:31:00 +0000</resolved>
                                                    <fixVersion>0.94.1</fixVersion>
                    <fixVersion>0.95.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>13</watches>
                                                                <comments>
                            <comment id="13164188" author="lhofhansl" created="Wed, 7 Dec 2011 06:47:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt; should help with too, as it would allow the overall set of threads that any HTable can use to perform IO (at least of batch operations).&lt;/p&gt;</comment>
                            <comment id="13164199" author="lhofhansl" created="Wed, 7 Dec 2011 07:17:47 +0000"  >&lt;p&gt;We might not have to go all the way to use netty (although that would be nice).&lt;br/&gt;
If we find that it is possible to avoid calling HBaseClient.Connection.sendParam from the client thread, but have the call actually be made from the Connection thread (after the callable representing the operation was queued), we have limited the number of threads that will have cached DirectBuffer on their behalf.&lt;br/&gt;
Stack suggested the only reason for the direct call might be to pass errors back to the client. We could hand the client a Deferred or Future that will eventually hold any encountered exception, the client could (and would by default to keep the current synchronous behavior) also wait on that object.&lt;/p&gt;</comment>
                            <comment id="13164798" author="zhihyu@ebaysf.com" created="Wed, 7 Dec 2011 22:49:08 +0000"  >&lt;p&gt;Since the proposal involves asynchronous communication, we should devise new API which can be used to validate the reduction in use of direct memory buffer.&lt;/p&gt;</comment>
                            <comment id="13173917" author="lhofhansl" created="Wed, 21 Dec 2011 07:04:32 +0000"  >&lt;p&gt;Just checked the HTable code in trunk... Puts (even single put operations) already go through a thread from the HTable&apos;s thread pool. A single get request is issuing a request directly, but get(List&amp;lt;Get&amp;gt;) is also using the pool.&lt;br/&gt;
So limiting the number of threads in a single thread pool and using the new HTable constructor from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt;, this can be controlled. Need to make one is using only get(List&amp;lt;Get&amp;gt;) in addition to a relatively small global threadpool.&lt;br/&gt;
At Salesforce we&apos;ll shoot for pool with ~100 threads and a waiting queue of about 100 as well (for very large clusters that may be too small though).&lt;/p&gt;</comment>
                            <comment id="13174884" author="stack" created="Thu, 22 Dec 2011 16:32:46 +0000"  >&lt;p&gt;@Lars So supposition is that we bound the number of direct buffers client allocates by bounding the number of threads in the thread pool (and have all accesses go via the HTable thread pool)?  If so, sounds good.  What happens if as many HTable instances as there are application-level threads and say the application spawns lots of threads?  The application should then go via HTablePool?  Or rather, you are suggesting that application uses new &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt; api and pass common executor to all HTable instances?&lt;/p&gt;</comment>
                            <comment id="13174892" author="zhihyu@ebaysf.com" created="Thu, 22 Dec 2011 16:53:04 +0000"  >&lt;p&gt;Currently lily-project uses a hack to share ExecutorService among all HTable instances.&lt;br/&gt;
Once &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5084&quot; title=&quot;Allow different HTable instances to share one ExecutorService&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5084&quot;&gt;&lt;del&gt;HBASE-5084&lt;/del&gt;&lt;/a&gt; is integrated, client application would have full control over reusing ExecutorService across all HTable instances.&lt;/p&gt;</comment>
                            <comment id="13181872" author="lhofhansl" created="Sat, 7 Jan 2012 05:44:12 +0000"  >&lt;p&gt;@Stack and Ted: Yes, if the application uses the API introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt; the number of threads doing network ops is limited to a single thread pools (that can be sized accordingly).&lt;/p&gt;

&lt;p&gt;Still not ideal. The regionserver side of things does chunking (8k chunks by default), so even if a lot of buffers are cached for a lot of threads it won&apos;t add to much memory - even for 1000 threads each caching 3 buffer you&apos;d only use 24mb, and since they are the same size they are reusable). Maybe the client do the same.&lt;/p&gt;</comment>
                            <comment id="13182851" author="lhofhansl" created="Mon, 9 Jan 2012 22:09:03 +0000"  >&lt;p&gt;Looking at the stacktrace that Jonathan posted to the asynchhabse group:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        java.nio.Buffer.&amp;lt;init&amp;gt;(Buffer.java:172)
        java.nio.ByteBuffer.&amp;lt;init&amp;gt;(ByteBuffer.java:259)
        java.nio.ByteBuffer.&amp;lt;init&amp;gt;(ByteBuffer.java:267)
        java.nio.MappedByteBuffer.&amp;lt;init&amp;gt;(MappedByteBuffer.java:64)
        java.nio.DirectByteBuffer.&amp;lt;init&amp;gt;(DirectByteBuffer.java:97)
        java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288)
        sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:155)
        sun.nio.ch.IOUtil.write(IOUtil.java:37)
        sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
        org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
        org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
--&amp;gt;     java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
--&amp;gt;     java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        java.io.DataOutputStream.flush(DataOutputStream.java:106)
        org.apache.hadoop.hbase.ipc.HBaseClient$Connection.sendParam(HBaseClient.java:518)
        org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:751)
        org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
        $Proxy11.getProtocolVersion(&amp;lt;Unknown Source&amp;gt;:Unknown line) 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Since the writes go through a BufferOutputStream they are already limited to 8k chunks (default buffer size).&lt;br/&gt;
So I do not really know what the problem is... In order to make this any kind of problem you need have buffers cached for 1m threads (to get to 8GB).&lt;br/&gt;
Anyway with the optional global ExecutorService of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt; this should no longer be a problem.&lt;/p&gt;</comment>
                            <comment id="13182908" author="stack" created="Mon, 9 Jan 2012 23:05:10 +0000"  >&lt;p&gt;Hmmm... Looking in code, we allocate a BufferedOutputStream using default buffer size:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.out = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DataOutputStream
            (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BufferedOutputStream(NetUtils.getOutputStream(socket)));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... so, it seems that yeah, at any one time, you&apos;d think the maximum allocation 8k as you say Lars.&lt;/p&gt;

&lt;p&gt;Jonathan says 3 of these buffers allocated when reading.  Presume similar writing (easy to check I suppose.  I havent&apos;).  Thats 6*8k per thread since these are thread local.... which still don&apos;t seem like that much.  You&apos;d need lots of threads to run into trouble.&lt;/p&gt;

&lt;p&gt;We need to reproduce.&lt;/p&gt;</comment>
                            <comment id="13267797" author="bcopeland" created="Thu, 3 May 2012 21:03:04 +0000"  >&lt;p&gt;I have this bug on our servers (0.90.4-cdh3).  I think Johnathan&apos;s analysis was spot on, but it matters much more for read path since readFully() allocates a direct buffer as large as the target read.&lt;/p&gt;

&lt;p&gt;Here&apos;s an easy way to reproduce:&lt;/p&gt;

&lt;p&gt;1) create a ~10 meg row in some table&lt;br/&gt;
2) edit bin/hbase script to have max direct memory size &amp;lt; 10m, e.g.:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+JAVA_HEAP_MAX=&lt;span class=&quot;code-quote&quot;&gt;&quot;-Xmx1000m -XX:MaxDirectMemorySize=1m&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;3) now have any client try to read this row.  If read side were chunking with &amp;lt; 1m pieces, it should still be able to read.&lt;/p&gt;

&lt;p&gt;Obviously this example is contrived, but we hit the problem with around 1-2000 threads (we have large rows in our DB).  Here&apos;s what our stack trace looks like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:633)
        at java.nio.DirectByteBuffer.&amp;lt;init&amp;gt;(DirectByteBuffer.java:98)
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288)
        at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:155)
        at sun.nio.ch.IOUtil.read(IOUtil.java:174)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:55)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        at java.io.FilterInputStream.read(FilterInputStream.java:116)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:299)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.hbase.client.Result.readFields(Result.java:504)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:521)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readFields(HbaseObjectWritable.java:259)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:554)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:477)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following works around the problem by only doing 8k allocations:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
diff --git a/src/main/java/org/apache/hadoop/hbase/client/Result.java b/src/main/java/org/apache/hadoop/hbase/client/Result.java
index 8a0c1a9..378ca88 100644
--- a/src/main/java/org/apache/hadoop/hbase/client/Result.java
+++ b/src/main/java/org/apache/hadoop/hbase/client/Result.java
@@ -502,10 +502,18 @@ &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class Result &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; Writable, WritableWithSize {
       &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
     }
     &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; [] raw = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[totalBuffer];
-    in.readFully(raw, 0, totalBuffer);
+    readChunked(in, raw, 0, totalBuffer);
     bytes = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ImmutableBytesWritable(raw, 0, totalBuffer);
   }
 
+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void readChunked(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; DataInput in, &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] dest, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; ofs, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; len)
+  &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
+    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; maxread = 8192;
+
+    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (; ofs &amp;lt; len; ofs += maxread)
+      in.readFully(dest, ofs, &lt;span class=&quot;code-object&quot;&gt;Math&lt;/span&gt;.min(len - ofs, maxread));
+  }
+
   &lt;span class=&quot;code-comment&quot;&gt;//Create KeyValue[] when needed
&lt;/span&gt;   &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void readFields() {
     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (bytes == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13267832" author="lhofhansl" created="Thu, 3 May 2012 21:39:27 +0000"  >&lt;p&gt;This looks like a good approach. I&apos;d be OK committing this (after passing tests, etc).&lt;/p&gt;</comment>
                            <comment id="13267838" author="lhofhansl" created="Thu, 3 May 2012 21:45:02 +0000"  >&lt;p&gt;Potentially every call to readFully over the wire is suspect. Just did a quick check. It&apos;s used in Put/Append/KeyValue/Bytes/etc/etc.&lt;/p&gt;</comment>
                            <comment id="13267839" author="tlipcon" created="Thu, 3 May 2012 21:46:17 +0000"  >&lt;p&gt;I&apos;d think we should make the max read configurable. Otherwise we should do some benchmarks to show that this doesn&apos;t cause a regression. My worry is that this would translate to a bunch more syscalls, no?&lt;/p&gt;</comment>
                            <comment id="13267849" author="lhofhansl" created="Thu, 3 May 2012 21:51:39 +0000"  >&lt;p&gt;Yeah, we should quantify this. 8k seems like a very reasonable chunk size, though (probably anything between 1k and 64k would be OK &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ).&lt;/p&gt;

&lt;p&gt;@Bob: Any chance you can whip up a quick performance test for this?&lt;/p&gt;</comment>
                            <comment id="13268435" author="bcopeland" created="Fri, 4 May 2012 15:16:23 +0000"  >&lt;p&gt;With regard to readFully() being everywhere &amp;#8211; perhaps it would be less intrusive to subclass DataInput, or to modify one of the read functions further down in the stack like org.apache.hadoop.net.SocketInputStream.read() to do the chunking.  You guys know the code better than I.&lt;/p&gt;

&lt;p&gt;These numbers are for standalone mode.  I can&apos;t easily do a test on our distributed cluster but I don&apos;t think it would change the numbers much.  I&apos;ll upload my test script and then anyone who wants can repeat the experiment on their setup.&lt;/p&gt;

&lt;p&gt;Each trial consists of a run of &quot;hbase org.jruby.Main thread_get.rb 1&quot;, which does 100 gets of a ~10 MB row, 3 times in a row, computing avg &amp;amp; stddev of wall-clock time.  Maxread variable is changed for every run.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
size            avg          stddev
~~~~         ~~~~~~          ~~~~~~
1            33.690             .10
1k            3.072             .18
2k            3.395             .19
4k            3.493             .16
8k            3.306             .19
16k           2.918             .21
32k           2.835             .06
64k           2.689             .08
128k          3.048             .03
len           3.328             .08
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13268439" author="bcopeland" created="Fri, 4 May 2012 15:19:32 +0000"  >&lt;p&gt;Test script for reproducing the error (with max direct memory limits in place) or benchmarking different chunk sizes.  Although it can do threading, I have mostly passed &quot;1&quot; to do the tests in a single thread.&lt;/p&gt;</comment>
                            <comment id="13418330" author="bcopeland" created="Thu, 19 Jul 2012 14:43:02 +0000"  >&lt;p&gt;Just a follow-up &amp;#8211; we&apos;ve been running with this patch (8k window size) for a couple of months now, and have no more OOMs, and no noticeable performance or functionality regressions.&lt;/p&gt;</comment>
                            <comment id="13418348" author="zhihyu@ebaysf.com" created="Thu, 19 Jul 2012 15:09:51 +0000"  >&lt;p&gt;Patch from Bob.&lt;/p&gt;</comment>
                            <comment id="13418383" author="hadoopqa" created="Thu, 19 Jul 2012 15:58:16 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12537187/4956.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12537187/4956.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 12 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2410//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13418384" author="lhofhansl" created="Thu, 19 Jul 2012 15:58:19 +0000"  >&lt;p&gt;Thanks Bob! I think we should commit this. To 0.94 and 0.96 at least.&lt;br/&gt;
If there&apos;re no objections I&apos;ll do that.&lt;/p&gt;

&lt;p&gt;From your analysis it looks like 32k would be the sweet spot, but since you ran it in production with an 8k window, let&apos;s commit this.&lt;/p&gt;</comment>
                            <comment id="13418394" author="bcopeland" created="Thu, 19 Jul 2012 16:05:57 +0000"  >&lt;p&gt;Yeah, buffer size doesn&apos;t really matter to our workload so out of laziness I stuck with 8k, but a larger one should be OK.  Worth noting that 8k is better than the full length (i.e. unpatched) in my tests, presumably because of the additional allocation overhead.&lt;/p&gt;</comment>
                            <comment id="13418646" author="zhihyu@ebaysf.com" created="Thu, 19 Jul 2012 20:32:29 +0000"  >&lt;p&gt;Patch integrated to trunk.&lt;/p&gt;

&lt;p&gt;Thanks for the patch, Bob.&lt;/p&gt;

&lt;p&gt;Thanks for the review, Lars.&lt;/p&gt;</comment>
                            <comment id="13418655" author="zhihyu@ebaysf.com" created="Thu, 19 Jul 2012 20:38:43 +0000"  >&lt;p&gt;Integrated to 0.94 since this JIRA is marked against 0.94.1 release.&lt;/p&gt;</comment>
                            <comment id="13418657" author="lhofhansl" created="Thu, 19 Jul 2012 20:42:41 +0000"  >&lt;p&gt;Thanks Ted.&lt;/p&gt;</comment>
                            <comment id="13418680" author="hudson" created="Thu, 19 Jul 2012 21:25:36 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3153 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3153/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3153/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4956&quot; title=&quot;Control direct memory buffer consumption by HBaseClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4956&quot;&gt;&lt;del&gt;HBASE-4956&lt;/del&gt;&lt;/a&gt; Control direct memory buffer consumption by HBaseClient (Bob Copeland) (Revision 1363526)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/client/Result.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13418683" author="lhofhansl" created="Thu, 19 Jul 2012 21:30:48 +0000"  >&lt;p&gt;Added you as contributor Bob.&lt;/p&gt;</comment>
                            <comment id="13418705" author="hudson" created="Thu, 19 Jul 2012 21:50:20 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #341 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/341/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/341/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4956&quot; title=&quot;Control direct memory buffer consumption by HBaseClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4956&quot;&gt;&lt;del&gt;HBASE-4956&lt;/del&gt;&lt;/a&gt; Control direct memory buffer consumption by HBaseClient (Bob Copeland) (Revision 1363533)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/client/Result.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13418797" author="hudson" created="Thu, 19 Jul 2012 23:36:51 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #100 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/100/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/100/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4956&quot; title=&quot;Control direct memory buffer consumption by HBaseClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4956&quot;&gt;&lt;del&gt;HBASE-4956&lt;/del&gt;&lt;/a&gt; Control direct memory buffer consumption by HBaseClient (Bob Copeland) (Revision 1363526)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/client/Result.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13419652" author="hudson" created="Fri, 20 Jul 2012 22:53:20 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #44 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/44/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/44/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4956&quot; title=&quot;Control direct memory buffer consumption by HBaseClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4956&quot;&gt;&lt;del&gt;HBASE-4956&lt;/del&gt;&lt;/a&gt; Control direct memory buffer consumption by HBaseClient (Bob Copeland) (Revision 1363533)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/client/Result.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13428750" author="hudson" created="Sun, 5 Aug 2012 00:51:16 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security-on-Hadoop-23 #6 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/6/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/6/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4956&quot; title=&quot;Control direct memory buffer consumption by HBaseClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4956&quot;&gt;&lt;del&gt;HBASE-4956&lt;/del&gt;&lt;/a&gt; Control direct memory buffer consumption by HBaseClient (Bob Copeland) (Revision 1363533)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/client/Result.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12527865">HBASE-4633</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12834145">HBASE-13819</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12531712">HBASE-4805</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12534135">HBASE-4970</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12455264">HBASE-2182</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12498300">HBASE-3523</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12537187" name="4956.txt" size="895" author="zhihyu@ebaysf.com" created="Thu, 19 Jul 2012 15:09:51 +0000"/>
                            <attachment id="12525631" name="thread_get.rb" size="2777" author="bcopeland" created="Fri, 4 May 2012 15:19:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 7 Dec 2011 06:47:07 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>219640</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 19 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08rx3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>49123</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12310230" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Tags</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.96notable</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>