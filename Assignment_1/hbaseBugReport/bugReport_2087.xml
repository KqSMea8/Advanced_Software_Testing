<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:58:51 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2087/HBASE-2087.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2087] The wait on compaction because &quot;Too many store files&quot; holds up all flushing</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2087</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;The method MemStoreFlusher#checkStoreFileCount is called from flushRegion.  flushRegion is called by MemStoreFlusher#run thread.  If the checkStoreFileCount finds too many store files, it&apos;ll stick around waiting on a compaction to happen.  While its hanging, the MemStoreFlusher#run is held up.  No other region can flush.  Meantime WALs will be rolling and memory will be accumulating writes.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12444468">HBASE-2087</key>
            <summary>The wait on compaction because &quot;Too many store files&quot; holds up all flushing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jdcryans">Jean-Daniel Cryans</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Jan 2010 00:26:33 +0000</created>
                <updated>Fri, 12 Oct 2012 06:14:57 +0000</updated>
                            <resolved>Fri, 2 Apr 2010 00:55:44 +0000</resolved>
                                                    <fixVersion>0.20.4</fixVersion>
                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12795715" author="jdcryans" created="Fri, 1 Jan 2010 01:02:27 +0000"  >&lt;p&gt;So either we have too much WALs or too much store files right? Like I said in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2053&quot; title=&quot;Upper bound of outstanding WALs can be overrun&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2053&quot;&gt;&lt;del&gt;HBASE-2053&lt;/del&gt;&lt;/a&gt;, our WAL is set very small so that master splits fast and we don&apos;t lose data. In 0.21 we won&apos;t lose data so speeding up the spit time then set a higher/bigger WAL would solve this problem?&lt;/p&gt;</comment>
                            <comment id="12795716" author="jdcryans" created="Fri, 1 Jan 2010 01:07:34 +0000"  >&lt;p&gt;Another thing to keep in mind, flushing incomplete memstores is highly inefficient. Let&apos;s say you want to drop the number of WALs by flushing 10 regions. Those are probably not full, maybe 2MB or 10MB big, but they still take time to flush and clogger HDFS with even more new files. Those files then have to be compacted, it&apos;s even worse if we hit the &quot;Too many store files&quot; problem and it&apos;s likely that one causes the other.&lt;/p&gt;</comment>
                            <comment id="12796046" author="stack" created="Sun, 3 Jan 2010 22:51:14 +0000"  >&lt;p&gt;@J-D: &quot;...flushing incomplete memstores is highly inefficent..&quot;  ... yeah but if the edit is old, its probably worth the flush if you take a systems view.  And this issue is about something else anyway, never holding up flushes.  Should we open a blanket issue in which we discuss undoing &quot;compensating&quot; changes now hdfs has a working sync; i.e.undo all the weird stuff we did to try and minimize losing edits when there was no working sync.&lt;/p&gt;</comment>
                            <comment id="12796057" author="apurtell" created="Mon, 4 Jan 2010 01:09:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;Should we open a blanket issue in which we discuss undoing &quot;compensating&quot; changes now hdfs has a working sync&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Like we did with the compaction limiting thread and region server &quot;safe mode&quot; after the transition to 0.20.&lt;/p&gt;</comment>
                            <comment id="12796078" author="jdcryans" created="Mon, 4 Jan 2010 04:49:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;And this issue is about something else anyway, never holding up flushes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As I said in my first comment, it&apos;s either too much WALs or too much store files. If we let all flushes go then we are overrun by store files. If we force flush memstores to be able to roll WALs then we easily create too much store files. We have seen stores that needed to compact 100 files and this is why we have a limit.&lt;/p&gt;

&lt;p&gt;So, I question the feasibility of this jira.&lt;/p&gt;

&lt;p&gt;In the particular case of WALs waiting on flushes waiting on too many store files, what I said is that it&apos;s by setting a very low number of WALs that we easily hit the limit. Setting it to a higher number means less chance of hitting this jira&apos;s problem, hence making it invalid?&lt;/p&gt;</comment>
                            <comment id="12796271" author="stack" created="Mon, 4 Jan 2010 18:11:25 +0000"  >&lt;p&gt;The problem this issue covers is case where a regionserver has say 1k regions and it so happens that one of these is over the store file upper limit.  As is all flushing on the regionserver is held up because one region is over the limit.   Because no flushing we will block writes and so on&lt;/p&gt;</comment>
                            <comment id="12796283" author="jdcryans" created="Mon, 4 Jan 2010 18:38:12 +0000"  >&lt;p&gt;Oh right I didn&apos;t see it like that. Yes we don&apos;t want to hold flushes for every region, just those concerned.&lt;/p&gt;</comment>
                            <comment id="12833684" author="stack" created="Mon, 15 Feb 2010 05:26:08 +0000"  >&lt;p&gt;Moving into 0.20.4.&lt;/p&gt;</comment>
                            <comment id="12835521" author="stack" created="Fri, 19 Feb 2010 00:20:12 +0000"  >&lt;p&gt;I was going to explore blocking the problematic store only by removing its flush request from the flush queue readding it later after the timer elapses (or after compaction completes)&lt;/p&gt;</comment>
                            <comment id="12852182" author="jdcryans" created="Wed, 31 Mar 2010 23:55:22 +0000"  >&lt;p&gt;Here&apos;s a patch that does what Stack describes. &lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If the flush comes from flushSomeRegions, we will wait since that doesn&apos;t hold up the other regions.&lt;/li&gt;
	&lt;li&gt;If the flush comes from the main flushing thread, we check if there&apos;s too many store files. If so, we wait a bit and add it back to the queue.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I tried it on the randomWrite PE, works as advertised but it may be a bit chatty when the compactions are taking a long time. Could be improved by doing the &quot;triggered&quot; thing ensureStoreFileCount is doing.&lt;/p&gt;</comment>
                            <comment id="12852619" author="stack" created="Fri, 2 Apr 2010 00:25:47 +0000"  >&lt;p&gt;+1 on patch.  Its a big improvement even though it means lots of ugly logs.   We can revisit the latter later.  I&apos;d say change the 500ms to 1s sleep at least since unlikely compaction will complete in this time.  You wanted to change the name of checkStoreFileCount?  Change it on commit?&lt;/p&gt;</comment>
                            <comment id="12852626" author="jdcryans" created="Fri, 2 Apr 2010 00:55:44 +0000"  >&lt;p&gt;Committed to branch and trunk with a 1000ms sleep, thanks for the review Stack!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12440426" name="HBASE-2087.patch" size="2044" author="jdcryans" created="Wed, 31 Mar 2010 23:55:22 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 1 Jan 2010 01:02:27 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26137</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 38 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08t0f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>49300</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>