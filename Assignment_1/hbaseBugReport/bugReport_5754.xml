<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:30:40 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5754/HBASE-5754.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5754] data lost with gora continuous ingest test (goraci)</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5754</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Keith Turner re-wrote the accumulo continuous ingest test using gora, which has both hbase and accumulo back-ends.&lt;/p&gt;

&lt;p&gt;I put a billion entries into HBase, and ran the Verify map/reduce job.  The verification failed because about 21K entries were missing.  The goraci &lt;a href=&quot;https://github.com/keith-turner/goraci&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;README&lt;/a&gt; explains the test, and how it detects missing data.&lt;/p&gt;

&lt;p&gt;I re-ran the test with 100 million entries, and it verified successfully.  &lt;br/&gt;
Both of the times I tested using a billion entries, the verification failed.&lt;br/&gt;
If I run the verification step twice, the results are consistent, so the problem is&lt;br/&gt;
probably not on the verify step.&lt;/p&gt;

&lt;p&gt;Here&apos;s the versions of the various packages:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;package&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;version&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;hadoop&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.20.205.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;hbase&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.92.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;gora&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;http://svn.apache.org/repos/asf/gora/trunk r1311277&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;goraci&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;https://github.com/ericnewton/goraci  tagged 2012-04-08&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;The change I made to goraci was to configure it for hbase and to allow it to build properly.&lt;/p&gt;</description>
                <environment>&lt;p&gt;10 node test cluster&lt;/p&gt;</environment>
        <key id="12550283">HBASE-5754</key>
            <summary>data lost with gora continuous ingest test (goraci)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="5">Cannot Reproduce</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="ecn">Eric Newton</reporter>
                        <labels>
                    </labels>
                <created>Mon, 9 Apr 2012 21:15:34 +0000</created>
                <updated>Thu, 6 Jun 2013 14:29:02 +0000</updated>
                            <resolved>Thu, 6 Jun 2013 14:29:02 +0000</resolved>
                                    <version>0.92.1</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                                <comments>
                            <comment id="13250195" author="stack" created="Mon, 9 Apr 2012 21:22:07 +0000"  >&lt;p&gt;Let me take this one.  I&apos;ve been playing w/ goraci.  I did 25M for my first test and that seemed to work.  Let me do the bigger numbers.  Thanks Eric.&lt;/p&gt;</comment>
                            <comment id="13250316" author="enis" created="Mon, 9 Apr 2012 23:53:05 +0000"  >&lt;p&gt;@Stack, if you need any help with Gora, I can help with that. &lt;/p&gt;</comment>
                            <comment id="13250940" author="stack" created="Tue, 10 Apr 2012 18:34:50 +0000"  >&lt;p&gt;@Enis Gora seems to work just fine.  Thanks for the offer.&lt;/p&gt;</comment>
                            <comment id="13251017" author="kturner" created="Tue, 10 Apr 2012 20:12:51 +0000"  >&lt;p&gt;You may run into &lt;a href=&quot;https://issues.apache.org/jira/browse/GORA-116&quot; title=&quot;gora treats split points as if they represent actual values in the table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;GORA-116&quot;&gt;GORA-116&lt;/a&gt;, a bug in the gora-hbase store.&lt;/p&gt;</comment>
                            <comment id="13251043" author="stack" created="Tue, 10 Apr 2012 20:36:21 +0000"  >&lt;p&gt;@Keith Thanks for the headsup.&lt;/p&gt;</comment>
                            <comment id="13251173" author="ecn" created="Tue, 10 Apr 2012 23:29:36 +0000"  >&lt;p&gt;@stack&lt;/p&gt;

&lt;p&gt;Save yourself a mountain of time during the Verify step by taking a look at &lt;a href=&quot;https://issues.apache.org/jira/browse/GORA-117&quot; title=&quot;gora hbase does not have a mechanism to set the caching on a scanner, which makes for poor performance on map/reduce jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;GORA-117&quot;&gt;&lt;del&gt;GORA-117&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When I pre-split the data, my 100M runs were clean.  When I leaned on the split button during ingest, I had problems even at 100M.  Could be I just got (un)lucky, but I hope it will reduce your debugging time.&lt;/p&gt;
</comment>
                            <comment id="13251181" author="stack" created="Tue, 10 Apr 2012 23:39:50 +0000"  >&lt;p&gt;@Eric No worries.  Let me make sure of your 1B test regardless (I messed up and loaded many billions ... need to start over).  Thanks for the heads up.&lt;/p&gt;</comment>
                            <comment id="13252178" author="stack" created="Thu, 12 Apr 2012 04:00:06 +0000"  >&lt;p&gt;I managed to get things to work after a few detours and figuring the tool (I&apos;m a little slow).  Here is output of a verify run after uploading 1B rows using the Generator tool (I have a five node cluster):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
12/04/12 03:53:54 INFO mapred.JobClient:  map 100% reduce 99%
12/04/12 03:54:06 INFO mapred.JobClient:  map 100% reduce 100%
12/04/12 03:54:11 INFO mapred.JobClient: Job complete: job_201204092039_0040
12/04/12 03:54:11 INFO mapred.JobClient: Counters: 31
12/04/12 03:54:11 INFO mapred.JobClient:   Job Counters
12/04/12 03:54:11 INFO mapred.JobClient:     Launched reduce tasks=103
12/04/12 03:54:11 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=43090396
12/04/12 03:54:11 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
12/04/12 03:54:11 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
12/04/12 03:54:11 INFO mapred.JobClient:     Rack-local map tasks=75
12/04/12 03:54:11 INFO mapred.JobClient:     Launched map tasks=256
12/04/12 03:54:11 INFO mapred.JobClient:     Data-local map tasks=181
12/04/12 03:54:11 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=8911236
12/04/12 03:54:11 INFO mapred.JobClient:   goraci.Verify$Counts
12/04/12 03:54:11 INFO mapred.JobClient:     REFERENCED=564459547
12/04/12 03:54:11 INFO mapred.JobClient:     UNREFERENCED=1040000000
12/04/12 03:54:11 INFO mapred.JobClient:   File Output Format Counters
12/04/12 03:54:11 INFO mapred.JobClient:     Bytes Written=0
12/04/12 03:54:11 INFO mapred.JobClient:   FileSystemCounters
12/04/12 03:54:11 INFO mapred.JobClient:     FILE_BYTES_READ=80913119406
12/04/12 03:54:11 INFO mapred.JobClient:     HDFS_BYTES_READ=156449
12/04/12 03:54:11 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=107202716633
12/04/12 03:54:11 INFO mapred.JobClient:   File Input Format Counters
12/04/12 03:54:11 INFO mapred.JobClient:     Bytes Read=0
12/04/12 03:54:11 INFO mapred.JobClient:   Map-Reduce Framework
12/04/12 03:54:11 INFO mapred.JobClient:     Map output materialized bytes=28369514665
12/04/12 03:54:11 INFO mapred.JobClient:     Map input records=1604459547
12/04/12 03:54:11 INFO mapred.JobClient:     Reduce shuffle bytes=28259732158
12/04/12 03:54:11 INFO mapred.JobClient:     Spilled Records=8195463443
12/04/12 03:54:11 INFO mapred.JobClient:     Map output bytes=24031522877
12/04/12 03:54:11 INFO mapred.JobClient:     CPU time spent (ms)=20730410
12/04/12 03:54:11 INFO mapred.JobClient:     Total committed heap usage (bytes)=150411739136
12/04/12 03:54:11 INFO mapred.JobClient:     Combine input records=0
12/04/12 03:54:11 INFO mapred.JobClient:     SPLIT_RAW_BYTES=156449
12/04/12 03:54:11 INFO mapred.JobClient:     Reduce input records=2168919094
12/04/12 03:54:11 INFO mapred.JobClient:     Reduce input groups=1604459547
12/04/12 03:54:11 INFO mapred.JobClient:     Combine output records=0
12/04/12 03:54:11 INFO mapred.JobClient:     Physical memory (bytes) snapshot=144318976000
12/04/12 03:54:11 INFO mapred.JobClient:     Reduce output records=0
12/04/12 03:54:11 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=522892115968
12/04/12 03:54:11 INFO mapred.JobClient:     Map output records=2168919094
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Going by the README, it would seem we&apos;re basically working.  Should I close this issue or would you like me to look at something else?  Thanks.&lt;/p&gt;

&lt;p&gt;I like this goraci tool.  Will play some more with it.&lt;/p&gt;</comment>
                            <comment id="13252454" author="ecn" created="Thu, 12 Apr 2012 14:25:22 +0000"  >&lt;p&gt;Yes, that looks like a clean run.&lt;/p&gt;

&lt;p&gt;My run of 1B completed correctly yesterday.&lt;/p&gt;

&lt;p&gt;However, I just did a run of 100M.  I suspect split/balancing is causing the data loss.  So, during the 100M run, I pushed the split button on the master web page, every 5 seconds until I had several hundred tablets.  Then I left it alone.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;12/04/12 14:17:01 INFO mapred.JobClient:   goraci.Verify$Counts
12/04/12 14:17:01 INFO mapred.JobClient:     UNDEFINED=37099
12/04/12 14:17:01 INFO mapred.JobClient:     REFERENCED=89961385
12/04/12 14:17:01 INFO mapred.JobClient:     UNREFERENCED=10001516
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Perhaps if you do this you can replicate the problem.&lt;/p&gt;</comment>
                            <comment id="13252491" author="kturner" created="Thu, 12 Apr 2012 15:31:51 +0000"  >&lt;p&gt;The counts for the 1B run seem odd to me , but maybe thats just an artifact of how many map task you ran for the generator and how much data each task generated.  If a map task does not does not generate a multiple of 25,000,000 then it will leave some unreferenced.  It generates a circular linked list every 25M.   &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;12/04/12 03:54:11 INFO mapred.JobClient:     REFERENCED=564459547
12/04/12 03:54:11 INFO mapred.JobClient:     UNREFERENCED=1040000000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you were to run 10 map task each generating 100M, then this should generate 1B with all nodes referenced.  Minimizing the number of unreferenced is ideal, because the test can not detect the loss of unreferenced nodes.  I should probably add this info to the readme.&lt;/p&gt;</comment>
                            <comment id="13252502" author="kturner" created="Thu, 12 Apr 2012 15:40:46 +0000"  >&lt;p&gt;When Eric ran 10 generators each adding 100M against HBase and no data was lost, he saw 1B referenced, 0 unref, and 0 undef.&lt;/p&gt;</comment>
                            <comment id="13252504" author="stack" created="Thu, 12 Apr 2012 15:43:08 +0000"  >&lt;p&gt;Let me do the same.  I did not match generator map tasks to verify reducers.  Then let me recreate the split issue Eric describes above.  Thanks lads.&lt;/p&gt;</comment>
                            <comment id="13253135" author="stack" created="Fri, 13 Apr 2012 05:48:26 +0000"  >&lt;p&gt;I ran w/ 10 generators and 10 slots for the verify step and got the below which doesn&apos;t prints out only a REFERENCED count.&lt;/p&gt;

&lt;p&gt;Running these recent tests I let it do its natural splitting so it grew from zero to 260odd regions so maybe the issue you see Eric comes of manual splits coming out of the UI.  Let me try that next.&lt;/p&gt;

&lt;p&gt;Thanks lads.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
12/04/13 05:16:23 INFO mapred.JobClient:  map 100% reduce 99%
12/04/13 05:16:54 INFO mapred.JobClient:  map 100% reduce 100%
12/04/13 05:16:59 INFO mapred.JobClient: Job complete: job_201204092039_0046
12/04/13 05:16:59 INFO mapred.JobClient: Counters: 30
12/04/13 05:16:59 INFO mapred.JobClient:   Job Counters
12/04/13 05:16:59 INFO mapred.JobClient:     Launched reduce tasks=10
12/04/13 05:16:59 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=30125694
12/04/13 05:16:59 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
12/04/13 05:16:59 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
12/04/13 05:16:59 INFO mapred.JobClient:     Rack-local map tasks=6
12/04/13 05:16:59 INFO mapred.JobClient:     Launched map tasks=256
12/04/13 05:16:59 INFO mapred.JobClient:     Data-local map tasks=250
12/04/13 05:16:59 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=5832198
12/04/13 05:16:59 INFO mapred.JobClient:   goraci.Verify$Counts
12/04/13 05:16:59 INFO mapred.JobClient:     REFERENCED=1000000000
12/04/13 05:16:59 INFO mapred.JobClient:   File Output Format Counters
12/04/13 05:16:59 INFO mapred.JobClient:     Bytes Written=0
12/04/13 05:16:59 INFO mapred.JobClient:   FileSystemCounters
12/04/13 05:16:59 INFO mapred.JobClient:     FILE_BYTES_READ=83022967343
12/04/13 05:16:59 INFO mapred.JobClient:     HDFS_BYTES_READ=156414
12/04/13 05:16:59 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=112881560332
12/04/13 05:16:59 INFO mapred.JobClient:   File Input Format Counters
12/04/13 05:16:59 INFO mapred.JobClient:     Bytes Read=0
12/04/13 05:16:59 INFO mapred.JobClient:   Map-Reduce Framework
12/04/13 05:16:59 INFO mapred.JobClient:     Map output materialized bytes=29992170602
12/04/13 05:16:59 INFO mapred.JobClient:     Map input records=1000000000
12/04/13 05:16:59 INFO mapred.JobClient:     Reduce shuffle bytes=29874879887
12/04/13 05:16:59 INFO mapred.JobClient:     Spilled Records=7527086436
12/04/13 05:16:59 INFO mapred.JobClient:     Map output bytes=25992155242
12/04/13 05:16:59 INFO mapred.JobClient:     CPU time spent (ms)=20182570
12/04/13 05:16:59 INFO mapred.JobClient:     Total committed heap usage (bytes)=99953082368
12/04/13 05:16:59 INFO mapred.JobClient:     Combine input records=0
12/04/13 05:16:59 INFO mapred.JobClient:     SPLIT_RAW_BYTES=156414
12/04/13 05:16:59 INFO mapred.JobClient:     Reduce input records=2000000000
12/04/13 05:16:59 INFO mapred.JobClient:     Reduce input groups=1000000000
12/04/13 05:16:59 INFO mapred.JobClient:     Combine output records=0
12/04/13 05:16:59 INFO mapred.JobClient:     Physical memory (bytes) snapshot=91762372608
12/04/13 05:16:59 INFO mapred.JobClient:     Reduce output records=0
12/04/13 05:16:59 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=391126540288
12/04/13 05:16:59 INFO mapred.JobClient:     Map output records=2000000000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13271036" author="enis" created="Wed, 9 May 2012 03:05:55 +0000"  >&lt;p&gt;In one of my 0.92.x tests on a 10 node cluster, 250M inserts, I did manage to get the verify to fail: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
12/05/08 11:11:18 INFO mapred.JobClient:   goraci.Verify$Counts
12/05/08 11:11:18 INFO mapred.JobClient:     UNDEFINED=972506
12/05/08 11:11:18 INFO mapred.JobClient:     REFERENCED=248051318
12/05/08 11:11:18 INFO mapred.JobClient:     UNREFERENCED=972506
12/05/08 11:11:18 INFO mapred.JobClient:   Map-Reduce Framework
12/05/08 11:11:18 INFO mapred.JobClient:     Map input records=249023824
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that map input records is 1M less that 250M, which indicates that the inputformat did not provide all records in the table. The missing rows all belong to the single region. I rerun the test again after a couple of hours, and it passed. But the failed test created 244 maps, instead of 246, which is the current region count, so I am suspecting there is something wrong in the split calculation or in the supposed transactional behavior for split/balance operations in the meta table. I am still inspecting the code and the logs, but any pointers are welcome. &lt;/p&gt;</comment>
                            <comment id="13415889" author="lhofhansl" created="Tue, 17 Jul 2012 04:28:27 +0000"  >&lt;p&gt;Trying to figure out whether this is something to worry about or not.&lt;br/&gt;
Is this a permanent data loss issue, or is it &quot;just&quot; .META. being out of whack temporarily (or at least fixable with hbck)?&lt;/p&gt;</comment>
                            <comment id="13416003" author="enis" created="Tue, 17 Jul 2012 08:15:00 +0000"  >&lt;p&gt;@Lars, &lt;br/&gt;
We have been running this for a while as nightlies, and apart from the reported &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5986&quot; title=&quot;Clients can see holes in the META table when regions are being split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5986&quot;&gt;&lt;del&gt;HBASE-5986&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6060&quot; title=&quot;Regions&amp;#39;s in OPENING state from failed regionservers takes a long time to recover&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6060&quot;&gt;&lt;del&gt;HBASE-6060&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6160&quot; title=&quot;META entries from daughters can be deleted before parent entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6160&quot;&gt;&lt;del&gt;HBASE-6160&lt;/del&gt;&lt;/a&gt;, we did not run into more issues. All of them can be considered META issues w/o actual data loss. Let&apos;s see what Eric would say. &lt;/p&gt;</comment>
                            <comment id="13416177" author="ecn" created="Tue, 17 Jul 2012 13:38:44 +0000"  >&lt;p&gt;I will try to reproduce the bug with 0.94 and hadoop 1.0.3 sometime in the next week.&lt;/p&gt;

&lt;p&gt;We normally run this test against Accumulo for 24 hours, both with and without agitation (random killing of servers).  I would be concerned if there was apparent data loss, even if it was transient.&lt;/p&gt;</comment>
                            <comment id="13416188" author="nkeywal" created="Tue, 17 Jul 2012 13:47:46 +0000"  >&lt;p&gt;@Eric&lt;br/&gt;
When you kill servers, do you stop the whole hardware, just the accumulo processes, or both hdfs &amp;amp; accumulo?&lt;/p&gt;</comment>
                            <comment id="13416207" author="kturner" created="Tue, 17 Jul 2012 14:00:43 +0000"  >&lt;p&gt;In the past we just killed Accumulo processes, the tablet servers, loggers, and master process(es).  We were not looking to test HDFS.  For the next release, Accumulo has moved its write ahead logging to HDFS.  So for the next release we plan to start killing HDFS processes as part of our testing.  See &lt;a href=&quot;https://issues.apache.org/jira/browse/ACCUMULO-578&quot; title=&quot;use hdfs for the walog&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ACCUMULO-578&quot;&gt;&lt;del&gt;ACCUMULO-578&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/ACCUMULO-623&quot; title=&quot;Data lost with hdfs write ahead log&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ACCUMULO-623&quot;&gt;&lt;del&gt;ACCUMULO-623&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/ACCUMULO-636&quot; title=&quot;Must kill hdfs processes during testing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ACCUMULO-636&quot;&gt;&lt;del&gt;ACCUMULO-636&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13416243" author="enis" created="Tue, 17 Jul 2012 14:30:38 +0000"  >&lt;p&gt;Just FYI in case, I&apos;ve been working on adding &quot;long running ingestion tests while randomly killing of servers&quot;, and other types of integration tests over at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6241&quot; title=&quot;HBaseCluster interface for interacting with the cluster from system tests &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6241&quot;&gt;&lt;del&gt;HBASE-6241&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6201&quot; title=&quot;HBase integration/system tests&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6201&quot;&gt;&lt;del&gt;HBASE-6201&lt;/del&gt;&lt;/a&gt;. Feel free to chime in. &lt;/p&gt;</comment>
                            <comment id="13416246" author="nkeywal" created="Tue, 17 Jul 2012 14:35:06 +0000"  >&lt;p&gt;@keith: thanks for the info. It will be interesting to see the results. fyi, I&apos;ve just created &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6401&quot; title=&quot;HBase may lose edits after a crash if used with HDFS 1.0.3 or older&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6401&quot;&gt;&lt;del&gt;HBASE-6401&lt;/del&gt;&lt;/a&gt; on an hdfs linked issue.&lt;/p&gt;</comment>
                            <comment id="13676653" author="ivarley" created="Thu, 6 Jun 2013 03:58:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;, this issue has been dormant for 11 months. Do you think all these issues were resolved based on the other lined JIRAs above (e.g. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5987&quot; title=&quot;HFileBlockIndex improvement&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5987&quot;&gt;&lt;del&gt;HBASE-5987&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6060&quot; title=&quot;Regions&amp;#39;s in OPENING state from failed regionservers takes a long time to recover&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6060&quot;&gt;&lt;del&gt;HBASE-6060&lt;/del&gt;&lt;/a&gt;, etc)? Or is this still reproducible?&lt;/p&gt;</comment>
                            <comment id="13676979" author="ecn" created="Thu, 6 Jun 2013 12:57:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ivarley&quot; class=&quot;user-hover&quot; rel=&quot;ivarley&quot;&gt;Ian Varley&lt;/a&gt; I don&apos;t know if it has been fixed.  I have not re-tested more recent versions of HBase.  Since the test has now been incorporated into HBase without any additional findings, perhaps it has been fixed.&lt;/p&gt;</comment>
                            <comment id="13677060" author="ivarley" created="Thu, 6 Jun 2013 14:27:22 +0000"  >&lt;p&gt;OK, thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;. I would move to close unless anyone has reported something similar in the meantime or is actively working on it. Reopen if you disagree. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13677061" author="ivarley" created="Thu, 6 Jun 2013 14:29:02 +0000"  >&lt;p&gt;Per discussion, this functionality is covered by many other tests, and hasn&apos;t been reproducible recently. Please reopen if anyone sees this again.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12554839">HBASE-5986</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 9 Apr 2012 21:22:07 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>235148</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 28 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02d33:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11721</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>