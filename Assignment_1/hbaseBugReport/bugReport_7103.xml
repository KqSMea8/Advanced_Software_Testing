<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:42:40 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-7103/HBASE-7103.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-7103] Need to fail split if SPLIT znode is deleted even before the split is completed.</title>
                <link>https://issues.apache.org/jira/browse/HBASE-7103</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This came up after the following mail in dev list&lt;br/&gt;
&apos;infinite loop of RS_ZK_REGION_SPLIT on .94.2&apos;.&lt;br/&gt;
The following is the reason for the problem&lt;br/&gt;
The following steps happen&lt;br/&gt;
-&amp;gt; Initially the parent region P1 starts splitting.&lt;br/&gt;
-&amp;gt; The split is going on normally.&lt;br/&gt;
-&amp;gt; Another split starts at the same time for the same region P1. (Not sure why this started).&lt;br/&gt;
-&amp;gt; Rollback happens seeing an already existing node.&lt;br/&gt;
-&amp;gt; This node gets deleted in rollback and nodeDeleted Event starts.&lt;br/&gt;
-&amp;gt; In nodeDeleted event the RIT for the region P1 gets deleted.&lt;br/&gt;
-&amp;gt; Because of this there is no region in RIT.&lt;br/&gt;
-&amp;gt; Now the first split gets over.  Here the problem is we try to transit the node to SPLITTING to SPLIT. But the node even does not exist.&lt;br/&gt;
But we don take any action on this.  We think it is successful.&lt;br/&gt;
-&amp;gt; Because of this SplitRegionHandler never gets invoked.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12614929">HBASE-7103</key>
            <summary>Need to fail split if SPLIT znode is deleted even before the split is completed.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ram_krish">ramkrishna.s.vasudevan</assignee>
                                    <reporter username="ram_krish">ramkrishna.s.vasudevan</reporter>
                        <labels>
                    </labels>
                <created>Tue, 6 Nov 2012 10:09:51 +0000</created>
                <updated>Tue, 26 Feb 2013 08:20:45 +0000</updated>
                            <resolved>Mon, 12 Nov 2012 19:46:34 +0000</resolved>
                                                    <fixVersion>0.94.3</fixVersion>
                    <fixVersion>0.95.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                <comments>
                            <comment id="13491358" author="ram_krish" created="Tue, 6 Nov 2012 10:11:39 +0000"  >&lt;p&gt;Will try to come up with a patch for this.&lt;/p&gt;</comment>
                            <comment id="13491653" author="ram_krish" created="Tue, 6 Nov 2012 17:51:51 +0000"  >&lt;p&gt;Trying to write a testcase for this?  Can we fail the split if the znode is not present? But my doubt is if a split is currently going on for the region A and if another split is called for the same region how should we handle it?  Ideally this prob is caused because of the rollback that is done by the second split.&lt;br/&gt;
Thinking on this.  Any suggestions?&lt;/p&gt;</comment>
                            <comment id="13491656" author="jdcryans" created="Tue, 6 Nov 2012 17:56:24 +0000"  >&lt;p&gt;I wonder which jira introduced this issue as it seems that it wasn&apos;t present before 0.94.2&lt;/p&gt;</comment>
                            <comment id="13491669" author="mcorgan" created="Tue, 6 Nov 2012 18:09:04 +0000"  >&lt;p&gt;From step 3, do you think the double-splitting is a new phenomenon?  It doesn&apos;t sound like something that should happen very often.  Maybe that would explain why i didn&apos;t get this error in .94.0.&lt;/p&gt;

&lt;p&gt;Also, please note I went straight from .94.0 to .94.2, so I don&apos;t know if it was present in .94.1.&lt;/p&gt;</comment>
                            <comment id="13491770" author="lhofhansl" created="Tue, 6 Nov 2012 20:05:24 +0000"  >&lt;p&gt;On the mailing list I had posted these as candidates:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6329&quot; title=&quot;Stopping META regionserver when splitting region could cause daughter region to be assigned twice&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6329&quot;&gt;&lt;del&gt;HBASE-6329&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6070&quot; title=&quot;AM.nodeDeleted and SSH races creating problems for regions under SPLIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6070&quot;&gt;&lt;del&gt;HBASE-6070&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6713&quot; title=&quot;Stopping META/ROOT RS may take 50mins when some region is splitting&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6713&quot;&gt;&lt;del&gt;HBASE-6713&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5986&quot; title=&quot;Clients can see holes in the META table when regions are being split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5986&quot;&gt;&lt;del&gt;HBASE-5986&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Some of these deal with split during regionserver stops.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mcorgan&quot; class=&quot;user-hover&quot; rel=&quot;mcorgan&quot;&gt;Matt Corgan&lt;/a&gt; I assume you&apos;re still restarting servers occasionally?&lt;/p&gt;</comment>
                            <comment id="13491798" author="mcorgan" created="Tue, 6 Nov 2012 20:45:05 +0000"  >&lt;p&gt;So far I don&apos;t think it has to do with stopping the regionserver since we&apos;re only doing that after this happens.  I also haven&apos;t seen anything suggesting it has to do with the META table.&lt;/p&gt;

&lt;p&gt;Any ideas on what would cause a split to fail and retry?  Is it more likely caused by some internal regionserver problem, or the region being moved during the split, etc?&lt;/p&gt;</comment>
                            <comment id="13491995" author="lhofhansl" created="Wed, 7 Nov 2012 00:49:45 +0000"  >&lt;p&gt;Did you change how frequently the balancer runs?&lt;/p&gt;</comment>
                            <comment id="13492012" author="mcorgan" created="Wed, 7 Nov 2012 01:03:17 +0000"  >&lt;p&gt;I disabled my custom balancer (external java program that calls HBaseAdmin.move()) and it&apos;s been working without error for longer than usual.  I&apos;ve been using the same balancer since .90 series I think, so possibly something changed where calling move() on a new daughter region soon after a split leaves ZK in a bad state.&lt;/p&gt;</comment>
                            <comment id="13492057" author="lhofhansl" created="Wed, 7 Nov 2012 01:56:48 +0000"  >&lt;p&gt;I have very limited knowledge in this area... From looking through the code briefly, if two splits happen roughly the parallel the 2nd one will fail due to the split node already existing (see SplitTransaction.createNodeSplitting), but I then it already wrote STARTED_SPLITTING to its journal. Now the transaction is rolled back and will cleanup the ZK state.&lt;/p&gt;

&lt;p&gt;So I guess we can either:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;track whether the split transaction failed because of a concurrent split, in that case we won&apos;t clean the zk state.&lt;/li&gt;
	&lt;li&gt;First try to create a ZK node, then write to the journal.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Both cases probably have bad side effects and races.&lt;/p&gt;</comment>
                            <comment id="13492122" author="ram_krish" created="Wed, 7 Nov 2012 04:41:54 +0000"  >&lt;p&gt;@JD&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; is a reason for this. But prior to this i can check and tell you the behaviour.&lt;/p&gt;</comment>
                            <comment id="13492131" author="lhofhansl" created="Wed, 7 Nov 2012 05:13:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; Would it be most expedient to revert &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; for now?&lt;br/&gt;
Then we can tackle these two issues together. Thoughts?&lt;/p&gt;</comment>
                            <comment id="13492367" author="ram_krish" created="Wed, 7 Nov 2012 14:11:34 +0000"  >&lt;p&gt;A testcase that shows the problem.&lt;br/&gt;
Ok Lars.. let me check this if not will revert it by tomorrow.  Is that fine?&lt;/p&gt;</comment>
                            <comment id="13492404" author="ram_krish" created="Wed, 7 Nov 2012 15:00:10 +0000"  >&lt;p&gt;So just checked without &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt;.  The problem is something similar.  What happens is from the master we do not remove from RIT if &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; is not present.&lt;br/&gt;
But RS thinks split is completed but as the node got deleted due to second split&apos;s rollback there is no transition from SPLITTING to SPLIT.  So the master is never notified about this. &lt;br/&gt;
After &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; the entry in RIt is removed but still the master does not know that split has happened.  Before that RIT is removed which allows atleast the balancer to run.&lt;br/&gt;
So we may have to come up with a better one &lt;/p&gt;</comment>
                            <comment id="13492502" author="ram_krish" created="Wed, 7 Nov 2012 17:01:20 +0000"  >&lt;p&gt;Actually &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt; introduced the STARTED_SPLITTING.  This was done so that first time when we try to create the znode with RS_ZK_SPLITTING state if there is any exception rollback was not taking any action.  This was leading to subsequent split failures and thus split never happened.&lt;br/&gt;
Now the new state STARTED_SPLITTING will delete the node on rollback if any error while setting the data.  Even if any exception happens in SET_SPLITTING_IN_ZK even then the same clean up is getting done.&lt;/p&gt;</comment>
                            <comment id="13492583" author="lhofhansl" created="Wed, 7 Nov 2012 18:49:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; Thanks again Ram. Nice test.&lt;br/&gt;
So rolling back &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; would not be good enough anyway (and reverting &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; is excessive).&lt;/p&gt;

&lt;p&gt;Hopefully with the test we have a good chance of fixing this.&lt;br/&gt;
No haste here, Ram. Will delay the next 0.94RC until this we can fix this.&lt;/p&gt;</comment>
                            <comment id="13492614" author="stack" created="Wed, 7 Nov 2012 19:19:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;-&amp;gt; Another split starts at the same time for the same region P1. (Not sure why this started).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we know more on the above?   Why two splits at same time are even possible?  We only check split at end of a compaction so this region is compacting frequently, so frequently, we can queue up splits so they can run near concurrent?&lt;/p&gt;
</comment>
                            <comment id="13492961" author="ram_krish" created="Thu, 8 Nov 2012 05:11:32 +0000"  >&lt;p&gt;Yes Stack.  Even am not sure why two splits started.&lt;br/&gt;
But a forceful split can happen parallely right? I may be wrong here.&lt;br/&gt;
Not only compaction, frequent flushes that results in big store files also may result in this?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; shouldCompact = region.flushcache();
      &lt;span class=&quot;code-comment&quot;&gt;// We just want to check the size
&lt;/span&gt;      &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; shouldSplit = region.checkSplit() != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (shouldSplit) {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.server.compactSplitThread.requestSplit(region);
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (shouldCompact) {
        server.compactSplitThread.requestCompaction(region, getName());
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Anyway to chec if a split request has come for an already going on split? Because currently every split request creates a new split transaction.&lt;/p&gt;</comment>
                            <comment id="13493018" author="stack" created="Thu, 8 Nov 2012 07:19:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;But a forceful split can happen parallely right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, probably.  I see no checks to prevent it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Not only compaction, frequent flushes that results in big store files also may result in this?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is that correct?  Should we only be doing it after compaction?  Is that why we are doing concurrent split?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Anyway to chec if a split request has come for an already going on split? Because currently every split request creates a new split transaction.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No.  Could start the transaction and if can&apos;t set SPLITTING znode, fail out.&lt;/p&gt;</comment>
                            <comment id="13493023" author="mcorgan" created="Thu, 8 Nov 2012 07:41:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;Not only compaction, frequent flushes that results in big store files also may result in this?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When triggering this problem I was doing frequent flushes, and compactions were probably backlogged for the region.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is that correct? Should we only be doing it after compaction? Is that why we are doing concurrent split?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It would be nice to keep the ability (if it already exists) for a region to split without waiting for all the flushing/compacting to stop because the flushing/compacting may go on indefinitely.  The split is important in this scenario since it spreads the load to another server.&lt;/p&gt;</comment>
                            <comment id="13493025" author="ram_krish" created="Thu, 8 Nov 2012 07:45:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;Could start the transaction and if can&apos;t set SPLITTING znode, fail out.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But how to determine if a request is coming parallely and a request that is coming newly after a previous one had failed.&lt;br/&gt;
Because if the node got created and the same thing failed due to some exception we will rollback.  Here we need to delete the node.&lt;br/&gt;
Next again if the new request comes this will succeed.&lt;br/&gt;
If we try to handle the failure by not deleting the node how can we diff a new request and a parallel request. Will think more on this.&lt;/p&gt;</comment>
                            <comment id="13493258" author="lhofhansl" created="Thu, 8 Nov 2012 15:29:12 +0000"  >&lt;p&gt;Does my idea from above:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;First try to create a ZK node, then write to the journal.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fix this? In that case the parallel split request would fail before it writes anything in its journal and hence would not attempt to clean up the ZK state.&lt;/p&gt;</comment>
                            <comment id="13493354" author="stack" created="Thu, 8 Nov 2012 18:09:56 +0000"  >&lt;p&gt;Yeah, Lars&apos; idea is like I was saying.  Else, can&apos;t we keep dictionary keyed by region of currently splitting regions in the RS?&lt;/p&gt;</comment>
                            <comment id="13493741" author="ram_krish" created="Fri, 9 Nov 2012 05:20:37 +0000"  >&lt;p&gt;Ok.. now i dont have the code with me.  Let me check the code and comment on this.  Thanks Lars and Stack.&lt;/p&gt;</comment>
                            <comment id="13494025" author="ram_krish" created="Fri, 9 Nov 2012 14:39:29 +0000"  >&lt;p&gt;@Lars&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt; added the new journal entry.  Because previously the STARTED_SPLITTING was never added.  So what happened was once we try to write the data RS_ZK_SPLITTING after creating the node and if that fails then on rollback we don take action and so subsequent splitting never happened.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;can&apos;t we keep dictionary keyed by region of currently splitting regions in the RS?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But the clearing of the dictionary should be done properly after the transition is done.  Chances of race between the time we remove and the time we check if already present.  May be we need to cross verify with the online regions list in the RS side.&lt;/p&gt;
</comment>
                            <comment id="13494369" author="lhofhansl" created="Fri, 9 Nov 2012 22:27:15 +0000"  >&lt;p&gt;I would like to entertain the thought of revert both &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6854&quot; title=&quot;Deletion of SPLITTING node on split rollback should clear the region from RIT&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6854&quot;&gt;&lt;del&gt;HBASE-6854&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt; for 0.94. (possible scheduling them both to 0.94.4 along with this one and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7101&quot; title=&quot;HBase stuck in Region SPLIT &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7101&quot;&gt;&lt;del&gt;HBASE-7101&lt;/del&gt;&lt;/a&gt; to fix these all together).&lt;/p&gt;

&lt;p&gt;I ran your test with these two patches reverted. It now fails in the last assert (where the RS and Master disagree whether the region is online or not). That is not ideal, but was a longstanding issue (I think).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; I realize this is frustrating. At the same time I think that for 0.94 we have start thinking about an expectation of stability.&lt;/p&gt;

&lt;p&gt;Thoughts about this?&lt;/p&gt;</comment>
                            <comment id="13494432" author="lhofhansl" created="Fri, 9 Nov 2012 23:38:43 +0000"  >&lt;p&gt;In fact just reverting &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt; seems to be fine. That is what I am proposing now.&lt;/p&gt;</comment>
                            <comment id="13494573" author="lhofhansl" created="Sat, 10 Nov 2012 05:59:43 +0000"  >&lt;p&gt;Here&apos;s a patch against trunk, reverting &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6088&quot; title=&quot; Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6088&quot;&gt;&lt;del&gt;HBASE-6088&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13494589" author="hadoopqa" created="Sat, 10 Nov 2012 08:00:21 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12552963/7103-6088-revert.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12552963/7103-6088-revert.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 87 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 18 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.util.TestMiniClusterLoadEncoded&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3301//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13494652" author="ram_krish" created="Sat, 10 Nov 2012 12:49:36 +0000"  >&lt;p&gt;Ok Lars.  I understand.  No problem.  &lt;br/&gt;
Just before we commit this i have a suggestion&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; node = ZKAssign.getNodeName(zkw, region.getEncodedName());
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!ZKUtil.createEphemeralNodeAndWatch(zkw, node, data.getBytes())) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Failed create of ephemeral &quot;&lt;/span&gt; + node);
    }
    &lt;span class=&quot;code-comment&quot;&gt;// Transition node from SPLITTING to SPLITTING and pick up version so we
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// can be sure &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; znode is ours; version is needed deleting.
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; transitionNodeSplitting(zkw, region, serverName, -1);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here after creating the node we once transit the node from SPLITTING to SPLITTING to get znode version.  Can we get the znode version just after creating the node.&lt;br/&gt;
So if creation itself fails there is no node at all.  If it succeeds anyway as next step will add the journal SET_SPLITTING_IN_ZK.&lt;br/&gt;
Now the transition will result in the version as 1 but if we don do the transition it will be 0.&lt;br/&gt;
Now what advantage we get is next time if any parallel split comes the node will already exist when it tries to create the znode and this will not do anything with the znode while rollback.  What do you feel?  My intention was to solve both 7103 and 6088.  &lt;br/&gt;
Lars, i leave it to you.  If you think we can revert this and address this in next version 0.94.4.  If not we can try for a patch this version.  If you are ok with that i can submit a patch for the same.&lt;/p&gt;</comment>
                            <comment id="13494657" author="ram_krish" created="Sat, 10 Nov 2012 12:59:45 +0000"  >&lt;p&gt;Also Lars the last assert should be asserting for false.  Because once split is successful the main parent region should not be in RIT&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
assertTrue(&lt;span class=&quot;code-quote&quot;&gt;&quot;The region should be online&quot;&lt;/span&gt;, rit.containsKey(hri.getTableNameAsString()));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13494691" author="ram_krish" created="Sat, 10 Nov 2012 13:54:34 +0000"  >&lt;p&gt;Doing as said above has one implication, because the SPLITTING node is created on the RS side the master does not get the callback for node created.  Hence the in memory RIT SPLITTING state is not added to master.&lt;br/&gt;
But once the transtion happens to SPLIT the nodeDataChange adds the state to RIT&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (regionState == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
            regionState = addSplittingToRIT(sn, encodedName);
            &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; message = &lt;span class=&quot;code-quote&quot;&gt;&quot;Received SPLIT &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region &quot;&lt;/span&gt; + prettyPrintedRegionName +
              &lt;span class=&quot;code-quote&quot;&gt;&quot; from server &quot;&lt;/span&gt; + sn;
            &lt;span class=&quot;code-comment&quot;&gt;// If still &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, it means we cannot find it and it was already processed
&lt;/span&gt;            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (regionState == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
              LOG.warn(message + &lt;span class=&quot;code-quote&quot;&gt;&quot; but it doesn&apos;t exist anymore,&quot;&lt;/span&gt; +
                  &lt;span class=&quot;code-quote&quot;&gt;&quot; probably already processed its split&quot;&lt;/span&gt;);
              &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
            }
            LOG.info(message +
                &lt;span class=&quot;code-quote&quot;&gt;&quot; but region was not first in SPLITTING state; continuing&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13494723" author="lhofhansl" created="Sat, 10 Nov 2012 16:41:29 +0000"  >&lt;p&gt;Hi Ram,&lt;/p&gt;

&lt;p&gt;yeah I found that assert as well. Btw. the test you attached does not work in trunk, because there is no AssignmentManager.getRegionsOfTable(...) in trunk.&lt;/p&gt;

&lt;p&gt;I like the idea of getting/verifying the version. Looks like this would solve the 6088 without the extra state (unless I misunderstand).&lt;/p&gt;

&lt;p&gt;I don&apos;t follow your 2nd comment. Are you saying your first suggestion does not work?&lt;br/&gt;
Otherwise +1 on your idea to check the version. If you have a patch on top of the rollback patch that&apos;d be awesome.&lt;br/&gt;
Thanks for working on this stuff Ram! It&apos;s hard to make changes to this without breaking something somewhere else, because this code is so fragile.&lt;/p&gt;</comment>
                            <comment id="13494825" author="stack" created="Sun, 11 Nov 2012 04:48:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; I don&apos;t think it possible getting version on create (Let me ask one of the zk lads).  That is why we do the SPLITTING to SPLITTING transition to get the versoin.  Its true though that there is a hole in here because if we fail on create, there should be no rollback but if we fail moving SPLITTING to SPLITTING, then we should remove the created znode but ONLY if we have its version (it could have been created by someone else).  Maybe when we create, we write some unique data into the znode and get it after creating it to see what the version is &amp;#8211; and if the unique data is not the same, we know that someone else owns the znode and we should not rollback .... but that won&apos;t work either given it won&apos;t be backward compatible.&lt;/p&gt;

&lt;p&gt;If we fail the create of the znode, we should not rollback. It looks like we are doing that now since adding the STARTED_SPLITTING state &amp;#8211; right?  That seems wrong... should we be inserting the STARTED_SPLITTING state after the create of the znode?  But even then, I&apos;m not sure about deleting a znode unless we are sure we own it &amp;#8211; that the version matches.&lt;/p&gt;

&lt;p&gt;Should the following code be checking we did NOT get a -1?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.znodeVersion = createNodeSplitting(server.getZooKeeper(),
          &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.parent.getRegionInfo(), server.getServerName());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It seems like createNodeSplitting could be returning -1 if it fails.&lt;/p&gt;

&lt;p&gt;(Weird that transitionNode has explicit mention of M_ZK_REGION_OFFLINE and RS_ZK_REGION_OPENING though it takes beginState and endState but that is another issue).&lt;/p&gt;</comment>
                            <comment id="13494826" author="stack" created="Sun, 11 Nov 2012 04:57:29 +0000"  >&lt;p&gt;I&apos;d be good w/ applying the revert patch for now &amp;#8211; would be interested in what you say to the above first though Ram and who knows, maybe the zk fellas will come back w/ a little bit of magic&lt;/p&gt;</comment>
                            <comment id="13494843" author="ram_krish" created="Sun, 11 Nov 2012 08:17:53 +0000"  >&lt;p&gt;This patch makes the transition to SPLITTING after creating the node once the first journal entry is added.&lt;br/&gt;
What do we get out of this is&lt;br/&gt;
-&amp;gt; If any parallel split request comes the second one will fail because the znode creation will fail saying node already exists.  So there is no impact due to rollback as nothing is added in the journal for the second split and so no deletion of the znode happens.&lt;br/&gt;
-&amp;gt; Now if while transitioning to SPLITTING if it fails, then it will lead to rollback that will delete the znode.  Anyway that is not going to impact anything on the RIT in master side as only after the transition is done the RIT will be populated first time. If nothing is there then no impact.&lt;br/&gt;
Pls provide your comments on this, i can prepare for trunk too if this is fine. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13494845" author="hadoopqa" created="Sun, 11 Nov 2012 08:22:54 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12553013/HBASE-7103_0.94.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12553013/HBASE-7103_0.94.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3306//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3306//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13495008" author="lhofhansl" created="Sun, 11 Nov 2012 21:46:49 +0000"  >&lt;p&gt;This is not really my area of expertise, but the patch makes sense. We should probably remove STARTED_SPLITTING from JournalEntry (it&apos;s not use anywhere after this patch). Otherwise +1.&lt;/p&gt;

&lt;p&gt;@Stack: Wanna have a look?&lt;/p&gt;</comment>
                            <comment id="13495040" author="lhofhansl" created="Sun, 11 Nov 2012 23:20:19 +0000"  >&lt;p&gt;Also a trunk patch would be awesome so we can run against HadoopQA.&lt;/p&gt;</comment>
                            <comment id="13495101" author="stack" created="Mon, 12 Nov 2012 05:42:55 +0000"  >&lt;p&gt;+1 on patch.  There is still a hole in here (deleting a znode though we&apos;re not sure it is ours) but it is narrower than the hole that was there previous.  I would suggest removing the unused state as per Lars comment above and adding comment that we could be removing znode that we do not own if the transition from SPLITTING to SPLITTING fails (maybe we should create it w/o data or w/ another state but can do that in another issue... just note the problem in a TODO comment for now).  Good on you Ram.&lt;/p&gt;</comment>
                            <comment id="13495120" author="ram_krish" created="Mon, 12 Nov 2012 06:52:36 +0000"  >&lt;p&gt;Attached patch for trunk and 0.94.&lt;br/&gt;
I think i have removed the unused state Lars. Added the comments and also the TODO.&lt;br/&gt;
@Stack&lt;br/&gt;
I was thinking about new state and infact had some idea on mind.  But did not want to complicate it now with new states and handling it in master side should be done with proper care.&lt;br/&gt;
Anyway will come up with some idea sooner.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;deleting a znode though we&apos;re not sure it is ours&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This could be a problem?  Why so?  Because now if the znode exists we will not start the split anyway so there is only one split right going on?  Anyway the node deletion is done by master.  May be am missing something Stack.&lt;br/&gt;
Thanks a lot for review.  &lt;/p&gt;
</comment>
                            <comment id="13495141" author="hadoopqa" created="Mon, 12 Nov 2012 07:44:23 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12553079/HBASE-7103_trunk.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12553079/HBASE-7103_trunk.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 87 warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 18 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/3309//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13495471" author="lhofhansl" created="Mon, 12 Nov 2012 18:25:57 +0000"  >&lt;p&gt;Will commit in the next our unless I hear objections. It&apos;s time to tie a bow around 0.94.3.&lt;/p&gt;</comment>
                            <comment id="13495540" author="lhofhansl" created="Mon, 12 Nov 2012 19:34:11 +0000"  >&lt;p&gt;The unused state is still on the enum. No need for a new patch, Ram, will remove it on commit.&lt;/p&gt;</comment>
                            <comment id="13495556" author="lhofhansl" created="Mon, 12 Nov 2012 19:46:34 +0000"  >&lt;p&gt;Committed to 0.94 and 0.96. Thanks again for the patch and the reviews.&lt;/p&gt;</comment>
                            <comment id="13495625" author="hudson" created="Mon, 12 Nov 2012 20:57:12 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3532 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3532/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3532/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7103&quot; title=&quot;Need to fail split if SPLIT znode is deleted even before the split is completed.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7103&quot;&gt;&lt;del&gt;HBASE-7103&lt;/del&gt;&lt;/a&gt; Need to fail split if SPLIT znode is deleted even before the split is completed. (Ram) (Revision 1408418)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13495639" author="hudson" created="Mon, 12 Nov 2012 21:09:19 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #580 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/580/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/580/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7103&quot; title=&quot;Need to fail split if SPLIT znode is deleted even before the split is completed.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7103&quot;&gt;&lt;del&gt;HBASE-7103&lt;/del&gt;&lt;/a&gt; Need to fail split if SPLIT znode is deleted even before the split is completed. (Ram) (Revision 1408421)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13495747" author="hudson" created="Mon, 12 Nov 2012 23:26:33 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #257 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/257/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/257/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7103&quot; title=&quot;Need to fail split if SPLIT znode is deleted even before the split is completed.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7103&quot;&gt;&lt;del&gt;HBASE-7103&lt;/del&gt;&lt;/a&gt; Need to fail split if SPLIT znode is deleted even before the split is completed. (Ram) (Revision 1408418)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13495871" author="ram_krish" created="Tue, 13 Nov 2012 02:31:45 +0000"  >&lt;p&gt;Am very sorry Lars.  I forgot to remove the state from the enum.  I was thinking that it to be removed from the journal alone. &lt;/p&gt;</comment>
                            <comment id="13495909" author="stack" created="Tue, 13 Nov 2012 03:52:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-1297&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/ZOOKEEPER-1297&lt;/a&gt; adds a Stat to the create call.  It is not yet committed.  Patrick says that what we are doing is the best that can be done given current state of the API.&lt;/p&gt;

&lt;p&gt;TRUNK patch looks good to me.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why so? Because now if the znode exists we will not start the split anyway so there is only one split right going on? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That sounds right Ram.  So the execute that created the SPLITTING znode should be legit in rollback removing it.  Good stuff.&lt;/p&gt;</comment>
                            <comment id="13496025" author="ram_krish" created="Tue, 13 Nov 2012 07:47:38 +0000"  >&lt;p&gt;Thanks for the info on the ZK stuff. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13496086" author="hudson" created="Tue, 13 Nov 2012 10:24:16 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #83 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/83/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/83/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7103&quot; title=&quot;Need to fail split if SPLIT znode is deleted even before the split is completed.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7103&quot;&gt;&lt;del&gt;HBASE-7103&lt;/del&gt;&lt;/a&gt; Need to fail split if SPLIT znode is deleted even before the split is completed. (Ram) (Revision 1408421)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13544415" author="hudson" created="Sat, 5 Jan 2013 00:42:26 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security-on-Hadoop-23 #10 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/10/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security-on-Hadoop-23/10/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7103&quot; title=&quot;Need to fail split if SPLIT znode is deleted even before the split is completed.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7103&quot;&gt;&lt;del&gt;HBASE-7103&lt;/del&gt;&lt;/a&gt; Need to fail split if SPLIT znode is deleted even before the split is completed. (Ram) (Revision 1408421)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12557439">HBASE-6088</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12608625">HBASE-6854</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12552963" name="7103-6088-revert.txt" size="7503" author="lhofhansl" created="Sat, 10 Nov 2012 05:59:43 +0000"/>
                            <attachment id="12553078" name="HBASE-7103_0.94.patch" size="11876" author="ram_krish" created="Mon, 12 Nov 2012 06:48:42 +0000"/>
                            <attachment id="12553013" name="HBASE-7103_0.94.patch" size="11488" author="ram_krish" created="Sun, 11 Nov 2012 08:11:58 +0000"/>
                            <attachment id="12552465" name="HBASE-7103_testcase.patch" size="7255" author="ram_krish" created="Wed, 7 Nov 2012 14:11:34 +0000"/>
                            <attachment id="12553079" name="HBASE-7103_trunk.patch" size="14088" author="ram_krish" created="Mon, 12 Nov 2012 06:49:01 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 6 Nov 2012 17:56:24 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>255442</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 49 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0erz3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>84296</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>