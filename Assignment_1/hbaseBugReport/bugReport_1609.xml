<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:54:55 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1609/HBASE-1609.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1609] [part of hbase-1583] We wait on leases to expire before regionserver goes down.  Rather, just let client fail</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1609</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Addressing this issue will help hbase-1583.  We should do for 0.20.0 and perhaps for 0.19.x even.&lt;/p&gt;

&lt;p&gt;Currently, if outstanding leases, in HRegion close, we&apos;ll hang until lease expires.  Could be a minute.  Could be worse, the client might come in and renew the lease a few times at least till it finishes out the region.  This gets in way of regionserver shutting down fast.  &lt;/p&gt;

&lt;p&gt;J-D suggests that regionserver should just go down and outstanding clients should fail rather than try and be nice to outstanding clients (in his case, his MR job had failed so no clients... but we insist on lease expiring).&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12429498">HBASE-1609</key>
            <summary>[part of hbase-1583] We wait on leases to expire before regionserver goes down.  Rather, just let client fail</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Fri, 3 Jul 2009 17:58:38 +0000</created>
                <updated>Sun, 13 Sep 2009 22:24:48 +0000</updated>
                            <resolved>Fri, 17 Jul 2009 04:39:02 +0000</resolved>
                                                    <fixVersion>0.20.0</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12727069" author="stack" created="Fri, 3 Jul 2009 18:01:00 +0000"  >&lt;p&gt;Here is a patch for the serverside.  It removes all the fancy scanner counting we used do.&lt;/p&gt;

&lt;p&gt;In testing, serverside logs seem fine when outstanding scanner..... scanners start to get notserveringregionexception as the regions go down.&lt;/p&gt;

&lt;p&gt;Client-side though goes crazy.... retries of zk and retries of scanner stuff.  Need to clean up client handling of this case before can commit, i&apos;d suggest.&lt;/p&gt;</comment>
                            <comment id="12727070" author="jdcryans" created="Fri, 3 Jul 2009 18:04:47 +0000"  >&lt;p&gt;I might also add that even if a client is still doing it&apos;s stuff with a lease opened, some other regions will already be down so you get an inconsistent view of the state of the cluster. Let&apos;s have a contract here: if the cluster goes down, don&apos;t try anything fancy. &lt;/p&gt;</comment>
                            <comment id="12727105" author="stack" created="Fri, 3 Jul 2009 19:42:17 +0000"  >&lt;p&gt;You mean don&apos;t do anything fancy serverside?  (I&apos;ll sign that contract)&lt;/p&gt;</comment>
                            <comment id="12727110" author="jdcryans" created="Fri, 3 Jul 2009 19:49:08 +0000"  >&lt;p&gt;Yes, and the client can just ask ZK to know if the cluster is really going down... could even set a watch on it (ZKW.setClusterStateWatch)&lt;/p&gt;</comment>
                            <comment id="12727262" author="davelatham" created="Sat, 4 Jul 2009 17:36:33 +0000"  >&lt;p&gt;+1 for this on 0.19&lt;/p&gt;</comment>
                            <comment id="12727263" author="ryanobjc" created="Sat, 4 Jul 2009 18:13:55 +0000"  >&lt;p&gt;We should shut down a regionserver (when asked via sigkill or whatever) by first waiting for existing clients to finish their current operation, then refusing new connections, then shutting down.  By shutting down faster we can reassign regions faster during a rolling restart or other partial shutdown scenario.&lt;/p&gt;</comment>
                            <comment id="12727275" author="stack" created="Sat, 4 Jul 2009 21:42:29 +0000"  >&lt;p&gt;I see that in HRegionServer#next, we do a checkOpen and will do the below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; void checkOpen() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.stopRequested.get() || &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.abortRequested) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Server not running&quot;&lt;/span&gt; +
        (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.abortRequested? &lt;span class=&quot;code-quote&quot;&gt;&quot;, aborting&quot;&lt;/span&gt;: &quot;&quot;));
    }
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fsOk) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;File system not available&quot;&lt;/span&gt;);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... sending an IOE back to the scanner &amp;#8211; but we&apos;re not cancelling its lease.  Cancelling lease would help some at least in case where Scanners are active.&lt;/p&gt;

&lt;p&gt;But for inactives scanners &amp;#8211; e.g. a killed client &amp;#8211; we&apos;d still be waiting around.&lt;/p&gt;

&lt;p&gt;Regards Ryan&apos;s refusing new requests but finishing the outstanding, a get takes out a row lock which also takes a splitsAndClosesLock.readLock().lock();.... so you can&apos;t close region while a get is going on (it needs to get the write lock here).&lt;/p&gt;

&lt;p&gt;Individual put does the same.&lt;/p&gt;

&lt;p&gt;A scanner next does not take out the splitsAndCloses lock.  I suppose it never had too because scanner can ride over splits and we&apos;d never close while outstanding lease.&lt;/p&gt;

&lt;p&gt;Lets make it so next takes at least a close lock.  Then in actual shutdown, cancel all leases and not wait on any.  This should make it so at least the current scanner next completes but all subsequent next&apos;s will be rejected.&lt;/p&gt;</comment>
                            <comment id="12732257" author="stack" created="Fri, 17 Jul 2009 00:07:11 +0000"  >&lt;p&gt;This patch cancels leases when next is called and we&apos;re going down.&lt;/p&gt;

&lt;p&gt;It also just closes all leases in HRS before we run close of all regions.&lt;/p&gt;

&lt;p&gt;I don&apos;t see why we have to pussy-foot around with Scanners.  They are reading.   If they get exception in middle of reading, then no big deal.   It looks like Scanners will get UnknownScannerException if cluster is shutting down on them.  I added javadoc saying this could be one of the reasons for an USE.&lt;/p&gt;</comment>
                            <comment id="12732259" author="stack" created="Fri, 17 Jul 2009 00:09:40 +0000"  >&lt;p&gt;Testing...&lt;/p&gt;</comment>
                            <comment id="12732264" author="jdcryans" created="Fri, 17 Jul 2009 00:30:33 +0000"  >&lt;p&gt;+1 on patch. Good idea adding this to the javadoc.&lt;/p&gt;</comment>
                            <comment id="12732284" author="streamy" created="Fri, 17 Jul 2009 01:07:36 +0000"  >&lt;p&gt;+1 as well.  Patch looks good, like the javadoc, we get lots of users asking what those exceptions mean.&lt;/p&gt;</comment>
                            <comment id="12732334" author="stack" created="Fri, 17 Jul 2009 04:25:34 +0000"  >&lt;p&gt;Testing puts, I see the below in client when we shut down in middle of an upload:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-07-17 04:10:04,645 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=0 of max=10, waiting=2000ms
2009-07-17 04:10:06,904 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=1 of max=10, waiting=2000ms
2009-07-17 04:10:09,015 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=2 of max=10, waiting=2000ms
2009-07-17 04:10:11,068 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=3 of max=10, waiting=4000ms
2009-07-17 04:10:15,107 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=4 of max=10, waiting=4000ms
2009-07-17 04:10:19,216 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=5 of max=10, waiting=8000ms
2009-07-17 04:10:27,490 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=6 of max=10, waiting=8000ms
2009-07-17 04:10:35,534 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=7 of max=10, waiting=16000ms
2009-07-17 04:10:52,446 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Reloading region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 location because regionserver didn&apos;t accept updates; tries=8 of max=10, waiting=32000ms
2009-07-17 04:11:24,514 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server Some server, retryOnlyOne=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, index=0, islastrow=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, tries=9, numtries=10, i=0, listsize=8643, location=address: X.X.X.141:60020, regioninfo: REGION =&amp;gt; {NAME =&amp;gt; &apos;TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498&apos;, STARTKEY =&amp;gt; &apos;\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03&apos;, ENDKEY =&amp;gt; &apos;\x00\x03\x02\x04\x06\x07\x07\x00\x06\x06&apos;, ENCODED =&amp;gt; 1615573, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;TestTable&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;info&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, VERSIONS =&amp;gt; &apos;3&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}, region=TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region TestTable,\x00\x03\x02\x04\x05\x07\x09\x04\x00\x03,1247725178498, row &apos;\x00\x03\x02\x04\x06\x03\x09\x01\x02\x06&apos;, but failed after 10 attempts.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think I see connection refused too.&lt;/p&gt;

&lt;p&gt;That ain&apos;t bad I&apos;d say.&lt;/p&gt;

&lt;p&gt;This is with zk not managed by hbase.  If I shut down a cluster where hbase is managing the zk quorum &amp;#8211; i.e. its shutdown as part of hbase shutdown &amp;#8211; then I see client log filled with zk complaints with above intermixed.&lt;/p&gt;

&lt;p&gt;Scanning, I see EOFException because server went down returning result it looks like.&lt;/p&gt;

&lt;p&gt;Exceptions ain&apos;t pretty but I don&apos;t see anything inherently wrong.  Will go ahead and commit.&lt;/p&gt;

&lt;p&gt;With this new commit,  our new philosophy is no more trying to be mr. nice guy regards clients if admin wants cluster to go down.&lt;/p&gt;</comment>
                            <comment id="12732336" author="stack" created="Fri, 17 Jul 2009 04:39:02 +0000"  >&lt;p&gt;Committed to branch and trunk.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12413759" name="1609-v2.patch" size="5664" author="stack" created="Fri, 17 Jul 2009 00:07:11 +0000"/>
                            <attachment id="12412514" name="1609.patch" size="2284" author="stack" created="Fri, 3 Jul 2009 18:01:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 3 Jul 2009 18:04:47 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32205</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 23 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0he9j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99573</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>No more trying to be sweet to clients.  If admin wants cluster to go down, we go down and let the clients each EOFExceptions (because RPC was cutoff midways) and failed connect type exceptions.  Puts take out a row lock so should make it in before their Region closes (though if part of a batch, all in batch are not guaranteed to make it in)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>