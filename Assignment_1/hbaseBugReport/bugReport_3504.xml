<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:10:45 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3504/HBASE-3504.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3504] HLog performance improvement</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3504</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;The HLog.updateLock protects the rolling of logs with concurrent writes to the HDFS log file. This is a scalability bottleneck for a workload that comprises mostly of counter-increments.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12497659">HBASE-3504</key>
            <summary>HLog performance improvement</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="dhruba">dhruba borthakur</assignee>
                                    <reporter username="dhruba">dhruba borthakur</reporter>
                        <labels>
                    </labels>
                <created>Fri, 4 Feb 2011 06:57:02 +0000</created>
                <updated>Sun, 26 Jan 2014 22:35:59 +0000</updated>
                            <resolved>Sun, 26 Jan 2014 22:35:59 +0000</resolved>
                                                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12990470" author="dhruba" created="Fri, 4 Feb 2011 07:00:27 +0000"  >&lt;p&gt;One proposal is to make it a ReadWrite lock. The log roller should acquire the lock in Write mode while the HLog.append() calls could acquire it in Read mode.&lt;/p&gt;

&lt;p&gt;On a related note, I see a possible bug in the existing code where the logSyncer thread invokes HDFS.sync on a writer that has already been closed by the log-roller.This could happen because the logSyncer thread does not acquire the updateLock while invoking HDFS sync on the writer.&lt;/p&gt;</comment>
                            <comment id="12990472" author="ryanobjc" created="Fri, 4 Feb 2011 07:06:18 +0000"  >&lt;p&gt;But doesn&apos;t each appending thread need to atomically do some work? A read&lt;br/&gt;
write lock would not maintain that atomic semantics?&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3504?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12990470#comment-12990470&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3504?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12990470#comment-12990470&lt;/a&gt;]&lt;br/&gt;
the lock in Write mode while the HLog.append() calls could acquire it in&lt;br/&gt;
Read mode.&lt;br/&gt;
logSyncer thread invokes HDFS.sync on a writer that has already been closed&lt;br/&gt;
by the log-roller.This could happen because the logSyncer thread does not&lt;br/&gt;
acquire the updateLock while invoking HDFS sync on the writer.&lt;br/&gt;
to the HDFS log file. This is a scalability bottleneck for a workload that&lt;br/&gt;
comprises mostly of counter-increments.&lt;/p&gt;</comment>
                            <comment id="12990475" author="dhruba" created="Fri, 4 Feb 2011 07:17:04 +0000"  >&lt;p&gt;The HDFS write call is already synchronized within the HDFS client. So the HBase code does not need the updateLock for writing into a HDFS file handle.&lt;/p&gt;

&lt;p&gt;The reason HBase needs the updateLock is to ensure that the current writer is not closed (by the log roller) when it is being used to sync/write by another thread. The other operations like the invocation to  obtainSeqNum() and updating lastSeqWritten does not need to be protected by the updateLock, isn&apos;t it?&lt;/p&gt;</comment>
                            <comment id="12990476" author="tlipcon" created="Fri, 4 Feb 2011 07:25:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;The HDFS write call is already synchronized within the HDFS client. So the HBase code does not need the updateLock for writing into a HDFS file handle.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Don&apos;t we also need to ensure that updates happen in sequential sequence numbers?&lt;/p&gt;</comment>
                            <comment id="12990479" author="dhruba" created="Fri, 4 Feb 2011 07:29:44 +0000"  >&lt;p&gt;from my understanding of the code that processes log splits, it appears that we do not need to ensure that the seqNum of the KeyValues of a HLog file need to be in ascending order. Please do let me know if this assumption is valid.&lt;/p&gt;</comment>
                            <comment id="12990672" author="stack" created="Fri, 4 Feb 2011 18:42:51 +0000"  >&lt;p&gt;It looks like we write them out to the recovered.edits file in the order in which we encounter them: &lt;a href=&quot;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.html#659&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.html#659&lt;/a&gt;.  RegionEntryBuffer does not sort them, &lt;a href=&quot;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.html#581&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.html#581&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is where we replay the recovered edits on region open: &lt;a href=&quot;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#1837&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#1837&lt;/a&gt;  It looks like this code, on cursory review, is immune to out of order edits (Edits will be sorted on insertion into memstore).&lt;/p&gt;

&lt;p&gt;Out of order edits though could be a problem in case where we got a new value and a delete of that same value coming in at around the same time. Out of order could change result seen on other side of the log split.&lt;/p&gt;

&lt;p&gt;Sorting at split time would be an option.  Would be big change.&lt;/p&gt;</comment>
                            <comment id="12990683" author="streamy" created="Fri, 4 Feb 2011 19:10:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;Out of order edits though could be a problem in case where we got a new value and a delete of that same value coming in at around the same time. Out of order could change result seen on other side of the log split.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This would only be an issue if it is the &lt;em&gt;exact&lt;/em&gt; same time (same timestamp).  The same potential issue would exist with two Put operations of the same row/col/ts (we expect the later one to win).  However, in this case, it&apos;s not actually an issue because the row lock ensures that these two things (even if at the same ms timestamp) will happen serially from POV of HLog... even without the update lock, the writes will happen in order because they are done under the row lock.&lt;/p&gt;

&lt;p&gt;So the guarantee is that for a given row, it is always increasing seqids.  Across rows, it mostly will be but there is no guarantee.  In my code inspection, it doesn&apos;t seem like this is a problem&lt;/p&gt;</comment>
                            <comment id="12990690" author="tlipcon" created="Fri, 4 Feb 2011 19:28:37 +0000"  >&lt;p&gt;Makes me pretty nervous, though, to have something called sequenceID that isn&apos;t written sequentially... like you I can&apos;t think of any exact issues yet, but I bet there&apos;s something subtle and nefarious.&lt;/p&gt;

&lt;p&gt;What is the actual performance improvement to be gained measured by a benchmark?&lt;/p&gt;</comment>
                            <comment id="12990696" author="streamy" created="Fri, 4 Feb 2011 19:42:48 +0000"  >&lt;p&gt;We&apos;re measuring 20-40ms increment operations with all time being spent in WAL.  When jstacking, all threads are waiting on updateLock.  At some point (and this seems to be a common behavior under heavy concurrency) the lock becomes such a point of contention that performance all of a sudden drops dramatically.  We&apos;ll see 0-2ms WAL on lower loaded servers and then 20-40ms on higher loaded servers... seemingly there is a tipping point.  This happened with other locks as well that we have removed or moved to read/write locks.&lt;/p&gt;

&lt;p&gt;Basically, the cluster is crawling because a couple RS have insanely long WAL times with every thread piled on the updateLock.  It is the only current bottleneck we see (unless we turn off the WAL).  The actual sync latency is still low, it&apos;s just the updateLock contention.&lt;/p&gt;

&lt;p&gt;The critical point of seqids and HLogs is that the range of seqids in each HLog must never overlap (that&apos;s where the write lock comes in) and we must be able to determine the highest seqid in an HLog.  Also, that seqids within a row are in order.  Seqids across rows do not need to be in order; within a region, it&apos;s only important to ensure we properly track the oldest seqid in the MemStore and the latest seqid in each HLog.&lt;/p&gt;</comment>
                            <comment id="12990704" author="jdcryans" created="Fri, 4 Feb 2011 19:55:17 +0000"  >&lt;p&gt;I did some work previously to do as much as we can outside the updateLock, and looking at the code there&apos;s only one thing remaining that slows us down inside rollWriter once it gets the lock which is cleanupCurrentWriter that calls close on the file. This is a bit involved and takes a long time. I think we could switch the writing to the new log first and then close the file after we&apos;re done with the lock (but we have to be careful).&lt;/p&gt;</comment>
                            <comment id="12990720" author="streamy" created="Fri, 4 Feb 2011 20:12:13 +0000"  >&lt;p&gt;There is still some extra stuff inside the updateLock like a number of currentTimeMillis and other metrics-only operations.  These can be pulled out.  Unfortunately some of it would require a bit of refactoring.&lt;/p&gt;

&lt;p&gt;The work done inside the log-rolling is not really the issue, it&apos;s that every single write operation on the server requires grabbing a single, exclusive lock.  That&apos;s going to be (and is) a bottleneck.  We&apos;ve already done work around removing other locking bottlenecks, for example, around onlineRegions and the row lock data structures.  It showed marked improvements (when running w/o WAL).  Patches on all that forthcoming.&lt;/p&gt;

&lt;p&gt;A much bigger improvement will come from removing the exclusive lock and changing it to a read/write lock.  I&apos;ve gone through the code multiple times and I&apos;m not convinced that having out-of-order seqids is a problem as long as we retain the properties I described above.&lt;/p&gt;</comment>
                            <comment id="13882460" author="stack" created="Sun, 26 Jan 2014 22:35:59 +0000"  >&lt;p&gt;There is no update lock any more in trunk, the focus of this issue.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 4 Feb 2011 07:06:18 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>33056</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 46 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02fh3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12108</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>