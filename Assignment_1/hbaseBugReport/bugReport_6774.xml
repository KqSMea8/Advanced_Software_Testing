<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:39:43 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6774/HBASE-6774.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6774] Immediate assignment of regions that don&apos;t have entries in HLog</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6774</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;The algo is today, after a failure detection:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;split the logs&lt;/li&gt;
	&lt;li&gt;when all the logs are split, assign the regions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But some regions can have no entries at all in the HLog. There are many reasons for this:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;kind of reference or historical tables. Bulk written sometimes then read only.&lt;/li&gt;
	&lt;li&gt;sequential rowkeys. In this case, most of the regions will be read only. But they can be in a regionserver with a lot of writes.&lt;/li&gt;
	&lt;li&gt;tables flushed often for safety reasons. I&apos;m thinking about meta here.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For meta; we can imagine flushing very often. Hence, the recovery for meta, in many cases, will be the failure detection time.&lt;/p&gt;

&lt;p&gt;There are different possible algos:&lt;br/&gt;
Option 1)&lt;br/&gt;
 A new task is added, in parallel of the split. This task reads all the HLog. If there is no entry for a region, this region is assigned.&lt;br/&gt;
 Pro: simple&lt;br/&gt;
 Cons: We will need to read all the files. Add a read.&lt;br/&gt;
Option 2)&lt;br/&gt;
 The master writes in ZK the number of log files, per region.&lt;br/&gt;
 When the regionserver starts the split, it reads the full block (64M) and decrease the log file counter of the region. If it reaches 0, the assign start. At the end of its split, the region server decreases the counter as well. This allow to start the assign even if not all the HLog are finished. It would allow to make some regions available even if we have an issue in one of the log file.&lt;br/&gt;
 Pro: parallel&lt;br/&gt;
 Cons: add something to do for the region server. Requites to read the whole file before starting to write. &lt;br/&gt;
Option 3)&lt;br/&gt;
 Add some metadata at the end of the log file. The last log file won&apos;t have meta data, as if we are recovering, it&apos;s because the server crashed. But the others will. And last log file should be smaller (half a block on average).  &lt;br/&gt;
Option 4) Still some metadata, but in a different file. Cons: write are increased (but not that much, we just need to write the region once). Pros: if we lose the HLog files (major failure, no replica available) we can still continue with the regions that were not written at this stage.&lt;/p&gt;

&lt;p&gt;I think it should be done, even if none of the algorithm above is totally convincing yet. It&apos;s linked as well to locality and short circuit reads: with these two points reading the file twice become much less of an issue for example. My current preference would be to open the file twice in the region server, once for splitting as of today, once for a quick read looking for unused regions. Who knows, may be it would even be faster this way, the quick read thread would warm-up the different caches for the splitting thread.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12607467">HBASE-6774</key>
            <summary>Immediate assignment of regions that don&apos;t have entries in HLog</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="v.himanshu">Himanshu Vashishtha</assignee>
                                    <reporter username="nkeywal">Nicolas Liochon</reporter>
                        <labels>
                    </labels>
                <created>Thu, 13 Sep 2012 14:02:26 +0000</created>
                <updated>Mon, 6 May 2013 20:37:03 +0000</updated>
                                            <version>0.95.2</version>
                                                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>15</watches>
                                                                <comments>
                            <comment id="13502132" author="nkeywal" created="Wed, 21 Nov 2012 17:09:18 +0000"  >&lt;p&gt;After thinking again about this one, here is another possible solution:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;put the memstore state in ZooKeeper&lt;/li&gt;
	&lt;li&gt;when we create a new memstore, we asynchronously write the state in ZK (region with empty memstore &amp;amp; region server name)&lt;/li&gt;
	&lt;li&gt;When the first put is written in the WAL, we synchronously write to ZK that this region has now an non empty memstore.&lt;/li&gt;
	&lt;li&gt;then the other puts don&apos;t need any ZK writes or synchronisation&lt;/li&gt;
	&lt;li&gt;on memstore flush, we asynchronously update the state in ZK to empty memstore region.&lt;/li&gt;
	&lt;li&gt;on crash, the master checks the region memstore states. If region is assigned but its memstore is empty, we can reassign the region immediately. If there is no data in ZK, or this data says the memstore is not empty, the master does nothing.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is high level, I obviously need to tune it for multiple memstore case and study all error cases. But it seems doable.&lt;/p&gt;

&lt;p&gt;So we would have a maximum of 100K znodes (1 per region) in ZK, with one viewer (the master), and one writer (the region server).&lt;br/&gt;
These objects would be written on memstore creation &amp;amp; flush, so not very often.&lt;br/&gt;
If we don&apos;t have the znode in ZK, we split as today. We could loose the whole ZK data without any impact.&lt;br/&gt;
This can be made optional (and may be even activated per table: it could be activated only for reference tables and meta. Tables heavily written would not do that. This lowers the number of znode to write into ZK)&lt;br/&gt;
Region servers are already connected to zookeeper, we don&apos;t add any ZK connection.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;do the job: the region non written will be reassigned immediately&lt;/li&gt;
	&lt;li&gt;add a security if we can&apos;t split the logs: the table that were not written can be made available immediately&lt;/li&gt;
	&lt;li&gt;optional, and configurable per table&lt;/li&gt;
	&lt;li&gt;should not decrease write performances; only the first put is impacted (by about 10-15ms). With a block size of 128Mb or more, it&apos;s acceptable imho.&lt;/li&gt;
	&lt;li&gt;don&apos;t add workload (read nor write) on HDFS&lt;/li&gt;
	&lt;li&gt;no dependency on ZK content: we continue to work if the ZK content &apos;disappears&apos;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add workload on ZooKeeper: but it&apos;s configurable per table, so we can limit to whatever we want. We can even imagine heuristic (wait before creating the znode, don&apos;t create it if a put occurs before 10 seconds for example)&lt;/li&gt;
	&lt;li&gt;as always, any new feature adds complexity to the whole thing... Could nearly be done with coprocessors (likely not the master part however).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13502181" author="jxiang" created="Wed, 21 Nov 2012 18:32:09 +0000"  >&lt;p&gt;Can we just split log and assign region in parallel?  In opening a region, we check if the region is involved in log splitting somehow.  If not, open it.  Otherwise, hold there till the log splitting is done for that region.&lt;/p&gt;</comment>
                            <comment id="13502309" author="stack" created="Wed, 21 Nov 2012 21:00:47 +0000"  >&lt;p&gt;I like Jimmy&apos;s suggestion for first step at lowering MTTR hereabouts.&lt;/p&gt;

&lt;p&gt;What if we wrote on the end of a WAL a list of all regions mentioned?&lt;/p&gt;

&lt;p&gt;On crash, we&apos;d look at the tail of all WALs and scan fully all WALs that were not properly closed to get the list of regions with edits.  Could be done in master.  Before a region opens, could query master if it needs to pick up edits from a split?  (Maybe only do this is the assign is because of regionserver crash &amp;#8211; add a marker to the assign message).&lt;/p&gt;

&lt;p&gt;Writing stuff to zk could work but would be better if we could avoid having to do this?&lt;/p&gt;</comment>
                            <comment id="13502375" author="nkeywal" created="Wed, 21 Nov 2012 22:22:52 +0000"  >&lt;p&gt;Yes, everything is in the &quot;somehow&quot; of &lt;cite&gt;we check if the region is involved in log splitting somehow&lt;/cite&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The advantage of doing that in ZK is that we don&quot;t have to open all the WALs, with the risk of going to a dead datanode (bad datanode often means 60 seconds delay). And we don&apos;t have to read fully the last one (as well, if we finally implement the multi WALs, we will have all these WALs to fully read). For the others, technically, reading backward may be difficult to optimize. As well, if the WAL is corrupted, we save the regions that were not written to.&lt;/p&gt;


&lt;p&gt;This said, I agree that writing to ZK is not an easy decision. I think on the long term, having a widely shared real time status on the region is interesting, but we need the middleware (ZK here) to support this (lots of znodes with lots of readers). It&apos;s my famous  &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-1147&quot; title=&quot;Add support for local sessions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-1147&quot;&gt;&lt;del&gt;ZOOKEEPER-1147&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Devaraj told me an idea from Enis for the .meta. case: just do a specific WAL for it. It can be generalized with the multiwals as well.&lt;/p&gt;

&lt;p&gt;All these solutions are not incompatible between themselves anyway...&lt;/p&gt;</comment>
                            <comment id="13502413" author="stack" created="Wed, 21 Nov 2012 23:14:57 +0000"  >&lt;p&gt;If multiwals, yeah, should dedicate one for .META.&lt;/p&gt;

&lt;p&gt;Agree all suggestions are not incompatible: i.e. we should do Jimmy&apos;s suggestion (You may have suggested similar a while back IIRC).&lt;/p&gt;

&lt;p&gt;I like the issues you raise w/ the soln. I suggest.  While we could read the last WAL while splitting, just reading metadata off the end of all WALs concerned would take say ... about a second for each unless we did it in //... and if tens of WALs, thats tens of seconds before we could open a region even when all is functioning without hiccups (add hiccups, bad DN and it goes up significantly).&lt;/p&gt;

&lt;p&gt;I do like not having to have another subsystem in the mix doing log splitting. I suppose we already have an optional dependency on zk farming out the work.&lt;/p&gt;

&lt;p&gt;Could the regionserver send the master the regions mentioned in a WAL and let it do accounting or it could send sequenceids by flush to the master and let it figure out up to what entry it can skip edits?  It could do the writing to zk instead of every regionserver doing it for every WAL roll.  We are already sending over seqids on heartbeat so we can skip stale edits on crash?  Could expand this functionality so a by region dimension?  (Haven&apos;t thought it through... just making suggestion)&lt;/p&gt;
</comment>
                            <comment id="13502451" author="nkeywal" created="Wed, 21 Nov 2012 23:59:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;If multiwals, yeah, should dedicate one for .META.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Is someone working on multiwals implementation, or is it still in the &quot;currently studied&quot; state?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we should do Jimmy&apos;s suggestion. (You may have suggested similar a while back IIRC)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, it&apos;s option 3) in this jira description &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. I was not totally satisfied, that why I tried to find something different. I agree it&apos;s more a different balance than a better solution.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could the regionserver send the master the regions mentioned in a WAL and let it do accounting or it could send sequenceids by flush to haven&apos;t though about the master option, it could be a solution. I need to think about it.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13502845" author="nkeywal" created="Thu, 22 Nov 2012 16:24:59 +0000"  >&lt;p&gt;For the master based solution&lt;br/&gt;
If we go for the regionserver -&amp;gt; master  -&amp;gt; zookeeper solution, it&apos;s not perfect imho, because we just add an agent in the middle.&lt;/p&gt;

&lt;p&gt;The master could store the region information, without going to ZK&lt;br/&gt;
-&amp;gt; Faster than the solution with ZK, because we would not write to the disk&lt;br/&gt;
-&amp;gt; If we lose the master, we lose the date, but it&apos;s not an issue (just that the recovery will be slower: we will have to read all the logs)&lt;br/&gt;
-&amp;gt; The master becomes an element of the write path (for the first write in a memstore). I&apos;m not at ease with that.&lt;/p&gt;

&lt;p&gt;At the end of the day, I agree with what Stack said previously: let&apos;s not add a new component in the write path. This is valid for both the master &amp;amp; ZK.&lt;/p&gt;

&lt;p&gt;So we&apos;re left with the other options:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;specific WAL for .meta.&lt;/li&gt;
	&lt;li&gt;adding meta data at the end of the WAL.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m currently looking at them.&lt;/p&gt;</comment>
                            <comment id="13502985" author="devaraj" created="Fri, 23 Nov 2012 02:01:33 +0000"  >&lt;p&gt;I am starting to prototype the specific wal for .meta. approach (leveraging the implementation of FSHlog) to get a feel for the complexity, etc. Will keep folks posted (and probably raise a separate jira as well).&lt;/p&gt;</comment>
                            <comment id="13635351" author="v.himanshu" created="Thu, 18 Apr 2013 17:05:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devaraj&quot; class=&quot;user-hover&quot; rel=&quot;devaraj&quot;&gt;Devaraj Das&lt;/a&gt;: I am interested to know whether there is any progress on this issue (making regions available which do not have a WAL entry, i.e., not waiting for log splitting to finish). Faced this when working on a read intensive workload. As Nkeywal commented earlier, it is quite useful for some use-cases. There is already a separate WAL for .META., thanks to Devaraj. If you guys are OK, I would like to work on this.&lt;/p&gt;</comment>
                            <comment id="13635384" author="nkeywal" created="Thu, 18 Apr 2013 17:29:47 +0000"  >&lt;p&gt;Ok for me of course &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. Thanks for this. I don&apos;t have an ideal solution in mind, I guess there is some design work to do here, but may be Devaraj is more advanced than me. I assign the jira to you in case you don&apos;t have the ar for this.&lt;/p&gt;</comment>
                            <comment id="13635395" author="devaraj" created="Thu, 18 Apr 2013 17:33:25 +0000"  >&lt;p&gt;I am fine with that, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;.. I guess we should start with a proposal and agree on (this jira had multiple proposals).&lt;/p&gt;</comment>
                            <comment id="13635427" author="lhofhansl" created="Thu, 18 Apr 2013 17:55:24 +0000"  >&lt;p&gt;This mingles (somewhat at least) with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8375&quot; title=&quot;Durability setting per table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8375&quot;&gt;&lt;del&gt;HBASE-8375&lt;/del&gt;&lt;/a&gt; that I just opened. One of the options proposed there are &quot;unlogged tables&quot; (tables that never write WAL entries). All regions of those tables could be assigned immediately.&lt;/p&gt;</comment>
                            <comment id="13644240" author="v.himanshu" created="Mon, 29 Apr 2013 03:43:31 +0000"  >&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I have come up with a proposal. Suggestions are welcome. &lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="13644252" author="ram_krish" created="Mon, 29 Apr 2013 04:06:31 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
This is to cover all the regions that were updated after the last ServerLoad report and shutdown event. Once it has read the WAL files (usually only the last one) which have sequenceIds greater than max_completeSequenceId, it sends an open request &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; regions which has allWALEntriesFlushed set to &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, as they don&#8217;t need to wait &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; log splitting/replaying to complete
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Am not clear in this area.  I may be missing something in my understanding. Pls do correct me if am wrong.&lt;br/&gt;
I have allWALEntriesFlushed set to true, but the region has some additional wal entries in HLog just before the report and abrupt shutdown event happened.&lt;br/&gt;
When you say they don&apos;t need to wait for Log Splitting? &lt;br/&gt;
Also did you see Jeffrey&apos;s latest work on Log Splitting.  His proposal also uses the LatestCompleteFlushSeqId.  &lt;br/&gt;
Thanks for the write up.&lt;/p&gt;</comment>
                            <comment id="13644274" author="v.himanshu" created="Mon, 29 Apr 2013 05:14:52 +0000"  >&lt;p&gt;Hey Ram,&lt;/p&gt;

&lt;p&gt;Thanks for reading it through.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When you say they don&apos;t need to wait for Log Splitting? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So in case when there are some mutations after the last Serverreport and shutdown event, we need to look at the last WAL. Once we have read it and updated the  region:allWalEntriesFlushed mapping for WALEdits which has logSeqNum &amp;gt; max_completeSequenceId, we can open those regions which has allWalEntriesFlushed still set to true.&lt;/p&gt;

&lt;p&gt;Yes, I have looked at Jeffrey&apos;s work and will review it more this week. I didn&apos;t think he proposed any change in the usage of latestCompleteFlushSeqId, though. Also, that jira will make regions available for writes, and this is about making &quot;pristine&quot; regions available for reads.&lt;/p&gt;</comment>
                            <comment id="13645213" author="enis" created="Tue, 30 Apr 2013 03:43:42 +0000"  >&lt;p&gt;If I understand this correctly, this allWALEntriesFlushed does not seem to contain reliable information. With this proposal, it seems that we have to read the last WAL files to update the allWalEntriesFlushed to make up for the fact that the last heartbeat might not complete etc. But hartbeats themselves are not reliable as well. We cannot assume that by just reading the last WAL file, allWalEntriesFlushed will be correct, since we might have been missing hearthbeats for some time. The only reliable way is to read up the wal backwards, until for each region we make sure that we have read up to latestCompleteFlushSeqId. Which makes allWALEntriesFlushed redundant. &lt;br/&gt;
If a region has not got any update for some time, its latestCompleteFlushSeqId wont be updated at all, since there will be no flushes. To reassign this region, we have to ensure that all wals are read. &lt;/p&gt;

</comment>
                            <comment id="13645599" author="v.himanshu" created="Tue, 30 Apr 2013 14:08:36 +0000"  >&lt;p&gt;Hey Enis,&lt;/p&gt;

&lt;p&gt;Thanks for asking these questions.&lt;/p&gt;

&lt;p&gt;There is a &lt;b&gt;max_completeSequenceId&lt;/b&gt; per regionserver field in the attached doc, which is updated after receiving the heartbeat from a regionserver. When master processes the server shutdown event, it will use the max_completeSequenceId for the regionserver in order to determine how much WAL is relevant (it has missed) and need to read before finalizing allWALEntriesFlushed. The goal is to process all WALEdits which have walEdit#key#logSequenceId &amp;gt; max_completeSequenceId. If that means reading second last WAL also, it will process that too. The invariant is to read latest WAL files first, until we reach the point where some waledits in the wal are s.t. WALedit#key#logSequenceId &amp;lt; max_completeSequenceId. We no longer need to read older WALs then. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If a region has not got any update for some time, its latestCompleteFlushSeqId wont be updated at all, since there will be no flushes. To reassign this region, we have to ensure that all wals are read. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It uses max_completeSequenceId to read the remaining WAL. Once it has read all the WALEdits after max_completeSequenceId, allWALEntriesFlushed will have the correct information, and it can be used to assign a region or not. &lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;The only reliable way is to read up the wal backwards, &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I am not sure whether a sequenceFile can be read backwards, or how efficient it would be. That&apos;s why I propose to read a WAL file from its head and re-use the existing WALReader code.&lt;/p&gt;

&lt;p&gt;As soon as any region is flushed, master will have the most updated information for all regions for that regionserver once it receives the next heartbeat.&lt;/p&gt;

&lt;p&gt;Consider a rogue scenario: A regionserver sends a report and the max_completeSequenceId = 100. There is a write heavy workload and WAL is rolled and then server abort. And master missed all its heartbeats before the rs aborted. Based on max_completeSequenceId, we need to read last 2 WAL files (1 + 1): 1 new one, and 1 at which master got the last heartbeat (it has some entries &amp;gt; 100). Since we are reading most current ones first, it is easy to determine whether we need to older WALs or not. Let&apos;s call those files f1 and f2 where f1 is the latest. &lt;br/&gt;
It reads f1 first and see that the first waledit#key#logSequenceId &amp;gt; 100, so it en-queues f2 also as there might be some entries at f2&apos;s tail which are missed.&lt;br/&gt;
Once it has read f1 and f2, and updated the allWALEntriesFlushed for the regions, master can decide which regions can be assigned right away.&lt;/p&gt;

&lt;p&gt;Hope this helps.&lt;/p&gt;</comment>
                            <comment id="13645829" author="enis" created="Tue, 30 Apr 2013 18:45:08 +0000"  >&lt;p&gt;Thanks for the explanation. It seems that this can work, but the relative gain may not be that much to justify it. Other proposal for writing the list of region names in wal header, and reading them to determine, which tasks should be complete before to make sure the assignment seems more cleaner to me. &lt;/p&gt;</comment>
                            <comment id="13646005" author="v.himanshu" created="Tue, 30 Apr 2013 21:26:01 +0000"  >&lt;p&gt;Thanks Enis.&lt;/p&gt;

&lt;p&gt;Yes, WAL approach is also there but I think they both have their own plus and minus points. I proposed the ServerLoad approach because it is self contained and doesn&apos;t involve any changes in WAL/SequenceFile, etc, and re-uses existing ServerLoad object. &lt;/p&gt;

&lt;p&gt;In WAL meta data case, some meta data should be appended at the end of a WAL file. This involves adding custom key-value while closing the WAL file, and a check while reading every record (whether it is a meta record or not, etc).&lt;br/&gt;
Since it will be added at the end, master needs to open the reader and seek to the end of the file. This meta data should be read for all the log files, in a sequential manner starting from the oldest wal file in order to track a region timeline. This is in addition to reading the last WAL file.&lt;br/&gt;
An application that have high write rates, a regionserver may have larger number of WALs to replay.&lt;/p&gt;

&lt;p&gt;Another point is, IMHO, this feature should be made configurable as there might be some workloads which may not require this (writes distributed on all key-space, etc). With WAL approach, it becomes little bit tricky to make this feature optional, as it is inserting meta data in the WAL. With some meta entry in a WAL file, LogReader should always be aware of such entries, be it ReplicationLogReaders or LogSplitter as they might be reading some old logs, etc.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It seems that this can work, but the relative gain may not be that much to justify it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is just an alternative approach to the WAL one, and I think it is less intrusive. But I am open to both and would like to hear more of your opinions on the above points. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12646267">HBASE-8497</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12617472">HBASE-7213</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                                                <inwardlinks description="is required by">
                                        <issuelink>
            <issuekey id="12551766">HBASE-5843</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12580929" name="HBase-6774-approach.pdf" size="89933" author="v.himanshu" created="Mon, 29 Apr 2013 03:43:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 21 Nov 2012 18:32:09 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>241864</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 33 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02f4f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12051</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>