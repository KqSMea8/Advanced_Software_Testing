<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:10:36 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3484/HBASE-3484.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3484] Replace memstore&apos;s ConcurrentSkipListMap with our own implementation</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3484</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;By copy-pasting ConcurrentSkipListMap into HBase we can make two improvements to it for our use case in MemStore:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add an iterator.replace() method which should allow us to do upsert much more cheaply&lt;/li&gt;
	&lt;li&gt;implement a Set directly without having to do Map&amp;lt;KeyValue,KeyValue&amp;gt; to save one reference per entry&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It turns out CSLM is in public domain from its development as part of JSR 166, so we should be OK with licenses.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12496970">HBASE-3484</key>
            <summary>Replace memstore&apos;s ConcurrentSkipListMap with our own implementation</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="tlipcon">Todd Lipcon</reporter>
                        <labels>
                    </labels>
                <created>Thu, 27 Jan 2011 18:49:19 +0000</created>
                <updated>Mon, 4 Jan 2016 04:58:48 +0000</updated>
                                            <version>0.92.0</version>
                                                    <component>Performance</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>25</watches>
                                                                <comments>
                            <comment id="12987702" author="tlipcon" created="Thu, 27 Jan 2011 18:50:16 +0000"  >&lt;p&gt;Here&apos;s the link to the class in Apache Harmony:&lt;br/&gt;
&lt;a href=&quot;https://svn.apache.org/repos/asf/harmony/enhanced/java/branches/java6/classlib/modules/concurrent/src/main/java/java/util/concurrent/ConcurrentSkipListMap.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/repos/asf/harmony/enhanced/java/branches/java6/classlib/modules/concurrent/src/main/java/java/util/concurrent/ConcurrentSkipListMap.java&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13025463" author="jpallas" created="Tue, 26 Apr 2011 21:58:49 +0000"  >&lt;p&gt;This issue was cited by jdcryans as related to unfortunate performance seen in the following case:&lt;/p&gt;

&lt;p&gt;A test program fills a single row of a family with tens of thousands of sequentially increasing qualifiers.  Then it performs random gets (or exists) of those qualifiers.  The response time seen is (on average) proportional to the ordinal position of the qualifier.  If the table is flushed before the random tests begin, then the average response time is basically constant, independent of the qualifier&apos;s ordinal position.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure that either of the two points in the description actually covers this case, but I don&apos;t know enough to say.&lt;/p&gt;</comment>
                            <comment id="13029172" author="stack" created="Thu, 5 May 2011 06:35:54 +0000"  >&lt;p&gt;Upping this to critical.  It keeps coming up as an issue.&lt;/p&gt;</comment>
                            <comment id="13047342" author="jpallas" created="Fri, 10 Jun 2011 18:13:14 +0000"  >&lt;p&gt;I think the performance issue I mentioned above may actually be &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3855&quot; title=&quot;Performance degradation of memstore because reseek is linear&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3855&quot;&gt;&lt;del&gt;HBASE-3855&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13047511" author="stack" created="Fri, 10 Jun 2011 22:25:17 +0000"  >&lt;p&gt;Moving out of 0.92.  Don&apos;t see it happening in time.&lt;/p&gt;</comment>
                            <comment id="13213526" author="tlipcon" created="Wed, 22 Feb 2012 11:06:31 +0000"  >&lt;p&gt;Here&apos;s something I hacked together tonight which maps the memstore maps hierarchical. It should save a bit of CPU especially when doing wide puts, but I haven&apos;t done any serious benchmarking. It probably has negative memory effects in its current incarnation. Seems to kind-of work.&lt;/p&gt;</comment>
                            <comment id="13214132" author="stack" created="Thu, 23 Feb 2012 00:36:00 +0000"  >&lt;blockquote&gt;&lt;p&gt;It probably has negative memory effects in its current incarnation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How you think Todd?  Because of the tiering cost more or is it something to do w/ mslab allocations?&lt;/p&gt;

&lt;p&gt;What would you like to see test-wise proving this direction better than what we currently have? I could work up some tests?&lt;/p&gt;</comment>
                            <comment id="13214179" author="tlipcon" created="Thu, 23 Feb 2012 01:33:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;How you think Todd? Because of the tiering cost more or is it something to do w/ mslab allocations?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Extra container costs with the fact that we have extra CSLM objects for each row. I haven&apos;t measured but I bet there is some map-wide overhead that we&apos;re paying.&lt;/p&gt;

&lt;p&gt;There are some other things I noticed that could be improved, though. In particular, CSLM optimizes for Comparable keys, so if you specify a custom comparator, then it has to wrap every key you insert with a wrapper object. Specializing CSLM for our purposes would easily save 64 bytes per entry on this.&lt;/p&gt;

&lt;p&gt;Another thought I had was to do the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;have the actual entries in rowMap be Object-typed, rather than CSLMs.&lt;/li&gt;
	&lt;li&gt;when the first insert happens, just insert the KeyValue itself (optimization for the case where each row has only one cell)&lt;/li&gt;
	&lt;li&gt;when more inserts happen, swap it out for a proper container type&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The proper container type&apos;s also interesting to consider here. We never have contention on update within a row, since the updates happen under a row lock, right? So, we can consider any map type that supports single-writer multiple-reader efficiently, which is a wider range of data structures than support multi-writer multi-reader. One possibility is snap trees or even copy-on-write sorted array lists.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What would you like to see test-wise proving this direction better than what we currently have? I could work up some tests?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Would be great if we had a benchmark focused on memstore-only which allowed a mix of the following operations from different threads:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;full scans&lt;/li&gt;
	&lt;li&gt;range scans&lt;/li&gt;
	&lt;li&gt;updates to existing rows which just touch 1 or a few columns&lt;/li&gt;
	&lt;li&gt;updates to existing rows which touch lots of columns&lt;/li&gt;
	&lt;li&gt;inserts of new rows (few or lots of columns)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But it&apos;s a bit of work to do all that. So, a microbenchmark which just timed something like having 20 threads each do a bunch of inserts with multi-column rows would at least show whether there&apos;s promise here.&lt;/p&gt;</comment>
                            <comment id="13215446" author="stack" created="Fri, 24 Feb 2012 06:52:40 +0000"  >&lt;p&gt;Great stuff Todd.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...copy-on-write sorted array lists.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could we do this?  We&apos;d allocate a new array everytime we did an insert?  An array would be cheaper space wise and more efficient scanning, etc., I&apos;d think.... It&apos;d just be the insert and sort that&apos;d be &apos;expensive&apos;.&lt;/p&gt;

&lt;p&gt;Let me have a go at your suggested microbenchmark.&lt;/p&gt;</comment>
                            <comment id="13247736" author="jdcryans" created="Thu, 5 Apr 2012 21:50:16 +0000"  >&lt;p&gt;This is what the slow down currently looks like when using big MemStores. In this graph I flush at around 6GB and there&apos;s only 1 region per RS. It seems that the top speed is 100k/s which happens at the very beginning and it can go down to 40k/s.&lt;/p&gt;

&lt;p&gt;For those wondering, the dips are caused because we max out our links when the flushes happen all at the same time.&lt;/p&gt;</comment>
                            <comment id="13409901" author="otis" created="Mon, 9 Jul 2012 21:59:49 +0000"  >&lt;p&gt;@JD - what would/should the ideal graph look like, roughly?&lt;/p&gt;</comment>
                            <comment id="13410818" author="jdcryans" created="Tue, 10 Jul 2012 20:20:56 +0000"  >&lt;p&gt;Something that&apos;s not log( n ), so a straight line would be ideal &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Edit: apparently this is thumbdown &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13410934" author="mcorgan" created="Tue, 10 Jul 2012 21:24:42 +0000"  >&lt;p&gt;I&apos;ve been pondering how to better compact the data in the memstore.  Sometimes we see a 100MB memstore flush that is really 10MB of KeyValues, which gzips to like 2MB, meaning there is a ton of pointer overhead.&lt;/p&gt;

&lt;p&gt;One thing that came to mind was splitting each memstore into &quot;regions&quot; of consecutive cell ranges and fronting these regions with an index of some sort.  Instead of Set&amp;lt;KeyValue&amp;gt; the memstore is Set&amp;lt;Set&amp;lt;KeyValue&amp;gt;&amp;gt;.  When an internal region crosses a certain size we split it in half.  With a good index structure in front of the memstore blocks, it might get closer to a linear performance/size curve.  It&apos;s comparable with hbase splitting a table into regions.&lt;/p&gt;

&lt;p&gt;Then, to address the pointer overhead problem, you could use DataBlockEncoding to encode each memstore region individually.  A memstore region could accumulate several blocks that get compacted periodically.  Given a region size of ~64-256KB, the compaction could be very aggressive and could even be done by the thread writing the data.  Again, very similar to how hbase manages the internals of a single region.&lt;/p&gt;

&lt;p&gt;This adds moving pieces and complexity but could be developed as a pluggable module that passes the same unit tests as the current memstore.&lt;/p&gt;</comment>
                            <comment id="13410944" author="zhihyu@ebaysf.com" created="Tue, 10 Jul 2012 21:30:45 +0000"  >&lt;p&gt;+1 on the above suggestion.&lt;br/&gt;
We can trade some complexity for better compression rate.&lt;/p&gt;</comment>
                            <comment id="13756557" author="anoop.hbase" created="Tue, 3 Sep 2013 12:24:51 +0000"  >&lt;p&gt;Trying out some thing like how there can be multiple HFiles within a store. &lt;br/&gt;
Within a memstore there can be more than one KeyValueSkipListSet object at a time (and so CSLM)&lt;br/&gt;
For each of the KeyValueSkipListSet slice there is a configurable max size . Initially there will be only one KeyValueSkipListSet in the Memstore. Once the size reaches the threshold, we will create another KeyValueSkipListSet (So a new CSLM) and new KVs are inserted into this.  The old datastructure wont get KVs again. So within &lt;b&gt;one KeyValueSkipListSet&lt;/b&gt;  KVs will be sorted. This continues and finally all these KeyValueSkipListSets are taken in to Snapshots and written to HFile.  We will need changes in the MemstoreScanner so as to consider this as a heap and emit KVs in the correct order.&lt;br/&gt;
Once the flush is over again there will be only one KeyValueSkipListSet in a memstore and this continues.   Basically trying to avoid a single CSLM to grow to very big size with more #entries.&lt;/p&gt;

&lt;p&gt;By default there is no max size for a slice so single CSLM becoming bigger as long as KVs are inserted into memstore before a flush.&lt;/p&gt;

&lt;p&gt;Done a POC and tested also. The initial test with LoadTestTool shows that we can avoid the decrease in throughput with size of the memstore.  Will attach a patch with this change by this weekend.&lt;/p&gt;</comment>
                            <comment id="13756558" author="anoop.hbase" created="Tue, 3 Sep 2013 12:26:11 +0000"  >&lt;p&gt;Also seeing optimization possibilities as such to CSLM so as to have our own CSLM. Todd already mentioned some points above. Will be working on that as well as some other things. &lt;/p&gt;</comment>
                            <comment id="13760088" author="anoop.hbase" created="Fri, 6 Sep 2013 09:41:44 +0000"  >&lt;p&gt;Doing the memstore slicing&lt;/p&gt;</comment>
                            <comment id="13760178" author="anoop.hbase" created="Fri, 6 Sep 2013 12:39:44 +0000"  >&lt;p&gt;In current patch the max size of the memstore slice is specified in terms of heap size. But what matters is the #entries in the CSLM. I am thinking this can be #entries in the slice.&lt;/p&gt;</comment>
                            <comment id="13850170" author="lhofhansl" created="Tue, 17 Dec 2013 06:31:25 +0000"  >&lt;p&gt;From &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mcorgan&quot; class=&quot;user-hover&quot; rel=&quot;mcorgan&quot;&gt;Matt Corgan&lt;/a&gt;...&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I&apos;ve been pondering how to better compact the data in the memstore. Sometimes we see a 100MB memstore flush that is really 10MB of KeyValues, which gzips to like 2MB, meaning there is a ton of pointer overhead.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This should better now. In various patches I removed:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;caching of the row key (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7279&quot; title=&quot;Avoid copying the rowkey in RegionScanner, StoreScanner, and ScanQueryMatcher&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7279&quot;&gt;&lt;del&gt;HBASE-7279&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;caching of the timestamp (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7279&quot; title=&quot;Avoid copying the rowkey in RegionScanner, StoreScanner, and ScanQueryMatcher&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7279&quot;&gt;&lt;del&gt;HBASE-7279&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;caching of the KV length (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9956&quot; title=&quot;Remove keyLength cache from KeyValue&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9956&quot;&gt;&lt;del&gt;HBASE-9956&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That saves 44 bytes + sizeOf(rowKey) for each KeyValue in the memstore.&lt;br/&gt;
The KV in memory overhead now is: 56 bytes. (the memstoreTS is also stored in the HFiles).&lt;/p&gt;</comment>
                            <comment id="13925805" author="haosdent@gmail.com" created="Mon, 10 Mar 2014 15:35:33 +0000"  >&lt;p&gt;Anybody know about Order Maintenance Tree(&lt;a href=&quot;https://github.com/shuttler/omt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/shuttler/omt&lt;/a&gt;), which is used in TokuDB? As is know to us, SkipList isn&apos;t cpu cache-efficiency which OMT is cpu cache-efficiency.&lt;/p&gt;</comment>
                            <comment id="15080698" author="lhofhansl" created="Mon, 4 Jan 2016 04:58:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;CSLM optimizes for Comparable keys, so if you specify a custom comparator, then it has to wrap every key you insert with a wrapper object. Specializing CSLM for our purposes would easily save 64 bytes per entry on this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;At least with JDK 8 this is not the case. CSLM works without extra objects with or without a custom comparator.&lt;br/&gt;
(The Harmony class linked above has that problem, though.)&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12510490">HBASE-3993</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12601815" name="WIP_HBASE-3484.patch" size="52249" author="anoop.hbase" created="Fri, 6 Sep 2013 12:25:03 +0000"/>
                            <attachment id="12515575" name="hierarchical-map.txt" size="41018" author="tlipcon" created="Wed, 22 Feb 2012 11:06:31 +0000"/>
                            <attachment id="12521588" name="memstore_drag.png" size="32248" author="jdcryans" created="Thu, 5 Apr 2012 21:50:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 26 Apr 2011 21:58:49 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>33049</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            49 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02fcv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12089</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>