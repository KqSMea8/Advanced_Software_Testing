<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:40:31 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HADOOP-11693/HADOOP-11693.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HADOOP-11693] Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-11693</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;One of our customers&apos; production HBase clusters was periodically throttled by Azure storage, when HBase was archiving old WALs. HMaster aborted the region server and tried to restart it.&lt;/p&gt;

&lt;p&gt;However, since the cluster was still being throttled by Azure storage, the upcoming distributed log splitting also failed. Sometimes hbase:meta table was on this region server and finally showed offline, which cause the whole cluster in bad state.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:
ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller
Cause:
org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)
	at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)
	at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)
	... 8 more

2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing event M_META_SERVER_SHUTDOWN
java.io.IOException: failed log splitting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)
	at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)
	at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)
	... 4 more
Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.
	at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)
	at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)
	at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)
	at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)
	at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)
	at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)
	... 11 more

Sun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When archiving old WALs, WASB will do rename operation by copying src blob to destination blob and deleting the src blob. Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled. The throttling by Azure storage usually ends within 15mins. Current WASB retry policy is exponential retry, but only last at most for 2min. Short term fix will be adding a more intensive exponential retry when copy blob is throttled.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12779927">HADOOP-11693</key>
            <summary>Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="onpduo">Duo Xu</assignee>
                                    <reporter username="onpduo">Duo Xu</reporter>
                        <labels>
                    </labels>
                <created>Thu, 5 Mar 2015 23:19:13 +0000</created>
                <updated>Wed, 1 Jul 2015 23:02:42 +0000</updated>
                            <resolved>Wed, 11 Mar 2015 21:45:55 +0000</resolved>
                                                    <fixVersion>2.7.0</fixVersion>
                                    <component>tools</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="14349612" author="thomas.jungblut" created="Thu, 5 Mar 2015 23:26:42 +0000"  >&lt;p&gt;Good find Duo, this explains some throttling I&apos;ve seen already!&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Why can&apos;t we just &quot;move&quot; (CopyFromBlob) it to a different container? If I&apos;m not mistaken it is a pretty cheap linkage operation, O(1) so to say.&lt;/p&gt;</comment>
                            <comment id="14349625" author="onpduo" created="Thu, 5 Mar 2015 23:36:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thomas.jungblut&quot; class=&quot;user-hover&quot; rel=&quot;thomas.jungblut&quot;&gt;Thomas Jungblut&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We discussed with Azure storage team.&lt;/p&gt;

&lt;p&gt;The problem here is during copying blob, temp tables will be created. Azure storage has cleaner threads to clean these temp tables. When the number of temp tables reaches a certain number, Azure storage will throtte the copy blob operation so that no more temp tables will be created.&lt;/p&gt;

&lt;p&gt;However, during Azure storage gc, cleaner is blocked, which restricts the number of copy blob operations meanwhile.&lt;/p&gt;

&lt;p&gt;It seems not simply a linkage operation. I do not fully know the details though.&lt;/p&gt;</comment>
                            <comment id="14349800" author="onpduo" created="Fri, 6 Mar 2015 02:15:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnauroth&quot; class=&quot;user-hover&quot; rel=&quot;cnauroth&quot;&gt;Chris Nauroth&lt;/a&gt; Please take a look.&lt;/p&gt;</comment>
                            <comment id="14350588" author="thomas.jungblut" created="Fri, 6 Mar 2015 17:13:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onpduo&quot; class=&quot;user-hover&quot; rel=&quot;onpduo&quot;&gt;Duo Xu&lt;/a&gt; you are right- I was mistaking it with deletes of containers vs. blobs. &lt;/p&gt;

&lt;p&gt;Can you please shed some light on why we archive old WALs? I would assume we can just queue them up for deletion and delete them at a rate that doesn&apos;t cause throttling while it was splitting logs. &lt;/p&gt;

&lt;p&gt;I know this might be a very &quot;localized&quot; solution to HBase, do you see any better fix than just changing the retry backoffs?&lt;/p&gt;</comment>
                            <comment id="14351084" author="onpduo" created="Fri, 6 Mar 2015 22:53:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thomas.jungblut&quot; class=&quot;user-hover&quot; rel=&quot;thomas.jungblut&quot;&gt;Thomas Jungblut&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&apos;s some words from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;,&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&quot;There is currently two services which may keep the files in the archive dir. First is a TTL process, which ensures that the WAL files are kept at least &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 10 min. This is mainly &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; debugging.  The other one is replication. If you have replication setup, the replication processes will hang on to the WAL files until they are replicated.

There was some related discussion about directly deleting those files, but that was not implemented AFAIK. HBase assumes that rename() is a cheap operation, and uses rename &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; not only WAL files but &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; data files as well.&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, in cloud, especially in Azure storage, rename() is not a cheap operation. Currently Azure storage gc only happens on page blobs and not as frequently as you imagined. So changing retry backoffs on copyblob operation seems the only short term fix here.&lt;/p&gt;</comment>
                            <comment id="14353141" author="cnauroth" created="Mon, 9 Mar 2015 15:56:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;, although the issue title mentions HBase, the root cause for this problem actually resides in Hadoop Common, specifically the Azure Storage &lt;tt&gt;FileSystem&lt;/tt&gt; implementation.  I have updated the issue title to try to make this clearer.  It looks like I don&apos;t have access to move HBASE issues, so would you mind moving this back to HADOOP?  Thanks!&lt;/p&gt;</comment>
                            <comment id="14353180" author="apurtell" created="Mon, 9 Mar 2015 16:38:52 +0000"  >&lt;p&gt;Sure, moving it back! Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnauroth&quot; class=&quot;user-hover&quot; rel=&quot;cnauroth&quot;&gt;Chris Nauroth&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14353635" author="enis" created="Mon, 9 Mar 2015 21:22:48 +0000"  >&lt;p&gt;Patch looks good to me. Is copyBlob the only operation that may get ServerBusy ? &lt;/p&gt;</comment>
                            <comment id="14353644" author="onpduo" created="Mon, 9 Mar 2015 21:28:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Currently yes. Since our WASB driver is slow due to the synchronized hsync() method when writing WALs to Azure storage, we have not seen other operations being throttled.&lt;/p&gt;</comment>
                            <comment id="14353661" author="hadoopqa" created="Mon, 9 Mar 2015 21:34:43 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12702958/HADOOP-11681.01.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12702958/HADOOP-11681.01.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision d6e05c5.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 1 new or modified test files.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-tools/hadoop-azure.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5896//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5896//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5896//artifact/patchprocess/newPatchFindbugsWarningshadoop-azure.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5896//artifact/patchprocess/newPatchFindbugsWarningshadoop-azure.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5896//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5896//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14355491" author="onpduo" created="Tue, 10 Mar 2015 19:08:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnauroth&quot; class=&quot;user-hover&quot; rel=&quot;cnauroth&quot;&gt;Chris Nauroth&lt;/a&gt; Could you take a look?&lt;/p&gt;</comment>
                            <comment id="14355736" author="cnauroth" created="Tue, 10 Mar 2015 21:14:19 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onpduo&quot; class=&quot;user-hover&quot; rel=&quot;onpduo&quot;&gt;Duo Xu&lt;/a&gt;.  I have just one question.  Right now, the patch waits to see if it encounters a &lt;tt&gt;SERVER_BUSY&lt;/tt&gt; error, and then restarts the operation with the redefined retry policy.  Is there any reason not to just use this retry policy right from the beginning on the initial call to &lt;tt&gt;startCopyFromBlob&lt;/tt&gt;?&lt;/p&gt;

&lt;p&gt;The patch will need to be reformatted to fit Hadoop coding conventions.  We indent by 2 spaces, and we wrap lines that exceed 80 characters.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="14356054" author="onpduo" created="Wed, 11 Mar 2015 01:06:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnauroth&quot; class=&quot;user-hover&quot; rel=&quot;cnauroth&quot;&gt;Chris Nauroth&lt;/a&gt;&lt;br/&gt;
There is a default retry policy for all the Azure storage calls, and it is initialized with the Azure storage client.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_MIN_BACKOFF_INTERVAL = 1 * 1000; &lt;span class=&quot;code-comment&quot;&gt;// 1s
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_MAX_BACKOFF_INTERVAL = 30 * 1000; &lt;span class=&quot;code-comment&quot;&gt;// 30s
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_BACKOFF_INTERVAL = 1 * 1000; &lt;span class=&quot;code-comment&quot;&gt;// 1s
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_MAX_RETRY_ATTEMPTS = 15;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The backoff interval is 1s. Azure storage throttling issue caused by Azure storage GC happens very rarely. Until now only one customer met this issue and only last 10-15 mins every day. &lt;/p&gt;

&lt;p&gt;Below is the retry policy for copyblob,&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_COPYBLOB_MIN_BACKOFF_INTERVAL   = 3  * 1000; &lt;span class=&quot;code-comment&quot;&gt;// 3s
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_COPYBLOB_MAX_BACKOFF_INTERVAL   = 90 * 1000; &lt;span class=&quot;code-comment&quot;&gt;// 90s
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_COPYBLOB_BACKOFF_INTERVAL       = 30 * 1000;  &lt;span class=&quot;code-comment&quot;&gt;// 30s
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; DEFAULT_COPYBLOB_MAX_RETRY_ATTEMPTS     = 15;  
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The backoff is longer, 15s. We set these values in order to let it retry as much as 15min. We only apply this to the rare Azure storage GC case so we do not lose performance in the normal cases.&lt;/p&gt;

&lt;p&gt;I have changed the format.&lt;/p&gt;</comment>
                            <comment id="14356087" author="hadoopqa" created="Wed, 11 Mar 2015 01:44:43 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12703803/HADOOP-11681.02.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12703803/HADOOP-11681.02.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision a5cf985.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 1 new or modified test files.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-tools/hadoop-azure.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5913//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5913//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5913//artifact/patchprocess/newPatchFindbugsWarningshadoop-azure.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5913//artifact/patchprocess/newPatchFindbugsWarningshadoop-azure.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5913//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5913//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14357332" author="cnauroth" created="Wed, 11 Mar 2015 18:21:56 +0000"  >&lt;p&gt;Thanks for the explanation about normal case vs. the new backoff policy.  That makes sense.&lt;/p&gt;

&lt;p&gt;The eclipse:eclipse failure looks unrelated.  I couldn&apos;t reproduce it locally.&lt;/p&gt;

&lt;p&gt;Sorry to nitpick, but there are still some lines in &lt;tt&gt;AzureNativeFileSystemStore&lt;/tt&gt; that exceed the 80 character limit.  I know there are some existing lines in this file that already break the rule.  Don&apos;t worry about cleaning up all of the existing code, but please make sure all lines touched in the patch adhere to the 80 character limit.&lt;/p&gt;

&lt;p&gt;The findbugs warning is legitimate.  I&apos;m not sure why it&apos;s triggering now with this patch, as it appears the problem existed before the patch.  We can fix this by changing the &lt;tt&gt;catch (Exception e)&lt;/tt&gt; so that there are 2 separate catch clauses for &lt;tt&gt;catch (StorageException e)&lt;/tt&gt; and &lt;tt&gt;catch (URISyntaxException e)&lt;/tt&gt;.  Each one can be rethrown wrapped as an &lt;tt&gt;AzureException&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;We&apos;re almost there.  Thanks, Duo!&lt;/p&gt;</comment>
                            <comment id="14357350" author="cnauroth" created="Wed, 11 Mar 2015 18:30:31 +0000"  >&lt;p&gt;Please disregard the mention of an eclipse:eclipse failure in my last comment.  That comment was meant for a different patch.  Sorry about that.&lt;/p&gt;</comment>
                            <comment id="14357517" author="onpduo" created="Wed, 11 Mar 2015 20:19:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnauroth&quot; class=&quot;user-hover&quot; rel=&quot;cnauroth&quot;&gt;Chris Nauroth&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have wrapped those lines execeeding 80 characters, and indent by 2 characters. &lt;/p&gt;</comment>
                            <comment id="14357590" author="hadoopqa" created="Wed, 11 Mar 2015 20:59:23 +0000"  >&lt;p&gt;&lt;font color=&quot;green&quot;&gt;+1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12704005/HADOOP-11681.03.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12704005/HADOOP-11681.03.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 344d7cb.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 1 new or modified test files.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-tools/hadoop-azure.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5919//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5919//testReport/&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/5919//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/5919//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14357674" author="cnauroth" created="Wed, 11 Mar 2015 21:45:55 +0000"  >&lt;p&gt;I have committed this to trunk, branch-2 and branch-2.7.  Duo, thank you for contributing the patch and incorporating the feedback.  Thomas and Enis, thank you for helping with code review.&lt;/p&gt;</comment>
                            <comment id="14357688" author="hudson" created="Wed, 11 Mar 2015 21:54:53 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-trunk-Commit #7306 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-trunk-Commit/7306/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-trunk-Commit/7306/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14358535" author="hudson" created="Thu, 12 Mar 2015 11:36:55 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Yarn-trunk #864 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Yarn-trunk/864/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-Yarn-trunk/864/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14358541" author="hudson" created="Thu, 12 Mar 2015 11:37:43 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Yarn-trunk-Java8 #130 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/130/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-Yarn-trunk-Java8/130/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14358734" author="hudson" created="Thu, 12 Mar 2015 14:36:41 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Hdfs-trunk #2062 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Hdfs-trunk/2062/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-Hdfs-trunk/2062/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14358752" author="hudson" created="Thu, 12 Mar 2015 14:49:02 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Hdfs-trunk-Java8 #121 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/121/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/121/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14358830" author="hudson" created="Thu, 12 Mar 2015 15:35:18 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Mapreduce-trunk-Java8 #130 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/130/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/130/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14358883" author="hudson" created="Thu, 12 Mar 2015 16:12:50 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Mapreduce-trunk #2080 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2080/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/2080/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt;. Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving. Contributed by Duo Xu. (cnauroth: rev 7a346bcb4fa5b56191ed00a39e72e51c9bdf1b56)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azure/MockStorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterfaceImpl.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/AzureNativeFileSystemStore.java&lt;/li&gt;
	&lt;li&gt;hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/StorageInterface.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14573880" author="onpduo" created="Fri, 5 Jun 2015 00:38:55 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnauroth&quot; class=&quot;user-hover&quot; rel=&quot;cnauroth&quot;&gt;Chris Nauroth&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have submitted a new patch, which does the retries in WASB rather than rely on Azure Storage SDK. As I looked into the source code this week, Azure Storage SDK regards storage exception as non-retryable, so when throttling happens, the current code might still not work.&lt;/p&gt;

&lt;p&gt;Could you reopen this JIRA and review it ASAP?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="14573965" author="cnauroth" created="Fri, 5 Jun 2015 02:44:27 +0000"  >&lt;p&gt;Hello &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=onpduo&quot; class=&quot;user-hover&quot; rel=&quot;onpduo&quot;&gt;Duo Xu&lt;/a&gt;.  The &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11693&quot; title=&quot;Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11693&quot;&gt;&lt;del&gt;HADOOP-11693&lt;/del&gt;&lt;/a&gt; patch already shipped in Apache Hadoop 2.7.0.  At this point, please create a new jira to track the new change instead of attaching new patches here.&lt;/p&gt;

&lt;p&gt;BTW, I noticed that this new patch appears to be in a multi-byte character encoding (UTF-16 with BOM?).  When you create the new jira, please attach an ASCII patch file.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12702958" name="HADOOP-11681.01.patch" size="7222" author="onpduo" created="Fri, 6 Mar 2015 02:15:14 +0000"/>
                            <attachment id="12703803" name="HADOOP-11681.02.patch" size="7225" author="onpduo" created="Wed, 11 Mar 2015 01:06:08 +0000"/>
                            <attachment id="12704005" name="HADOOP-11681.03.patch" size="7544" author="onpduo" created="Wed, 11 Mar 2015 20:19:03 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 5 Mar 2015 23:26:42 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 28 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i26fbj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue>12327583</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>