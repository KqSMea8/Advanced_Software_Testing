<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:20:27 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-17040/HBASE-17040.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-17040] HBase Spark does not work in Kerberos and yarn-master mode</title>
                <link>https://issues.apache.org/jira/browse/HBASE-17040</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We are loading hbase records  to RDD with the hbase-spark library in Cloudera. &lt;/p&gt;

&lt;p&gt;The hbase-spark code works if  we submit the job with client mode, but does not work in cluster mode. We got below exceptions:&lt;br/&gt;
```&lt;br/&gt;
16/11/07 05:43:28 WARN security.UserGroupInformation: PriviledgedActionException as:spark (auth:SIMPLE) cause:javax.security.sasl.SaslException: GSS initiate failed &lt;span class=&quot;error&quot;&gt;&amp;#91;Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&amp;#93;&lt;/span&gt;&lt;br/&gt;
16/11/07 05:43:28 WARN ipc.RpcClientImpl: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed &lt;span class=&quot;error&quot;&gt;&amp;#91;Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&amp;#93;&lt;/span&gt;&lt;br/&gt;
16/11/07 05:43:28 ERROR ipc.RpcClientImpl: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider &apos;kinit&apos;.&lt;br/&gt;
javax.security.sasl.SaslException: GSS initiate failed &lt;span class=&quot;error&quot;&gt;&amp;#91;Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&amp;#93;&lt;/span&gt;&lt;br/&gt;
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:181)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupSaslConnection(RpcClientImpl.java:617)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.access$700(RpcClientImpl.java:162)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection$2.run(RpcClientImpl.java:743)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection$2.run(RpcClientImpl.java:740)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:422)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:740)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.writeRequest(RpcClientImpl.java:906)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.tracedWriteRequest(RpcClientImpl.java:873)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1242)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:226)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:331)&lt;br/&gt;
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:34118)&lt;br/&gt;
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1627)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:92)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:89)&lt;br/&gt;
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:95)&lt;br/&gt;
	at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:73)&lt;br/&gt;
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:86)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:111)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:108)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:422)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:340)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:108)&lt;br/&gt;
	at org.apache.hadoop.hbase.security.token.TokenUtil.addTokenForJob(TokenUtil.java:329)&lt;br/&gt;
	at org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:490)&lt;br/&gt;
	at org.apache.hadoop.hbase.spark.HBaseContext.&amp;lt;init&amp;gt;(HBaseContext.scala:70)&lt;br/&gt;
```&lt;/p&gt;</description>
                <environment>&lt;p&gt;HBase&lt;br/&gt;
Kerberos&lt;br/&gt;
Yarn&lt;br/&gt;
Cloudera&lt;/p&gt;</environment>
        <key id="13018743">HBASE-17040</key>
            <summary>HBase Spark does not work in Kerberos and yarn-master mode</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="caobinzi">Binzi Cao</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 Nov 2016 05:54:27 +0000</created>
                <updated>Mon, 7 Nov 2016 23:58:08 +0000</updated>
                                            <version>2.0.0</version>
                                                    <component>spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="15645247" author="busbey" created="Mon, 7 Nov 2016 19:57:56 +0000"  >&lt;p&gt;what&apos;s the &lt;tt&gt;spark-submit&lt;/tt&gt; line you&apos;re using?&lt;/p&gt;</comment>
                            <comment id="15645605" author="caobinzi" created="Mon, 7 Nov 2016 22:03:06 +0000"  >&lt;p&gt;My &lt;tt&gt;spark-sbumit&lt;/tt&gt; command is as below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
HADOOP_USER_NAME=spark spark-submit   \
--jars &lt;span class=&quot;code-quote&quot;&gt;&quot;local:&lt;span class=&quot;code-comment&quot;&gt;///opt/cloudera/parcels/CDH/jars/hbase-spark-1.2.0-cdh5.8.2.jar&quot;&lt;/span&gt; \
&lt;/span&gt;--driver-memory 5G \
--executor-memory 5G \
--num-executors  5 \
--deploy-mode cluster \
--files &lt;span class=&quot;code-quote&quot;&gt;&quot;my_application.conf,hbase.keytab&quot;&lt;/span&gt;  \
--class &lt;span class=&quot;code-quote&quot;&gt;&quot;MySparkApp&quot;&lt;/span&gt; --master yarn  \
--driver-java-options &lt;span class=&quot;code-quote&quot;&gt;&quot;-Dconfig.file=my_application.conf&quot;&lt;/span&gt; \
MyHBaseApp.jar
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I tried two different ways to start the job:&lt;/p&gt;

&lt;p&gt;1. &lt;tt&gt;kinit&lt;/tt&gt; first with a user with spark and hbase permissions. Spark job can be started successfully but will fail to create &lt;tt&gt;HBaseContext&lt;/tt&gt; with above exceptions. &lt;/p&gt;


&lt;p&gt;2. Pass the keytab file to job and load it in code, the principal/keytab file can be loaded properly, but the &lt;tt&gt;HBaseContext&lt;/tt&gt; could not be created as it does not use the keytab credentials &lt;/p&gt;

&lt;p&gt;The submit command &lt;b&gt;works&lt;/b&gt; well if I change the &lt;tt&gt;deploy-mode&lt;/tt&gt; to &lt;tt&gt;client&lt;/tt&gt;.&lt;/p&gt;
</comment>
                            <comment id="15645707" author="busbey" created="Mon, 7 Nov 2016 22:40:45 +0000"  >&lt;p&gt;(Since this is an unreleased feature, the most expedient help will generally come from using the support mechanism of the vendor who is publishing it to their downstream users. For future reference, for Cloudera that&apos;s &lt;a href=&quot;https://community.cloudera.com/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;community.cloudera.com&lt;/a&gt;. But I&apos;m likely to be the person answering in any case :joy_cat:. If this had been in an Apache HBase release, usually user@hbase would be the place to start.)&lt;/p&gt;

&lt;p&gt;Can you post the stacktrace for the error using &lt;tt&gt;&amp;#45;&amp;#45;principal&lt;/tt&gt; and &lt;tt&gt;&amp;#45;&amp;#45;keytab&lt;/tt&gt;? That&apos;s what I would expect to be needed for &lt;tt&gt;&amp;#45;&amp;#45;deploy-mode cluster&lt;/tt&gt; (and what I&apos;ve previously tested prior to the hbase-spark module as documented in &lt;a href=&quot;https://github.com/saintstack/hbase-downstreamer#spark-streaming-test-application&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;the hbase-downstreamer project&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="15645827" author="caobinzi" created="Mon, 7 Nov 2016 23:39:13 +0000"  >&lt;p&gt;Hi Sean, &lt;/p&gt;

&lt;p&gt;Thank you very much for the quick update! It seems the Cloudera hbase-spark libarary is a fork from apache hbase now , so I put the ticket here as I did not find a proper place to raise it. Sorry that if this is not the right place. &lt;/p&gt;

&lt;p&gt;I tried to use &lt;tt&gt;principal&lt;/tt&gt; and &lt;tt&gt;keytab&lt;/tt&gt; in &lt;tt&gt;spark-submit&lt;/tt&gt; and I got the same exception.  I also had a look at the code in &lt;tt&gt;hbase-downstreamer&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The test code created ugi and a connection with the principal and keytab in doAs, which works in my environment as well. &lt;/p&gt;

&lt;p&gt;However, my project is using HBaseContext to load the HBase Records to a RDD. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        val hbaseContext = createHbaseContext()
        val scan         = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Scan()
        val scanRdd      = hbaseContext.hbaseRDD(TableName.valueOf(table_name), scan)
        scanRdd.map(_._2)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;tt&gt;HBaseContext&lt;/tt&gt; handles the connection and credentials automatically but not working well in Cluster mode. &lt;/p&gt;






</comment>
                            <comment id="15645851" author="busbey" created="Mon, 7 Nov 2016 23:51:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;However, my project is using HBaseContext to load the HBase Records to a RDD.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        val hbaseContext = createHbaseContext()
        val scan         = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Scan()
        val scanRdd      = hbaseContext.hbaseRDD(TableName.valueOf(table_name), scan)
        scanRdd.map(_._2)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The HBaseContext handles the connection and credentials automatically but not working well in Cluster mode.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for the additional context. Can you get me the specific stack trace from a submittal that uses &lt;tt&gt;-&lt;del&gt;principal&lt;/tt&gt; and &lt;tt&gt;&lt;/del&gt;-keytab&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="15645866" author="caobinzi" created="Mon, 7 Nov 2016 23:58:08 +0000"  >&lt;p&gt;The call trace is as below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;

16/11/07 23:18:14 WARN security.UserGroupInformation: PriviledgedActionException as:spark (auth:SIMPLE) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
16/11/07 23:18:14 WARN ipc.RpcClientImpl: Exception encountered &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
16/11/07 23:18:14 ERROR ipc.RpcClientImpl: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider &apos;kinit&apos;.
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
	at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:181)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupSaslConnection(RpcClientImpl.java:617)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.access$700(RpcClientImpl.java:162)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection$2.run(RpcClientImpl.java:743)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection$2.run(RpcClientImpl.java:740)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:740)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.writeRequest(RpcClientImpl.java:906)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.tracedWriteRequest(RpcClientImpl.java:873)
	at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1242)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:226)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:331)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:34118)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1627)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:92)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:89)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)
	at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:95)
	at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:73)
	at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:86)
	at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:111)
	at org.apache.hadoop.hbase.security.token.TokenUtil$1.run(TokenUtil.java:108)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:340)
	at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:108)
	at org.apache.hadoop.hbase.security.token.TokenUtil.addTokenForJob(TokenUtil.java:329)
	at org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:490)
	at org.apache.hadoop.hbase.spark.HBaseContext.&amp;lt;init&amp;gt;(HBaseContext.scala:70)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The app code is as below&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;

 hbaseContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HBaseContext(sparkContext, configuration)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;



&lt;p&gt;Basically, I could not create an &lt;tt&gt;HBaseConext&lt;/tt&gt; successfully&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 7 Nov 2016 19:57:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35zlj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>