<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:09:15 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3323/HBASE-3323.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3323] OOME in master splitting logs</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3323</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In testing a RS failure under heavy increment workload I ran into an OOME when the master was splitting the logs.&lt;/p&gt;

&lt;p&gt;In this test case, I have exactly 136 bytes per log entry in all the logs, and the logs are all around 66-74MB). With a batch size of 3 logs, this means the master is loading about 500K-600K edits per log file. Each edit ends up creating 3 byte[] objects, the references for which are each 8 bytes of RAM, so we have 160 (136+8*3) bytes per edit used by the byte[]. For each edit we also allocate a bunch of other objects: one HLog$Entry, one WALEdit, one ArrayList, one LinkedList$Entry, one HLogKey, and one KeyValue. Overall this works out to 400 bytes of overhead per edit. So, with the default settings on this fairly average workload, the 1.5M log entries takes about 770MB of RAM. Since I had a few log files that were a bit larger (around 90MB) it exceeded 1GB of RAM and I got an OOME.&lt;/p&gt;

&lt;p&gt;For one, the 400 bytes per edit overhead is pretty bad, and we could probably be a lot more efficient. For two, we should actually account this rather than simply having a configurable &quot;batch size&quot; in the master.&lt;/p&gt;

&lt;p&gt;I think this is a blocker because I&apos;m running with fairly default configs here and just killing one RS made the cluster fall over due to master OOME.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12492801">HBASE-3323</key>
            <summary>OOME in master splitting logs</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tlipcon">Todd Lipcon</assignee>
                                    <reporter username="tlipcon">Todd Lipcon</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 Dec 2010 08:02:45 +0000</created>
                <updated>Fri, 20 Nov 2015 12:41:57 +0000</updated>
                            <resolved>Mon, 20 Dec 2010 20:41:16 +0000</resolved>
                                    <version>0.90.0</version>
                                    <fixVersion>0.90.0</fixVersion>
                                    <component>master</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12969895" author="clehene" created="Thu, 9 Dec 2010 19:43:46 +0000"  >
&lt;p&gt;Here&apos;s the object distribution tlipcon mentioned:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
The values of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; map contain the 1.5M+ edits (in Entry objects) tlipcon mentioned

Map&amp;lt;&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[], LinkedList&amp;lt;Entry&amp;gt;&amp;gt; editsByRegion
      |                  |
      |                  |
      |                  |
(encodedRegionName)      |
      .                  |
      .                  |
      .                  |
      .                  |
      .                  |
      .                  | 
      .                  --- WalEdit edit
      .                  |      |
      .                  |      |
      .                  |      |
      .                  |      --- ArrayList&amp;lt;KeyValue&amp;gt; kvs
      .                  |                      |
      .                  |                      |
      .                  |                      |
      .                  |                      --- &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] bytes
      .                  |                          
      .                  |
      .  ----------------------------------------------------------
      .  |               |                                        |
      .  |               |                                        |
      .  |               --- HLogKey key                          |
      .  |                    |                                   |
      .  |                    |                                   |
      .  |                    |                                   |
      .  |                    |                                   |
      . .| . . . . . . . . . .--- &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] encodedRegionName        |
         |                    |                                   |
         |                    |                                   |
         |                    |                                   |
         |                    --- &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] tableName                |
         |                                                        |
         |                                                        |
         | &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is useless as we could have &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; in the map key   |
         ----------------------------------------------------------

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The splitLog workflow loads all the edits in a map indexed by region, and then uses a thread pool to write them to per region directories.&lt;/p&gt;


&lt;p&gt;As you can see from this diagram, each edit duplicates the tableName and the encodedRegionName (hence the 2 extra byte[]). &lt;/p&gt;

&lt;p&gt;&lt;b&gt;One simple, partial solution:&lt;/b&gt;&lt;br/&gt;
We can reduce the memory footprint by putting the tableName in the map key with the encodedRegionName (it&apos;s free). This would leave us with a LinkedList of WalEdit objects (ArrayList + KeyValue + the actual info: byte[]). Of course this could be further compressed, but it might not be worth it (WalEdit has a replication scope as well IIUC). &lt;br/&gt;
This is a partial solution since we still don&apos;t solve the case when we have too much data in the HLogs.&lt;/p&gt;


&lt;p&gt;&lt;b&gt;A second solution/suggestion:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;We can change the split process a bit. Let me explain how HLogs are organized and how we split (please correct me if I&apos;m wrong):&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Context:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Eeach region server has one HLog directory in HDFS (under /hbase/.logs)&lt;/li&gt;
	&lt;li&gt;In each HRegionServer corresponding directory there&apos;s a bunch of HLog files.&lt;/li&gt;
	&lt;li&gt;There&apos;s a strict order of the HLog files within a region&apos;s dir and edits inside are ordered as well.&lt;/li&gt;
	&lt;li&gt;We read all the files in memory first because we need all the edits for a particular region and to respect the order of the edits.&lt;/li&gt;
	&lt;li&gt;Only after everything is read, we use a thread pool to distribute the log entries per regions.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;b&gt;Suggestion:&lt;/b&gt;&lt;br/&gt;
We could read the files in parallel, and instead of writing a single file in the HRegion corresponding directory, we write one file for each HLog. This should keep all the edits in strict order. Then HRegionServer could safely load them in the same order and apply edits. &lt;/p&gt;

&lt;p&gt;While we read the files in parallel we don&apos;t have to read the entire content in memory: we can just read and write to the corresponding destination file. This should solve the memory footprint problem. &lt;/p&gt;


&lt;p&gt;I haven&apos;t spent too much time analyzing the second option; it might have been discussed in the past, so if I&apos;m missing something let me know.&lt;/p&gt;


&lt;p&gt;Cosmin&lt;/p&gt;</comment>
                            <comment id="12969926" author="clehene" created="Thu, 9 Dec 2010 20:19:50 +0000"  >&lt;p&gt;Talking with JD, we figured &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1364&quot; title=&quot;[performance] Distributed splitting of regionserver commit logs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1364&quot;&gt;&lt;del&gt;HBASE-1364&lt;/del&gt;&lt;/a&gt; might be a solution as well.&lt;/p&gt;</comment>
                            <comment id="12970025" author="tlipcon" created="Fri, 10 Dec 2010 00:14:07 +0000"  >&lt;p&gt;Hi Cosmin. I agree that the log split process can be fixed up a bit to use a smaller amount of memory. Fixing the data structure to get rid of all those extra objects is one thing that&apos;s pretty straightfowrard like you mentioned. As for how to change the split process itself, I think following a more typical producer/consumer model makes more sense. For example, something like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;buffers = map&amp;lt;region name, arraylist&amp;lt;edit&amp;gt;&amp;gt;
for each log:
  for each edit:
    buf = map.get(region) [inserting a new arraylist if necessary]
    buf.add(edit)
    if buf length &amp;gt; some number of bytes:
      while workqueue length &amp;gt; some threshold: wait
      workqueue.add(buf)
      buffers.remove(region)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then we have a set of threads which pull chunks of edits off the work queue and write into the appropriate file (where the file handles are kept open).&lt;/p&gt;</comment>
                            <comment id="12970063" author="tlipcon" created="Fri, 10 Dec 2010 03:33:50 +0000"  >&lt;p&gt;I&apos;m going to take a leap and assume I still remember how to code and work on this one tonight.&lt;/p&gt;</comment>
                            <comment id="12970091" author="stack" created="Fri, 10 Dec 2010 06:57:36 +0000"  >&lt;p&gt;Cosmin, on the &apos;One simple, partial solution&apos;, I played with changing the LinkedList from Entry to LinkedList of WALEdits and changing the Map Key to be a data structure of encodedRegionName plus tablename (I used HLogKey rather than add a new data structure and then made a special Comparator that equated HLogKeys that had same encodedRegionName).  All was going nicely till it came to dump out the per region recovered.edit files.  At this point I need the Entry.  A WALEdit will not do.  So it seems.  We need seqid at least (This could change when KV has seqid &amp;#8211; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2856&quot; title=&quot;TestAcidGuarantee broken on trunk &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2856&quot;&gt;&lt;del&gt;HBASE-2856&lt;/del&gt;&lt;/a&gt; &apos;TestAcidGuarantee broken on trunk&apos;).&lt;/p&gt;</comment>
                            <comment id="12970092" author="stack" created="Fri, 10 Dec 2010 07:06:44 +0000"  >&lt;p&gt;The second option seems viable to me Cosmin.  &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2727&quot; title=&quot;Splits writing one file only is untenable; need dir of recovered edits ordered by sequenceid.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2727&quot;&gt;&lt;del&gt;HBASE-2727&lt;/del&gt;&lt;/a&gt; made it so we could pick up multiple files from recovered.edits directory.  The files we write are named fro the seqid of the first edit so we can pick them up in order.&lt;/p&gt;</comment>
                            <comment id="12970097" author="tlipcon" created="Fri, 10 Dec 2010 07:49:34 +0000"  >&lt;p&gt;Here&apos;s a patch which basically redoes the way log splitting happens. It needs to be commented up and I want to rename some things, but the basic architecture is this:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Main thread reads logs in order and writes into a structure called EntrySink (I want to rename this to EntryBuffer or sometihng)&lt;/li&gt;
	&lt;li&gt;EntrySink maintains some kind of approximate heap size (I don&apos;t think I calculated it quite right, but c&apos;est la vie) and also takes care of managing a RegionEntryBuffer for each region key.
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;The RegionEntryBuffer just has a LinkedList of Entries right now, but it does size accounting, and I think we could change these to a fancier data structure for more efficient memory usage (eg a linked list of 10000-entry arrays)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;If the main thread tries to append into the EntrySink but the heap usage has hit a max threshold, it waits.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Meanwhile, there are N threads called WriterThread-n which do the following in a loop:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;poll the EntrySink to grab a RegionEntryBuffer
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;The EntrySink returns the one with the most outstanding edits (hope is to write larger sequential chunks if possible)&lt;/li&gt;
		&lt;li&gt;The EntrySink also keeps track of which regions already have some thread working on them, so we don&apos;t end up with out-of-order appends&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;The EntrySink then drains the RegionEntryBuffer into the &quot;OutputSink&quot; which maintains the map from region key to WriterAndPath (bug in patch uploaded: this map needs to be synchronizedMap)&lt;/li&gt;
	&lt;li&gt;Once the buffer is drained, it notifies the EntrySink that the memory is no longer in use (hence unblocking the producer thread)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In summary, it&apos;s a fairly standard producer-consumer pattern with some trickery to make a separate queue per region so as not to reorder edits.&lt;/p&gt;

&lt;p&gt;As a non-scientific test I patched this into my cluster which was getting the OOME on master startup, and it not only started up fine, the log splits ran about 50% faster than they did before!&lt;/p&gt;

&lt;p&gt;Known bug: the &quot;log N of M&quot; always says &quot;log 1 of M&quot;&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="12970098" author="tlipcon" created="Fri, 10 Dec 2010 07:53:03 +0000"  >&lt;p&gt;Oh, I should also note that you probably want to turn off DEBUG logs or comment them out before testing this on any real log. It&apos;s quite noisy at the moment. Also, it seems like the 128M buffer I hardcoded is plenty fine - the writers stayed completely ahead of the reader in the test I ran.&lt;/p&gt;

&lt;p&gt;We should make a standalone benchmark for this algorithm too.&lt;/p&gt;</comment>
                            <comment id="12970333" author="stack" created="Fri, 10 Dec 2010 23:08:03 +0000"  >&lt;p&gt;I&apos;m part way through a review but have to leave.  So far it looks like less moving parts and cleaner overall.  Will finish up review tomorrow.   We could set the number of splits to 1 and ship the RC with that but at the moment, going by the other issues that need fixing, its looking like next week before new RC and that might be time to test this redo of splits. &lt;/p&gt;</comment>
                            <comment id="12970354" author="tlipcon" created="Sat, 11 Dec 2010 00:48:14 +0000"  >&lt;p&gt;Here&apos;s an updated patch, refactored some stuff a bit.&lt;/p&gt;

&lt;p&gt;Also fixed up one of the test cases which was flaky in the old version - it used the presence of region directories to determine whether the split had started running, but those were created by generateHLogs, so it was only winning the race sometimes.&lt;/p&gt;</comment>
                            <comment id="12970355" author="tlipcon" created="Sat, 11 Dec 2010 00:56:50 +0000"  >&lt;p&gt;er, there&apos;s a deadlock in this version of the patch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; but general structure should be good enough to review at this point&lt;/p&gt;</comment>
                            <comment id="12970359" author="tlipcon" created="Sat, 11 Dec 2010 01:32:32 +0000"  >&lt;p&gt;Now certified deadlock free by jcarder&lt;/p&gt;</comment>
                            <comment id="12970498" author="stack" created="Sat, 11 Dec 2010 18:42:10 +0000"  >&lt;p&gt;I think I pulled the right patch &amp;#8211; the latest one &amp;#8211; 62820 bytes in size (Todd, add a version to your patches?).  I was trying it and got this on the very end&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-12-11 18:39:00,914 INFO org.apache.hadoop.hbase.util.FSUtils: Recovering file hdfs:&lt;span class=&quot;code-comment&quot;&gt;//sv2borg180:10000/hbase/.logs/sv2borg188,60020,1291841481545/sv2borg188%3A60020.1291993339759
&lt;/span&gt;2010-12-11 18:39:00,915 ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Error in log splitting write thread
java.util.ConcurrentModificationException
        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:761)
        at java.util.LinkedList$ListItr.next(LinkedList.java:696)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:669)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:649)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:621)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12970507" author="stack" created="Sat, 11 Dec 2010 19:34:45 +0000"  >&lt;p&gt;So, on restart, the split completed (this is ten servers whose logs were to be split; one split set was of 33 logs... all in a 1G Master heap).&lt;/p&gt;

&lt;p&gt;The logging is profuse but I&apos;m grand w/ that.&lt;/p&gt;

&lt;p&gt;The below looks like a version of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2471&quot; title=&quot;Splitting logs, we&amp;#39;ll make an output file though the region no longer exists&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2471&quot;&gt;&lt;del&gt;HBASE-2471&lt;/del&gt;&lt;/a&gt;... is that so?  If so, good stuff.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-12-11 18:35:43,243 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: This region&apos;s directory doesn&apos;t exist: hdfs:&lt;span class=&quot;code-comment&quot;&gt;//sv2borg180:10000/hbase/TestTable/1d4a17311a6bc9f7b34f121bf121f42b. It is very likely that it was already split so it&apos;s safe to discard those edits.&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the patch, why flip parameter order &amp;#8211; i.e. moving conf from end of list to start?  People want to know!&lt;/p&gt;

&lt;p&gt;Whats this about?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void internTableName(&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; []tablename) {
+    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt; Bytes.equals(tablename, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.tablename);
+    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.tablename = tablename;
+  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We could call this method mutiple times?  And it&apos;d be an error if tablename was different on an invocation?  The method is public w/o doc.  It has to be public because called from different package?&lt;/p&gt;

&lt;p&gt;Sorry... still not done w/ review.  Back later.&lt;/p&gt;</comment>
                            <comment id="12970539" author="tlipcon" created="Sat, 11 Dec 2010 23:33:21 +0000"  >&lt;p&gt;It seems in my eagerness to fix up the deadlock I ended up undersynchronizing in the latest patch. Oops! I&apos;m going to write some standalone slower tests that actually split larger amounts of data, so we can get proper benchmarks and do a better job of finding these races.&lt;/p&gt;

&lt;p&gt;Will address your other comments in the next version as well.&lt;/p&gt;</comment>
                            <comment id="12970555" author="tlipcon" created="Sun, 12 Dec 2010 01:40:52 +0000"  >&lt;p&gt;Here&apos;s a new version including some new unit tests that set up a mock log reader and mock writers so as to test a lot of edits going through the splitter without actually taking any IO. This reliably reproduced the split bug Stack found - fixed that now.&lt;/p&gt;

&lt;p&gt;Didn&apos;t get a chance to do a true benchmark yet.&lt;/p&gt;</comment>
                            <comment id="12970640" author="stack" created="Sun, 12 Dec 2010 20:12:14 +0000"  >&lt;p&gt;I have a bunch of praise/+1s for changes made in this patch &amp;#8211; the refactoring of where params are passed to HLogSplitter, the new javadoc on the intern&apos;ing methods, the overall less moving parts &amp;#8211; but will say no more on that other than the improvements are great.&lt;/p&gt;

&lt;p&gt;Whats this for:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Path rootDir;
+  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Path srcDir;
+  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Path oldLogDir;
+  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; FileSystem fs;
+  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Configuration conf;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is stuff protected rather than private for the subclassers &amp;#8211; the transactional hbasers?&lt;/p&gt;


&lt;p&gt;Minor.... the below javadoc is now stale..&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    * Create a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HLogSplitter using the given {@link Configuration} and the
    * &amp;lt;code&amp;gt;hbase.hlog.splitter.impl&amp;lt;/code&amp;gt; property to derived the instance
    * class to use.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Address on commit?&lt;/p&gt;

&lt;p&gt;Same for this stuff:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * @param oldLogDir  directory where processed (split) logs will be archived to
+   * @param fs FileSystem
+   * @param conf Configuration
+   * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException will &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; corrupted hlogs aren&apos;t tolerated
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Its javadoc of params no longer present on this method.&lt;/p&gt;


&lt;p&gt;Mistype &apos;+        &quot;An HLogSplitter instance may only be used one&quot;);&apos;&lt;/p&gt;

&lt;p&gt;Extremely minor comment, the below formatting will be destroyed when rendered by javadoc:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+   * through the logs to be split. For each log, we:
+   *   - Recover it (take and drop HDFS lease) to ensure no other process can write
+   *   - Read each edit (see {@link #parseHLog}
+   *   - Mark as &lt;span class=&quot;code-quote&quot;&gt;&quot;processed&quot;&lt;/span&gt; or &lt;span class=&quot;code-quote&quot;&gt;&quot;corrupt&quot;&lt;/span&gt; depending on outcome
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(... but good documentation).&lt;/p&gt;

&lt;p&gt;It would be sweeter if this were a percentage of heap rather than hard MB number --&amp;gt; maxHeapUsage .... but no biggie.  Can do in later issue.&lt;/p&gt;

&lt;p&gt;So, it looks like we keep the order in which edits were written across the split process as best as I can tell.  We just append to the Entry List in RegionEntryBuffer.  That looks right.&lt;/p&gt;

&lt;p&gt;(Reading this patch makes me reconsider asserts)&lt;/p&gt;

&lt;p&gt;You iterate logWriters, a synchronized Map, a couple of times.  Is this safe at the time of iteration?&lt;/p&gt;

&lt;p&gt;You keep the old format for naming edit files?  Naming them for the sequenceid of their first edit, it seems (you use getRegionSplitEditsPath &amp;#8211; not in the patch).&lt;/p&gt;

&lt;p&gt;On the below&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (scopes != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
+      ret += ClassSize.TREEMAP;
+      ret += ClassSize.align(scopes.size() * ClassSize.MAP_ENTRY);
+      &lt;span class=&quot;code-comment&quot;&gt;// TODO &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; isn&apos;t quite right, need help here
&lt;/span&gt;+    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... maybe Jon can help &amp;#8211; but its fine for the moment I&apos;d say.&lt;/p&gt;




</comment>
                            <comment id="12971136" author="tlipcon" created="Tue, 14 Dec 2010 02:55:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is stuff protected rather than private for the subclassers - the transactional hbasers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yep, I made it protected since inheritors will need to access this stuff. Hopefully we can get rid of the subclasses in favor of log coprocessors some day.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Minor.... the below javadoc is now stale..&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;...Its javadoc of params no longer present on this method.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Mistype &apos;+ &quot;An HLogSplitter instance may only be used one&quot;);&apos;&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Extremely minor comment, the below formatting will be destroyed when rendered by javadoc:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fixed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, it looks like we keep the order in which edits were written across the split process as best as I can tell. We just append to the Entry List in RegionEntryBuffer. That looks right.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Added verification to the testThreading test that makes sure the edits come in the right order&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You iterate logWriters, a synchronized Map, a couple of times. Is this safe at the time of iteration?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It was safe in the current usage, but I added synchronization on this map for getOutputCounts just in case someone starts to call it in a different context.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You keep the old format for naming edit files? Naming them for the sequenceid of their first edit, it seems (you use getRegionSplitEditsPath - not in the patch).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yep, left that as-is.&lt;/p&gt;</comment>
                            <comment id="12971508" author="stack" created="Wed, 15 Dec 2010 01:00:25 +0000"  >&lt;p&gt;I&apos;m +1 on committing this patch.  I tried v5 on my little test cluster here killing RSs a few times.  It does the right thing as best as I can tell verified by rowcount of table subsequently to ensure all regions online after the killing.&lt;/p&gt;</comment>
                            <comment id="12971970" author="stack" created="Thu, 16 Dec 2010 05:58:32 +0000"  >&lt;p&gt;I did some more testing and came across the following after split was done and HLogSplitter was moving to close all files:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-12-16 01:07:20,394 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; split writer threads to finish
2010-12-16 01:07:20,640 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Split writers finished
2010-12-16 01:07:20,650 ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Couldn&apos;t close log at hdfs:&lt;span class=&quot;code-comment&quot;&gt;//sv2borg180:10000/hbase/TestTable/02495f8b7cb6404cb7ea0521cc183d56/recovered.edits/0000000000000012854
&lt;/span&gt;org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/TestTable/02495f8b7cb6404cb7ea0521cc183d56/recovered.edits/0000000000000012854 File does not exist. [Lease.  Holder: DFSClient_hb_m_sv2borg180:         
60000_1292460569453, pendingcreates: 178]
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1418)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1409)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1464)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1452)
    at org.apache.hadoop.hdfs.server.namenode.NameNode.complete(NameNode.java:471)
    at sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)
    at org.apache.hadoop.ipc.Client.call(Client.java:740)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
    at $Proxy5.complete(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
    at $Proxy5.complete(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3457)
    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3381)
    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
    at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
    at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966)
    at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.close(SequenceFileLogWriter.java:138)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.closeStreams(HLogSplitter.java:756)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWritingAndClose(HLogSplitter.java:741)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:291)
    at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:186)
    at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:194)
    at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:96)
    at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:662)
2010-12-16 01:07:20,685 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs:&lt;span class=&quot;code-comment&quot;&gt;//sv2borg180:10000/hbase/TestTable/03dbd921c75876d6bc3f86c10201fa93/recovered.edits/0000000000000014196 (wrote 12 edits in 470ms)
&lt;/span&gt;2010-12-16 01:07:20,719 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs:&lt;span class=&quot;code-comment&quot;&gt;//sv2borg180:10000/hbase/TestTable/04f81e343d032d43946393636b2b4d2d/recovered.edits/0000000000000012928 (wrote 15 edits in 390ms)
&lt;/span&gt;2010-12-16 01:07:20,725 ERROR org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Couldn&apos;t close log at hdfs:&lt;span class=&quot;code-comment&quot;&gt;//sv2borg180:10000/hbase/TestTable/06d137bd176e2604761243d396c11b3a/recovered.edits/0000000000000012945
&lt;/span&gt;org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hbase/TestTable/06d137bd176e2604761243d396c11b3a/recovered.edits/0000000000000012945 File does not exist. [Lease.  Holder: DFSClient_hb_m_sv2borg180:         
60000_1292460569453, pendingcreates: 176]
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1418)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1409)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:1464)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:1452)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It then closes a bunch and fails one or two more until its done with all.&lt;/p&gt;

&lt;p&gt;Eventually the split &apos;completes&apos; and we start assigning out regions:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-12-16 01:07:32,581 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Reassigning 186 region(s) that sv2borg185,60020,1292460570976 was carrying (skipping 1 regions(s) that are already in transition)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="12971971" author="tlipcon" created="Thu, 16 Dec 2010 06:03:04 +0000"  >&lt;p&gt;Strange... can you grep the NN logs for that directory and try to figure it out? Usually that kind of error happens if someone deletes the file while you&apos;re writing it.&lt;/p&gt;</comment>
                            <comment id="12972370" author="stack" created="Fri, 17 Dec 2010 06:32:59 +0000"  >&lt;p&gt;Ok. I&apos;ve overwritten the logs from above.  I&apos;ve tested more since and haven&apos;t run into the issue.  Will keep at it.   Will open new issue if I run into it.   I&apos;d say go ahead still w/ commit of this.&lt;/p&gt;</comment>
                            <comment id="12973350" author="stack" created="Mon, 20 Dec 2010 20:40:59 +0000"  >&lt;p&gt;Here is what I committed.  Its Todd&apos;s patch updated so it&apos;d apply to branch and trunk.&lt;/p&gt;</comment>
                            <comment id="12973987" author="jdcryans" created="Wed, 22 Dec 2010 00:01:37 +0000"  >&lt;p&gt;I think the patch is missing this, at least adding it back in HLogSplitter fixes TestHLogSplitting:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
} &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (EOFException eof) {
  &lt;span class=&quot;code-comment&quot;&gt;// truncated files are expected &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; a RS crashes (see HBASE-2643)
&lt;/span&gt;  LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;EOF from hlog &quot;&lt;/span&gt; + logPath + &lt;span class=&quot;code-quote&quot;&gt;&quot;.  continuing&quot;&lt;/span&gt;);
  processedLogs.add(logPath);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12974516" author="hudson" created="Thu, 23 Dec 2010 04:31:50 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #1697 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/HBase-TRUNK/1697/&lt;/a&gt;)&lt;/p&gt;
</comment>
                            <comment id="15017116" author="lars_francke" created="Fri, 20 Nov 2015 12:41:57 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12492861">HBASE-3325</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12424367">HBASE-1364</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12466089" name="hbase-3323.4.txt" size="72384" author="tlipcon" created="Sun, 12 Dec 2010 01:40:52 +0000"/>
                            <attachment id="12466188" name="hbase-3323.5.txt" size="72737" author="tlipcon" created="Tue, 14 Dec 2010 02:57:59 +0000"/>
                            <attachment id="12466657" name="hbase-3323.6.txt" size="72811" author="stack" created="Mon, 20 Dec 2010 20:40:59 +0000"/>
                            <attachment id="12466034" name="hbase-3323.txt" size="62820" author="tlipcon" created="Sat, 11 Dec 2010 01:32:32 +0000"/>
                            <attachment id="12466032" name="hbase-3323.txt" size="61746" author="tlipcon" created="Sat, 11 Dec 2010 00:48:14 +0000"/>
                            <attachment id="12465969" name="hbase-3323.txt" size="27894" author="tlipcon" created="Fri, 10 Dec 2010 07:49:34 +0000"/>
                            <attachment id="12465886" name="sizes.png" size="83430" author="tlipcon" created="Thu, 9 Dec 2010 08:33:05 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 9 Dec 2010 19:43:46 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26784</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hlrb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100787</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>