<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:38:57 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-13031/HBASE-13031.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-13031] Ability to snapshot based on a key range</title>
                <link>https://issues.apache.org/jira/browse/HBASE-13031</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Posted on the mailing list and seems like some people are interested.  A little background for everyone.&lt;/p&gt;

&lt;p&gt;We have a very large table, we would like to snapshot and transfer the data to another cluster (compressed data is always better to ship).  Our problem lies in the fact it could take many weeks to transfer all of the data and during that time with major compactions, the data stored in dfs has the potential to double which would cause us to run out of disk space.&lt;/p&gt;

&lt;p&gt;So we were thinking about allowing the ability to snapshot a specific key range.  &lt;/p&gt;

&lt;p&gt;Ideally I feel the approach is that the user would specify a start and stop key, those would be associated with a region boundary.  If between the time the user submits the request and the snapshot is taken the boundaries change (due to merging or splitting of regions) the snapshot should fail.&lt;/p&gt;

&lt;p&gt;We would know which regions to snapshot and if those changed between when the request was submitted and the regions locked, the snapshot could simply fail and the user would try again, instead of potentially giving the user more / less than what they had anticipated.  I was planning on storing the start / stop key in the SnapshotDescription and from there it looks pretty straight forward where we just have to change the verifier code to accommodate the key ranges.  &lt;/p&gt;

&lt;p&gt;If this design sounds good to anyone, or if I am overlooking anything please let me know.  Once we agree on the design, I&apos;ll write and submit the patches.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12774685">HBASE-13031</key>
            <summary>Ability to snapshot based on a key range</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="churromorales">churro morales</assignee>
                                    <reporter username="churromorales">churro morales</reporter>
                        <labels>
                    </labels>
                <created>Thu, 12 Feb 2015 20:27:45 +0000</created>
                <updated>Mon, 20 Jun 2016 18:38:46 +0000</updated>
                            <resolved>Tue, 23 Feb 2016 17:31:51 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                                <comments>
                            <comment id="14318944" author="jesse_yates" created="Thu, 12 Feb 2015 20:37:36 +0000"  >&lt;p&gt;+1 seems like a reasonable approach.&lt;/p&gt;</comment>
                            <comment id="14319165" author="apurtell" created="Thu, 12 Feb 2015 22:54:58 +0000"  >&lt;p&gt;Carrying over the counterpoint from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vrodionov&quot; class=&quot;user-hover&quot; rel=&quot;vrodionov&quot;&gt;Vladimir Rodionov&lt;/a&gt; from the mailing list discussion for reference here:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I do not think there is a need for new API. Take a look at TableSnapshotInputFormat which you can customize to to work with key ranges. It allows M/R over snapshots. You make a snapshot of a full table, then you run  first batch of keys in M/R job then you delete snapshot and create new one ... repeat until last key range.&lt;/p&gt;

&lt;p&gt;You will need to control major compaction during this migration.&lt;/p&gt;

&lt;p&gt;How to output the data - is your choice: TableOutputFormat or HFileOutputFormat2&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="14319168" author="apurtell" created="Thu, 12 Feb 2015 22:56:17 +0000"  >&lt;p&gt;I can see how snapshotting ranges for transfer (then local removal) can be useful if you can&apos;t control major compaction. It will be up to the operator to figure out how stitching together the different snapshot ranges on the remote would work, right? &lt;/p&gt;</comment>
                            <comment id="14319171" author="jesse_yates" created="Thu, 12 Feb 2015 22:57:47 +0000"  >&lt;p&gt;The problem I saw with that is they (flurry) don&apos;t want to keep around an entire second copy of the table if there is a compaction happening.... which does bring up the question of, what good is a snapshot if it doesn&apos;t capture the state of an entire table? If you have writes happening at the same time (which causes the compaction) then the partial snapshots will never be &apos;up to date&apos;.&lt;/p&gt;

&lt;p&gt;or, what Andrew said &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14319190" author="apurtell" created="Thu, 12 Feb 2015 23:07:03 +0000"  >&lt;p&gt;Would it be better to use the Export utility with timerange queries? Or a modified exporter that runs over snapshot files as Vladimir suggested. Start replication between the existing and new cluster. Then use Export to go back from that point in time in keyrange-chunks until everything is backfilled? It would be more time consuming, of course, but we could build something that uses MR to take edits from a backup and make HFiles for bulk import to avoid round trips through the regionservers when restoring data into the new cluster.&lt;/p&gt;</comment>
                            <comment id="14320401" author="davelatham" created="Fri, 13 Feb 2015 17:20:03 +0000"  >&lt;p&gt;The question is how to best bootstrap a new cluster from an old one that doesn&apos;t have enough disk space around to store an additional full table snapshot if major compaction creates new HFiles.&lt;/p&gt;

&lt;p&gt;The general idea is start replicating to the new cluster.  Then take a snapshot, copy it to the new cluster, and bulk load it into the table.  Data is copied only a single time.  However, since the data is so large (~1PB compressed) the amount of time to copy it over a WAN link will be weeks, during which time the table will undergo major compaction (we are also investigating the major compaction schedule, but let&apos;s assume we can&apos;t afford to simply disable it for weeks), would end up doubling the storage usage on the source cluster which would fill it up.  If we can break the snapshot/copy up into 2 or 4 chunks then it wouldn&apos;t be a problem.&lt;/p&gt;

&lt;p&gt;Jesse asks &quot;what good is a snapshot if it doesn&apos;t capture the state of an entire table?&quot; It would certainly help with this use case, but I could also see it being used for efficient sampling (and exporting) of a portion of a table&apos;s data, or for tables using time ranges, map to a data for a period of time as an example.&lt;/p&gt;

&lt;p&gt;Andrew suggests using Export.  That would require exporting to local dfs (into compressed sequence files) - DistCp the sequence files, then Import them into the destination cluster.  That&apos;s two extra data copies on the surface, not counting the write amplification of Importing data through the memstore / flush / compaction process.  I&apos;m not sure exactly what all the steps that Vladimir is suggesting, but it again sounds like extra data copies.  &lt;/p&gt;

&lt;p&gt;This proposal seems like a small change to allow for an efficient bootstrap of a remote cluster under storage constraints.&lt;/p&gt;</comment>
                            <comment id="14320444" author="apurtell" created="Fri, 13 Feb 2015 17:48:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;Andrew suggests using Export.  That would require exporting to local dfs (into compressed sequence files) - DistCp the sequence files, then Import them into the destination cluster.  That&apos;s two extra data copies on the surface, not counting the write amplification of Importing data through the memstore / flush / compaction process.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You missed the part where I said to make HFiles of the exports for bulk ingest instead of writing with Import to the API.&lt;/p&gt;</comment>
                            <comment id="14320460" author="davelatham" created="Fri, 13 Feb 2015 17:59:55 +0000"  >&lt;blockquote&gt;
&lt;p&gt;You missed the part where I said to make HFiles of the exports for bulk ingest instead of writing with Import to the API.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Oops - sorry for missing that part.  So you&apos;re suggesting Export to sequence files, then build another MR job to translate those to HFiles, then copy those to the remote cluster for bulk load?  Definitely better than Importing through regionservers, but still seems to require a couple of extra copies when the data is already sitting in HFiles.&lt;/p&gt;</comment>
                            <comment id="14320557" author="davelatham" created="Fri, 13 Feb 2015 18:55:27 +0000"  >&lt;p&gt;Added an alternate idea at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13042&quot; title=&quot;MR Job to export HFiles directly from an online cluster&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13042&quot;&gt;&lt;del&gt;HBASE-13042&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14320582" author="apurtell" created="Fri, 13 Feb 2015 19:05:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;So you&apos;re suggesting Export to sequence files, then build another MR job to translate those to HFiles, then copy those to the remote cluster for bulk load? Definitely better than Importing through regionservers, but still seems to require a couple of extra copies when the data is already sitting in HFiles.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Close.&lt;/p&gt;

&lt;p&gt;0. Set up replication at time T. &lt;/p&gt;

&lt;p&gt;1. Export manageable keyranges from time 0 to T-1 to compressed sequencefiles. Use bzip or even xz (slow compressor!!!) to squeeze out as much redundancy as possible. This will be better than compression you&apos;ll ever see in an HFile. Also is independent of compaction, you won&apos;t get junk/redundant HFiles in the mix, just exactly the KVs present in the keyspace. &lt;/p&gt;

&lt;p&gt;2. Transfer the maximally compressed sequencefiles over the wide area link.&lt;/p&gt;

&lt;p&gt;3. Convert the sequencefiles to HFiles. Resources here should be mostly spare until the cluster goes live so there&apos;s headroom to spare. &lt;/p&gt;

&lt;p&gt;4. Bulk load.&lt;/p&gt;

&lt;p&gt;5. Rinse and repeat until all of the keyspace has been transferred.&lt;/p&gt;</comment>
                            <comment id="14320589" author="ianfriedman" created="Fri, 13 Feb 2015 19:11:25 +0000"  >&lt;p&gt;Andrew - can you think of how would we do this in a streaming fashion? We&apos;d have to copy the sequence files to the remote dfs as they are generated by the local export and then immediately delete them in order to not blow up our local dfs.&lt;/p&gt;</comment>
                            <comment id="14320616" author="apurtell" created="Fri, 13 Feb 2015 19:28:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;can you think of how would we do this in a streaming fashion?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sure. I&apos;ve been aiming for minimum dev time, considering existing tools. Above suggestion needs a simple MR job that takes sequence files as input and produces HFiles, not difficult. Could also require adding lzma codec support to your Hadoop if you&apos;re crazy enough to try it (smile), which I&apos;ve done before, the coding isn&apos;t so bad, but the compressor might be too slow... Anyway, if you can invest some dev time then there&apos;s no reason the export/compress job workers need write the stream of compressed KVs to the local DFS for a copy, they could contact workers running at the remote site for streaming transfer and there those workers could direct write to HFiles for bulk load. You&apos;d have to think about how to handle broken connections. This could be a fair amount of work but still better for a couple of reasons offhand:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Can compress data for WAN transfer better than we ever could/should in HFiles&lt;/li&gt;
	&lt;li&gt;Minimizing data copies at petascale saves a lot of time.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14320629" author="davelatham" created="Fri, 13 Feb 2015 19:36:45 +0000"  >&lt;p&gt;Ah, thanks for the explanation.  So Export with a key range (guess we&apos;d backport &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11436&quot; title=&quot;Support start Row and stop Row in HBase Export&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11436&quot;&gt;&lt;del&gt;HBASE-11436&lt;/del&gt;&lt;/a&gt; to 0.94 - very small patch).  Then DistCp, convert to HFile (looks like Import already supports seq file to HFile), then bulk load.  That may be simplest and get some extra compression as you suggest, at the expense of a couple of extra copies.&lt;/p&gt;</comment>
                            <comment id="14320993" author="lhofhansl" created="Fri, 13 Feb 2015 23:51:19 +0000"  >&lt;p&gt;Yes, Import can already do that (added that for scenarios like this).&lt;br/&gt;
Moving a snapshot is still preferable, though (IMHO). It can simply be mapped into a table.&lt;/p&gt;</comment>
                            <comment id="14339228" author="churromorales" created="Thu, 26 Feb 2015 21:47:58 +0000"  >&lt;p&gt;Attached the trunk patch for ability to snapshot key ranges.  If folks are interested in getting this upstream, I can provide backports for 1.x, 98.x and 94.&lt;/p&gt;
</comment>
                            <comment id="14339238" author="mbertozzi" created="Thu, 26 Feb 2015 21:52:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=churromorales&quot; class=&quot;user-hover&quot; rel=&quot;churromorales&quot;&gt;churro morales&lt;/a&gt; from a quick look looks good. what about restore? should it leave only the subset regions or replace the subset regions and leave the others?&lt;/p&gt;</comment>
                            <comment id="14339354" author="churromorales" created="Thu, 26 Feb 2015 22:55:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; good point.  What do you think about this.  We check to see if the snapshot is partial (start or end keys) not empty byte arrays, if that is the case.  &lt;/p&gt;

&lt;p&gt;When we do the check for metaChanges.hasRegionsToRemove() in RestoreSnapshotHandler we can replace that with &lt;br/&gt;
    metaChanges.hasRegionsToRemove() &amp;amp;&amp;amp; snapshotIsNotPartial&lt;/p&gt;

&lt;p&gt;That way we don&apos;t remove regions from META for partial snapshots, but for complete snapshots we do.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</comment>
                            <comment id="14339375" author="mbertozzi" created="Thu, 26 Feb 2015 23:06:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=churromorales&quot; class=&quot;user-hover&quot; rel=&quot;churromorales&quot;&gt;churro morales&lt;/a&gt; sounds good to me. restore of a partial snapshot should only replace the subset of regions, and not remove the others (add that to the restore doc).&lt;br/&gt;
clone may need a flag to make sure everyone is happy. prevent to create a new table from a partial snapshot or allow to create a new table from a partial snapshot. but this one is easier since it is just a check before starting the clone.&lt;/p&gt;</comment>
                            <comment id="14339425" author="hadoopqa" created="Thu, 26 Feb 2015 23:46:40 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12701177/HBASE-13031.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12701177/HBASE-13031.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 49b4f3737eb0dd7c5d88d6dcbe8b5d4f167c6a2b.&lt;br/&gt;
  ATTACHMENT ID: 12701177&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 12 new or modified tests.&lt;br/&gt;
    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 release audit&lt;/font&gt;.  The applied patch generated 1 release audit warnings (more than the master&apos;s current 0 warnings).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +              new java.lang.String[] &lt;/p&gt;
{ &quot;Name&quot;, &quot;Table&quot;, &quot;CreationTime&quot;, &quot;Type&quot;, &quot;Version&quot;, &quot;Owner&quot;, &quot;StartRow&quot;, &quot;StopRow&quot;, }
&lt;p&gt;);&lt;br/&gt;
+          start = org.apache.hadoop.hbase.util.Bytes.toStringBinary(snapshot.getStartRow.toByteArray)&lt;br/&gt;
+          formatter.row([ snapshot.getName, snapshot.getTable + &quot; (&quot; + creation_time + &quot;)&quot;, &quot;[ &quot; +  start + &quot; =&amp;gt; &quot; + stop + &quot; ]&quot;])&lt;br/&gt;
+  hbase&amp;gt; snapshot &apos;sourceTable&apos;, &apos;snapshotName&apos;, &lt;/p&gt;
{SKIP_FLUSH =&amp;gt; true}
&lt;p&gt;, &lt;/p&gt;
{STARTROW =&amp;gt; &apos;key&apos;}
&lt;p&gt;, &lt;/p&gt;
{STOPROW =&amp;gt; &apos;key&apos;}

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithCustomVisLabService&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.TestIOFencing.testFencingAroundCompactionAfterWALSync(TestIOFencing.java:240)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//testReport/&lt;/a&gt;&lt;br/&gt;
Release audit warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/patchReleaseAuditWarnings.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/patchReleaseAuditWarnings.txt&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12985//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14343537" author="churromorales" created="Mon, 2 Mar 2015 18:24:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; Thank you for the feedback.  I am attaching a new patch which updates Clone / Restore Snapshot per our conversation.  If this is acceptable, I will go ahead and backport to 1x and 98x.  Thank you for the feedback.&lt;/p&gt;</comment>
                            <comment id="14343709" author="hadoopqa" created="Mon, 2 Mar 2015 20:31:48 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12701930/HBASE-13031-v1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12701930/HBASE-13031-v1.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 7b5c9eabacf5019d5b6aba95ba5a4fcb7dc8d8e5.&lt;br/&gt;
  ATTACHMENT ID: 12701930&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 12 new or modified tests.&lt;br/&gt;
    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1938 checkstyle errors (more than the master&apos;s current 1937 errors).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 release audit&lt;/font&gt;.  The applied patch generated 1 release audit warnings (more than the master&apos;s current 0 warnings).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +              new java.lang.String[] &lt;/p&gt;
{ &quot;Name&quot;, &quot;Table&quot;, &quot;CreationTime&quot;, &quot;Type&quot;, &quot;Version&quot;, &quot;Owner&quot;, &quot;StartRow&quot;, &quot;StopRow&quot;, }
&lt;p&gt;);&lt;br/&gt;
+      if (metaChanges.hasRegionsToRemove() &amp;amp;&amp;amp; !ClientSnapshotDescriptionUtils.isPartialSnapshot(snapshot))&lt;br/&gt;
+          start = org.apache.hadoop.hbase.util.Bytes.toStringBinary(snapshot.getStartRow.toByteArray)&lt;br/&gt;
+          formatter.row([ snapshot.getName, snapshot.getTable + &quot; (&quot; + creation_time + &quot;)&quot;, &quot;[ &quot; +  start + &quot; =&amp;gt; &quot; + stop + &quot; ]&quot;])&lt;br/&gt;
+  hbase&amp;gt; snapshot &apos;sourceTable&apos;, &apos;snapshotName&apos;, &lt;/p&gt;
{SKIP_FLUSH =&amp;gt; true}
&lt;p&gt;, &lt;/p&gt;
{STARTROW =&amp;gt; &apos;key&apos;}
&lt;p&gt;, &lt;/p&gt;
{STOPROW =&amp;gt; &apos;key&apos;}

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//testReport/&lt;/a&gt;&lt;br/&gt;
Release audit warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/patchReleaseAuditWarnings.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/patchReleaseAuditWarnings.txt&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13037//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14343957" author="hadoopqa" created="Mon, 2 Mar 2015 23:07:47 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12701978/HBASE-13031-v1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12701978/HBASE-13031-v1.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 74e36f8ddd03cf94c17bdb30ecd81cc5dff4d063.&lt;br/&gt;
  ATTACHMENT ID: 12701978&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 8 new or modified tests.&lt;br/&gt;
    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +              new java.lang.String[] &lt;/p&gt;
{ &quot;Name&quot;, &quot;Table&quot;, &quot;CreationTime&quot;, &quot;Type&quot;, &quot;Version&quot;, &quot;Owner&quot;, &quot;StartRow&quot;, &quot;StopRow&quot;, }
&lt;p&gt;);&lt;br/&gt;
+          start = org.apache.hadoop.hbase.util.Bytes.toStringBinary(snapshot.getStartRow.toByteArray)&lt;br/&gt;
+          formatter.row([ snapshot.getName, snapshot.getTable + &quot; (&quot; + creation_time + &quot;)&quot;, &quot;[ &quot; +  start + &quot; =&amp;gt; &quot; + stop + &quot; ]&quot;])&lt;br/&gt;
+  hbase&amp;gt; snapshot &apos;sourceTable&apos;, &apos;snapshotName&apos;, &lt;/p&gt;
{SKIP_FLUSH =&amp;gt; true}
&lt;p&gt;, &lt;/p&gt;
{STARTROW =&amp;gt; &apos;key&apos;}
&lt;p&gt;, &lt;/p&gt;
{STOPROW =&amp;gt; &apos;key&apos;}

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 2 zombie test(s): 	at org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA.testKillAppWhenFailoverHappensAtRunningState(TestKillApplicationWithRMHA.java:89)&lt;br/&gt;
	at org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.testRegionMerge(TestNamespaceAuditor.java:308)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/13041//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14344056" author="apurtell" created="Mon, 2 Mar 2015 23:58:20 +0000"  >&lt;p&gt;Moving to 0.98.12&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12701978" name="HBASE-13031-v1.patch" size="49894" author="churromorales" created="Mon, 2 Mar 2015 21:08:14 +0000"/>
                            <attachment id="12701177" name="HBASE-13031.patch" size="43972" author="churromorales" created="Thu, 26 Feb 2015 21:47:58 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 12 Feb 2015 20:37:36 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 41 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i25kav:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>