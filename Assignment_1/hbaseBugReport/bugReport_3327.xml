<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:09:17 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3327/HBASE-3327.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3327] For increment workloads, retain memstores in memory after flushing them</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3327</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This is an improvement based on our observation of what happens in an increment workload. The working set is typically small and is contained in the memstores. &lt;br/&gt;
1. The reason the memstores get flushed is because the number of wal logs limit gets hit. &lt;br/&gt;
2. This in turn triggers compactions, which evicts the block cache. &lt;br/&gt;
3. Flushing of memstore and eviction of the block cache causes disk reads for increments coming in after this because the data is no longer in memory.&lt;/p&gt;

&lt;p&gt;We could solve this elegantly by retaining the memstores AFTER they are flushed into files. This would mean we can quickly populate the new memstore with the working set of data from memory itself without having to hit disk. We can throttle the number of such memstores we retain, or the memory allocated to it. In fact, allocating a percentage of the block cache to this would give us a huge boost.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12492871">HBASE-3327</key>
            <summary>For increment workloads, retain memstores in memory after flushing them</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="karthik.ranga">Karthik Ranganathan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 Dec 2010 19:08:20 +0000</created>
                <updated>Sat, 11 Apr 2015 00:48:30 +0000</updated>
                            <resolved>Sat, 11 Apr 2015 00:48:30 +0000</resolved>
                                                                    <component>regionserver</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="12969877" author="jdcryans" created="Thu, 9 Dec 2010 19:14:08 +0000"  >&lt;p&gt;I&apos;m wondering... if those Memstores are flushed because of HLogs, wouldn&apos;t HLog compactions (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3242&quot; title=&quot;HLog Compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3242&quot;&gt;&lt;del&gt;HBASE-3242&lt;/del&gt;&lt;/a&gt;) solve the issue more elegantly than special casing ICVs?&lt;/p&gt;</comment>
                            <comment id="12969892" author="karthik.ranga" created="Thu, 9 Dec 2010 19:39:36 +0000"  >&lt;p&gt;True - I mentioned HLog limit because we observed it because of that, but this would address the underlying issue for any of the reasons to flush. Additionally, this also makes it resilient in the face of compactions, which HLog compactions would not help with. &lt;/p&gt;

&lt;p&gt;HLog compactions would also be most effective for the ICV kind of workload (frequent updates to existing data) right?&lt;/p&gt;</comment>
                            <comment id="12969897" author="jdcryans" created="Thu, 9 Dec 2010 19:47:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Additionally, this also makes it resilient in the face of compactions, which HLog compactions would not help with.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, but if you don&apos;t flush then you don&apos;t compact meaning that it won&apos;t screw up the BC. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;HLog compactions would also be most effective for the ICV kind of workload (frequent updates to existing data) right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m pretty sure we both agree on that, and this jira is also about helping that case as far as I understand it.&lt;/p&gt;</comment>
                            <comment id="12969917" author="kannanm" created="Thu, 9 Dec 2010 20:09:53 +0000"  >&lt;p&gt;I think this scheme helps more than the ICV case. For example, workloads that mostly tend to access recent data. You still bound your recovery time by flushing the memstores into HFiles-- but now continue to keep them around as a &quot;read-cache&quot;. &lt;span class=&quot;error&quot;&gt;&amp;#91;This scheme provides some of the benefits (granted, not all) of doing a &amp;quot;scan cache&amp;quot; (as described in the big table paper), but with much less implementation complexity.&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="12969919" author="kannanm" created="Thu, 9 Dec 2010 20:10:59 +0000"  >&lt;p&gt;Typo:&lt;br/&gt;
I think this scheme helps more than the ICV case.&lt;br/&gt;
meant to say:&lt;br/&gt;
I think this scheme helps more than just the ICV case.&lt;/p&gt;</comment>
                            <comment id="12969932" author="ryanobjc" created="Thu, 9 Dec 2010 20:35:03 +0000"  >&lt;p&gt;What about the write a block cache on hfile write patch? Does that not help?&lt;/p&gt;</comment>
                            <comment id="12969939" author="streamy" created="Thu, 9 Dec 2010 20:48:08 +0000"  >&lt;p&gt;It does help.  For flushes the different between cacheOnWrite and this are not that big.  This helps mostly in the face of compactions, I think.&lt;/p&gt;

&lt;p&gt;One potential downside of keeping stuff in MemStore vs. block cache via CacheOnWrite is the relative efficiencies.  With a full increment workload what I see is major reductions in storage between MemStore -&amp;gt; block cache -&amp;gt; compressed files.  I&apos;m seeing approximately 128MB -&amp;gt; 32MB -&amp;gt; 2-3MB (so block cache is 4X more efficient at storing the same data as MemStore, and compressed files another 10X).&lt;/p&gt;

&lt;p&gt;There&apos;s also the suspicion that I think many of us have that reads out of MemStore are actually slower than reads out of the block cache.&lt;/p&gt;

&lt;p&gt;I still think this is a really interesting potential direction but w/ CacheOnWrite and the difference in space efficiency, I think other optimizations may be better to target first.&lt;/p&gt;</comment>
                            <comment id="12969944" author="kannanm" created="Thu, 9 Dec 2010 20:53:32 +0000"  >&lt;p&gt;Ryan: If this happened only for recent HFiles or compactions of recent files, and not for say bigger compactions-- then yes, the two schemes start to have more similarities. The trouble with writing to block cache on all HFile creations (i.e. not just flushes but also on all compactions) is too much old data could be rewritten, and you might have storms that fully clear out items in the block cache. Jonathan has suggested knobs to throttle how much &quot;write through&quot; happens--- but they are size based rather than recency of data based.&lt;/p&gt;

&lt;p&gt;But I agree your suggestion sounds like a viable alternative with the right tweaks.&lt;/p&gt;</comment>
                            <comment id="12970047" author="karthik.ranga" created="Fri, 10 Dec 2010 02:07:11 +0000"  >&lt;p&gt;Ryan: was talking to Kannan as well about this. The only thing the writing into block cache on flushes works for flushes. But for compactions, it gets a bit complicated - and any algorithm will become a little dependent on the compaction policy.&lt;/p&gt;</comment>
                            <comment id="12971831" author="paultuckfield" created="Wed, 15 Dec 2010 20:52:19 +0000"  >&lt;p&gt;I see the logic behind compact memory and cacheOnWrite, but still for some distribution of keys being updated, the memory tradeoffs can favor memstore in terms of ram consumption. I suppose the tradeoff point exists somewhere in reasonable tuning range. So it seems like this gives the user control to understand their datalocality and make tuning tradeoffs.&lt;/p&gt;

&lt;p&gt;If memstore reads are slower (presumably because of contention with writers to memstore) that seems like a global problem, especially if check-and-miss is slow (I&quot;m ignorant as to whether checking existence of a key is as expensive as checking+readingvalue) Because that&apos;s the first check any read must do, block cache, snapshot or physical IO, all check memstore first i think.&lt;/p&gt;

&lt;p&gt;I&apos;d very much like to test this just by a boolean setting allowing a snapshot to remain in ram until the next memstore must be converted to a snapshot. I suspect 1 memstore plus one snapshot gives most of the benefit, and is tuneable by existing memstore size affecting parameters. But maybe this could be a memstore + N snapshots.&lt;/p&gt;

</comment>
                            <comment id="12971837" author="streamy" created="Wed, 15 Dec 2010 21:09:11 +0000"  >&lt;p&gt;I actually disagree that the biggest benefit is 1 memstore plus snapshot.  That would then cover flushes but not compactions.  As stated, flushing w/ cacheOnWrite would be virtually the same but consume 25% the memory.  So for this case, I don&apos;t see the clear benefit of retaining the snapshot vs. cacheOnWrite of the flushed file.&lt;/p&gt;

&lt;p&gt;This change is significant and would require a good bit of modifications to the tracking of aggregate MemStore sizes and the rules around eviction when under global heap pressure.  I still do like this idea in general but not sure it&apos;s the best direction for effort to be spent right now.&lt;/p&gt;</comment>
                            <comment id="12971843" author="tlipcon" created="Wed, 15 Dec 2010 21:21:13 +0000"  >&lt;p&gt;Here&apos;s a brainstormy idea: we don&apos;t like the cacheOnWrite with compactions because it tries to cache everything instead of just the warm keys. So our goal should be to figure out how to only cache blocks that contain the warm ones.&lt;/p&gt;

&lt;p&gt;What if we maintained a counting bloom filter which was periodically cleared in order to determine which keys in the region were potentially hot. Then as we flush those keys, those hfile blocks are the ones that get pre-cached?&lt;/p&gt;</comment>
                            <comment id="12971845" author="streamy" created="Wed, 15 Dec 2010 21:28:47 +0000"  >&lt;p&gt;I looked into doing some kind of intelligent block selection for caching on cacheOnWrite.  Was not going to be simple.  To start I was thinking that I would re-cache blocks if the originating block(s) were already cached.  If the originating blocks were not cached, I would skip caching on write of those block(s).&lt;/p&gt;

&lt;p&gt;Counting bloom filter sounds interesting.&lt;/p&gt;

&lt;p&gt;If we can ever get fast local fs reads, seems like we wouldn&apos;t need to do cacheOnWrite because the recently written file would be in the fs cache?&lt;/p&gt;</comment>
                            <comment id="12971848" author="tlipcon" created="Wed, 15 Dec 2010 21:35:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we can ever get fast local fs reads, seems like we wouldn&apos;t need to do cacheOnWrite because the recently written file would be in the fs cache?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That should help, but also keep in mind that our block cache is post-decompression, so we&apos;d still pay the decompression tax even if we&apos;re reading from OS cache, right?&lt;/p&gt;</comment>
                            <comment id="12971851" author="streamy" created="Wed, 15 Dec 2010 21:39:47 +0000"  >&lt;p&gt;yes, but just for the first read.  we&apos;d then load into block cache.  but in this way, we&apos;d have &quot;intelligent&quot; selection of which blocks to cache (those that get used).&lt;/p&gt;</comment>
                            <comment id="13030970" author="karthick" created="Tue, 10 May 2011 00:32:49 +0000"  >&lt;p&gt;Just out of curiosity, is this issue still open? In other words, when we read from a &lt;tt&gt;HFile&lt;/tt&gt; right after it has been flushed (or compacted), will that strictly be an in-memory call? If not, will the following approach address this issue (at the risk of sounding uneducated):&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Define a &lt;tt&gt;Map&amp;lt;Path, BlockCache&amp;gt;&lt;/tt&gt; in &lt;tt&gt;StoreFile&lt;/tt&gt; that captures the &lt;tt&gt;BlockCache&lt;/tt&gt; objects used by writes, regardless of if it&apos;s triggered by a flush or a compaction.&lt;/li&gt;
	&lt;li&gt;Lookup the &lt;tt&gt;BlockCache&lt;/tt&gt; from that map based on the &lt;tt&gt;StoreFile&lt;/tt&gt;&apos;s &lt;tt&gt;Path&lt;/tt&gt;, at the time we create a reader for it, and use that as opposed to an empty &lt;tt&gt;BlockCache&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Correct me if I&apos;m wrong, but when &quot;hbase.rs.cacheblocksonwrite&quot; is true, we seem to be caching blocks on writes regardless of whether we&apos;re flushing or compacting. If that&apos;s already the case, we might as well make those block caches visible in the read path.&lt;/p&gt;</comment>
                            <comment id="13081361" author="yiliang" created="Tue, 9 Aug 2011 01:48:06 +0000"  >&lt;p&gt;Is there a patch for 0.90.3?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12475662">HBASE-3067</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 9 Dec 2010 19:14:08 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32989</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 19 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02fnr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12138</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>