<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:54:46 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-8389/HBASE-8389.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-8389] HBASE-8354 forces Namenode into loop with lease recovery requests</title>
                <link>https://issues.apache.org/jira/browse/HBASE-8389</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We ran hbase 0.94.3 patched with 8354 and observed too many outstanding lease recoveries because of the short retry interval of 1 second between lease recoveries.&lt;/p&gt;

&lt;p&gt;The namenode gets into the following loop:&lt;br/&gt;
1) Receives lease recovery request and initiates recovery choosing a primary datanode every second&lt;br/&gt;
2) A lease recovery is successful and the namenode tries to commit the block under recovery as finalized - this takes &amp;lt; 10 seconds in our environment since we run with tight HDFS socket timeouts.&lt;br/&gt;
3) At step 2), there is a more recent recovery enqueued because of the aggressive retries. This causes the committed block to get preempted and we enter a vicious cycle&lt;/p&gt;

&lt;p&gt;So we do,  &amp;lt;initiate_recovery&amp;gt; --&amp;gt; &amp;lt;commit_block&amp;gt; --&amp;gt; &amp;lt;commit_preempted_by_another_recovery&amp;gt;&lt;/p&gt;

&lt;p&gt;This loop is paused after 300 seconds which is the &quot;hbase.lease.recovery.timeout&quot;. Hence the MTTR we are observing is 5 minutes which is terrible. Our ZK session timeout is 30 seconds and HDFS stale node detection timeout is 20 seconds.&lt;/p&gt;

&lt;p&gt;Note that before the patch, we do not call recoverLease so aggressively - also it seems that the HDFS namenode is pretty dumb in that it keeps initiating new recoveries for every call. Before the patch, we call recoverLease, assume that the block was recovered, try to get the file, it has zero length since its under recovery, we fail the task and retry until we get a non zero length. So things just work.&lt;/p&gt;

&lt;p&gt;Fixes:&lt;br/&gt;
1) Expecting recovery to occur within 1 second is too aggressive. We need to have a more generous timeout. The timeout needs to be configurable since typically, the recovery takes as much time as the DFS timeouts. The primary datanode doing the recovery tries to reconcile the blocks and hits the timeouts when it tries to contact the dead node. So the recovery is as fast as the HDFS timeouts.&lt;/p&gt;

&lt;p&gt;2) We have another issue I report in HDFS 4721. The Namenode chooses the stale datanode to perform the recovery (since its still alive). Hence the first recovery request is bound to fail. So if we want a tight MTTR, we either need something like HDFS 4721 or we need something like this&lt;/p&gt;

&lt;p&gt;  recoverLease(...)&lt;br/&gt;
  sleep(1000)&lt;br/&gt;
  recoverLease(...)&lt;br/&gt;
  sleep(configuredTimeout)&lt;br/&gt;
  recoverLease(...)&lt;br/&gt;
  sleep(configuredTimeout)&lt;/p&gt;

&lt;p&gt;Where configuredTimeout should be large enough to let the recovery happen but the first timeout is short so that we get past the moot recovery in step #1.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12643831">HBASE-8389</key>
            <summary>HBASE-8354 forces Namenode into loop with lease recovery requests</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="varunsharma">Varun Sharma</assignee>
                                    <reporter username="varunsharma">Varun Sharma</reporter>
                        <labels>
                    </labels>
                <created>Sun, 21 Apr 2013 17:35:04 +0000</created>
                <updated>Sun, 9 Mar 2014 06:25:08 +0000</updated>
                            <resolved>Wed, 15 May 2013 06:26:18 +0000</resolved>
                                                    <fixVersion>0.94.8</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>17</watches>
                                                                <comments>
                            <comment id="13637606" author="varunsharma" created="Sun, 21 Apr 2013 17:52:12 +0000"  >&lt;p&gt;Attached the Namenode logs showing a large number of recoveries in progress...&lt;/p&gt;

&lt;p&gt;1) nn.log - showing a huge number of initiated recoveries&lt;br/&gt;
2) nn1.log - showing a huge number of block finalization/commit failures&lt;/p&gt;

&lt;p&gt;We ran hbase with an increased sleep period b/w recoveries - 25 seconds and the time to recovery came down substantially.&lt;/p&gt;</comment>
                            <comment id="13637610" author="varunsharma" created="Sun, 21 Apr 2013 18:12:46 +0000"  >&lt;p&gt;Attached sample patch which works in our setup. We run with dfs.socket.timeout=3000 and dfs.socket.write.timeout=5000 - so recovery typically takes &amp;lt; 20 seconds since there is 1 WAL and we recover only 1 block.&lt;/p&gt;

&lt;p&gt;If you closely observe the logs - the first useful recovery starts @49:05 since the first recovery chooses the dead datanode as the primary DN to do the recovery and first commit block is around 49:08 - hence, the recovery is finished within 3 seconds - this is the same as dfs.socket.timeout which is 3 seconds (the primary DN times out on the dead DN while trying to reconcile the replicas).&lt;/p&gt;

&lt;p&gt;I believe if we do not pick stale node replicas (&amp;gt; 20 second heart beat) as primary DN(s) and when we choose a non stale replica as the primary DN, we do not reconcile blocks against the stale replica, we can get the lease recovery to finish under 1 second. Currently that is not the case. &lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13637616" author="yuzhihong@gmail.com" created="Sun, 21 Apr 2013 18:26:11 +0000"  >&lt;p&gt;Patch for trunk where I introduced &apos;hbase.lease.recovery.retry.interval&apos; with default value of 10 seconds.&lt;/p&gt;

&lt;p&gt;If DistributedFileSystem provides API that returns recovery status for given Path, we can utilize that to reduce the number of times we request lease recovery.&lt;/p&gt;</comment>
                            <comment id="13637626" author="hadoopqa" created="Sun, 21 Apr 2013 19:34:17 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12579741/8389-trunk-v1.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12579741/8389-trunk-v1.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5374//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13637645" author="varunsharma" created="Sun, 21 Apr 2013 21:07:47 +0000"  >&lt;p&gt;Ted, thanks for the patch - I just attached a v2 with the corrected comments.&lt;/p&gt;

&lt;p&gt;Basically, there are two things here:&lt;br/&gt;
a) The DDoS is independent of whether the namenode chose the stale data node as the primary DN to do the recovery. All it needs is a slower recovery time than the retry interval. Because then recoveries pile up faster than they actually complete. As a result, any recovery that succeeds gets preempted by a recovery that starts later. So it needs to be as big as the HDFS underlying timeout.&lt;br/&gt;
b) The very first recoverLease call is always a no-op. In fact every third call is a no-op since the NN chooses DN1-&lt;del&gt;&amp;gt;DN2&lt;/del&gt;&lt;del&gt;&amp;gt;DN3&lt;/del&gt;-&amp;gt;DN1 in a cyclic order to do the recoveries. Note that DN1 is the dead datanode here.&lt;/p&gt;

&lt;p&gt;Currently, I think it will take 900 seconds for the cluster to recover if it accepts write traffic across all region servers.&lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13637652" author="lhofhansl" created="Sun, 21 Apr 2013 21:34:59 +0000"  >&lt;p&gt;Making this critical. Either we fix this here or revert &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13637662" author="hadoopqa" created="Sun, 21 Apr 2013 22:15:02 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12579752/8389-trunk-v2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12579752/8389-trunk-v2.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.TestFullLogReconstruction&lt;br/&gt;
                  org.apache.hadoop.hbase.replication.TestReplicationQueueFailover&lt;br/&gt;
                  org.apache.hadoop.hbase.security.access.TestAccessController&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5375//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13637665" author="varunsharma" created="Sun, 21 Apr 2013 22:21:42 +0000"  >&lt;p&gt;I believe the test failed because i increased the default retry interval in patch v2.&lt;/p&gt;

&lt;p&gt;I think before HBASE 8354 - we essentially have the following situation:&lt;/p&gt;

&lt;p&gt;1) Split task picked up by region server - recover lease called&lt;br/&gt;
2) Ignore return value&lt;br/&gt;
3) Try to read file and get a file length=0 and sometimes try to grab the 0 length file from a DN&lt;br/&gt;
4) Mostly fail because of 0 length file or because the DN has not &quot;finalized&quot; the block and its under recovery&lt;br/&gt;
5) Task unassigned and bounce back and forth b/w multiple region servers (I see multiple region servers holdign the same task sometimes)&lt;/p&gt;

&lt;p&gt;This process is equally bad - multi minute recovery (not sure exactly how long)&lt;/p&gt;</comment>
                            <comment id="13637686" author="yuzhihong@gmail.com" created="Sun, 21 Apr 2013 23:41:31 +0000"  >&lt;p&gt;I am reading &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12445209/appendDesign3.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12445209/appendDesign3.pdf&lt;/a&gt; as well as related code in FSNamesystem so that I gain better understanding.&lt;br/&gt;
I am also in contact with hdfs developers about this matter.&lt;/p&gt;

&lt;p&gt;If no satisfactory solution is found Monday, I will revert &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13637688" author="varunsharma" created="Sun, 21 Apr 2013 23:54:47 +0000"  >&lt;p&gt;Hi Ted,&lt;/p&gt;

&lt;p&gt;Seems like lease recovery is the real thorn when it comes to recovery for HBase. The stale node detection patches work very well for splitting the finalized WAL but not the WAL being currently written into. I basically see a very long time to recovery because it always takes a long time for HDFS with the stock timeouts.&lt;/p&gt;

&lt;p&gt;I am trying out a patch for HDFS 4721 which basically avoids all those datanodes which have not heart beated for 20 seconds during block recovery. That seems to enable me to recover the block within 1 second. With that fix, we can survive loss of a single datanode and recover the last WAL within 2-3 seconds.&lt;/p&gt;

&lt;p&gt;I have not heard from the HDFS community on it yet but I think once we lose a datanode - that should not be chosen as the Primary Datanode for lease recovery nor for block reconciliation...&lt;/p&gt;</comment>
                            <comment id="13637689" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 00:02:03 +0000"  >&lt;p&gt;Thanks for the continued effort, Varun.&lt;/p&gt;

&lt;p&gt;Please post your findings on &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt; as well.&lt;/p&gt;</comment>
                            <comment id="13637701" author="varunsharma" created="Mon, 22 Apr 2013 00:35:17 +0000"  >&lt;p&gt;Cool - I attached a rough patch with some logs to show how it allows lease recovery within 1-2 seconds. Now waiting for input from the HDFS community.&lt;/p&gt;

&lt;p&gt;Do we know what the status was w.r.t to lease recovery before HBASE 8354 - we call recoverLease and access the file - mostly error out and then do we retry or do we unassign the zk task for the split ?&lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13637759" author="xieliang007" created="Mon, 22 Apr 2013 04:04:54 +0000"  >&lt;p&gt;w/o &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt;, it may lead to data loss, please refer to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7878&quot; title=&quot;recoverFileLease does not check return value of recoverLease&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7878&quot;&gt;&lt;del&gt;HBASE-7878&lt;/del&gt;&lt;/a&gt; for detail info, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=varunsharma&quot; class=&quot;user-hover&quot; rel=&quot;varunsharma&quot;&gt;Varun Sharma&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13637760" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 04:05:14 +0000"  >&lt;p&gt;Patch for 0.94&lt;br/&gt;
Lease recovery retry is turned off by default.&lt;/p&gt;</comment>
                            <comment id="13637792" author="varunsharma" created="Mon, 22 Apr 2013 06:33:55 +0000"  >&lt;p&gt;Hi Ted,&lt;/p&gt;

&lt;p&gt;+1 on patch for 0.94&lt;/p&gt;

&lt;p&gt;This basically retains the old behaviour prior to hbase 7878. For people who run clusters with tight hdfs timeouts and know that lease/block recovery for them shall occur within x seconds, they can make use of the configurable retry interval to wait for the real recovery to happen.&lt;/p&gt;

&lt;p&gt;I feel that HBase 7878 does not cause data loss for hbase but I am not sure. My theory is that for HBase, size of 1 WAL is &amp;lt; size of one HDFS block before it is rolled. Hence each rolled WAL has one block and one on which the lease is being held contains 1 block under_recovery/under_construction - that file has a size=0 in the namenode until the lease recovery is complete - this is because there are no finalized blocks for the WAL. So, when we do the following, try to replay the WAL without lease recovery being complete, we get a file of size 0 from the namenode.&lt;/p&gt;

&lt;p&gt;From the region server logs, it seems that we do not take size 0 for the file as the truth but instead treat it as a failure until we get a WAL file size &amp;gt; 0.&lt;/p&gt;

&lt;p&gt;However, if the size of the WAL is &amp;gt; HDFS block size then it is possible that some HDFS blocks have been finalized and we get a file size &amp;gt;= 0 because of the finalized blocks. In that case we could end up throwing away the last block belonging to the WAL. Maybe that is why this was observed for accumulo but not for HBase.&lt;/p&gt;</comment>
                            <comment id="13637877" author="nkeywal" created="Mon, 22 Apr 2013 09:43:55 +0000"  >&lt;p&gt;The error scenario is a recovery occurring while the region server is still up &amp;amp; writing: the recovery could be finished (wal file read) while the not-so-dead-regionserver will continue writing, so the last writes will be lost.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;From the region server logs, it seems that we do not take size 0 for the file as the truth but instead treat it as a failure until we get a WAL file size &amp;gt; 0.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What logs are you referring to?&lt;/p&gt;
</comment>
                            <comment id="13637890" author="nkeywal" created="Mon, 22 Apr 2013 10:10:18 +0000"  >&lt;p&gt;In 0.94, before 7878, we were waiting 1s after the releaseFileLease. May be it was a bug, but it was buying some time for the regionserver to abort. May be we could just increase this wait time to something like 5s. We need a proper fix for trunk, but I fear we will need something from hdfs for this. Increasing the sleep would limit the risk. We can be this configurable of course.&lt;/p&gt;

&lt;p&gt;boolean recover = hdfs.recoverLease()&lt;br/&gt;
if (!recover ) // HDFS has started an asynchronous lease recovery.&lt;br/&gt;
    sleep 5s  // hope that it buys enough time for the RS to stop writing&lt;/p&gt;

&lt;p&gt;wdyt?&lt;/p&gt;</comment>
                            <comment id="13637910" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 10:33:22 +0000"  >&lt;p&gt;I plan to rename the new config parameter hbase.lease.recovery.interval&lt;br/&gt;
This is in anticipation of new hdfs API which allows us to query lease recovery progress. &lt;br/&gt;
For 0.94, the retry would be off. We wait hbase.lease.recovery.interval before returning. &lt;/p&gt;</comment>
                            <comment id="13637914" author="nkeywal" created="Mon, 22 Apr 2013 10:42:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted_yu&quot; class=&quot;user-hover&quot; rel=&quot;ted_yu&quot;&gt;Ted Yu&lt;/a&gt; Ok for me. What&apos;s the HDFS jira?&lt;/p&gt;</comment>
                            <comment id="13637933" author="ram_krish" created="Mon, 22 Apr 2013 11:26:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;We need a proper fix for trunk, but I fear we will need something from hdfs for this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think we the original parent JIRA was to make HDFS to provide something so that HBase knows the status of the recovery.  I think that would be a more cleaner way to solve this.&lt;/p&gt;</comment>
                            <comment id="13637977" author="nkeywal" created="Mon, 22 Apr 2013 12:46:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think that would be a more cleaner way to solve this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, I agree. There is a JIRA to make it faster as well (&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt;). I wonder if it&apos;s possible to make synchronous, I expect the answer to be no, but the tradeoff is not that simple. As well, the lease removal could be started by the master, before starting the recovery). If there are a lot of file to read, it would be faster.&lt;/p&gt;</comment>
                            <comment id="13638006" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 13:50:39 +0000"  >&lt;p&gt;Patch v2 renames the config param as &quot;hbase.lease.recovery.interval&quot; with default of 2.5 seconds.&lt;br/&gt;
Test suite run for 8389-0.94.txt was green:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Tests run: 1341, Failures: 0, Errors: 0, Skipped: 13

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13638023" author="nkeywal" created="Mon, 22 Apr 2013 14:17:40 +0000"  >&lt;p&gt;Seems it&apos;s ok. I think 5s is much safer (empirically, it seems the NN needs ~4s when it can speak to the datanode: this would cover the case of dead regionserver but live datanode.&lt;/p&gt;

&lt;p&gt;It&apos;s worth a comment and a log line imho.&lt;br/&gt;
&quot;LOG.info(&quot;Finished lease recover attempt for &quot; + p);&quot; could be changed.&lt;/p&gt;</comment>
                            <comment id="13638049" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 14:49:12 +0000"  >&lt;p&gt;Patch v3 increases default wait time to 4 seconds.&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721?focusedCommentId=13637698&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13637698:&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HDFS-4721?focusedCommentId=13637698&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13637698:&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Earlier - recovery takes as long as dfs.socket.timeout but now it takes roughly 1-2 seconds (which is basically the heartbeat interval).&lt;/p&gt;</comment>
                            <comment id="13638058" author="nkeywal" created="Mon, 22 Apr 2013 14:53:29 +0000"  >&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Ok for 4 seconds.&lt;br/&gt;
You don&apos;t want to add a comment for the &quot;break&quot;?&lt;/p&gt;</comment>
                            <comment id="13638089" author="nkeywal" created="Mon, 22 Apr 2013 15:13:36 +0000"  >&lt;p&gt;+1, thanks Ted.&lt;/p&gt;</comment>
                            <comment id="13638173" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 17:01:05 +0000"  >&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4525&quot; title=&quot;Provide an API for knowing that whether file is closed or not.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4525&quot;&gt;&lt;del&gt;HDFS-4525&lt;/del&gt;&lt;/a&gt;, the following API was added:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /**
   * Get the close status of a file
   * @param src The path to the file
   *
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; file is closed
   * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; FileNotFoundException &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the file does not exist.
   * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException If an I/O error occurred     
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isFileClosed(Path src) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Since it is not available in hadoop 1.1, I will utilize it, through reflection, for 0.95 / trunk.&lt;/p&gt;

&lt;p&gt;A new HBASE JIRA would be logged for the above improvement.&lt;/p&gt;</comment>
                            <comment id="13638187" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 17:22:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;:&lt;br/&gt;
What do you think of 8389-0.94-v4.txt ?&lt;/p&gt;</comment>
                            <comment id="13638230" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 17:57:47 +0000"  >&lt;p&gt;Patch for trunk utilizes isFileClosed() API to check for whether file is closed.&lt;/p&gt;

&lt;p&gt;I moved the call to recoverLease() out of the loop because we don&apos;t need to call it a second time.&lt;/p&gt;

&lt;p&gt;Reflection is used for isFileClosed().&lt;/p&gt;

&lt;p&gt;TestHLogSplit and TestHLog passed.&lt;/p&gt;</comment>
                            <comment id="13638247" author="enis" created="Mon, 22 Apr 2013 18:11:02 +0000"  >&lt;p&gt;With this patch, still Varun&apos;s observation above about the WAL&apos;s having 2 or more blocks would still cause data loss. Can we do exponential backoff at the recoverLease() retries? &lt;br/&gt;
Other than &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4724&quot; title=&quot;Provide API for checking whether lease is recovered or not&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4724&quot;&gt;&lt;del&gt;HDFS-4724&lt;/del&gt;&lt;/a&gt;, should we log another hdfs jira for the recoverLease() call not retrying on outstanding recovery attempts? &lt;/p&gt;</comment>
                            <comment id="13638283" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 18:29:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can we do exponential backoff at the recoverLease() retries?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There is a balance we try to achieve here: reasonably fast recovery without data loss. Since isFileClosed() is absent in hdfs 1.0 /1.1, we don&apos;t have a reliable way of knowing when to return from FSHDFSUtils#recoverFileLease().&lt;br/&gt;
So for 0.94, I prefer to restore previous behavior.&lt;/p&gt;

&lt;p&gt;For trunk, we can continue discussion to achieve the best solution (in another JIRA, if needed).&lt;/p&gt;</comment>
                            <comment id="13638288" author="nkeywal" created="Mon, 22 Apr 2013 18:33:16 +0000"  >&lt;p&gt;For trunk:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;This should not be in compat?&lt;/li&gt;
	&lt;li&gt;Catching raw Exception is dangerous imho.&lt;/li&gt;
	&lt;li&gt;If I understand well, in trunk with HDFS 1.x we do as in 0.94: we sleep to reduce the probability of a dataloss (I don&apos;t have a better solution). Then we should log it explicitly, and wait for a few seconds (4).&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="13638292" author="lhofhansl" created="Mon, 22 Apr 2013 18:40:47 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.blocksize = conf.getLong(&lt;span class=&quot;code-quote&quot;&gt;&quot;hbase.regionserver.hlog.blocksize&quot;&lt;/span&gt;,
        getDefaultBlockSize());
    &lt;span class=&quot;code-comment&quot;&gt;// Roll at 95% of block size.
&lt;/span&gt;    &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt; multi = conf.getFloat(&lt;span class=&quot;code-quote&quot;&gt;&quot;hbase.regionserver.logroll.multiplier&quot;&lt;/span&gt;, 0.95f);
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.logrollsize = (&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;)(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.blocksize * multi);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It seems we&apos;re trying to keep the WAL file size &amp;lt; than the HDFS block size by default.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tedyu%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;tedyu@apache.org&quot;&gt;Ted Yu&lt;/a&gt; Patch looks good. I&apos;d be happier if we increase the interval to 5s, though. If I understand the discussion correctly we&apos;ll pile up recovery request if that interval is less than the actual time it takes to do the recovery at the NN. Losing 1s in return for safety seems acceptable (I&apos;m also fine with 4s, if you feel that would be better).&lt;/p&gt;</comment>
                            <comment id="13638325" author="hadoopqa" created="Mon, 22 Apr 2013 19:04:25 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12579863/8389-trunk-v2.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12579863/8389-trunk-v2.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5383//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13638336" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 19:11:05 +0000"  >&lt;p&gt;I updated Release Notes.&lt;/p&gt;

&lt;p&gt;I prefer to keep the wait time at 4 seconds since HLog would be rolled at 95% of block size.&lt;/p&gt;</comment>
                            <comment id="13638374" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 19:51:19 +0000"  >&lt;p&gt;Revisied Release Notes to be consistent with patch v5.&lt;/p&gt;

&lt;p&gt;hbase.lease.recovery.interval would be used in trunk patch. It represents the interval between successive calls to isFileClosed(). I changed the config param in 0.94 patch to hbase.lease.recovery.waiting.period&lt;/p&gt;

&lt;p&gt;Trunk patch would go through longer review / testing cycle - I will create a new JIRA once patch for 0.94 is integrated.&lt;/p&gt;</comment>
                            <comment id="13638410" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 20:44:13 +0000"  >&lt;p&gt;Patch v6 for 0.94 detects whether hdfs supports isFileClosed() and uses it in the loop if the method is available.&lt;/p&gt;</comment>
                            <comment id="13638418" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 20:49:26 +0000"  >&lt;p&gt;Trunk patch v3 adjusts the waiting period according to whether isFileClosed is supported or not.&lt;/p&gt;</comment>
                            <comment id="13638477" author="hadoopqa" created="Mon, 22 Apr 2013 21:56:08 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12579899/8389-trunk-v3.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12579899/8389-trunk-v3.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop2.0&lt;/font&gt;.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/5384//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13638478" author="lhofhansl" created="Mon, 22 Apr 2013 21:56:45 +0000"  >&lt;p&gt;Wait, so 0.94 we&apos;re only calling isFileClosed() in a loop but not recoverLease(...)?&lt;/p&gt;</comment>
                            <comment id="13638486" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 22:01:29 +0000"  >&lt;p&gt;recoverLease() should not be called repeatedly. hdfs would take care of lease recovery.&lt;br/&gt;
8389-0.94-v6.txt is same as 8389-0.94-v5.txt when isFileClosed is not supported.&lt;br/&gt;
Otherwise we poll DistributedFileSystem for the completion of lease recovery.&lt;/p&gt;</comment>
                            <comment id="13638492" author="varunsharma" created="Mon, 22 Apr 2013 22:09:40 +0000"  >&lt;p&gt;My feeling is that this will not truly work because we will get stuck.&lt;/p&gt;

&lt;p&gt;We call recoverLease - lease recovery gets added to primary datanode which is already dead. Now, we keep calling isClosed() but hte file never closes since the lease recovery does not really start (unless we have something like HDFS 4721).&lt;/p&gt;

&lt;p&gt;Eventually, I suspect there is a timeout for how long HLog tasks can be outstanding.&lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13638531" author="nkeywal" created="Mon, 22 Apr 2013 22:44:33 +0000"  >&lt;p&gt;I think that for 0.94.8, we should stick to the version that acts like before except the increased sleep from 1s to 4s. That&apos;s Ted&quot;s version v5 if I&apos;m not wrong. This version lowers the probability to have a dataloss on false positive regionserver timeout, at a very low cost on mttr.&lt;/p&gt;

&lt;p&gt;For 0.95, we need to test the fixes. By test, I mean unplugging a computer to see what is the mttr. In the tests done here, it&apos;s over 10 minutes (more tests to come tomorrow, but it&apos;s repeatable)&lt;/p&gt;

&lt;p&gt;This will take a little bit longer, but given the complexity and the criticality, it&apos;s our best bet imho...&lt;/p&gt;</comment>
                            <comment id="13638534" author="szetszwo" created="Mon, 22 Apr 2013 22:49:08 +0000"  >&lt;p&gt;&amp;gt; We call recoverLease - lease recovery gets added to primary datanode which is already dead. Now, we keep calling isClosed() but hte file never closes since the lease recovery does not really start ...&lt;/p&gt;

&lt;p&gt;You are right that client calls recoverLease to Namenode and then Namenode will choose a live datanode as the primary for recovery. However, if the primary datanode indeed is dead, a second lease recovery won&apos;t be started until the lease expires.  This will take a long time unless we have &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13638543" author="szetszwo" created="Mon, 22 Apr 2013 22:56:04 +0000"  >&lt;p&gt;&amp;gt; ... a second lease recovery won&apos;t be started ...&lt;/p&gt;

&lt;p&gt;I forgot to say that the second lease recovery will be started automatically by the namenode again but it has to wait for LEASE_HARDLIMIT_PERIOD, which is one hour by default.  If HBase wants to speed up the second recovery, it may wait and check file close for some reasonable time period, say 1 minute, and then call recoverLease again.&lt;/p&gt;</comment>
                            <comment id="13638546" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 23:00:06 +0000"  >&lt;p&gt;According to Nicolas Sze and Nicolas Liochon&apos;s comments above, I would integrate patch v5 to 0.94 later today and open new JIRA for trunk.&lt;/p&gt;</comment>
                            <comment id="13638551" author="varunsharma" created="Mon, 22 Apr 2013 23:02:41 +0000"  >&lt;p&gt;I think &quot;isFileClosed()&quot; loop basically assumes that recoverLease() has started the recovery. But it is quite possible that recoverLease() has never started the recovery. So we need to be a little more careful with the interactions b/w recoverLease and isFileClosed().&lt;/p&gt;

&lt;p&gt;For now we can perhaps stick to v5. I think the behaviour there would be, for a cluster run with default settings:&lt;br/&gt;
a) recoverLease every 4 seconds&lt;br/&gt;
b) HLog split timeout expires&lt;br/&gt;
c) Let the task bounce back and forth b/w region servers&lt;/p&gt;

&lt;p&gt;The only way to fix this is to configure the HDFS cluster in a way such that lease recovery finishes within 4 seconds (like dfs.socket.timeout=3000 and connect timeouts to be low enough). I am going to test out some of these combinations today on 0.94&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Varun&lt;/p&gt;</comment>
                            <comment id="13638689" author="enis" created="Tue, 23 Apr 2013 01:26:33 +0000"  >&lt;p&gt;For Hadoop 2, I think it makes sense to do smt to this affect: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
bool closed = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (!closed) {
 closed = recoverLease();
 &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; !closed and timeout 1 min
   sleep(4 sec)
   closed = isFileClosed()
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For 0.94 and trunk running on Hadoop 1, we can do smt like: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
bool closed = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0;
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (!closed) {
 closed = recoverLease();
 &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (closed) 
   &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
 sleep Min(Pow(2, i), 60) sec
 i++;
}

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13638717" author="yuzhihong@gmail.com" created="Tue, 23 Apr 2013 02:11:49 +0000"  >&lt;p&gt;For Hadoop 2, I agree with the plan above. I would make 1 minute configurable.&lt;/p&gt;

&lt;p&gt;For Hadoop 1.0 / 1.1, a lot of hdfs optimizations are absent w.r.t. stale Data Node handling. I feel it is hard for optimization in HBase to achieve desirable effect under dynamic cluster config.&lt;br/&gt;
So I prefer patch v5 for 0.94&lt;/p&gt;</comment>
                            <comment id="13638741" author="lhofhansl" created="Tue, 23 Apr 2013 03:15:17 +0000"  >&lt;p&gt;Let&apos;s start with v5 for 0.94. We can always improve later.&lt;br/&gt;
If there is other configuration needed on the HDFS side (as Varun indicates) we should document this somewhere (at the very least in the release notes for this issue).&lt;/p&gt;</comment>
                            <comment id="13638743" author="yuzhihong@gmail.com" created="Tue, 23 Apr 2013 03:25:44 +0000"  >&lt;p&gt;Integrated to 0.94&lt;/p&gt;

&lt;p&gt;Thanks for the initial patch, Varun.&lt;/p&gt;

&lt;p&gt;Thanks for the reviews, Lars, Varun, Nicolas and Enis.&lt;/p&gt;

&lt;p&gt;Let&apos;s continue optimization work in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8394&quot; title=&quot;Utilize isFileClosed() so that the wait for lease recovery can be optimized&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8394&quot;&gt;&lt;del&gt;HBASE-8394&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13638768" author="stack" created="Tue, 23 Apr 2013 04:35:52 +0000"  >&lt;p&gt;I think this patch is intellectual wanking without tests or experience on a cluster where the issue is replicated.  I wouldn&apos;t mind but it seems like Varun even volunteered to try out the last version of patch but the patch was committed w/o waiting on his experience.  Would suggest revert until we hear back about Varun&apos;s effort especially since the man was the one to open the issue and was the person who figured out root cause.&lt;/p&gt;</comment>
                            <comment id="13638776" author="lhofhansl" created="Tue, 23 Apr 2013 04:48:14 +0000"  >&lt;p&gt;v5 doesn&apos;t do any harm.&lt;br/&gt;
I&apos;m fine with reverting from 0.94, but only if we also revert &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt; until this is worked out.&lt;/p&gt;</comment>
                            <comment id="13638845" author="hudson" created="Tue, 23 Apr 2013 06:53:14 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #964 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/964/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/964/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8389&quot; title=&quot;HBASE-8354 forces Namenode into loop with lease recovery requests&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8389&quot;&gt;&lt;del&gt;HBASE-8389&lt;/del&gt;&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt; DDoSes Namenode with lease recovery requests (Varun and Ted) (Revision 1470800)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13639174" author="ecn" created="Tue, 23 Apr 2013 15:43:23 +0000"  >&lt;p&gt;Hi, accumulo developer here.  I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7878&quot; title=&quot;recoverFileLease does not check return value of recoverLease&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7878&quot;&gt;&lt;del&gt;HBASE-7878&lt;/del&gt;&lt;/a&gt; based on our experience following HBase&apos;s approach to log recovery.&lt;/p&gt;

&lt;p&gt;From our observations:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;we have never seen the DDoS of the NN while testing on Hadoop 1.0.4 and 2.0.3-alpha&lt;/li&gt;
	&lt;li&gt;accumulo tries to keep the WALs in one HDFS block, too.&lt;/li&gt;
	&lt;li&gt;before checking the return code of recoverLease, we had occasional data loss.&lt;br/&gt;
   This was with tests designed to find data loss, while randomly killing servers every few minutes. These tests would run for hours before detecting data loss.  We were able to attribute data loss to incomplete log recovery.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;accumulo&apos;s approach to recovery:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;it waits a configurable period after the loss of a tablet (region) server lock (default: 10s).&lt;br/&gt;
   This gives the server a chance to die, and for zookeeper to propagate agreement about the lock to the other servers.&lt;/li&gt;
	&lt;li&gt;it calls recoverLease on the WALs until it returns true.&lt;/li&gt;
	&lt;li&gt;due to the discussion in this issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt;), I opened (&lt;a href=&quot;https://issues.apache.org/jira/browse/ACCUMULO-1328&quot; title=&quot;make delay between recoverLease calls configurable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ACCUMULO-1328&quot;&gt;&lt;del&gt;ACCUMULO-1328&lt;/del&gt;&lt;/a&gt;) to make the wait interval between calls to recoverLease configurable: it was hard coded to 1 second.&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="13639198" author="stack" created="Tue, 23 Apr 2013 16:21:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt; Thanks for the input boss&lt;/p&gt;</comment>
                            <comment id="13639205" author="varunsharma" created="Tue, 23 Apr 2013 16:33:04 +0000"  >&lt;p&gt;Thank a lot for chiming in, Eric.&lt;/p&gt;

&lt;p&gt;Its great to know that Accumulo also uses single WAL block - so indeed, 7878 is needed for HBase.&lt;/p&gt;

&lt;p&gt;I may have used a strong word DDos here - it does not mean the NN experiences high CPU, high load or high network. Its just that it enters a vicious cycle of block recoveries (on 2.0.0-alpha)&lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13639206" author="varunsharma" created="Tue, 23 Apr 2013 16:33:32 +0000"  >&lt;p&gt;FYI,&lt;/p&gt;

&lt;p&gt;I am going to test the v5 of this patch today and report back... Thanks !&lt;/p&gt;</comment>
                            <comment id="13640785" author="varunsharma" created="Wed, 24 Apr 2013 18:49:16 +0000"  >&lt;p&gt;Okay I did some testing with v5 and the MTTR was pretty good - 2-3 minutes - log splitting took 20-30 seconds - basically around 1 minute of this time was to replay the edits from the recovered_edits. This was with the stale node patches (3703 and 3912) however for HDFS. Also had tight dfs.socket.timeout=30. I mostly, suspended the Datanode and the region server packages at the same time. I also ran a test where I used iptables to firewall against all traffic to the host except &quot;ssh&quot; traffic.&lt;/p&gt;

&lt;p&gt;However, the weird thing was, I also tried to reproduce the failure scenario above, with is setting the timeout at 1 second and I could not. I looked into the NN logs and this is what happened. Lease recovery was called the 1st time and a block recovery was initiated with the dead datanode (no HDFS 4721). Lease recovery was called the 2nd time and it returned true almost every time I ran these tests.&lt;/p&gt;

&lt;p&gt;This is something that I did not see, the last time around. The logs I attached above show that a release recovery is called once by one SplitLogWorker, followed by 25 calls by another worker, followed by another 25 and eventually hundreds of calls the 3rd time. The 25 calls make sense since each split worker has a task level timeout of 25 seconds and we do recoverLease every second. Also there are 3 resubmissions, so the last worker is trying to get back the lease. I wonder if I hit a race condition which I can no longer reproduce, where one worker had the lease and did not give it up and subsequent workers just failed to recover the lease. In which case, 8354 is not the culprit but I still prefer the more relaxed timeout in this JIRA.&lt;/p&gt;

&lt;p&gt;Also, I am now a little confused with lease recovery. It seems that lease recovery can be separate from block recovery. Basically, recover lease is called the first time, we enqueue a block recovery (which is never going to happen since we try to hit the dead datanode thats not heartbeating). However the 2nd call still returns true which confuses me since the block is still not finalized.&lt;/p&gt;

&lt;p&gt;I wonder if lease recovery means anything other than, flipping something at the namenode saying who has the lease to the file. But its quite possible that the underlying block/file has not truly been recovered.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;&lt;br/&gt;
Do you see something similar in your namenode logs as you kill, lease recovery initiated but no real block recovery/commitSynchronization messages (both regionserver + datanode) ? When we kill region server + datanode, we basically kill the primary or the first datanode which holds the block - this is the same datanode which would be chosen for block recovery..&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Varun&lt;/p&gt;

</comment>
                            <comment id="13640909" author="ecn" created="Wed, 24 Apr 2013 20:31:25 +0000"  >&lt;p&gt;I will experiment tomorrow to see what happens when I firewall a whole node.&lt;/p&gt;

&lt;p&gt;Our master calls recoverLease and does not initiate log sorting until it returns true.  I know from our logs that it takes several calls to the NN, and sometimes as long as 30 seconds.  This is from memory, so I&apos;ll double check.&lt;/p&gt;
</comment>
                            <comment id="13640925" author="varunsharma" created="Wed, 24 Apr 2013 20:40:57 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is something that I am not seeing in the tests I ran yesterday. The 2nd call almost always succeeds and returns null. Also, when the master calls recoverLease, are they corresponding CommitBlockSynchronization messages in the NN log ?&lt;/p&gt;

&lt;p&gt;You may find this helpful - &lt;a href=&quot;http://www.cyberciti.biz/tips/linux-iptables-4-block-all-incoming-traffic-but-allow-ssh.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.cyberciti.biz/tips/linux-iptables-4-block-all-incoming-traffic-but-allow-ssh.html&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="13641043" author="enis" created="Wed, 24 Apr 2013 22:12:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;Also, I am now a little confused with lease recovery. It seems that lease recovery can be separate from block recovery.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Lease recovery is a two step process. It does lease recovery first, where NN assigns the lease to itself, generates a new GS, which should ensure that the client cannot do any more NN operations. Then it does block recovery with another GS, where it schedules the primary DN to recover the block. This document  &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12445209/appendDesign3.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12445209/appendDesign3.pdf&lt;/a&gt; has some details on the process. Not sure how current it is though. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;However the 2nd call still returns true which confuses me since the block is still not finalized.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do you have any logs that correspond to this. If so, this is clearly a bug in HDFS. If we can repro this, we can have HDFS folks look into it. &lt;/p&gt;</comment>
                            <comment id="13641045" author="yuzhihong@gmail.com" created="Wed, 24 Apr 2013 22:13:45 +0000"  >&lt;p&gt;Looking at Accumulo pom.xml, I think hadoop 1.0.4 is used.&lt;br/&gt;
I think Varun is testing something close to hdfs 2.0&lt;/p&gt;

&lt;p&gt;There may be difference when we compare logs from Namenode and Datanodes.&lt;/p&gt;

&lt;p&gt;@Varun:&lt;br/&gt;
If you can attach NN log showing what you described above, that would be nice.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13641096" author="ecn" created="Wed, 24 Apr 2013 22:39:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted_yu&quot; class=&quot;user-hover&quot; rel=&quot;ted_yu&quot;&gt;Ted Yu&lt;/a&gt;, accumulo is being stress-tested against 2.0.3-alpha these days; it requires building with a specific profile, but works well.&lt;/p&gt;

&lt;p&gt;We are presently testing the first release of accumulo to use HDFS for WALogs, so it is critical we know the behavior of recoverLease.  We have not had any data loss since we began watching the return result of recoverLease; we are growing more confident that it works correctly.&lt;/p&gt;</comment>
                            <comment id="13641101" author="varunsharma" created="Wed, 24 Apr 2013 22:44:45 +0000"  >&lt;p&gt;This is Hadoop 2.0.0 alpha CDH 4.2 - namenode logs - this all there is for this block&lt;br/&gt;
LOG LINE FOR BLOCK CREATION (NAMENODE)&lt;/p&gt;

&lt;p&gt;2013-04-24 05:40:30,282 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /hbase/.logs/ip-10-170-15-97.ec2.internal,60020,1366780717760/ip-10-170-15-97.ec2.internal%2C60020%2C1366780717760.1366782030238. BP-889095791-10.171.1.40-1366491606582 blk_-2482251885029951704_11942&lt;/p&gt;
{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[10.170.15.97:50010|RBW], ReplicaUnderConstruction[10.168.12.138:50010|RBW], ReplicaUnderConstruction[10.170.6.131:50010|RBW]]}

&lt;p&gt;LOG LINES FOR RECOVERY INITIATION (NAMENODE)&lt;/p&gt;

&lt;p&gt;2013-04-24 06:14:43,623 INFO BlockStateChange: BLOCK* blk_-2482251885029951704_11942&lt;/p&gt;
{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[10.170.15.97:50010|RBW], ReplicaUnderConstruction[10.168.12.138:50010|RBW], ReplicaUnderConstruction[10.170.6.131:50010|RBW]]}
&lt;p&gt; recovery started, primary=10.170.15.97:50010&lt;br/&gt;
2013-04-24 06:14:43,623 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseLease: File /hbase/.logs/ip-10-170-15-97.ec2.internal,60020,1366780717760-splitting/ip-10-170-15-97.ec2.internal%2C60020%2C1366780717760.1366782030238 has not been closed. Lease recovery is in progress. RecoveryId = 12012 for block blk_-2482251885029951704_11942&lt;/p&gt;
{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[10.170.15.97:50010|RBW], ReplicaUnderConstruction[10.168.12.138:50010|RBW], ReplicaUnderConstruction[10.170.6.131:50010|RBW]]}

&lt;p&gt;Note that the primary index is 0 - which is the datanode i killed. This was chosen as the primary DN for lease recovery. Obviously it will not work isnce the node is dead. But recoverLease returned true neverthless for the next call. Now I am not sure if that is expected behaviour since the real block recovery never happened.&lt;/p&gt;</comment>
                            <comment id="13641106" author="varunsharma" created="Wed, 24 Apr 2013 22:50:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;&lt;br/&gt;
Thanks Eric for checking (I guess we should have a similar data loss checker for hbase)&lt;/p&gt;

&lt;p&gt;I wonder if you could look at your NN logs and see if you do get commitBlockSynchronization() log messages when the recoverLease method is called. I am trying to figure out why the block is not getting recovered and recoverlease is still returning true. These show up like this&lt;/p&gt;

&lt;p&gt;2013-04-24 16:38:26,254 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-889095791-10.171.1.40-1366491606582:blk_-2482251885029951704_11942, newgenerationstamp=12012, newlength=7044280, newtargets=&lt;span class=&quot;error&quot;&gt;&amp;#91;10.170.15.97:50010&amp;#93;&lt;/span&gt;, closeFile=true, deleteBlock=false)&lt;/p&gt;</comment>
                            <comment id="13641291" author="enis" created="Thu, 25 Apr 2013 01:42:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;I guess we should have a similar data loss checker for hbase&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In fact we do. We have been using the goraci tool, which is a port of the accumulo test. Then we ported that to hbase proper as well. The class name is IntegrationTestBigLinkedList. We also have other load / verify tools as well, namely, LoadTestTool and IntegrationTestLoadAndVerify. &lt;br/&gt;
We also have a tool called ChaosMonkey that does the killing of servers (see &lt;a href=&quot;http://hbase.apache.org/book/hbase.tests.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/book/hbase.tests.html&lt;/a&gt;). &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But recoverLease returned true neverthless for the next call. Now I am not sure if that is expected behaviour since the real block recovery never happened.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For each call, NN will choose the primary NN in round robin fashion. See BlockInfoUnderConstruction#initializeBlockRecovery():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; previous = primaryNodeIndex;
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 1; i &amp;lt;= replicas.size(); i++) {
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; j = (previous + i)%replicas.size();
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (replicas.get(j).isAlive()) {
        primaryNodeIndex = j;
        DatanodeDescriptor primary = replicas.get(j).getExpectedLocation(); 
        primary.addBlockToBeRecovered(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;);
        NameNode.stateChangeLog.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;BLOCK* &quot;&lt;/span&gt; + &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;
          + &lt;span class=&quot;code-quote&quot;&gt;&quot; recovery started, primary=&quot;&lt;/span&gt; + primary);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Are those logs above from the first call? Do you have logs that correspond to second or third call? Before returning true, you should see a log from:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(nrCompleteBlocks == nrBlocks) {
      finalizeINodeFileUnderConstruction(src, pendingFile);
      NameNode.stateChangeLog.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;BLOCK*&quot;&lt;/span&gt;
        + &lt;span class=&quot;code-quote&quot;&gt;&quot; internalReleaseLease: All existing blocks are COMPLETE,&quot;&lt;/span&gt;
        + &lt;span class=&quot;code-quote&quot;&gt;&quot; lease removed, file closed.&quot;&lt;/span&gt;);
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;  &lt;span class=&quot;code-comment&quot;&gt;// closed!
&lt;/span&gt;    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13641294" author="varunsharma" created="Thu, 25 Apr 2013 01:46:27 +0000"  >&lt;p&gt;There is no second call - because the 2nd call returns true - I am following up on this in HDFS 4721&lt;/p&gt;</comment>
                            <comment id="13641393" author="yuzhihong@gmail.com" created="Thu, 25 Apr 2013 04:13:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;:&lt;br/&gt;
Thanks for the update about hadoop version that you use.&lt;/p&gt;

&lt;p&gt;2.0.4-alpha is being released. You may want to check it out.&lt;/p&gt;</comment>
                            <comment id="13641467" author="varunsharma" created="Thu, 25 Apr 2013 05:56:29 +0000"  >&lt;p&gt;Alright, so it seems I have been stupid in running the recent tests. The lease recovery is correct in hadoop. I forgot what v5 patch exactly does, it reverts to old behaviour - I kept searching the namenode logs for multiple lease recoveries &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;HDFS timeouts (for region server and HDFS) - socket timeout = 3 seconds, socket write timeout = 5 seconds and ipc connect retries = 0 (timeout is hardcoded at 20 seconds which is way too high)&lt;/p&gt;

&lt;p&gt;I am summarizing each case:&lt;br/&gt;
1) After this patch,&lt;br/&gt;
When we split a log, we will do the following:&lt;br/&gt;
  a) Call recoverLease, which will enqueue a block recovery to the dead datanode, so a noop&lt;br/&gt;
  b) sleep 4 seconds&lt;br/&gt;
  c) Break the loop and access the file irrespective of whether recovery happened&lt;br/&gt;
  d) Sometimes fail but eventually get through&lt;/p&gt;

&lt;p&gt;Note that lease recovery has not happened. If hbase finds a zero size hlog at any of the datanodes (the size is typically zero at the namenode since the file is not closed yet), it will error out and unassign the task, some other region server will pick up the split task. From the hbase console, I am always seeing non zero edits being split - so we are reading data. I am not sure if accumulo does similar checks for zero sized WALs, but &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt; will know better.&lt;/p&gt;

&lt;p&gt;Since lease recovery has not happened, we risk data loss but it again depends on what kind of data loss accumulo sees, whether entire WAL(s) are lost or portions of WAL(s). If its entire WAL(s), maybe the zero sized check in HBase saves it from data loss. But if portions of WAL are being lost in accumulo when recoverLease return value is not checked, then we can have data loss after v5 patch. Again I will let &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt; speak on that.&lt;/p&gt;

&lt;p&gt;The good news though is that I am seeing pretty good MTTR in this case. Its typically 2-3 minutes and WAL splitting accounts for maybe 30-40 seconds. But note that I am running with HDFS 3912, 3703 and that my HDFS timeouts are configured to fail fast.&lt;/p&gt;

&lt;p&gt;2) Before this patch but after 8354&lt;/p&gt;

&lt;p&gt;We have the issue where lease recoveries pile up on the namenode faster than they can be served (every second), the side effect is that each latter recovery preempts the earlier one. Basically with HDFS it is simply not possible to get lease recovery within 4 seconds unless we use some of the stale node patches and really tighten all the HDFS timeouts and retries. So recoveries never finish in one second and they keep piling up and preempting earlier recoveries. Eventually we wait for 300 seconds, hbase.lease.recovery.timeout, after which we just open the file and mostly the last recovery has succeeded by then.&lt;/p&gt;

&lt;p&gt;MTTR is not good in this case - at least 6 minutes for log splitting. On possibility could have been to reduce the number 300  seconds to maybe 20 seconds. &lt;/p&gt;

&lt;p&gt;3) One can have the best of both worlds - a good MTTR and no/little data loss by opening files after real lease recovery has happened to avoid data corruption. For that, one would need to tune their HDFS timeouts to be low, the connect + socket timeouts, so that lease recoveries can happen within 5-10 seconds. I think that, for such cases we should have a parameter, saying whether we want to force lease recovery before - I am going to raise a JIRA to discuss that configuration. Overall, if we had an isClosed() API life would be so much easier but a large number of hadoop releases do not have it, yet. I think this is more of a power user configuration but it probably makes sense to have one.&lt;/p&gt;

&lt;p&gt;Thanks !&lt;/p&gt;</comment>
                            <comment id="13641499" author="varunsharma" created="Thu, 25 Apr 2013 06:43:39 +0000"  >&lt;p&gt;As I said above, there is no bug in HDFS - for a moment I thought that patch v5 only increased the timeout from 1s to 4s but it also reverts to old behaviour of not enforcing lease recovery so that we can reduce MTTR. If we choose to enforce lease recovery, then if this timeout is significantly lower than the time it takes to recover the lease (if we can&apos;t recover within 4s), our MTTR will be poor.&lt;/p&gt;</comment>
                            <comment id="13641521" author="nkeywal" created="Thu, 25 Apr 2013 07:28:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Basically with HDFS it is simply not possible to get lease recovery within 4 seconds unless we use some of the stale node patches and really tighten all the HDFS timeouts and retries.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Stale node is mandatory if you care about mttr and performances during failures. I&apos;m more scare about putting the timeouts to very low numbers.&lt;/p&gt;

&lt;p&gt;I have the following improvements in mind:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;have the master starts the recovery and not the region server&lt;/li&gt;
	&lt;li&gt;in hdfs recovery, retry only once.&lt;/li&gt;
	&lt;li&gt;add an API in the namenode to mark a datanode as staled. This would be used by the master before doing the recoverLease.&lt;/li&gt;
	&lt;li&gt;when splitting, put a higher priority on files already closed&lt;/li&gt;
	&lt;li&gt;using the locality when reading the wal is in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt;. This should decrease the split time.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;A brittle option would be to start the split while the recovery is in progress, and synchronize on fileClosed only at the end. This would be may be reasonable on top of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7006&quot; title=&quot;[MTTR] Improve Region Server Recovery Time - Distributed Log Replay&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7006&quot;&gt;&lt;del&gt;HBASE-7006&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The v5 in 0.94 is just a way to buy time, without risking regression, not a long term solution. Points 1 to 3 are acceptable in 0.94 while point 4 &amp;amp; 5 is for trunk (at least at the beginning) imho.&lt;/p&gt;

&lt;p&gt;The tests here (on trunk) still show some random issues, but I would expect that we can make it work with a hdfs timeout of 30s.&lt;/p&gt;

&lt;p&gt;Varun, From your tests, with 0.94 and an hdfs that have stale mode &amp;amp; the 3 first point above, do you think it would work?&lt;/p&gt;</comment>
                            <comment id="13641542" author="varunsharma" created="Thu, 25 Apr 2013 07:52:21 +0000"  >&lt;p&gt;Hi Nicholas,&lt;/p&gt;

&lt;p&gt;Firstly I configure the HDFS cluster in the following way:&lt;/p&gt;

&lt;p&gt;dfs.socket.timeout = 3sec&lt;br/&gt;
dfs.socket.write.timeout = 5sec&lt;br/&gt;
ipc.client.connect.timeout = 1sec&lt;br/&gt;
ipc.client.connect.max.retries.on.timeouts = 2 (hence total 3 retries)&lt;/p&gt;

&lt;p&gt;The connect timeout is low since connecting should really be very fast unless something major is wrong. Our clusters are housed within the same AZ on amazon EC2 and it is very rare to see this timeouts even getting hit on EC2 which is known for poor I/O performance. I, for most, see this timeouts kick in during failures. Note that these timeouts are only used for avoiding bad datanodes and not for marking nodes as dead/stale, so i think these timeouts are okay for quick failovers - we already have high timeouts for dead node detection/zookeeper session (10&apos;s of seconds).&lt;/p&gt;

&lt;p&gt;stale node timeout = 20 seconds&lt;br/&gt;
dead node timeout = 10 minutes&lt;br/&gt;
ZooKeeper session timeout = 30 seconds&lt;/p&gt;

&lt;p&gt;HDFS is hadoop 2.0 with HDFS 3703, HDFS 3912 and HDFS 4721. The approach is the following:&lt;/p&gt;

&lt;p&gt;a) A node is failed artificially using&lt;br/&gt;
  1) Use iptables to only allow ssh traffic and drop all traffic&lt;br/&gt;
  2) Suspending the processes&lt;/p&gt;

&lt;p&gt;b) Even though we configure stale detection to be faster than hbase detection, lets assume that does not play out. The node is not marked stale.&lt;/p&gt;

&lt;p&gt;c) Lease recovery attempt # 1&lt;br/&gt;
   i) We choose a good primary node for recovery - since its likely that the bad node has the worst possible heartbeat (HDFS 4721)&lt;br/&gt;
   ii) But we point it to recover from all 3 nodes since we are considering the worst case where no node is marked stale&lt;br/&gt;
   iii) The primary tries to reconcile the block with all 3 nodes and hits either&lt;br/&gt;
        a) dfs.socket.timeout = 3 seconds - if process is suspended&lt;br/&gt;
        b) ipc.connect.timeout X ipc.connect.retries which is 3 * 1 second + 3 * 1 second sleep = 6 seconds - if we firewall the host using iptables&lt;/p&gt;

&lt;p&gt;d) If we use a value of 4 seconds, the first recovery attempt does not finish in time and we initiate lease recovery #2&lt;br/&gt;
   i) Either a rinse and repeat of c) happens&lt;br/&gt;
   ii) Or the node is now stale and the block is instantly recovered from the remaining two replicas&lt;/p&gt;

&lt;p&gt;I think by we could either adjust the timeout 4 seconds to be say 8 seconds and mostly be able to get the first attempt successful or otherwise, we just wait to get stale node detection and then we will have a fairly quick block recovery due to HDFS 4721.&lt;/p&gt;

&lt;p&gt;I will try to test these values tomorrow, by rebooting some nodes...&lt;/p&gt;</comment>
                            <comment id="13641711" author="nkeywal" created="Thu, 25 Apr 2013 12:12:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;b) Even though we configure stale detection to be faster than hbase detection, lets assume that does not play out. The node is not marked stale.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep. If we have the API to make a node as stale, it lowers a lot the probability to try to go to a stale node while recovering, and if we limit the number of tries to one, we should spent less than 1 minute trying the wrong node.&lt;/p&gt;</comment>
                            <comment id="13641804" author="varunsharma" created="Thu, 25 Apr 2013 13:52:19 +0000"  >&lt;p&gt;Okay. I am just not 100 % sure if we want to mark the datanode as stale if the region server crashes alone...&lt;/p&gt;</comment>
                            <comment id="13641815" author="nkeywal" created="Thu, 25 Apr 2013 14:00:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;I am just not 100 % sure if we want to mark the datanode as stale if the region server crashes alone...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In the HDFS jira, I proposed to put a duration: that limit the effect if there are false positive.&lt;br/&gt;
We could as well do a check before marking the datanode as stale. It&apos;s not necessary short term imho, but we can add a hook in case someone wants to do it.&lt;/p&gt;</comment>
                            <comment id="13642089" author="ecn" created="Thu, 25 Apr 2013 19:04:13 +0000"  >&lt;p&gt;I can confirm long recovery times.&lt;/p&gt;

&lt;p&gt;I stopped the data node and tablet server with SIGSTOP. I&apos;m seeing messages like this from the NN:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2013-04-25 17:55:39,923 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 6093, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.commitBlockSynchronization from 10.0.0.7:49820: error: java.io.IOException: The recovery id 2494 does not match current recovery id 2495 for block BP-189257095-10.0.0.3-1366907958658:blk_8428898362502069151_2174
java.io.IOException: The recovery id 2494 does not match current recovery id 2495 for block BP-189257095-10.0.0.3-1366907958658:blk_8428898362502069151_2174
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even when I use a long delay between calls to recoverLease (60s), it does not recover.&lt;/p&gt;

&lt;p&gt;If I kill the servers, recovery happens quickly.&lt;/p&gt;

&lt;p&gt;Unfortunately we see this kind of &quot;unresponsive, but not dead&quot; servers in practice.&lt;/p&gt;

&lt;p&gt;I tried these configurations, and it did not improve:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;dfs.socket.timeout = 3sec
dfs.socket.write.timeout = 5sec
ipc.client.connect.timeout = 1sec
ipc.client.connect.max.retries.on.timeouts = 2 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="13642099" author="varunsharma" created="Thu, 25 Apr 2013 19:11:51 +0000"  >&lt;p&gt;How long is the recovery time - the 1st recovery will always be moot unless u have HDFS 4721.&lt;/p&gt;

&lt;p&gt;Can you grep the namenode logs for &quot;8428898362502069151&quot; and paste them here ?&lt;/p&gt;</comment>
                            <comment id="13642101" author="varunsharma" created="Thu, 25 Apr 2013 19:14:03 +0000"  >&lt;p&gt;One more question - you did apply these timeouts to both your HDFS datanodes and your accumulo DFS clients (restart) ? If yes, I expect the second recovery to succeed, hence you should recover in 2 attempts = 120 seconds&lt;/p&gt;</comment>
                            <comment id="13642171" author="ecn" created="Thu, 25 Apr 2013 20:23:32 +0000"  >&lt;p&gt;I double-checked the configuration settings... and there were two typos.  Now files recover as expected.&lt;/p&gt;
</comment>
                            <comment id="13642175" author="varunsharma" created="Thu, 25 Apr 2013 20:26:19 +0000"  >&lt;p&gt;Okay, I am done with testing for hbase using the above configuration - tight timeouts and stale node patches. I am using patch v5 and HBASE 8434 on top, to force lease recovery and not skip it.&lt;/p&gt;

&lt;p&gt;The testing is done by doing &quot;kill -STOP &amp;lt;server_process&amp;gt;; kill -STOP &amp;lt;datanode_process&amp;gt;&quot;. I am forcing lease recovery so - I am applying HBASE 8434 on top which basically means keep calling recoverLease until it returns true every 5 seconds. I have not found a single case where it takes more than 30-40 seconds for recovery. The HDFS runs with 3703, 3912 and 4721 patches. So at some point the recovery succeeds within 1 second after a node becomes marked stale.&lt;/p&gt;

&lt;p&gt;So, I am able to consistently get log splitting finished within the 1st minute and either by the 2nd or 3rd minute all regions are back online. I have tried this a sufficient number of times to convince me that it works.&lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13642181" author="varunsharma" created="Thu, 25 Apr 2013 20:32:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ecn&quot; class=&quot;user-hover&quot; rel=&quot;ecn&quot;&gt;Eric Newton&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Files recover as expected.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you elaborate - how many recovery attempts for success and also how long b/w retries ?&lt;/p&gt;

&lt;p&gt;Varun&lt;/p&gt;</comment>
                            <comment id="13642629" author="stack" created="Fri, 26 Apr 2013 06:22:24 +0000"  >&lt;p&gt;Reading over this nice, fat, info-dense issue, I am trying to figure what we need to add to trunk right now.&lt;/p&gt;

&lt;p&gt;Sounds like checking the recoverFileLease return checking gained us little in the end (though Varun you think we want to keep going till its true though v5 here skips out on it).  The valuable finding hereabouts is the need for a pause before going ahead with file open it seems.  Trunk does not have this pause.  I need to add a version of v5 to trunk?  (Holding our breath until an api not yet generally available, isFileClosed hbase-8394, shows up is not an option for now; nor is an expectation that all will just upgrade to an hdfs that has this api on either.)&lt;/p&gt;

&lt;p&gt;hbase-7878 backport is now elided since we have added back the old behavior w/ patch applied here excepting the pause of an arbitrary enough 4seconds&lt;/p&gt;

&lt;p&gt;The applied patch here does not loop on recoverLease after the 4seconds expire.  It breaks. In trunk we loop.  We should break too (...and let it fail if 0 length and then let the next split task do a new recoverLease call?)&lt;/p&gt;

&lt;p&gt;On the 4seconds, it seems that it rather should be the dfs timeout dfs.socket.timeout that hdfs is using &amp;#8211; plus a second or so &amp;#8211; rather than &quot;4seconds&quot; if I follow Varuns&apos; reasoning above properly and just remove the new config &apos;hbase.lease.recovery.retry.interval&apos; (We have enough configs already)?&lt;/p&gt;

&lt;p&gt;Sounds like we are depending on WAL sizes being &amp;lt; HDFS block sizes.  This will not always be the case; we could go into a second block easily if a big edit comes in on the tail of the first block; and then there may be dataloss (TBD) because we have a file size (so we think the file recovered?)&lt;/p&gt;

&lt;p&gt;Sounds also like we are relying file size being zero as a marker that file is not yet closed (I suppose that is ok because an empty WAL will be &amp;gt; 0 length IIRC.  We should doc. our dependency though)&lt;/p&gt;

&lt;p&gt;Varun, i like your low timeouts.  Would you suggest we adjust hbase default timeouts down and recommend folks change their hdfs defaults if they want better MTTR?  If you had a blog post on your nice work done in here, I could at least point the refguide at it for those interested in improved MTTR (smile).&lt;/p&gt;</comment>
                            <comment id="13642669" author="nkeywal" created="Fri, 26 Apr 2013 08:18:21 +0000"  >&lt;p&gt;Varun, I +1 Stack: the timeout setting you mentionned are quite impressive!&lt;br/&gt;
Thanks a lot for all this work.&lt;/p&gt;

&lt;p&gt;Here is my understanding, please correct me where I&apos;m wrong.&lt;/p&gt;

&lt;p&gt;In don&apos;t think that single / multiple block is an issue, even if it&apos;s better to have single block (increased parallelism).&lt;/p&gt;

&lt;p&gt;HBase has a dataloss risk: we need to wait for the end of recoverFileLease before reading.&lt;br/&gt;
 =&amp;gt; Either by polling the NN and calling recoverFileLease multiple times&lt;br/&gt;
 =&amp;gt; Either calling isFileClosed (&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4525&quot; title=&quot;Provide an API for knowing that whether file is closed or not.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4525&quot;&gt;&lt;del&gt;HDFS-4525&lt;/del&gt;&lt;/a&gt;) (and polling as well) where it&apos;s available.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure that we can poll every second recoverFileLease. When I try I have the same logs as Eric: &quot;java.io.IOException: The recovery id 2494 does not match current recovery id 2495 for block&quot;, and the state of the namenode seems strange. &lt;/p&gt;

&lt;p&gt;In critical scenarios, the recoverFileLease won&apos;t happen at all. The probability is greatly decreased by &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt;, but it&apos;s not zero.&lt;/p&gt;

&lt;p&gt;In critical scenarios, the recoverFileLease will start, but will be stuck in bad datanodes. The probability is greatly decreased by &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4754&quot; title=&quot;Add an API in the namenode to mark a datanode as stale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4754&quot;&gt;HDFS-4754&lt;/a&gt;, but it&apos;s not zero. Here, we need to limit the number of retry in HDFS to one, whatever the global setting, to be on the safe side (no hdfs jira for this).&lt;/p&gt;

&lt;p&gt;I see a possible common implementation (trunk / hbase 0.94)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4754&quot; title=&quot;Add an API in the namenode to mark a datanode as stale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4754&quot;&gt;HDFS-4754&lt;/a&gt;, calls markAsStale to be sure this datanode won&apos;t be used.&lt;/li&gt;
	&lt;li&gt;call recoverFileLease a first time&lt;/li&gt;
	&lt;li&gt;if &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4525&quot; title=&quot;Provide an API for knowing that whether file is closed or not.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4525&quot;&gt;&lt;del&gt;HDFS-4525&lt;/del&gt;&lt;/a&gt; is available, call isFileClosed every second to detect that the recovery is done&lt;/li&gt;
	&lt;li&gt;every 60s, call again recoverFileLease (either isFileClosed is missing, either we went into one of the bad scenario above).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This would mean: no dataloss and a MTTR of:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;less than a minute if we have stale mode + &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt; + &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4754&quot; title=&quot;Add an API in the namenode to mark a datanode as stale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4754&quot;&gt;HDFS-4754&lt;/a&gt; + &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4525&quot; title=&quot;Provide an API for knowing that whether file is closed or not.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4525&quot;&gt;&lt;del&gt;HDFS-4525&lt;/del&gt;&lt;/a&gt; + no retry in HDFS recoverLease or Varun&apos;s settings.&lt;/li&gt;
	&lt;li&gt;around 12 minutes if we have none of the above. But that&apos;s what we have already without the stale mode imho.&lt;/li&gt;
	&lt;li&gt;in the middle if we have a subset of the above patches and config.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4721&quot; title=&quot;Speed up lease/block recovery when DN fails and a block goes into recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4721&quot;&gt;&lt;del&gt;HDFS-4721&lt;/del&gt;&lt;/a&gt; seems validated by the HDFS dev team, I think that my only question is: can we poll very frequently recoverFileLease if we don&apos;t have isFileClosed?&lt;/p&gt;

&lt;p&gt;As a side node, tests more or less similar to yours with HBase trunk and HDFS branch-2 trunk (without your settings but with a hack to skip the deadnodes) brings similar results.&lt;/p&gt;</comment>
                            <comment id="13642823" author="ecn" created="Fri, 26 Apr 2013 13:20:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can you elaborate - how many recovery attempts for success and also how long b/w retries ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;After the tablet server loses its lock in zookeeper, the master waits 10s and calls recoverLease which returns false.  After 5s, recoverLease is retried and succeeds.  These are the default values for the timeouts.&lt;/p&gt;
</comment>
                            <comment id="13643136" author="varunsharma" created="Fri, 26 Apr 2013 19:02:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt;&lt;br/&gt;
I can do a small write up that folks can refer to.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt;&lt;br/&gt;
One point regarding the low setting though. Its good for fast MTTR requirements such as online clusters but it does not work well if you pound a small cluster with mapreduce jobs. The write timeouts start kicking in on datanodes - we saw this on a small cluster. So it has to be taken with a pinch of salt.&lt;/p&gt;

&lt;p&gt;I think 4 seconds might be too tight. Because we have the following sequence -&lt;br/&gt;
1) recoverLease called&lt;br/&gt;
2) The primary node heartbeats (this can be 3 seconds in the worst case)&lt;br/&gt;
3) There are multiple timeouts during recovery at primary datanode:&lt;br/&gt;
    a) dfs.socket.timeout kicks in when we suspend the processes using &quot;kill -STOP&quot; - there is only 1 retry&lt;br/&gt;
    b) ipc.client.connect.timeout is the troublemaker - on old hadoop versions it is hardcoded at 20 seconds. On some versions, the # of retries is hardcoded at 45. This can be trigger by firewalling a host using iptables to drop all incoming/outgoing TCP packets. Another issue here is that b/w the timeouts there is a 1 second hardcoded sleep &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; - I just fixed it in HADOOP 9503. If we make sure that all the dfs.socket.timeout and ipc client settings are the same in hbase-site.xml and hdfs-site.xml. Then, we can&lt;/p&gt;

&lt;p&gt;The retry rate should be no faster than 3a and 3b - or lease recoveries will accumulate for 900 seconds in trunk. To get around this problem, we would want to make sure that hbase-site.xml has the same settings as hdfs-site.xml. And we calculate the recovery interval from those settings. Otherwise, we can leave a release note saying that this number should be max(dfs.socket.timeout, ipc.client.connect.max.retries.on.timeouts * ipc.client.connect.timeout, ipc.client.connect.max.retries).&lt;/p&gt;

&lt;p&gt;The advantage of having HDFS 4721 is that at some point the data node will be recognized as stale - maybe a little later than hdfs recovery. Once that happens, recoveries typically occuring within 2 seconds.&lt;/p&gt;</comment>
                            <comment id="13643220" author="yuzhihong@gmail.com" created="Fri, 26 Apr 2013 20:31:33 +0000"  >&lt;p&gt;@Varun:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Then, we can&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you complete the above sentence ?&lt;/p&gt;</comment>
                            <comment id="13643226" author="varunsharma" created="Fri, 26 Apr 2013 20:36:01 +0000"  >&lt;p&gt;Sorry about that...&lt;/p&gt;

&lt;p&gt;If we make sure that all the dfs.socket.timeout and ipc client settings are the same in hbase-site.xml and hdfs-site.xml. Then, we can &quot;do a custom calculation of recover lease retry interval inside hbase&quot;. But basically hbase needs to know in some way how the timeouts are setup underneath.&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Varun&lt;/p&gt;</comment>
                            <comment id="13643255" author="yuzhihong@gmail.com" created="Fri, 26 Apr 2013 21:24:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we make sure that all the dfs.socket.timeout and ipc client settings are the same in hbase-site.xml and hdfs-site.xml.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Should we add a check for the above at cluster startup ? If discrepancy is found, we can log a warning message.&lt;/p&gt;</comment>
                            <comment id="13643287" author="stack" created="Fri, 26 Apr 2013 22:08:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=varun&quot; class=&quot;user-hover&quot; rel=&quot;varun&quot;&gt;varun&lt;/a&gt; Thanks.  I made &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; for trunk patch (and to fix what is applied here &amp;#8211; the 4s in particular).&lt;/p&gt;</comment>
                            <comment id="13647885" author="jxiang" created="Thu, 2 May 2013 20:44:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=varunsharma&quot; class=&quot;user-hover&quot; rel=&quot;varunsharma&quot;&gt;Varun Sharma&lt;/a&gt;, when you played with the patch for this jira, does your HBase have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8314&quot; title=&quot;HLogSplitter can retry to open a 0-length hlog file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8314&quot;&gt;&lt;del&gt;HBASE-8314&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8321&quot; title=&quot;Log split worker should heartbeat to avoid timeout when the hlog is under recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8321&quot;&gt;&lt;del&gt;HBASE-8321&lt;/del&gt;&lt;/a&gt;?  I think &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8314&quot; title=&quot;HLogSplitter can retry to open a 0-length hlog file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8314&quot;&gt;&lt;del&gt;HBASE-8314&lt;/del&gt;&lt;/a&gt; may give you extra time for the file to recover.&lt;/p&gt;</comment>
                            <comment id="13647915" author="varunsharma" created="Thu, 2 May 2013 21:28:38 +0000"  >&lt;p&gt;No, it does not. We primarily fix it by fixing it in HDFS since thats where the core of the problem is (slow recovery).&lt;/p&gt;</comment>
                            <comment id="13654558" author="yuzhihong@gmail.com" created="Fri, 10 May 2013 15:52:42 +0000"  >&lt;p&gt;Can this issue be resolved ?&lt;/p&gt;</comment>
                            <comment id="13658102" author="lhofhansl" created="Wed, 15 May 2013 06:26:18 +0000"  >&lt;p&gt;Closing. Please re-open if I&apos;m confused.&lt;/p&gt;</comment>
                            <comment id="13658897" author="hudson" created="Wed, 15 May 2013 22:31:03 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #141 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/141/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/141/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8389&quot; title=&quot;HBASE-8354 forces Namenode into loop with lease recovery requests&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8389&quot;&gt;&lt;del&gt;HBASE-8389&lt;/del&gt;&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8354&quot; title=&quot;Backport HBASE-7878 &amp;#39;recoverFileLease does not check return value of recoverLease&amp;#39; to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8354&quot;&gt;&lt;del&gt;HBASE-8354&lt;/del&gt;&lt;/a&gt; DDoSes Namenode with lease recovery requests (Varun and Ted) (Revision 1470800)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13666871" author="hudson" created="Sat, 25 May 2013 00:55:11 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #4142 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/4142/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/4142/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; Refactor recoverLease retries and pauses informed by findings over in hbase-8389 (Revision 1486121)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; Refactor recoverLease retries and pauses informed by findings over in hbase-8389 (Revision 1486108)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;/p&gt;</comment>
                            <comment id="13666906" author="hudson" created="Sat, 25 May 2013 01:30:46 +0000"  >&lt;p&gt;Integrated in hbase-0.95 #214 (See &lt;a href=&quot;https://builds.apache.org/job/hbase-0.95/214/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/hbase-0.95/214/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; Refactor recoverLease retries and pauses informed by findings over in hbase-8389 (Revision 1486107)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;/p&gt;</comment>
                            <comment id="13666988" author="hudson" created="Sat, 25 May 2013 06:30:12 +0000"  >&lt;p&gt;Integrated in hbase-0.95-on-hadoop2 #111 (See &lt;a href=&quot;https://builds.apache.org/job/hbase-0.95-on-hadoop2/111/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/hbase-0.95-on-hadoop2/111/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; Refactor recoverLease retries and pauses informed by findings over in hbase-8389 (Revision 1486107)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;/p&gt;</comment>
                            <comment id="13667001" author="hudson" created="Sat, 25 May 2013 07:07:56 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #542 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/542/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/542/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; Refactor recoverLease retries and pauses informed by findings over in hbase-8389 (Revision 1486121)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8449&quot; title=&quot;Refactor recoverLease retries and pauses informed by findings over in hbase-8389&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8449&quot;&gt;&lt;del&gt;HBASE-8449&lt;/del&gt;&lt;/a&gt; Refactor recoverLease retries and pauses informed by findings over in hbase-8389 (Revision 1486108)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;/p&gt;</comment>
                            <comment id="13667184" author="hudson" created="Sun, 26 May 2013 01:47:48 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #4144 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/4144/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/4144/&lt;/a&gt;)&lt;br/&gt;
    Add section on mttr; cite nicolas/devaraj blog and point at hbase-8389 varun comments (Revision 1486358)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;/p&gt;</comment>
                            <comment id="13667196" author="hudson" created="Sun, 26 May 2013 03:02:40 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #544 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/544/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/544/&lt;/a&gt;)&lt;br/&gt;
    Add section on mttr; cite nicolas/devaraj blog and point at hbase-8389 varun comments (Revision 1486358)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12644952">HBASE-8449</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12698006">HBASE-10642</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12644043">HBASE-8394</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12650404">HBASE-8670</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12643830">HDFS-4721</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12579833" name="8389-0.94-v2.txt" size="1212" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 13:50:39 +0000"/>
                            <attachment id="12579840" name="8389-0.94-v3.txt" size="1278" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 14:49:12 +0000"/>
                            <attachment id="12579843" name="8389-0.94-v4.txt" size="1346" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 15:00:18 +0000"/>
                            <attachment id="12579893" name="8389-0.94-v5.txt" size="1362" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 20:12:37 +0000"/>
                            <attachment id="12579896" name="8389-0.94-v6.txt" size="4112" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 20:44:13 +0000"/>
                            <attachment id="12579777" name="8389-0.94.txt" size="1168" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 04:28:35 +0000"/>
                            <attachment id="12579741" name="8389-trunk-v1.txt" size="1406" author="yuzhihong@gmail.com" created="Sun, 21 Apr 2013 18:26:11 +0000"/>
                            <attachment id="12579752" name="8389-trunk-v2.patch" size="2426" author="varunsharma" created="Sun, 21 Apr 2013 21:03:02 +0000"/>
                            <attachment id="12579863" name="8389-trunk-v2.txt" size="4215" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 17:57:47 +0000"/>
                            <attachment id="12579899" name="8389-trunk-v3.txt" size="5012" author="yuzhihong@gmail.com" created="Mon, 22 Apr 2013 20:49:26 +0000"/>
                            <attachment id="12579736" name="nn.log" size="124074" author="varunsharma" created="Sun, 21 Apr 2013 17:48:40 +0000"/>
                            <attachment id="12579737" name="nn1.log" size="86111" author="varunsharma" created="Sun, 21 Apr 2013 17:52:23 +0000"/>
                            <attachment id="12579738" name="sample.patch" size="1078" author="varunsharma" created="Sun, 21 Apr 2013 18:05:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>13.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sun, 21 Apr 2013 18:26:11 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>324198</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 29 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1jxmf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>324543</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>A new config param, hbase.lease.recovery.waiting.period, is introduced for FSHDFSUtils#recoverFileLease().&lt;br/&gt;
It represents the amount of time, in milliseconds, that we wait after issuing recoverLease() request. The default value is 4 seconds.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>