<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:01:39 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2413/HBASE-2413.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2413] Master does not respect generation stamps, may result in meta getting permanently offlined</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2413</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This happens if the RS is restarted before the zk node expires. The sequence is as follows:&lt;/p&gt;

&lt;p&gt;1. RS1 dies - lets say its server string was HOST1:PORT1:TS1&lt;br/&gt;
2. In a few seconds RS1 is restarted, it comes up as HOST1:PORT1:TS2 (TS2 is more recent than TS1)&lt;br/&gt;
3. Master gets a start up message from RS1 with the server name as HOST1:PORT1:TS2&lt;br/&gt;
4. Master adds this as a new RS, tries to red&lt;br/&gt;
---- The master does not use the generation stamps to detect that RS1 has already restarted.&lt;br/&gt;
---- Also, if RS1 contained meta, master would try to go to HOST1:PORT1:TS1. It would end up talking to HOST1:PORT1:TS2, which spews a bunch of not serving region exceptions.&lt;br/&gt;
5. zk node expires for HOST1:PORT1:TS1&lt;br/&gt;
6. Master tries to process shutdown for HOST1:PORT1:TS1 - this probably interferes with HOST1:PORT1:TS2  and ends up somehow removing the reassign meta in the master&apos;s queue.&lt;br/&gt;
---- Meta never comes online and master continues logging the following exception indefinitely:&lt;/p&gt;

&lt;p&gt;2010-04-06 11:02:23,988 DEBUG org.apache.hadoop.hbase.master.HMaster: Processing todo: ProcessRegionClose of test1,7094000000,1270220428234, false, reassign: true&lt;br/&gt;
2010-04-06 11:02:23,988 DEBUG org.apache.hadoop.hbase.master.ProcessRegionClose$1: Exception in RetryableMetaOperation:&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
        at org.apache.hadoop.hbase.master.RetryableMetaOperation.doWithRetries(RetryableMetaOperation.java:64)&lt;br/&gt;
        at org.apache.hadoop.hbase.master.ProcessRegionClose.process(ProcessRegionClose.java:63)&lt;br/&gt;
        at org.apache.hadoop.hbase.master.HMaster.processToDoQueue(HMaster.java:494)&lt;br/&gt;
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:429)&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12461310">HBASE-2413</key>
            <summary>Master does not respect generation stamps, may result in meta getting permanently offlined</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="karthik.ranga">Karthik Ranganathan</reporter>
                        <labels>
                    </labels>
                <created>Tue, 6 Apr 2010 22:20:58 +0000</created>
                <updated>Fri, 20 Nov 2015 12:41:42 +0000</updated>
                            <resolved>Sat, 8 May 2010 06:28:44 +0000</resolved>
                                    <version>0.20.3</version>
                                    <fixVersion>0.90.0</fixVersion>
                                    <component>master</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12854282" author="karthik.ranga" created="Wed, 7 Apr 2010 00:43:20 +0000"  >&lt;p&gt;Fix is to have master recognize the RS1 restart before the zk node expires and handle it right away. There should only be one entry for each HOST:PORT - the one with the latest TS.&lt;/p&gt;</comment>
                            <comment id="12854286" author="stack" created="Wed, 7 Apr 2010 00:55:47 +0000"  >&lt;p&gt;So, looking at log sent via back channel, a few things:&lt;/p&gt;

&lt;p&gt;1. When the restarted server shows up and we treat it as though it a new server, the load balancer rules the cluster unbalanced and starts to shed regions.  This is before the znode expires.  Looking in log, we&apos;ll have succesfully done a bit of distribution before the we start the processing of the failed server instance.  This is bad because this is churn.&lt;br/&gt;
2. Suspicious in the master is the serverAddressToServerInfo Map.  Its keyed by server+port.  When znode expires, server is removed from this list using server+port for key.  Will remove the server that had reported in as new ten seconds or so earlier.  Looking still for damage this causes.&lt;/p&gt;</comment>
                            <comment id="12854360" author="streamy" created="Wed, 7 Apr 2010 06:18:53 +0000"  >&lt;p&gt;Definitely need a good checkup/cleanup on where we use server+port vs server+port+startcode.  So I guess we already have the map to check if we exist when receiving a startup.  Good stuff guys.&lt;/p&gt;

&lt;p&gt;@stack, assign to u?&lt;/p&gt;</comment>
                            <comment id="12854850" author="stack" created="Thu, 8 Apr 2010 07:38:11 +0000"  >&lt;p&gt;Untested patch posted so we can discuss.  It does nothing but make code that was already in place actually work.&lt;/p&gt;

&lt;p&gt;Patch looks at new server and if it has same host+port as a currently registered server, we reject it.  On RS side, it pauses and then retries forever.&lt;/p&gt;

&lt;p&gt;A more involved soln. would have us run the code that is executed when znode expires but then when znode actually expires, the code will be run again and we&apos;d have to be careful we recognized the difference between the two runs and not knock out a server that was legit.&lt;/p&gt;</comment>
                            <comment id="12854865" author="stack" created="Thu, 8 Apr 2010 08:31:02 +0000"  >&lt;p&gt;As to why we failed allocate meta, looking in log I see two restarts of 019 ( 2010-03-31 17:37:39,882 and 2010-03-31 17:49:43,667).  After the first restart, all looks to be restored to normal only if you look at the emissions from load balancer it says:&lt;/p&gt;

&lt;p&gt; 1782 2010-03-31 17:48:46,521 INFO org.apache.hadoop.hbase.master.ServerManager: 2 region servers, 0 dead, average load 9.0&lt;/p&gt;

&lt;p&gt;So we are off kilter.   There should be 3 servers showing at this stage.  If you look at this message in src, you&apos;ll see that count is from a map keyed by host+port.  019 was removed from list as part of the processing of the crash.&lt;/p&gt;

&lt;p&gt;So, there will be more churn of regions than there should be.&lt;/p&gt;

&lt;p&gt;Next 020 expires at 2010-03-31 17:50:57,004.  It was carrying .META.  It expired &apos;naturally&apos; w/ znode expired.  Start message comes in at &apos;2010-03-31 17:51:15,385&apos;.  We remove meta from online regions list so we will immediately reassign it.  Meantime we are processing a message close from 018 because balancer is working to balance the churning cluster. &lt;/p&gt;

&lt;p&gt;The message close can&apos;t complete because it has built in the expectation that meta is available.&lt;/p&gt;

&lt;p&gt;Missing from this patch is review of all of these message processing items in the master package.  Need to make sure that close, open, etc., are requeued to try later if meta is null (as was the case here).&lt;/p&gt;

&lt;p&gt;Need to figure how to write a test for this stuff.&lt;/p&gt;






</comment>
                            <comment id="12855052" author="karthik.ranga" created="Thu, 8 Apr 2010 18:02:44 +0000"  >&lt;p&gt;&amp;lt;&amp;lt; A more involved soln. would have us run the code that is executed when znode expires but then when znode actually expires, the code will be run again and we&apos;d have to be careful we recognized the difference between the two runs and not knock out a server that was legit. &amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;I was thinking of something like this, not sure how easily this would translate to code:&lt;/p&gt;

&lt;p&gt;1. In the server restart and sending a new start code, we process the shutdown of the older instance:&lt;/p&gt;

&lt;p&gt;if (this.serverAddressToServerInfo.containsKey(hostAndPort)) {&lt;br/&gt;
     if(newStartCode &amp;gt; currentStartCode) &lt;/p&gt;
{
        // process shutdown of current incarnation of RS
        // register new incarnation of RS
     }
&lt;p&gt;}&lt;/p&gt;


&lt;p&gt;2. On the znode expire path:&lt;/p&gt;

&lt;p&gt;// znodeExpiredHostAndPort = ...&lt;br/&gt;
// znodeExpiredStartCode = ...&lt;br/&gt;
if(serverAddressToServerInfo.containsKey(znodeExpiredHostAndPort) &amp;amp;&amp;amp; znodeExpiredStartCode &amp;gt;= currentStartCode) &lt;/p&gt;
{
  // process shutdown
}
&lt;p&gt; else &lt;/p&gt;
{
  // no op - this should already have been handled
}</comment>
                            <comment id="12855066" author="stack" created="Thu, 8 Apr 2010 18:30:08 +0000"  >&lt;p&gt;Let me redo my patch to do as Karthik suggests.  Chatting on IRC, his suggestion would be effectivey immune from clock drift as long as the restart of the RS was done on the same host (the problem comes of same host+port combo so exception would be the unlikely new machine w/ same host+port coming up before old host&apos;s znode had expired).  Will make a separate issue to review processing open, close, etc. to ensure alls well for these processings when meta is offline (an offline meta while processing a close is what killed this cluster).&lt;/p&gt;

&lt;p&gt;Moving this issue to 0.20.5.&lt;/p&gt;</comment>
                            <comment id="12856172" author="stack" created="Mon, 12 Apr 2010 21:09:19 +0000"  >&lt;p&gt;Here is a cleaner patch.  Changed server expired to only expire servers it has registered.&lt;/p&gt;</comment>
                            <comment id="12856178" author="stack" created="Mon, 12 Apr 2010 21:14:59 +0000"  >&lt;p&gt;Here is more on the patch.&lt;/p&gt;

&lt;p&gt;+ If server we&apos;ve not seen before comes in and has a startcode &amp;gt; startcode for currently registered server of same host+port, then call serverExpire (synchronized).&lt;br/&gt;
+ When the zk watcher is triggered, we call serverExpire only now serverExpire first checks server is registered and not on deadServers list before it proceeds.&lt;/p&gt;

&lt;p&gt;Need to add tests.  Patch posted for review.&lt;/p&gt;</comment>
                            <comment id="12864190" author="stack" created="Wed, 5 May 2010 06:26:27 +0000"  >&lt;p&gt;This patch adds a unit test and updates v3 of the patch to match whats now in branch.  The test fails even with the &quot;fix&quot; in place; meta is never redeployed.  Fix must not be complete.  More work to do.&lt;/p&gt;</comment>
                            <comment id="12864590" author="stack" created="Wed, 5 May 2010 23:50:17 +0000"  >&lt;p&gt;Still not done.  Jiggering test so can have two filesystems in place so another can grab lease of former to recover files of &lt;b&gt;killed&lt;/b&gt; regionserver.&lt;/p&gt;</comment>
                            <comment id="12865046" author="stack" created="Fri, 7 May 2010 06:47:28 +0000"  >&lt;p&gt;Attached is my v7 patch.  I&apos;ve also posted it here, &lt;a href=&quot;http://review.hbase.org/r/36/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/36/&lt;/a&gt;, into Todds fancy-pants new code review tool (which is excellent).  Please review.&lt;/p&gt;</comment>
                            <comment id="12865051" author="stack" created="Fri, 7 May 2010 07:03:42 +0000"  >&lt;p&gt;All tests passed locally.&lt;/p&gt;</comment>
                            <comment id="12865052" author="tlipcon" created="Fri, 7 May 2010 07:13:53 +0000"  >&lt;p&gt;Posted a review here:&lt;br/&gt;
&lt;a href=&quot;http://review.hbase.org/r/36/#review1&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/36/#review1&lt;/a&gt;&lt;br/&gt;
(JIRA integration forthcoming in the next couple weeks, I suppose!)&lt;/p&gt;</comment>
                            <comment id="12865263" author="stack" created="Fri, 7 May 2010 18:35:47 +0000"  >&lt;p&gt;This patch includes fixes for all review comments made by Todd over in &lt;a href=&quot;http://review.hbase.org/r/36/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/36/&lt;/a&gt;.  I&apos;m not done yet.  I want to add more tests.  This is just interrim version.&lt;/p&gt;</comment>
                            <comment id="12865328" author="stack" created="Fri, 7 May 2010 21:55:40 +0000"  >&lt;p&gt;Main changes from previous patch are implementation of Todd suggestions. &lt;br/&gt;
Below we note changes other than those suggested by Todd.&lt;br/&gt;
Also uploaded patch to review.hbase.org.&lt;/p&gt;

&lt;p&gt;M  src/test/org/apache/hadoop/hbase/MiniHBaseCluster.java&lt;br/&gt;
  In here, if RS constructor, we conjure a new Configuration instance&lt;br/&gt;
  that has in it a differnent user so that later when we get a&lt;br/&gt;
  filesystem, then we get a filesystem instance per regionserver instance.&lt;br/&gt;
  (getLiveRegionServerThreads): Return live regionservers.&lt;br/&gt;
A  src/test/org/apache/hadoop/hbase/master/TestServerManager.java&lt;br/&gt;
  Add small junit test for isDead method.&lt;br/&gt;
A src/test/org/apache/hadoop/hbase/master/TestMasterTransitions.java&lt;br/&gt;
  Renamed from ...&lt;br/&gt;
D src/test/org/apache/hadoop/hbase/master/TestMasterTransistions.java&lt;br/&gt;
  This misspelled name.&lt;br/&gt;
M src/java/org/apache/hadoop/hbase/HServerInfo.java&lt;br/&gt;
  Redid javaodc.  Changed toString output.  Changed getName to getHostName&lt;br/&gt;
M src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java&lt;br/&gt;
  Added getLiverRegionServers&lt;br/&gt;
M src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java&lt;br/&gt;
  Formatting changes.&lt;br/&gt;
M src/java/org/apache/hadoop/hbase/master/HMaster.java&lt;br/&gt;
  getName changed to getHostName.&lt;br/&gt;
M  src/java/org/apache/hadoop/hbase/HServerAddress.java&lt;br/&gt;
  Javadoc cleanup.&lt;/p&gt;</comment>
                            <comment id="12865332" author="stack" created="Fri, 7 May 2010 22:02:34 +0000"  >&lt;p&gt;Pushed v9 to &lt;a href=&quot;http://review.hbase.org/r/36/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/36/&lt;/a&gt;.  Tests look like they are all passing (not done yet).&lt;/p&gt;</comment>
                            <comment id="12865403" author="stack" created="Sat, 8 May 2010 05:12:49 +0000"  >&lt;p&gt;This patch is the result of two rounds of tlipcon review over on &lt;a href=&quot;http://review.hbase.org/r/36/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/36/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12865405" author="tlipcon" created="Sat, 8 May 2010 05:30:02 +0000"  >&lt;p&gt;+1, assuming it passes tests&lt;/p&gt;</comment>
                            <comment id="12865417" author="stack" created="Sat, 8 May 2010 06:28:44 +0000"  >&lt;p&gt;Applied branch and trunk.  Thanks for the double-review over in &lt;a href=&quot;http://review.hbase.org/r/36/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://review.hbase.org/r/36/&lt;/a&gt; Todd.&lt;/p&gt;</comment>
                            <comment id="12866916" author="stack" created="Wed, 12 May 2010 23:54:08 +0000"  >&lt;p&gt;Marking these as fixed against 0.21.0 rather than against 0.20.5.&lt;/p&gt;</comment>
                            <comment id="15017049" author="lars_francke" created="Fri, 20 Nov 2015 12:41:42 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12444030" name="2413-v10.patch" size="73102" author="stack" created="Sat, 8 May 2010 05:12:49 +0000"/>
                            <attachment id="12443685" name="2413-v4.txt" size="18306" author="stack" created="Wed, 5 May 2010 06:26:27 +0000"/>
                            <attachment id="12443797" name="2413-v6.patch" size="26848" author="stack" created="Wed, 5 May 2010 23:50:17 +0000"/>
                            <attachment id="12443930" name="2413-v7.patch" size="48554" author="stack" created="Fri, 7 May 2010 06:47:27 +0000"/>
                            <attachment id="12443988" name="2413-v8.patch" size="61936" author="stack" created="Fri, 7 May 2010 18:35:47 +0000"/>
                            <attachment id="12444004" name="2413-v9.patch" size="92267" author="stack" created="Fri, 7 May 2010 21:55:40 +0000"/>
                            <attachment id="12441546" name="ASF.LICENSE.NOT.GRANTED--newserver-v3.txt" size="12530" author="stack" created="Mon, 12 Apr 2010 21:09:19 +0000"/>
                            <attachment id="12441123" name="newserver.txt" size="4856" author="stack" created="Thu, 8 Apr 2010 07:38:10 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 7 Apr 2010 00:55:47 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26295</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hhn3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100120</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>