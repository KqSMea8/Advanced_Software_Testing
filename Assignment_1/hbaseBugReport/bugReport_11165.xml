<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:20:46 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-11165/HBASE-11165.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-11165] Scaling so cluster can host 1M regions and beyond (50M regions?)</title>
                <link>https://issues.apache.org/jira/browse/HBASE-11165</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This discussion issue comes out of &quot;Co-locate Meta And Master &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10569&quot; title=&quot;Co-locate meta and master&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10569&quot;&gt;&lt;del&gt;HBASE-10569&lt;/del&gt;&lt;/a&gt;&quot; and comments on the doc posted there.&lt;/p&gt;

&lt;p&gt;A user &amp;#8211; our Francis Liu &amp;#8211; needs to be able to scale a cluster to do 1M regions maybe even 50M later.  This issue is about discussing how we will do that (or if not 50M on a cluster, how otherwise we can attain same end).&lt;/p&gt;

&lt;p&gt;More detail to follow.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12714102">HBASE-11165</key>
            <summary>Scaling so cluster can host 1M regions and beyond (50M regions?)</summary>
                <type id="13" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Brainstorming</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Wed, 14 May 2014 03:23:00 +0000</created>
                <updated>Thu, 21 Jan 2016 23:46:30 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>50</watches>
                                                                                                            <comments>
                            <comment id="13999047" author="ndimiduk" created="Thu, 15 May 2014 18:29:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; you&apos;re ears are burning &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13999573" author="vrodionov" created="Fri, 16 May 2014 03:45:36 +0000"  >&lt;p&gt;Single Master for 50M regions? Either it should be master-less or we need something similar to NN federation. Or FB Cell. Each cell is HBase cell-cluster and META cells create tree hierarchy. May be I am missing something?&lt;/p&gt;

</comment>
                            <comment id="13999605" author="mantonov" created="Fri, 16 May 2014 05:10:04 +0000"  >&lt;p&gt;Before we had META hosted on master, I&apos;d have said that this situation is different from NN - as for NN it&apos;s kind of known that 1Gb or RAM can hold ~1M inodes or so and whole FSImage must be in memory. With META tied to Master it&apos;s kind of same issue? Does someone have estimates - how many memory on avg. are needed for META table for each, say, 10k or 100k regions?&lt;/p&gt;

&lt;p&gt;Do we have some list of bottlenecks on this way? I remember from the hackathon @ Salesforce that one of important bottlenecks is number of splits/assignment ops / sec (do I recall correctly &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt;?)&lt;/p&gt;</comment>
                            <comment id="13999619" author="lhofhansl" created="Fri, 16 May 2014 05:49:18 +0000"  >&lt;p&gt;We&apos;ll run into other limitations before we hit META size issues I guess. Each column family and each region has a memstore. With a (say) 30gb heap and 128mb memstores, and 40% of heap used for the memstore you can only host 96 regions per region server. We&apos;d need 10k servers for 1m regions.&lt;br/&gt;
Even if we assume that on average the memstores are 50% filled we still need 5k servers for 1m regions.&lt;/p&gt;

&lt;p&gt;Now, maybe only a few regions are being written, in that case we need much less heap for the memstores.&lt;br/&gt;
And maybe we can make the memstores smaller (64 or 32mb); we&apos;d get lots flushes and great write amplification.&lt;/p&gt;

&lt;p&gt;We should also discuss why few, large regions are bad, and whether we can decouple the unit of distribution (a region) from whatever unit we&apos;re trying to operate on. Maybe a mapper per region is not good if regions can grows to 20gb (assuming we can ideally read around 100mb/s, we&apos;d need at least 3.5mins to scan through 20gb).&lt;/p&gt;</comment>
                            <comment id="13999672" author="mantonov" created="Fri, 16 May 2014 07:45:01 +0000"  >&lt;p&gt;I think the numbers mentioned for the particular usecase were somewhat like region size around 80gb being an issue.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We should also discuss why few, large regions are bad&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;d think that one of limitations of overly big regions (besides one-mapper-per-region) is that they&apos;re inconvenient to split/move around/compact?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Now, maybe only a few regions are being written, in that case we need much less heap for the memstores.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do you think that having an option to share memstore between group of regions (rather than having many small, like 64mb or 32mb, memstores) would help here? &lt;/p&gt;</comment>
                            <comment id="13999925" author="jxiang" created="Fri, 16 May 2014 16:07:37 +0000"  >&lt;p&gt;It may take a while for a single master to assign 50 million regions. It is also quite some load to the ZK if we use ZK for region assignment.&lt;/p&gt;</comment>
                            <comment id="14001097" author="apurtell" created="Sun, 18 May 2014 15:30:45 +0000"  >&lt;p&gt;I think we may want to be autotuning Memstores based on usage metrics. Lazy allocation of mslabs and such if recent history is read only or write rarely. Just pointing out that autotuning is a related independent interest that could benefit here. &lt;/p&gt;</comment>
                            <comment id="14001210" author="lhofhansl" created="Sun, 18 May 2014 21:19:09 +0000"  >&lt;p&gt;Yep. My point was that we need to tackle from a lot of different angles:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;autotuning memstore sizes, and lazy allocation (as Andy says), or sharing memstores&lt;/li&gt;
	&lt;li&gt;make large regions more workable, splits, compations, etc&lt;/li&gt;
	&lt;li&gt;allow more RAM to be used by region server (off heap memstores)&lt;/li&gt;
	&lt;li&gt;fast assignments (colocating HMaster and the RS hosting META, 50m should then not be a problem if everything is local to a single machine)&lt;/li&gt;
	&lt;li&gt;allow smaller units of computation in M/R&lt;/li&gt;
	&lt;li&gt;split META? And then colocate with multiple HMasters?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Right now it is hard to even utilize the disks on a reasonably sized commodity machine... Say you can host 100 regions on a server (30gb heap). With 20gb regions the max diskspace you can utilize per box is this 100*20gb*3 (HDFS replication factor) = 6tb... Of course the replicas are distributed across data nodes, but the averages still hold, you need 1 box for each 6tb of diskspace.&lt;br/&gt;
Larger regions and smaller memstores can fix that, but currently these lead to other issues.&lt;/p&gt;

&lt;p&gt;(See also &lt;a href=&quot;http://hadoop-hbase.blogspot.com/2013/01/hbase-region-server-memory-sizing.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hadoop-hbase.blogspot.com/2013/01/hbase-region-server-memory-sizing.html&lt;/a&gt;, where I blogged about this a while ago)&lt;/p&gt;</comment>
                            <comment id="14002365" author="mantonov" created="Mon, 19 May 2014 20:47:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; auto-tunable memstores sound interesting to me. As of the options to implement that, we probably can tune them so the frequency of flushes is kept the same (so say, when we see less flushes for a particular memstore last 3 minutes that the avg. last 24 hours, we make this memstore smaller, when we see more frequent flushes, we make it bigger? &lt;/p&gt;

&lt;p&gt;What do you think, how expensive is to resize memstore and how often we may need it? Another thing - how it might affect 99th latency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;&lt;br/&gt;
Fast assignment assuming that everything is local binds us to non-splittable META, right?&lt;/p&gt;
</comment>
                            <comment id="14002716" author="apurtell" created="Tue, 20 May 2014 02:26:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;What do you think, how expensive is to resize memstore and how often we may need it? Another thing - how it might affect 99th latency. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we are using a lower flush threshold for some regions, then we&apos;d flush more than otherwise if a burst of writes comes in, and increase compaction activity. Would be important to notice and adjust for workload changes. If we are lazily allocating mslabs also then we could trigger GC activity as those are large objects. It might be worth keeping a pool of mslabs around for use by memstores as needed, in aggregate still a lesser allocation than today, the size of the pool adjusted by heuristics. &lt;/p&gt;</comment>
                            <comment id="14002813" author="toffer" created="Tue, 20 May 2014 05:17:39 +0000"  >&lt;p&gt;Thanks for putting up the jira Stack. &lt;/p&gt;

&lt;p&gt;We are currently doing experiments as to what the potential bottlenecks would be when we scale regions to a few million. There&apos;ll be more information when it&apos;s done.&lt;/p&gt;</comment>
                            <comment id="14002864" author="toffer" created="Tue, 20 May 2014 06:32:14 +0000"  >&lt;blockquote&gt;
&lt;p&gt;autotuning memstore sizes, and lazy allocation (as Andy says), or sharing memstores&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If we increase the memstore multiplier to a high number won&apos;t that be a rough simulation of this? Also if the writes are uniformly distributed across regions then sharing is not needed. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;make large regions more workable, splits, compations, etc&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s seems to me it&apos;s in HBase&apos;s DNA to have small regions. My gut tells me that it would take less effort to support more regions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;allow more RAM to be used by region server (off heap memstores)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Or support larger heap &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;allow smaller units of computation in M/R&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We generally need a smarter way of calculating splits. ie Control number of tasks accessing an RS. If only there was some integration between an NM and RS.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;split META? And then colocate with multiple HMasters?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;IMHO HBase should be horizontally scalable with regards to # of regions. If I have too many regions I should be able to add more machines (ie master/regionserver). Currently at ~68k regions, it&apos;s consuming about ~200MB. Extrapolating at 6M it&apos;s 20GB and 60M it&apos;s 200GB. &lt;/p&gt;</comment>
                            <comment id="14003854" author="mantonov" created="Tue, 20 May 2014 19:20:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we are lazily allocating mslabs also then we could trigger GC activity as those are large objects.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Gc-ing mslabs should be fast, since they&apos;re few large (2mb) objects?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It might be worth keeping a pool of mslabs around for use by memstores as needed, in aggregate still a lesser allocation than today, the size of the pool adjusted by heuristics.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So have a global mslabs-pool in RS, versus mslabs per memstore?&lt;/p&gt;</comment>
                            <comment id="14003857" author="apurtell" created="Tue, 20 May 2014 19:24:00 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;If we are lazily allocating mslabs also then we could trigger GC activity as those are large objects.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Gc-ing mslabs should be fast, since they&apos;re few large (2mb) objects?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No, I meant that allocating a large object could trigger GC that wouldn&apos;t otherwise have happened at that time. &lt;/p&gt;</comment>
                            <comment id="14003869" author="mantonov" created="Tue, 20 May 2014 19:30:01 +0000"  >&lt;p&gt;Oh, I see.&lt;/p&gt;</comment>
                            <comment id="14004107" author="apurtell" created="Tue, 20 May 2014 23:13:32 +0000"  >&lt;p&gt;Looks like how the master uses ZooKeeper to manage assignments will need updating. I created a table with 1M regions using &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11211&quot; title=&quot;LoadTestTool option for specifying number of regions per server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11211&quot;&gt;&lt;del&gt;HBASE-11211&lt;/del&gt;&lt;/a&gt;. After creating all 1M regions, the master attempted IO of an enormous znode and aborted:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2014-05-20 16:07:40,402 WARN  [main-SendThread(localhost:2181)] zookeeper.ClientCnxn: Session 0x1461be2f6220000 for server localhost/127.0.0.1:2181, unexpected error, closing socket connection and attempting reconnect
java.io.IOException: Packet len6378500 is out of range!
	at org.apache.zookeeper.ClientCnxnSocket.readLength(ClientCnxnSocket.java:112)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:79)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:366)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2014-05-20 16:07:40,506 WARN  [master:localhost:8100] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/region-in-transition

...

2014-05-20 16:07:56,754 INFO  [master:localhost:8100] master.HMaster: Aborting
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14004211" author="mantonov" created="Wed, 21 May 2014 01:50:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;attempted a write of an enormous znode&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That seems exactly what&apos;s discussed here: &lt;a href=&quot;http://stackoverflow.com/questions/10249579/zookeeper-cli-failing-ioexception-packet-len12343123123-is-out-of-range&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/10249579/zookeeper-cli-failing-ioexception-packet-len12343123123-is-out-of-range&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Obviously, here we try to write 6M worth of data and it exceeds jute.maxBuffer.&lt;/p&gt;

&lt;p&gt;What if you run ZK with -Djute.maxbuffer=(something bigger), and see?&lt;/p&gt;</comment>
                            <comment id="14004231" author="mantonov" created="Wed, 21 May 2014 02:17:46 +0000"  >&lt;p&gt;That&apos;s the workaround option (but should work). ZK-less assignment proposed in other jira should help with that.&lt;/p&gt;</comment>
                            <comment id="14004254" author="apurtell" created="Wed, 21 May 2014 02:54:31 +0000"  >&lt;p&gt;Attached as &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11165&quot; title=&quot;Scaling so cluster can host 1M regions and beyond (50M regions?)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11165&quot;&gt;HBASE-11165&lt;/a&gt;.zip is a list of the counts and aggregate sizes of live objects in an otherwise idle regionserver after loading 10000 regions (e.g. 1M regions might be hosted on 100 servers), 241 MB of heap in total. I found some of it surprising.&lt;/p&gt;</comment>
                            <comment id="14004257" author="apurtell" created="Wed, 21 May 2014 02:55:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;That&apos;s the workaround option (but should work). ZK-less assignment proposed in other jira should help with that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Or assignment accounting stored into multiple znodes such that we don&apos;t hit a znode size limit if needing to assign a very large number of regions.&lt;/p&gt;</comment>
                            <comment id="14004261" author="mantonov" created="Wed, 21 May 2014 02:59:06 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; for sharing Interesting, will definitely look at it tonight. Yet 250M overhead (scales roughly to 1Gb of heap at RS per each 50K regions?) on RS memory is probably less concerning than ZK load?&lt;/p&gt;</comment>
                            <comment id="14004321" author="mantonov" created="Wed, 21 May 2014 04:28:59 +0000"  >&lt;p&gt;Sorry if that&apos;s trivial question, but there&apos;re no mslabs on the picture, right?..&lt;/p&gt;</comment>
                            <comment id="14004350" author="nkeywal" created="Wed, 21 May 2014 05:01:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;What if you run ZK with -Djute.maxbuffer=(something bigger), and see?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;fwiw, it&apos;s a client side setting, so it has to be set on the master process.&lt;/p&gt;</comment>
                            <comment id="14004361" author="stack" created="Wed, 21 May 2014 05:12:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4246&quot; title=&quot;Cluster with too many regions cannot withstand some master failover scenarios&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4246&quot;&gt;&lt;del&gt;HBASE-4246&lt;/del&gt;&lt;/a&gt; has related experience.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; and Virag were trying 3M regions on a cluster today.  Did not see the big packet issue but other interesting findings that will show in a JIRA issue near you soon.&lt;/p&gt;</comment>
                            <comment id="14004804" author="apurtell" created="Wed, 21 May 2014 15:51:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4246&quot; title=&quot;Cluster with too many regions cannot withstand some master failover scenarios&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4246&quot;&gt;&lt;del&gt;HBASE-4246&lt;/del&gt;&lt;/a&gt; has related experience.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for the pointer.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&quot;then, it called &quot;listChildren&quot; on the /hbase/unassigned znode, and crashed with &quot;Packet len6080218 is out of range!&quot; since the IPC response was larger than the default maximum.&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Oh I see that looks like a ZK API limitation then, we can&apos;t do partial enumeration. So this is an opportunity for an alternative &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14006485" author="mantonov" created="Thu, 22 May 2014 21:16:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;Oh I see that looks like a ZK API limitation then, we can&apos;t do partial enumeration. So this is an opportunity for an alternative Mikhail Antonov&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep (plus, in this particular case, it may also be a misconfiguration, or default configuration of zookeeper. Btw, were you able to overcome this error with the settings described at the link, if you had a chance to?)&lt;/p&gt;</comment>
                            <comment id="14006508" author="enis" created="Thu, 22 May 2014 21:49:35 +0000"  >&lt;p&gt;One other thing to consider is that we want to put even more data per region entry in meta. First, we would like to keep a history of RS assignments for the region in meta (this is for debugging + new assignment design). Second, ideally we would want to keep the region files in meta as well and get away without doing reference files, etc. It will ultimately depend on the new master design of course when these can go in. So I would say the current size calculations might not be relevant in the longer term.  &lt;/p&gt;</comment>
                            <comment id="14006625" author="mantonov" created="Thu, 22 May 2014 23:49:36 +0000"  >&lt;p&gt;I wonder if it makes sense to have google doc linked to this jira to save various proposals, findings and estimates? Like that summarizes current usage to be conservatively 3.5Gb in meta / 1M regions.&lt;/p&gt;</comment>
                            <comment id="14006672" author="apurtell" created="Fri, 23 May 2014 01:02:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;Btw, were you able to overcome this error with the settings described at the link, if you had a chance to?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sure, the jute packet limit is adjustable. &lt;/p&gt;

&lt;p&gt;If doing this again I would switch to a more plausible scenario that brings regions online in stages, up to 1M regions in total over 100 or 1000 tables brought online serially.&lt;/p&gt;</comment>
                            <comment id="14016181" author="toffer" created="Tue, 3 Jun 2014 03:33:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; and I have been doing experiments. Here is our findings so far. I&apos;ll be creating jira for the obvious ones. &lt;/p&gt;</comment>
                            <comment id="14016185" author="toffer" created="Tue, 3 Jun 2014 03:39:44 +0000"  >&lt;p&gt;I&apos;d like to create a jira to start discussion on distributed master but there&apos;s already something related  &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11241&quot; title=&quot;Support for multiple active HMaster replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11241&quot;&gt;HBASE-11241&lt;/a&gt;. Should we address it there?&lt;/p&gt;</comment>
                            <comment id="14016204" author="mantonov" created="Tue, 3 Jun 2014 04:33:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11241&quot; title=&quot;Support for multiple active HMaster replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11241&quot;&gt;HBASE-11241&lt;/a&gt; was opened to support multiple active masters with replicated state (vs. current active-backups) for better HA, and you are referring to partitioned master, right?&lt;/p&gt;

&lt;p&gt;Actually yes, I think we should discuss it there, as it&apos;s correlated. If we discuss distributed masters in different jiras, I&apos;d think it&apos;ll be much harder to track.&lt;/p&gt;</comment>
                            <comment id="14016829" author="jxiang" created="Tue, 3 Jun 2014 16:09:43 +0000"  >&lt;p&gt;With ZK-less region assignment, I think we can avoid the problems with ZK. We can also remove lots of complex code that handles all kinds of race conditions. For the first step (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11059&quot; title=&quot;ZK-less region assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11059&quot;&gt;&lt;del&gt;HBASE-11059&lt;/del&gt;&lt;/a&gt;), probably we won&apos;t see this benefit due to compatibility. In the next step (major release after 11059), we should simplify the state machine, remove ZK assignment event handling code and thread, test cases that test handling simulated ZK states, etc..&lt;/p&gt;</comment>
                            <comment id="14019184" author="stack" created="Thu, 5 Jun 2014 20:17:39 +0000"  >&lt;p&gt;Keep it coming.  Lets have this issue as location for high-level scaling discussion.  Lets hang actual scaling issues (and their fixes) off this umbrella issue too as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt;  and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; have been doing already.&lt;/p&gt;

&lt;p&gt;A few of us had a chat this morning on this topic (Francis, Andrew, LarsH).  Here are some very rough notes:&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;Howdoto1Mregionsoncluster%3F&quot;&gt;&lt;/a&gt;How do to 1M+ regions on cluster?&lt;/h4&gt;

&lt;ul&gt;
	&lt;li&gt;Distributed Master?&lt;/li&gt;
	&lt;li&gt;Split Meta? We&apos;ll want to store more in meta?&lt;/li&gt;
	&lt;li&gt;Fix assignment&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;3 high-level issues to address:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Scanning is too slow&lt;/li&gt;
	&lt;li&gt;Write amplification&lt;/li&gt;
	&lt;li&gt;Moving regions is hard because big (lots of data offline at a time)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;On amplification, on stripe compaction, need more data.&lt;/p&gt;

&lt;p&gt;Do we add complexity to region itself or to the container.&lt;/p&gt;

&lt;p&gt;Region is unit of distribution. Don&apos;t depend on changing say its size to address issues listed above if possible.  Disentangle dependencies.&lt;/p&gt;

&lt;p&gt;Support a lot of regions.  Can we disentangle need to have lots of regions so can do more parallel scans: e.g. keep region midkeys or one-hundredth-keys somewhere so mapreduce can do part-region-scan rather than all-region-scan?  Same for compactions.&lt;/p&gt;

&lt;p&gt;Distributed master will be hard.&lt;/p&gt;

&lt;p&gt;Multimaster will be hard.&lt;/p&gt;

&lt;p&gt;Where to put the complexity?  Make meta ops/master more complex?  Currently single master is &apos;simple&apos; (if slow, etc.).&lt;/p&gt;

&lt;p&gt;MultiMaster could partition the work but also do HA... Needs to be a plan for multimaster.&lt;/p&gt;

&lt;p&gt;Should we split meta.  Can distribute load for assignment when other metas (startup is a fat read then many .&lt;/p&gt;

&lt;p&gt;Issues with HDFS when millions of regions.  Need 4Gs of heap to  list a directory of millions of regions.  Six hours to create a million regions in HDFS.&lt;/p&gt;

&lt;p&gt;Treeify the regions or just take regions out of HDFS (Matteo&apos;s region facade, how BT does regions with files up in meta table).  Just so can scale.&lt;/p&gt;

&lt;p&gt;Action item.  Can we try zkless assigmment on the big cluster?  Can zkless assignment apply to 0.98?&lt;/p&gt;

&lt;p&gt;Going forward, lets keep high-level discussion up in this issue and break out sub-issues for sub-discussion and work.&lt;/p&gt;</comment>
                            <comment id="14019192" author="stack" created="Thu, 5 Jun 2014 20:21:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;Francis Liu &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11241&quot; title=&quot;Support for multiple active HMaster replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11241&quot;&gt;HBASE-11241&lt;/a&gt; was opened to support multiple active masters with replicated state (vs. current active-backups) for better HA, and you are referring to partitioned master, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Scaling, the thought is that we need more masters to spread the work (partitioning).  We also need the HA if we don&apos;t want to have state straddle processes (master and zk (and hdfs)).&lt;/p&gt;</comment>
                            <comment id="14019256" author="mantonov" created="Thu, 5 Jun 2014 20:48:31 +0000"  >&lt;p&gt;I would think the partitioned master and multi-active master (HA) is better to be discussed/designed together. Both of them are complex, but if one is done without accounting for other, adding the second one later may be harder.&lt;/p&gt;</comment>
                            <comment id="14019260" author="mantonov" created="Thu, 5 Jun 2014 20:50:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;Six hours to create a million regions in HDFS.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Is HDFS (NN?) deemed a bottleneck here?&lt;/p&gt;</comment>
                            <comment id="14019262" author="vrodionov" created="Thu, 5 Jun 2014 20:53:25 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Issues with HDFS when millions of regions. Need 4Gs of heap to list a directory of millions of regions. Six hours to create a million regions in HDFS.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We want to build a race car out of a shit rock, sorry - sheet rock (HDFS). Time has come to attack the major limitation of HDFS: single-NN design and lack of scalability.  Having ability to keep billions files in a file system w/o any performance penalty will allow HBase to consider compact-less design, for example. There is no need for compaction if random IO is cheap (SSD). You won&apos;t need large regions as well because you can afford millions of them and small regions will improve MTTR significantly.&lt;/p&gt;

&lt;p&gt;Region assignment/re-assignment  must be done in a completely decentralized manner with conflict resolution protocol in place to resolve possible rare conflicts. &lt;/p&gt;</comment>
                            <comment id="14098069" author="virag" created="Fri, 15 Aug 2014 02:13:50 +0000"  >&lt;p&gt;Attached is a comparison of Zk-less assignment (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11059&quot; title=&quot;ZK-less region assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11059&quot;&gt;&lt;del&gt;HBASE-11059&lt;/del&gt;&lt;/a&gt;) vs zk assignment on 0.98&lt;/p&gt;</comment>
                            <comment id="14098090" author="mantonov" created="Fri, 15 Aug 2014 03:08:02 +0000"  >&lt;p&gt;Interesting observation! I believe it&apos;s radically different from previous benchmark.&lt;/p&gt;

&lt;p&gt;Wondering why on new setup ZK is ~10 faster than it was before with same options for forceSync (off both times). Could you share any specs for ZK machines (disks, NICs, CPU)? &lt;/p&gt;</comment>
                            <comment id="14098232" author="virag" created="Fri, 15 Aug 2014 06:14:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;Interesting observation! I believe it&apos;s radically different from previous benchmark&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They are not comparable.&lt;br/&gt;
In previous setup, we never ran with forcesync=no on a bigger cluster (300 node). We only ran using that config on a smaller cluster with 23 RS and with 1M regions, each RS had more regions to open which itself would take more time causing the overall startup time to increase. Also,  the master was not lagging behind by more than few mins after the region servers had opened all region when using forcesync off. We had observed that opening 3.3M regions on 300 node takes only ~35 minutes on RS, but it seems that forceSync=yes was causing the master to receive late notifications from zk. So I think that if we had set forceSync=no during the previous benchmark, it would have taken a bit more than 35 minutes for 3.3M which is consistent with the current result of 10 mins for 1M.&lt;br/&gt;
Also, during that time we were measuring the entire clean startup time. In the current benchmark, we only measure the bulk assignment time. The entire startup time also included the cost of scanning META (3 times)&lt;br/&gt;
I will check if there were any significant differences between the hardware config of zk machines for both the setups.&lt;/p&gt;</comment>
                            <comment id="14098242" author="mantonov" created="Fri, 15 Aug 2014 06:28:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; I see, thanks for explanation. For whatever reason I thought that the previous benchmark on 300 nodes cluster was done with forcesync=off too. Curious to see possible difference in hardware then.&lt;/p&gt;

&lt;p&gt;Also you were saying &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;From the profiler, cpu spends around 50% of time in locking region states&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;are there maybe some profiling reports to look at?&lt;/p&gt;</comment>
                            <comment id="14098676" author="jxiang" created="Fri, 15 Aug 2014 15:37:57 +0000"  >&lt;p&gt;My test shows it took about 13 minutes to assign 1M regions in a 39 nodes cluster. I haven&apos;t tried with forceSync=no.&lt;/p&gt;</comment>
                            <comment id="14098770" author="virag" created="Fri, 15 Aug 2014 17:14:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;are there maybe some profiling reports to look at?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t saved any but it showed high thread contention (bulk assigner and rpc reader threads) on synchronized methods like RegionStates.updateRegionState() and RegionStates.getRegionState().&lt;/p&gt;</comment>
                            <comment id="14098801" author="mantonov" created="Fri, 15 Aug 2014 17:40:05 +0000"  >&lt;p&gt;(not sure it&apos;s right to discuss it here, may be better elsewhere?) but that urged me to look at those methods, and I&apos;m wondering, if the invariants there allow more fine-grained locking (sorry, may be it was already asked elsewhere and I missed conversation). Like:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;RegionStates#getRegionState(String) is synchronized, and does lookup in HashMap. Should it be ConcurrentHashMap instead?&lt;/li&gt;
	&lt;li&gt;in RegionStates#updateRegionState, regionsInTransition and regionsStates do form invariant, what about lastAssignments? Can updating lastAssignments happen under separate subsequent lock?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; what do you think?&lt;/p&gt;</comment>
                            <comment id="14098820" author="jxiang" created="Fri, 15 Aug 2014 17:50:38 +0000"  >&lt;p&gt;We can discuss it in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11290&quot; title=&quot;Unlock RegionStates&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11290&quot;&gt;HBASE-11290&lt;/a&gt;. Basically, we need to use a different data structure. It&apos;s better to make sure all of those maps in RegionStates to be consistent.&lt;/p&gt;</comment>
                            <comment id="14098833" author="mantonov" created="Fri, 15 Aug 2014 17:54:49 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; will take a look at this one.&lt;/p&gt;</comment>
                            <comment id="14098894" author="toffer" created="Fri, 15 Aug 2014 18:32:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; We&apos;ve observed just changing the data structure to something concurrent so we have unsynchronized getters significantly lowers cpu utilization to low levels. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; is looking into this more. we should discuss ideas in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11290&quot; title=&quot;Unlock RegionStates&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11290&quot;&gt;HBASE-11290&lt;/a&gt;.&lt;/p&gt;
</comment>
                            <comment id="14098900" author="toffer" created="Fri, 15 Aug 2014 18:35:48 +0000"  >&lt;p&gt;Guys, I&apos;d like to start the discussion on splitting meta (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11288&quot; title=&quot;Splittable Meta&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11288&quot;&gt;HBASE-11288&lt;/a&gt;). As our experiments shows splitting is a must for scaling. Thoughts? Anyone against?&lt;/p&gt;</comment>
                            <comment id="14098902" author="virag" created="Fri, 15 Aug 2014 18:37:11 +0000"  >&lt;p&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;, Based on your feedback, I have update the doc to make it more clear not to compare this with the previous benchmark. Thanks!&lt;/p&gt;</comment>
                            <comment id="14098910" author="apurtell" created="Fri, 15 Aug 2014 18:42:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;Guys, I&apos;d like to start the discussion on splitting meta (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11288&quot; title=&quot;Splittable Meta&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11288&quot;&gt;HBASE-11288&lt;/a&gt;). As our experiments shows splitting is a must for scaling. Thoughts?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As long as lower scale deploys don&apos;t run into potential new issues with a split meta in 0.98, we could be good, i.e. disabled by default in 0.98, enabled by default in later versions.&lt;/p&gt;</comment>
                            <comment id="14098923" author="mantonov" created="Fri, 15 Aug 2014 18:49:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;Guys, I&apos;d like to start the discussion on splitting meta (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11288&quot; title=&quot;Splittable Meta&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11288&quot;&gt;HBASE-11288&lt;/a&gt;). As our experiments shows splitting is a must for scaling. Thoughts? Anyone against?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; do you think split meta implies partitioned masters, each responsible for its piece? If so, I&apos;d think we could discuss partitioned master along with replicated master.&lt;/p&gt;</comment>
                            <comment id="14098949" author="apurtell" created="Fri, 15 Aug 2014 19:06:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;do you think split meta implies partitioned masters, each responsible for its piece&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That kind of change could not be backported&lt;/p&gt;</comment>
                            <comment id="14099005" author="mantonov" created="Fri, 15 Aug 2014 19:45:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;That kind of change could not be backported&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Right - that&apos;s why I brought this question up &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14099096" author="toffer" created="Fri, 15 Aug 2014 20:38:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; I think we can split meta and later on build/extend it to support partitioned master if need be, though as Andy mentioned the master part prolly not in 0.98. We&apos;ll be doing more investigation to motivate multi-master.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrew.purtell%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andrew.purtell@gmail.com&quot;&gt;Andrew Purtell&lt;/a&gt; That&apos;s great, as a first step will bring back root. And next patch would be do split meta. Does it sound reasonable?&lt;/p&gt;</comment>
                            <comment id="14099097" author="toffer" created="Fri, 15 Aug 2014 20:40:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; Have yet to read replicated master jira as well. &lt;/p&gt;</comment>
                            <comment id="14099107" author="virag" created="Fri, 15 Aug 2014 20:45:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; I had hackily fixed &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11290&quot; title=&quot;Unlock RegionStates&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11290&quot;&gt;HBASE-11290&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11758&quot; title=&quot;Meta region location should be cached&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11758&quot;&gt;&lt;del&gt;HBASE-11758&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11759&quot; title=&quot;TableZnode should be cached&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11759&quot;&gt;&lt;del&gt;HBASE-11759&lt;/del&gt;&lt;/a&gt; and saw that bulk assignment time reduced to ~5 mins from 12 mins. Will be working on proper fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11290&quot; title=&quot;Unlock RegionStates&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11290&quot;&gt;HBASE-11290&lt;/a&gt; soon..your thoughts will be helpful. &lt;/p&gt;</comment>
                            <comment id="14099118" author="mbertozzi" created="Fri, 15 Aug 2014 20:50:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;That&apos;s great, as a first step will bring back root. And next patch would be do split meta. Does it sound reasonable?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I haven&apos;t read the full discussion, so sorry if I missed this piece.&lt;br/&gt;
does splitting meta means having multiple master each one handing its own meta and its own set of RS?&lt;br/&gt;
otherwise we go back as before, the idea of having meta colocated with the master was have operation like assignment, or disable/enable, create/delete interact with &quot;local data&quot; and avoid complexity in handling failure when interacting with other machines.&lt;/p&gt;
</comment>
                            <comment id="14099168" author="apurtell" created="Fri, 15 Aug 2014 21:29:00 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;As long as lower scale deploys don&apos;t run into potential new issues with a split meta in 0.98, we could be good, i.e. disabled by default in 0.98, enabled by default in later versions.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That&apos;s great, as a first step will bring back root. And next patch would be do split meta. Does it sound reasonable?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, but older clients must be able to run against any given latest 0.98.x server side version, so these changes should hide behind a default-disabled configuration toggle.&lt;/p&gt;</comment>
                            <comment id="14099283" author="toffer" created="Fri, 15 Aug 2014 22:30:22 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I haven&apos;t read the full discussion, so sorry if I missed this piece.&lt;br/&gt;
does splitting meta means having multiple master each one handing its own meta and its own set of RS?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It does not have to be. So far we can see there are gains to be made in scalability by spliting meta so that it is served by multiple RS. We will do the work to motivate multi-master and justify the complexity if need be.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;otherwise we go back as before, the idea of having meta colocated with the master was have operation like assignment, or disable/enable, create/delete interact with &quot;local data&quot; and avoid complexity in handling failure when interacting with other machines.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t see why collocation is required to have the features you mentioned. IMHO all you need is the mutations/locking/etc to be centrally managed (ie all updates to meta is sent to the master), the meta itself can be anywhere. So I&apos;d argue that central management is needed, collocation is not required. Please correct me if I&apos;m missing something here.&lt;/p&gt;</comment>
                            <comment id="14099285" author="toffer" created="Fri, 15 Aug 2014 22:31:29 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Yes, but older clients must be able to run against any given latest 0.98.x server side version, so these changes should hide behind a default-disabled configuration toggle.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep will do this for 0.98. For trunk it should be fine to have it enabled by default or not even have the switch?&lt;/p&gt;</comment>
                            <comment id="14099287" author="apurtell" created="Fri, 15 Aug 2014 22:35:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;For trunk it should be fine to have it enabled by default or not even have the switch&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In my opinion for trunk it&apos;s not necessary to have a switch. &lt;/p&gt;</comment>
                            <comment id="14099401" author="enis" created="Sat, 16 Aug 2014 00:37:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;That&apos;s great, as a first step will bring back root.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do we need to bring root back? Can&apos;t we simply host all of root in zookeeper? We expect the number of meta regions in the tens / hundreds case right? &lt;/p&gt;</comment>
                            <comment id="14099505" author="lhofhansl" created="Sat, 16 Aug 2014 05:04:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do we need to bring root back? Can&apos;t we simply host all of root in zookeeper?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Came here to say exactly that. Enis beat me to it. Can we please not bring back root? We need a bootstrapping mechanism to find root anyway, and if we can do that we should be able to bootstrap a few dozen meta regions from there. Currently we&apos;d use ZK for that, but we don&apos;t have to.&lt;/p&gt;</comment>
                            <comment id="14099529" author="mbertozzi" created="Sat, 16 Aug 2014 06:49:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t see why collocation is required to have the features you mentioned. IMHO all you need is the mutations/locking/etc to be centrally managed (ie all updates to meta is sent to the master), the meta itself can be anywhere. So I&apos;d argue that central management is needed, collocation is not required. Please correct me if I&apos;m missing something here.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;meta is only modified by the master, and the master is mainly doing operations on meta. so if meta is down you have a master up which is not useful at much. Also since all writes to meta are controlled by the master having meta anywhere else will not improve things since the master is the one processing requests and you also have to go to another server to perform the operation.&lt;br/&gt;
if you are talking about the read side, I can understand why you want to split it, but isn&apos;t having multiple copies even as cache simpler and you&apos;ll get the same results?&lt;/p&gt;</comment>
                            <comment id="14100778" author="jxiang" created="Mon, 18 Aug 2014 16:13:50 +0000"  >&lt;p&gt;I agree with Matteo on this. One more benefit to have meta and master together is the meta/master recovery will be much simpler (I mean there won&apos;t be scenario like master is recovering,  meta regionserver may be down).&lt;/p&gt;</comment>
                            <comment id="14100953" author="apurtell" created="Mon, 18 Aug 2014 18:00:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;I agree with Matteo on this. One more benefit to have meta and master together is the meta/master recovery will be much simpler&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we need to split this conversation into what to do on master and what to do with 0.98? We could for example file two separate subtasks that approach the meta scaling problem in different ways for the respective branches. They are divergent enough so that would be a good idea IMHO&amp;gt;&lt;/p&gt;</comment>
                            <comment id="14101556" author="toffer" created="Mon, 18 Aug 2014 23:59:57 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Came here to say exactly that. Enis beat me to it. Can we please not bring back root? We need a bootstrapping mechanism to find root anyway, and if we can do that we should be able to bootstrap a few dozen meta regions from there. Currently we&apos;d use ZK for that, but we don&apos;t have to.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; My main intent is to be able to split meta to avoid having such a large region. The root approach is well understood and AFAIK has no real downsides? Having it ZK is one way as long as we can have around a thousand entries in there that should be fine for our use case. I thought the trend was to move a lot of heavy lifting out of ZK which sounds reasonable to me. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; where do you propose we put it? &lt;/p&gt;</comment>
                            <comment id="14101707" author="toffer" created="Tue, 19 Aug 2014 01:56:17 +0000"  >&lt;blockquote&gt;
&lt;p&gt;meta is only modified by the master, and the master is mainly doing operations on meta. so if meta is down you have a master up which is not useful at much. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hmm so in this context things are neither really better or worse with either option?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also since all writes to meta are controlled by the master having meta anywhere else will not improve things since the master is the one processing requests and you also have to go to another server to perform the operation. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For writes you already have to go to a bunch of other servers (datanodes) to perform the write operation and for reads worst case remote dfs read. Also as we&apos;ve pointed out in our experiments that overhead is not much (if any at all) and overshadowed by the gains you get from horizontal scalability through SoC. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;if you are talking about the read side, I can understand why you want to split it, but isn&apos;t having multiple copies even as cache simpler and you&apos;ll get the same results?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m talking about both read and writes. Being able to split it means being able to have read/write throughput equivalent to the number of machines hosting the table. Have less write amplification (which is already an issue) as well as horizontally scale to have enough memory to have meta in block cache. &lt;/p&gt;


</comment>
                            <comment id="14101710" author="toffer" created="Tue, 19 Aug 2014 01:59:38 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I agree with Matteo on this. One more benefit to have meta and master together is the meta/master recovery will be much simpler (I mean there won&apos;t be scenario like master is recovering, meta regionserver may be down).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Seemed to miss this. What is the other benefit?&lt;/p&gt;</comment>
                            <comment id="14101717" author="toffer" created="Tue, 19 Aug 2014 02:05:03 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Do we need to split this conversation into what to do on master and what to do with 0.98? We could for example file two separate subtasks that approach the meta scaling problem in different ways for the respective branches. They are divergent enough so that would be a good idea IMHO&amp;gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It sounds to me like we&apos;ve at least come to an agreement that splitting META is reasonable. Off-hand any implementation in trunk should be backportable 0.98. The parallel discussion was about multi-master and the merits of collocation which can be a separate issue. Correct me if I&apos;m wrong &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt;.  &lt;/p&gt;</comment>
                            <comment id="14101827" author="stack" created="Tue, 19 Aug 2014 04:26:17 +0000"  >&lt;p&gt;On the nice attached doc:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Thoughts:
Advantage	of	zk	less	assignment
1)	Better	API&apos;s	(ls	on	znode	vs	scan	table)
2)	Scalability	(If	META	is	split)
			a)	Read/Write	throughput
			b)	Storage	(1M	child	znodes	vs	1M	rows	in	META)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can I have some pointers on how to read the above.  Zk-less AM is better because you scan a table &amp;#8211; you don&apos;t have to ls znodes?  What is the 1M znodes vs 1M rows about in above?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; Is the above the basis for your &quot;...As our experiments shows splitting is a must for scaling.&quot;?  If split meta, then more read/write throughput?  Because the meta table could be served by many machines so field more reads/writes?   The reads/writes are needed at starttime or during cluster lifetime in your judgement?  Thanks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; Is the shrink in time from 12 mins to 5mins on zk-less assign or forceSync=no? (or for both)?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Do we need to bring root back? Can&apos;t we simply host all of root in zookeeper? We expect the number of meta regions in the tens / hundreds case right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Root access would be different to the access of any other table if we went this route. Everyone &amp;#8211; all clients, etc. &amp;#8211; would need to know how to do this new access type.  How would you do it in zk anyways?  Would be a single znode with all of root in it? Root would have zk enforced limits. Might be ok for a small table that changes infrequently. Not sure about znode with 1k rows in it (history, replicas? etc.). We&apos;d have to test.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We need a bootstrapping mechanism to find root anyway...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah. There is zk now. Elsewhere, a quorum of masters has been proposed; you&apos;d go to the master to figure where everything is.  That&apos;d be a big change. 2.0.x.&lt;/p&gt;</comment>
                            <comment id="14102555" author="toffer" created="Tue, 19 Aug 2014 17:59:56 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Can I have some pointers on how to read the above. Zk-less AM is better because you scan a table &#8211; you don&apos;t have to ls znodes? What is the 1M znodes vs 1M rows about in above?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Essentially the apis are better. ie 1M rows we can iterate over the rows instead of ls and get back a huge chunk of data. ie deleting 1M znodes takes too long, this could be parallelizable against an hbase table.&lt;/p&gt;

&lt;p&gt;For 2.a, response is below. For 2.b, it&apos;s mainly a concern wether we&apos;ll hit other ZK issues when having that many child znodes (1M and beyond). HDFS guys are already looking into scaling number of child directories for NN.&lt;/p&gt;

&lt;p&gt;Will update doc.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Francis Liu Is the above the basis for your &quot;...As our experiments shows splitting is a must for scaling.&quot;? If split meta, then more read/write throughput? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If split meta, then:  1) Less write amplification (ie no large compactions), Better W throughput. 2) More disks, more R/W throughput. 3. More heap to fit meta, better R throughput.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Because the meta table could be served by many machines so field more reads/writes? The reads/writes are needed at starttime or during cluster lifetime in your judgement? Thanks.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep needed for startup. We need to do experiments for 1 rack and 2 rack failure for cluster lifetime case. Though large compactions would creep up on you. So splitting would still be motivating for cluster lifetime IMHO.  &lt;/p&gt;</comment>
                            <comment id="14102764" author="stack" created="Tue, 19 Aug 2014 20:17:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;If split meta, then 1) Less write amplification (ie no large compactions) ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point. i.e. if we want to move to lots of small regions, it would be odd if there was an &quot;except for meta&quot; clause.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Better W throughput.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If Master is only writer, we&apos;d need to ensure we are writing in // (i.e. Virag&apos;s recent patches).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;2) More disks, more R/W throughput.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;More heap to fit meta...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;More heap to cache meta, yes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...We need to do experiments for 1 rack and 2 rack failure...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed that in time of catastrophic part-failure, we&apos;d need the better R/W throughput a split meta can give you.&lt;/p&gt;

&lt;p&gt;Other pluses are we would treat meta like any other table. Negatives are we need our root back and startup is more complicated (but at least all inside single master in this case).&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://docs.google.com/document/d/1xC-bCzAAKO59Xo3XN-Cl6p-5CM_4DMoR-WpnkmYZgpw/edit#&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1xC-bCzAAKO59Xo3XN-Cl6p-5CM_4DMoR-WpnkmYZgpw/edit#&lt;/a&gt; I (and others) argue for colocated meta and master going forward looking at options. Let me freshen it with arguments made here.&lt;/p&gt;

&lt;p&gt;Colocating meta and master has nice properties. The in-memory image of the cluster layout &amp;#8211; probably a severe sub-set of what is actually in meta &amp;#8211; would need to fit a single-server&apos;s RAM in either model.  When colocated, operations are faster, less prone-to-error when less RPC involved (We&apos;d still be subject to &lt;a href=&quot;http://writings.quilt.org/2014/05/12/distributed-systems-and-the-end-of-the-api/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://writings.quilt.org/2014/05/12/distributed-systems-and-the-end-of-the-api/&lt;/a&gt; if persisting meta in hdfs as francis notes above).  A single machine hosting single meta would not be able to service a 50M region startup with hundreds or regionservers as well as a deploy with split meta.  It could. It&apos;d just be slower. Colocated meta and master implies single meta forever and that single meta is served by one server only &amp;#8211; a 50M meta region would be an anomaly in the cluster being bigger than all the rest &amp;#8211; and until we have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10295&quot; title=&quot;Refactor the replication  implementation to eliminate permanent zk node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10295&quot;&gt;HBASE-10295&lt;/a&gt; &quot;Refactor the replication implementation to eliminate permanent zk node&quot; and/or &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11467&quot; title=&quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11467&quot;&gt;HBASE-11467&lt;/a&gt; &quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; (Maybe a later phase of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10070&quot; title=&quot;HBase read high-availability using timeline-consistent region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10070&quot;&gt;&lt;del&gt;HBASE-10070&lt;/del&gt;&lt;/a&gt; when followers can run closer in to the leader state would work here) or a new master layout where we partition meta across multiple master server.&lt;/p&gt;

&lt;p&gt;A plus split meta has over colocated master and meta is that master currently can be down for some period of time and the cluster keeps working; no splits and no merges and if a machine crashes while master is down, data is offline till master comes back (needs more exercise).  This is less the case when colocated master and meta.&lt;/p&gt;

&lt;p&gt;Please pile on all with thoughts. We need to put stake in grounds soon for hbase 2.0 cluster topology.  Francis needs something in 0.98 timeframe.  If the 0.98 is different to what folks want for 2.0, as per Andy lets split this issue.&lt;/p&gt;

&lt;p&gt;Thoughts-for-the-day:&lt;/p&gt;

&lt;p&gt;+ HBase is supposed to be able to scale&lt;br/&gt;
+ Single meta came about because way back, we were too lazy to fix issues that arose when meta was split (at the time, we didn&apos;t need to scale as much).&lt;/p&gt;</comment>
                            <comment id="14102942" author="virag" created="Tue, 19 Aug 2014 22:18:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is the shrink in time from 12 mins to 5mins on zk-less assign or forceSync=no? (or for both)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Zk-less. Not yet checked on forceSync=no. Will do the comparison once we have proper fixes for those issues.&lt;/p&gt;</comment>
                            <comment id="14104809" author="toffer" created="Wed, 20 Aug 2014 23:46:37 +0000"  >&lt;blockquote&gt;
&lt;p&gt;If the 0.98 is different to what folks want for 2.0, as per Andy lets split this issue.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We plan to start working on splitting meta the week after next or maybe even next. If there&apos;s no clear conclusion to the approach we will likely bring back root for 0.98. If there&apos;s an agreed upon solution prior we&apos;d be happy to work/collaborate to get it done. I&apos;m hoping to have splittable meta stable soon so we can avoid having to do 2 backward incompatible rollouts. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; hope you&apos;re okay with this.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We need to put stake in grounds soon for hbase 2.0 cluster topology.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So far it seems to me the driving requirements are:&lt;/p&gt;

&lt;p&gt;+ scale&lt;br/&gt;
+ high availability&lt;br/&gt;
+ stop using zookeeper completely/for persistence&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;There&apos;s a lot of unknowns specific requirements may change. Let&apos;s pile on the ideas and have a roadmap iteratively experimenting and adding features with clear gains.&lt;/p&gt;</comment>
                            <comment id="14105102" author="mantonov" created="Thu, 21 Aug 2014 06:50:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do we need to split this conversation into what to do on master and what to do with 0.98?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That makes lot of sense to me.&lt;/p&gt;</comment>
                            <comment id="14114582" author="mantonov" created="Thu, 28 Aug 2014 23:22:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; - do you guys have by chance some recent numbers (or maybe estimate) on how long does full master failover take on the cluster with 300k or 3M regions? I didn&apos;t find those in the recent doc, eager to see that.&lt;/p&gt;</comment>
                            <comment id="14114633" author="mantonov" created="Fri, 29 Aug 2014 00:04:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;Please pile on all with thoughts. We need to put stake in grounds soon for hbase 2.0 cluster topology.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;2 humble cents from my side:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I thought that the primary requirement for splittable meta is not really read-write throughput, but rather the thinking that on large enough cluster with 1M+ regions, meta may not simply fit in master memory (or JVM would have hard time keeping process consuming that much memory up)? That might be worsened if we take any actions towards keeping more metadata in meta table. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; I believe you brought up before possible other things we may want to add to meta table, which would inflate it in size? Couldn&apos;t find that jira/thread. I would think that if we want to keep regions as a unit of assignment/recovery, splittable meta is a the must (so far didn&apos;t see an approach describing how to avoid it)&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;..and until we have &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10295&quot; title=&quot;Refactor the replication  implementation to eliminate permanent zk node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10295&quot;&gt;HBASE-10295&lt;/a&gt; &quot;Refactor the replication implementation to eliminate permanent zk node&quot; and/or &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11467&quot; title=&quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11467&quot;&gt;HBASE-11467&lt;/a&gt; &quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; (Maybe a later phase of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10070&quot; title=&quot;HBase read high-availability using timeline-consistent region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10070&quot;&gt;&lt;del&gt;HBASE-10070&lt;/del&gt;&lt;/a&gt; when followers can run closer in to the leader state would work here) or a new master layout where we partition meta across multiple master server.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Unless I&apos;ve missed some recent developments, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10070&quot; title=&quot;HBase read high-availability using timeline-consistent region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10070&quot;&gt;&lt;del&gt;HBASE-10070&lt;/del&gt;&lt;/a&gt; is about region replicas, while &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11467&quot; title=&quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11467&quot;&gt;HBASE-11467&lt;/a&gt; is about ZK-less client (the patch there is about to grow big enough to provide for zk-less client, it&apos;s absorbing other subtasks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ). May be worth to reiterate that zk-less client is some sort of pre-requisite or component of multi-master approach we&apos;re working on now, but it would work fine with current single active-many backup-masters schema as well.&lt;br/&gt;
I&apos;m thinking that multi-masters and partitioned-masters (if we go in these approaches) need to be discussed closely together and considering each other, otherwise it&apos;d be really hard to merge them together later on.&lt;/p&gt;

&lt;p&gt;Also  on this:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A plus split meta has over colocated master and meta is that master currently can be down for some period of time and the cluster keeps working; no splits and no merges and if a machine crashes while master is down, data is offline till master comes back (needs more exercise). This is less the case when colocated master and meta.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;d be curious to hear more opinions/assessments on how bad is that when master is down, and what timeframe various people would consider as &quot;generally ok&quot;, &quot;kind of long, really want it to be faster&quot; and &quot;unacceptably long&quot;?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So far it seems to me the driving requirements are:&lt;br/&gt;
+ scale&lt;br/&gt;
+ high availability&lt;br/&gt;
+ stop using zookeeper completely/for persistence&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, I think that are exactly the points and they could be discussed together. Besides scale, HA here probably consists of 2 parts - HA for region replicas (read- and rw-), and improved HA for master. Improved master HA (multi-master) for master is being researched/worked on now.&lt;/p&gt;

&lt;p&gt;On &quot;stop using ZK completely&quot; there are general changes here coming along (like see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7767&quot; title=&quot;Get rid of ZKTable, and table enable/disable state in ZK &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7767&quot;&gt;&lt;del&gt;HBASE-7767&lt;/del&gt;&lt;/a&gt;, on stopping using ZK for keeping table state.. a patch from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=octo47&quot; class=&quot;user-hover&quot; rel=&quot;octo47&quot;&gt;Andrey Stepachev&lt;/a&gt; is there ready for reviews), and proposed changes on client side to make hbase client non-dependent on ZK (that&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11467&quot; title=&quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11467&quot;&gt;HBASE-11467&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; mentioned above, and that&apos;s what would be complementary to multi-master work).&lt;/p&gt;</comment>
                            <comment id="14114894" author="stack" created="Fri, 29 Aug 2014 06:01:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;I thought that the primary requirement for splittable meta is not really read-write throughput, but rather the thinking that on large enough cluster with 1M+ regions, meta may not simply fit in master memory (or JVM would have hard time keeping process consuming that much memory up)? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;d be both &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; Scale reads because pieces of meta served by many servers and master can write in // so scale writes too (though not all ops will be //izable).&lt;/p&gt;

&lt;p&gt;Yeah, adding features to meta comes up all the time (today it was adding region flush/compaction history, previous its keeping hfiles per cf in meta, region replica info, and so on)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I would think that if we want to keep regions as a unit of assignment/recovery, splittable meta is a the must (so far didn&apos;t see an approach describing how to avoid it)&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10070&quot; title=&quot;HBase read high-availability using timeline-consistent region replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10070&quot;&gt;&lt;del&gt;HBASE-10070&lt;/del&gt;&lt;/a&gt; is about region replicas ....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was thinking that we could have meta region replicas to scale out the read i/o using hbase-10070 (solving one of the objections to splitting meta arguments)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11467&quot; title=&quot;New impl of Registry interface not using ZK + new RPCs on master protocol&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11467&quot;&gt;HBASE-11467&lt;/a&gt; is about ZK-less client...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is the issue where client takes a quorum of masters instead of a quorum of zks, right?  I was extrapolating that the endpoint of this issue is a quorum of masters where we could read from any (or at least some reads could be stale...) as another means of scaling out the read i/o when master and meta colocated.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...but it would work fine with current single active-many backup-masters schema as well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good to know &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m thinking that multi-masters and partitioned-masters (if we go in these approaches) need to be discussed closely together and considering each other, otherwise it&apos;d be really hard to merge them together later on.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree.  This issue seems to be arriving at single master to serve mllions of regions. A quorum of masters or partitioning master responsibilities for sure should be discussed together but I don&apos;t think they are soln to this issues problem (maybe partitioned master but single server soln seems simpler?)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;d be curious to hear more opinions/assessments on how bad is that when master is down, and what timeframe various people would consider as &quot;generally ok&quot;, &quot;kind of long, really want it to be faster&quot; and &quot;unacceptably long&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. Will write something up. In fact I don&apos;t even think a client can connect to the cluster currently if master is down which makes a bit of a farce of the above notion and needs fixing.&lt;/p&gt;

&lt;p&gt;Let me look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7767&quot; title=&quot;Get rid of ZKTable, and table enable/disable state in ZK &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7767&quot;&gt;&lt;del&gt;HBASE-7767&lt;/del&gt;&lt;/a&gt;. I got burned by it today. Its annoying.... to say the least.&lt;/p&gt;

&lt;p&gt;Yeah, that seems to be the conclusion that is beginning to prevail  here.&lt;/p&gt;





</comment>
                            <comment id="14114920" author="mantonov" created="Fri, 29 Aug 2014 06:31:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;That is the issue where client takes a quorum of masters instead of a quorum of zks, right? I was extrapolating that the endpoint of this issue is a quorum of masters where we could read from any (or at least some reads could be stale...) as another means of scaling out the read i/o when master and meta colocated.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, that&apos;s this one. Yeah, it should help with scaling read I/O (which, I believe, is a vast majority of meta access calls? by the way, could somebody estimate the distribution or read vs writes to meta table, in terms of requests per second/networking traffic/disk access? are there metrics for that?)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This issue seems to be arriving at single master to serve mllions of regions&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I believe there are 2 dimensions here, right?&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;For multi-masters (replicated master) the objection I believe is to 1) maintain up-to-date in-memory state on &amp;gt;1 master, thus avoiding startup cost for second master (that&apos;s actually why I solicited the estimates of how long does the full restart takes now on that big cluster) and 2) scale out reads by serving them out of copies located at different machines. But multi-masters does not solve problem when meta doesn&apos;t fit in memory, of when writes need to scale better&lt;/li&gt;
	&lt;li&gt;partitioned masters, in turn, address this second aspect, fitting meta in memory and scaling writes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Does it look like accurate summary to you? The viable combined solution might be multi-master setup, when masters are serving split meta and grouped by meta region replicas  (like, HM1 and HM2 are serving replicas of metaRegion1, metaRegion2, MetaRegion3, and HM3 and HM4 are serving replicas of metaRegion4 and metaRegion5? with masters being a region server now, having really many masters in the cluster might be just right direction to go with?)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In fact I don&apos;t even think a client can connect to the cluster currently if master is down which makes a bit of a farce of the above notion and needs fixing.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That probably is an argument for multi-master layout too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Let me look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7767&quot; title=&quot;Get rid of ZKTable, and table enable/disable state in ZK &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7767&quot;&gt;&lt;del&gt;HBASE-7767&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That would be great. I also did a first pass to review the patch on the review board and planning to get back to get closer look.&lt;/p&gt;</comment>
                            <comment id="14115524" author="stack" created="Fri, 29 Aug 2014 17:23:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;..by the way, could somebody estimate the distribution or read vs writes to meta table, in terms of requests per second/networking traffic/disk access?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; Do you have any numbers from your experiments?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I believe there are 2 dimensions here, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There is a more immediate 3rd dimension/option and that is what we have now where we have a single master and if it fails, backup assumes its role.&lt;/p&gt;

&lt;p&gt;For your dimension 1., (also known as  &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10296&quot; title=&quot;Replace ZK with a consensus lib(paxos,zab or raft) running within master processes to provide better master failover performance and state consistency&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10296&quot;&gt;HBASE-10296&lt;/a&gt; Replace ZK with a consensus lib(paxos,zab or raft running within master processes to provide better master failover performance and state consistency&quot;), it would be a nice-to-have but if we can, lets try and avoid our having to have a HA master. As long as the master comes back inside some reasonable window and the cluster can keep on chugging while its figuring out its recovery, this would be a simpler deploy than one that needs a HA master mini-cluster.  Multi-master would help scale reads but as you note, doesn&apos;t help if one massive meta region and we want to cache it.&lt;/p&gt;

&lt;p&gt;We&apos;d go to partitioned masters, as I see it, if we can&apos;t make one master do.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The viable combined solution ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suggest we look at single master with split meta served by regionservers first?&lt;/p&gt;</comment>
                            <comment id="14115558" author="mantonov" created="Fri, 29 Aug 2014 17:48:47 +0000"  >&lt;p&gt;Oops, sorry, instead of &quot;For multi-masters (replicated master) the objection I believe is&quot;, I meant &quot;..objective&quot;.&lt;/p&gt;</comment>
                            <comment id="14115562" author="jxiang" created="Fri, 29 Aug 2014 17:50:02 +0000"  >&lt;p&gt;One meta region should be good enough for most of clusters. If there is just one meta region, it&apos;s better to put it together with the master to avoid possible complications.  Additional meta regions should be optional. It should be simple to add new meta regions. Ideally, the same code path should be used no matter how many meta regions there are.&lt;/p&gt;</comment>
                            <comment id="14115582" author="mantonov" created="Fri, 29 Aug 2014 17:54:27 +0000"  >&lt;p&gt;Totally agree that for most clusters, 1 meta region should suffice, and that if there&apos;s one region, it&apos;s easier to bind it to master (no rpcs, less complicated coordination and failure scenarios). Though, for clusters where &amp;gt;1 meta regions is required, that would need to be served on other machines using RPC, right? I mean, seems tricky to make it be the same code path?&lt;/p&gt;</comment>
                            <comment id="14115646" author="mantonov" created="Fri, 29 Aug 2014 18:48:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; regarding multi-master..yeah, I&apos;d make sure that it doesn&apos;t complicate the deployment (and to reduce scope of changes, that would be gradually in the direction of cold backup -&amp;gt; warm -&amp;gt; hot -&amp;gt; active-active).&lt;/p&gt;</comment>
                            <comment id="14119513" author="mantonov" created="Wed, 3 Sep 2014 07:52:29 +0000"  >&lt;p&gt;A side question to folks who recently benchmarked it on big clusters.. what&apos;s avg ratio of hdfs inodes / per region you observed? Trying to estimate the load the proposed 1M or 50M regions setup puts on NN.&lt;/p&gt;</comment>
                            <comment id="14119832" author="octo47" created="Wed, 3 Sep 2014 12:59:34 +0000"  >&lt;p&gt;Just thinking, did anyone tried to measure how meta uses memory and reduce usage of memory? It is interesting, why NN is able to handle much more data in memory, while HMaster can&apos;t.&lt;/p&gt;</comment>
                            <comment id="14120086" author="stack" created="Wed, 3 Sep 2014 16:58:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;I mean, seems tricky to make it be the same code path?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah. One of the code paths will &apos;suffer&apos; neglect.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; I like your &quot;cold backup -&amp;gt; warm -&amp;gt; hot -&amp;gt; active-active&quot;.... let me try and put together a bit of a summary so far on findings and arguments.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;...and reduce usage of memory&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, we&apos;ll have to go this route if we are trying to keep state of a big cluster in heap.  Could work on making the representation more compact.  You arguing for single meta region &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=octo47&quot; class=&quot;user-hover&quot; rel=&quot;octo47&quot;&gt;Andrey Stepachev&lt;/a&gt; then? There is also the on-hdfs size to consider (write-amplification) and the r/w i/os.&lt;/p&gt;</comment>
                            <comment id="14120142" author="octo47" created="Wed, 3 Sep 2014 17:44:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yeah, we&apos;ll have to go this route if we are trying to keep state of a big cluster in heap. Could work on making the representation more compact. You arguing for single meta region Andrey Stepachev then? There is also the on-hdfs size to consider (write-amplification) and the r/w i/os.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For sure, compact representation doesn&apos;t implicate single meta. Compact meta allows to bother with split meta only for really big installations. But how HDFS would handle that, as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; mentioned above.&lt;/p&gt;

&lt;p&gt;As for compact META representations we can use other technics to reduce HDFS impact for big meta.&lt;/p&gt;</comment>
                            <comment id="14120144" author="octo47" created="Wed, 3 Sep 2014 17:45:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;But how HDFS would handle that, as Mikhail Antonov mentioned above?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;that should be a question&lt;/p&gt;</comment>
                            <comment id="14120349" author="mantonov" created="Wed, 3 Sep 2014 20:06:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;,  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=octo47&quot; class=&quot;user-hover&quot; rel=&quot;octo47&quot;&gt;Andrey Stepachev&lt;/a&gt; - on this compaction topic,  I also mentioned early on in the thread:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I wonder if it makes sense to have google doc linked to this jira to save various proposals, findings and estimates? Like that summarizes current usage to be conservatively 3.5Gb in meta / 1M regions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So seems like we&apos;re using 3-3.5 Kb per region-row? That should be compressible, looking at the data in meta rows. Also I think it would help if we can post here some numbers and capture in the documents, so we have the baseline for our work. For example:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;how many kb in memory per-region in meta&lt;/li&gt;
	&lt;li&gt;how many hdfs inodes per region (depends on numbers of store files, but some estimate?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To estimate, how big would be a deployment where meta doesn&apos;t fit in memory? How many RSs, how many petabytes of data?&lt;/p&gt;</comment>
                            <comment id="14120360" author="octo47" created="Wed, 3 Sep 2014 20:10:45 +0000"  >&lt;p&gt;Also, it is very interesting, did big users of HBase with so many regions use NameNode federation or use enormous machine to handle NameNode with so many regions?&lt;/p&gt;</comment>
                            <comment id="14120408" author="toffer" created="Wed, 3 Sep 2014 20:41:33 +0000"  >&lt;p&gt;We&apos;re currently using huge NNs. &lt;/p&gt;

&lt;p&gt;We haven&apos;t looked into the number of inodes as that didn&apos;t seem to be an issue for the 1M case (We have a single NN running ~250M files). But we&apos;ll be watching it for the post 1M  benchmarks. Will post results here.&lt;/p&gt;
</comment>
                            <comment id="14120485" author="eclark" created="Wed, 3 Sep 2014 21:23:56 +0000"  >&lt;p&gt;If we can get into the same scaling range as HDFS&apos;s namenode then I don&apos;t see the urgency to split meta.&lt;/p&gt;

&lt;p&gt;Num Files &amp;gt;&amp;gt; Num Regions&lt;/p&gt;

&lt;p&gt;So it would seem that addressing in-memory representation of meta would mean that the scaling bottle neck would be back to the NN. At some point there will be limits there, but that seems fine as long as there are the same limits to our underlying foundation (hdfs).&lt;/p&gt;</comment>
                            <comment id="14120511" author="toffer" created="Wed, 3 Sep 2014 21:38:27 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Francis Liu, Virag Kothari - do you guys have by chance some recent numbers (or maybe estimate) on how long does full master failover take on the cluster with 300k or 3M regions? I didn&apos;t find those in the recent doc, eager to see that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; We don&apos;t have the numbers we&apos;ll get them next time. Tho failover recovery is essentially bounded on scanning meta and recovering dead servers. So without dead servers it would just be a fraction of the startup time.&lt;/p&gt;
</comment>
                            <comment id="14120516" author="mantonov" created="Wed, 3 Sep 2014 21:42:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; thanks! I&apos;d be really curious to look at those numbers.&lt;/p&gt;

&lt;p&gt;Is the NN you mentioned with 250M files is solely dedicated to HBase installation? I mean, could the assumption be made that the HBase cluster with 1M or large regions consumes about 250M of files in HDFS, so roughly 250 files / per region, or would it be too bold assumption?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; so if we take as a baseline that (num of files) &amp;gt;&amp;gt; (num regions), I wonder how close to NN limits we are? I mean, if we&apos;re talking about case with 10M regions (or even 50M), with the same ratio of region-to-files, 10M regions would give us 2.5B files in HDFS? How close is that to HDFS limits?&lt;/p&gt;</comment>
                            <comment id="14120535" author="toffer" created="Wed, 3 Sep 2014 21:50:27 +0000"  >&lt;blockquote&gt;
&lt;p&gt;If we can get into the same scaling range as HDFS&apos;s namenode then I don&apos;t see the urgency to split meta.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; We already can&apos;t scale the number of regions close to the number hdfs can&apos;t handle. See attached doc. Also the hdfs guys internally will be working to support hbase scaling requirements.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Is the NN you mentioned with 250M files is solely dedicated to HBase installation? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The NN I mentioned belongs to a different cluster. The files/per region&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I mean, could the assumption be made that the HBase cluster with 1M or large regions consumes about 250M of files in HDFS, so roughly 250 files / per region, or would it be too bold assumption?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah that wouldn&apos;t be accurate. It&apos;s hard to come up with a good estimate because it&apos;s use case dependent. Tho even for our use case I&apos;m making estimates as things are still in flux. I&apos;ll share something once we have more data.&lt;/p&gt;</comment>
                            <comment id="14120561" author="eclark" created="Wed, 3 Sep 2014 22:09:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;We already can&apos;t scale the number of regions close to the number hdfs can&apos;t handle.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah I know. What I&apos;m saying is that we should work on getting there before working on the more complex split meta and split master. I would argue that we can get on par (or better) than the NN since it&apos;s doing more active writes then meta on a stable cluster.  Then when that happens the NN will be the bottleneck and there will be no need for split meta.&lt;/p&gt;</comment>
                            <comment id="14120912" author="toffer" created="Thu, 4 Sep 2014 03:40:11 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Yeah I know. What I&apos;m saying is that we should work on getting there before working on the more complex split meta and split master. I would argue that we can get on par (or better) than the NN since it&apos;s doing more active writes then meta on a stable cluster. Then when that happens the NN will be the bottleneck and there will be no need for split meta.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s hard to make a comparison IMHO. NN uses a filer for WAL (at least for us). It&apos;s not an LSM so it doesn&apos;t suffer from write amplification. Major compaction could just creep up and you could get hosed till its done.  Having higher write throughput would definitely be a good thing but IMHO the clear way to scale, is to split meta as it addresses a bunch of issues and enables horizontal scalability for regions. Bottom line for us is we need to scale to 1M regions (soon) and beyond. The guys here will help us with any hdfs related blockers.&lt;/p&gt;</comment>
                            <comment id="14120938" author="eclark" created="Thu, 4 Sep 2014 04:22:52 +0000"  >&lt;p&gt;Yeah it&apos;s not a direct comparison, but I still think that making meta and meta look ups faster will be more beneficial than adding extra complexity.  Adding things like row caching and read replica aware meta look ups will get us a lot before drastically changing the deployment or complexity of normal users. Until we&apos;ve tuned everything we can I don&apos;t feel that it&apos;s on overall benefit to add complexity to an already very complex system.&lt;/p&gt;</comment>
                            <comment id="14120953" author="mantonov" created="Thu, 4 Sep 2014 04:50:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; thinking about that more..meta table size isn&apos;t what limits you now, is it? 1M regions is estimated to take up about 3-4Gb of space (7Gb is mentioned in last doc, for 10 last versions kept for meta), which should comfortably fit in memory of any single machine. So bottleneck isn&apos;t memory, but CPU, due to (possibly) overly coarse-grained locking, is that what it looks like?..&lt;/p&gt;</comment>
                            <comment id="14120970" author="stack" created="Thu, 4 Sep 2014 05:29:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...If we can get into the same scaling range as HDFS&apos;s namenode then I don&apos;t see the urgency to split meta.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There is a largish gap at the moment.&lt;/p&gt;

&lt;p&gt;In-memory representation of cluster is not the immediate barrier to scaling.  Getting more r/w iops, cache, and putting off mad compactions on a single big, critical meta region are the more immediate priorities. Row caching and read replicas are just as crazy pants as splitting meta but in the end you are only &apos;solving&apos; the read i/o issue and leaving aside how we&apos;d deal w/ fat meta, write iops, and cache &amp;#8211; nevermind we have all eggs in one basket.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...doing more active writes then meta on a stable cluster&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Stable cluster is uninteresting. Its the stop cluster, install software, restart cluster in minimal time is where all the fun is.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...changing the deployment or complexity of normal users. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, this is an issue.  Most people will not want/need split meta.  A &apos;go big&apos; button would be messy...&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; Let me put up doc. for all to throw darts at.  Has a few stats in it.&lt;/p&gt;

</comment>
                            <comment id="14120976" author="mantonov" created="Thu, 4 Sep 2014 05:39:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;Getting more r/w iops, cache, and putting off mad compactions on a single big, critical meta region are the more immediate priorities. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hm, sounds like that&apos;s the fit for multiple masters then? If each one of several active masters has its own co-located consistent replica of meta, then:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;we can do rolling major compactions&lt;/li&gt;
	&lt;li&gt;read IO (which I think is prevailing) is improved, + meta is cached on several machines?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14120984" author="stack" created="Thu, 4 Sep 2014 05:52:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; Wouldn&apos;t splitting meta be a simpler and less risky route to more read and write iops, more cache, and smaller meta regions than dev&apos;ing multiple masters with colocated replicas?&lt;/p&gt;</comment>
                            <comment id="14121038" author="mantonov" created="Thu, 4 Sep 2014 06:52:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; that&apos;s kind of hard to compare the relative complexity without proposed detailed designs for both, I believe both options worth research and comparison (also we should be able to combine them). W.r.t multiple masters, we do try to streamline the design (with that incremental approach for &quot;cold-&amp;gt;warm-&amp;gt;hot-&amp;gt; active-active&quot; transition) to make it less intrusive change + provide positive side effects like simplifying current workflow management around region splits, log splitting etc.&lt;/p&gt;</comment>
                            <comment id="14121051" author="eclark" created="Thu, 4 Sep 2014 07:12:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;There is a largish gap at the moment.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Then we should be focusing there on places where we have slowness.  Not on adding more complexity.&lt;/p&gt;

&lt;p&gt;It seems like everyone is rushing to following the example of HDFS to federation (that&apos;s what split meta, split masters gets us). I for one am terrified of going that way. From where I sit that was just a failure and following it isn&apos;t something I&apos;m ready to do without tangible benefits.&lt;/p&gt;</comment>
                            <comment id="14121455" author="stack" created="Thu, 4 Sep 2014 15:27:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...that&apos;s kind of hard to compare the relative complexity without proposed detailed designs for both&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hopefully we don&apos;t need detailed design.  I think a sketch will be sufficient.   TODO.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;....Then we should be focusing there on places where we have slowness. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&apos;slowness&apos; is but one of the dimensions that needs addressing.  There is also &apos;size&apos; &amp;#8211; size in HDFS, size of cache &amp;#8211; as well as availability&lt;/p&gt;

&lt;p&gt;No to federation. I don&apos;t think we need to split master.&lt;/p&gt;

&lt;p&gt;Is the failure you refer to our having a root and not making use of it?&lt;/p&gt;

&lt;p&gt;Let me post something for folks to skewer (listing tangible benefit).. Hopefully tonight (out for the day).&lt;/p&gt;

&lt;p&gt;Good stuff&lt;/p&gt;</comment>
                            <comment id="14121545" author="eclark" created="Thu, 4 Sep 2014 16:39:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is the failure you refer to our having a root and not making use of it?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No. I was referring to HDFS Federation as a failure.  In my mind it&apos;s a failure.  99% of HDFS users don&apos;t need or want it yet federation slows down all feature developent and increases complexity on just about every operation. &lt;/p&gt;

&lt;p&gt;In my mind the solutions being discussed here seem to run parallel. 99% of users don&apos;t need them but everyone&apos;s going to deal with extra lookpus on region location misses and extra complexity while running the system.&lt;/p&gt;</comment>
                            <comment id="14122350" author="stack" created="Fri, 5 Sep 2014 03:31:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;...In my mind it&apos;s a failure.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK. Lets avoid going that route.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...99% of users don&apos;t need them...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True.  We could just hang out in the realm of the 99% in an orbit just above &quot;webscale&quot;. Meantime the excluded 1% are our brothers and sisters who need to go bigger.  Lets club together and try and figure a way where we can go &apos;big&apos; w/o going complicated.&lt;/p&gt;
</comment>
                            <comment id="14122381" author="stack" created="Fri, 5 Sep 2014 04:12:15 +0000"  >&lt;p&gt;Here&apos;s some notes summarizing back and forth so far. Its public so lacerate to your hearts content:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.google.com/document/d/1eCuqf7i2dkWHL0PxcE1HE1nLRQ_tCyXI4JsOB6TAk60/edit#heading=h.i9al4pf1bajh&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1eCuqf7i2dkWHL0PxcE1HE1nLRQ_tCyXI4JsOB6TAk60/edit#heading=h.i9al4pf1bajh&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14122382" author="stack" created="Fri, 5 Sep 2014 04:14:02 +0000"  >&lt;p&gt;Try this link instead &lt;a href=&quot;https://docs.google.com/document/d/1eCuqf7i2dkWHL0PxcE1HE1nLRQ_tCyXI4JsOB6TAk60/edit?usp=sharing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1eCuqf7i2dkWHL0PxcE1HE1nLRQ_tCyXI4JsOB6TAk60/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14122383" author="octo47" created="Fri, 5 Sep 2014 04:16:41 +0000"  >&lt;p&gt;Thanks Stack, that works.&lt;/p&gt;






&lt;p&gt;&amp;#8211; &lt;br/&gt;
Andrey.&lt;/p&gt;</comment>
                            <comment id="14123667" author="apurtell" created="Fri, 5 Sep 2014 21:56:52 +0000"  >&lt;p&gt;There&apos;s also the issue that Francis and crew want to run a version of HBase in production today. I don&apos;t see the multi-master alternative as viable before 2.0, or perhaps 1.1 I suppose, but not 1.0.x. Split meta is conceivably something that could be backported like ZK-less assignment* was, although no doubt risky and would need to be carefully done, hidden behind a default-off toggle.&lt;/p&gt;</comment>
                            <comment id="14123670" author="stack" created="Fri, 5 Sep 2014 21:59:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; Yes.&lt;/p&gt;</comment>
                            <comment id="14123674" author="apurtell" created="Fri, 5 Sep 2014 22:00:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;It seems like everyone is rushing to following the example of HDFS to federation (that&apos;s what split meta, split masters gets us).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m concerned about this notion of a split meta colocated with a split master function also. Split meta on its own is one thing. Also splitting the master as described smacks of the dynamic subtree partitioning of Ceph&apos;s MDS, which has never been stable at scale as far as I know, and there&apos;s no stability in sight there either.&lt;/p&gt;</comment>
                            <comment id="14123679" author="apurtell" created="Fri, 5 Sep 2014 22:07:48 +0000"  >&lt;p&gt;With apologies to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;Steve Loughran&lt;/a&gt;, I&apos;m going to channel my inner Steve Loughran and suggest that a split meta + split master + active-active master (am I getting all the basics in there?) should be prototyped with TLA+ so we have some idea it could even work correctly as proposed. It would also be a worthwhile exercise to model the current AssignmentManager and related protocols with TLA+ but I&apos;ll understand if nobody volunteers. &lt;/p&gt;</comment>
                            <comment id="14126072" author="posix4e" created="Mon, 8 Sep 2014 20:48:14 +0000"  >&lt;p&gt;We have a ton of people in house who understand TLA+. The problem is TLA+ doesn&apos;t verify anything about the code (or a prototype). I am glad that Steve was happy that we wrote a TLA+ specification but in my humble opinion, no one could give us any feedback of what we proposed. Frankly from the lack of push back on the TLA+ we wrote up, it is apparent to me that no one understood what we wrote. That being said, if it&apos;s what the community needs TLA+ to feel confident about the approach we are taking, we can do it. Just remember TLA+ doesn&apos;t&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Verify anything about the code or implementation&lt;/li&gt;
	&lt;li&gt;Doesn&apos;t understand anything about your architecture&lt;/li&gt;
	&lt;li&gt;Won&apos;t tell you if your code isn&apos;t modeled by TLA+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It only verifies the theoretical behavior of concurrent systems, not the systems themselves.&lt;/p&gt;</comment>
                            <comment id="14126542" author="toffer" created="Tue, 9 Sep 2014 03:40:40 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I&apos;m concerned about this notion of a split meta colocated with a split master function also. Split meta on its own is one thing. Also splitting the master as described smacks of the dynamic subtree partitioning of Ceph&apos;s MDS, which has never been stable at scale as far as I know, and there&apos;s no stability in sight there either.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;From what I understand &quot;splitting meta&quot; supercedes &quot;collocating the meta&quot; as having both together would be counterintuitive. With splitting meta I think it is understood enough that code plus documentation should be enough?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There&apos;s also the issue that Francis and crew want to run a version of HBase in production today. I don&apos;t see the multi-master alternative as viable before 2.0, or perhaps 1.1 I suppose, but not 1.0.x. Split meta is conceivably something that could be backported like ZK-less assignment* was, although no doubt risky and would need to be carefully done, hidden behind a default-off toggle.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For our use case we don&apos;t have data yet to motivate sharded master or HA master. For now it seems splitting meta will address our short-term scaling use case.&lt;/p&gt;</comment>
                            <comment id="14126549" author="posix4e" created="Tue, 9 Sep 2014 03:47:40 +0000"  >&lt;p&gt;We also support splitting meta as being important for scalability. In fact&lt;br/&gt;
we&apos;d love to help. I think some of the work required for splitting meta&lt;br/&gt;
could be aided by some of the work we have done in regards to rdsm. I&lt;br/&gt;
assume that there is a significant master change required to make it work.&lt;br/&gt;
Perhaps we can join forces in a way that won&apos;t get in your way.&lt;/p&gt;
</comment>
                            <comment id="14126556" author="cos" created="Tue, 9 Sep 2014 03:53:43 +0000"  >&lt;p&gt;Honestly speaking the latest fashion (not to say obsession &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; with TLA+ doesn&apos;t help anything. It isn&apos;t a formal proof of the code correctness nor help to reconciles architecture contracts, as Alex has pointed out. So while someone can produce a fancy TLA+ diagram I am not sure I am not sure how many others will even pay the attention to it. It&apos;s like a UML - a good way to share the responsibility for a bad decision &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;We have posted the spec in that HDFS JIRA because it was easier to do that instead of keep the endless argument about nothingness. Will the same be required here, I wonder?&lt;/p&gt;</comment>
                            <comment id="14126564" author="apurtell" created="Tue, 9 Sep 2014 04:03:30 +0000"  >&lt;p&gt;I hope the tongue-in-cheek nature of that post would have been evident. But it is true that consensus is hard.&lt;/p&gt;

&lt;p&gt;The concern I have is this discussion not downplay issues and solutions with scaling that people have today in lieu of grander designs for tomorrow&apos;s versions. (Both are important IMHO). Looks like that concern is not an issue given Alex&apos;s comment above. Great. &lt;/p&gt;</comment>
                            <comment id="14126581" author="cos" created="Tue, 9 Sep 2014 04:20:23 +0000"  >&lt;p&gt;I actually was trying to play along, Andrew - I guess my Russian&apos;s toughness got into the way of this comment being accepted as a joke &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14136860" author="mantonov" created="Wed, 17 Sep 2014 06:43:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; - guys, getting back for a moment to the size of meta table..there&apos;s an interesting thing about it. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=octo47&quot; class=&quot;user-hover&quot; rel=&quot;octo47&quot;&gt;Andrey Stepachev&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sergey.soldatov&quot; class=&quot;user-hover&quot; rel=&quot;sergey.soldatov&quot;&gt;Sergey Soldatov&lt;/a&gt; and myself are doing some prototyping of more compact &amp;amp; fast in-memory representation of meta, and have noticed interesting thing inspecting the data structures under the microscope (like &lt;a href=&quot;https://code.google.com/p/memory-measurer&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://code.google.com/p/memory-measurer&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In the discussion about, and in the doc put together by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; it&apos;s mentioned that in best case, for fully compacted meta with single-versioned cells 1 row in meta takes up 7-10kb (please correct me if I&apos;m wrong).&lt;/p&gt;

&lt;p&gt;If I run a minicluster test, create some simple table with regions and then do MTA.fullScan() to get list of Result, a single Result won&apos;t take more than 1Kb, normally less than that. And the list of Results is not exactly super-compacted structure, if initial experiments we were able to compact it further quite a bit.&lt;/p&gt;

&lt;p&gt;So I&apos;m curious how exactly the size heap occupied by meta was calculated (was it some sort of direct sizeOf, like using Unsafe or instrumentation etc), or default impl provided by HeapSize for hregion, or size of HFiles, or something else? Also, I&apos;d appreciate a lot if you could share a sample representative row from your meta, so we can see the typical size of elements in it?&lt;/p&gt;</comment>
                            <comment id="14137450" author="apurtell" created="Wed, 17 Sep 2014 16:05:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;And the list of Results is not exactly super-compacted structure, if initial experiments we were able to compact it further quite a bit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Would you mind filing a new issue for that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; and say more about this on that tangent?&lt;/p&gt;</comment>
                            <comment id="14137613" author="virag" created="Wed, 17 Sep 2014 17:40:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;fully compacted meta with single-versioned cells 1 row in meta takes up 7-10k&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The 7GB for 1M is with 10 versions of meta. Meta has 10 versions by default and we dint change that for our experiments. But I see the confusion. The attached pdf says 10 versions while the shared google doc says one version. Sorry about that.&lt;br/&gt;
Also 7GB was the size of store file on hdfs. The table was simply created using HexStringSplit with nothing extra. Also this was on 0.98 (I think master  code adds some stuff about region replicas in meta) with zk-less assignment.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;d appreciate a lot if you could share a sample representative row from your meta, so we can see the typical size of elements in it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;On prod, we currently run 0.94. I can check if we can share some sample row from there. &lt;/p&gt;</comment>
                            <comment id="14137632" author="octo47" created="Wed, 17 Sep 2014 17:50:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt;, do you have some thoughts, why 10 versions are needed? Seems thats a great overhead (ten times more memory).&lt;/p&gt;</comment>
                            <comment id="14137662" author="virag" created="Wed, 17 Sep 2014 18:05:12 +0000"  >&lt;p&gt;why 10 versions are needed?&lt;/p&gt;

&lt;p&gt;Looking at the code for META_TABLEDESC in HTableDescriptor&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// Ten is arbitrary number.  Keep versions to help debugging.
&lt;/span&gt;.setMaxVersions(10)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;May be we need to revisit this?&lt;/p&gt;</comment>
                            <comment id="14137715" author="octo47" created="Wed, 17 Sep 2014 18:37:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; thanks. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; didn&apos;t see that comment somehow.&lt;/p&gt;</comment>
                            <comment id="14137754" author="mantonov" created="Wed, 17 Sep 2014 19:01:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;Would you mind filing a new issue for that Mikhail Antonov and say more about this on that tangent?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks for the interest &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;, I sure will file it, but I&apos;d just like to do that in a day or maybe couple of days, when we have run more experiments and also put together actual code to show. Once filed, I&apos;ll make sure to link it to this jira.&lt;/p&gt;</comment>
                            <comment id="14137783" author="octo47" created="Wed, 17 Sep 2014 19:24:09 +0000"  >&lt;p&gt;Looks like we can do two things to solve that on up to 1.0 version of hbase (without significant changes)&lt;br/&gt;
1. Meta should keep 1 version (with atomic row updates we definitely need no more then one actual version), even better make that configurable.&lt;br/&gt;
2. Meta HLogs should be archived for debugging&lt;/p&gt;

&lt;p&gt;That will reduce scan overhead (much less KVs to scan) and reduce memory footprint and reduce load times for very big metas.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;, do we have significant objections on that?&lt;/p&gt;</comment>
                            <comment id="14137789" author="mantonov" created="Wed, 17 Sep 2014 19:30:18 +0000"  >&lt;p&gt;Thanks for explanation &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Yeah, as comment suggest it seems there were no special reason to have it set to 10? Making it configurable seems trivial change?&lt;/p&gt;</comment>
                            <comment id="14138478" author="stack" created="Thu, 18 Sep 2014 04:40:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;...do we have significant objections on that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not from me.  Even keeping just keeping three versions would be an improvement.&lt;/p&gt;

&lt;p&gt;A compact representation in master will help (Let me update the attached google doc and fix my misstatements).&lt;/p&gt;</comment>
                            <comment id="14138802" author="octo47" created="Thu, 18 Sep 2014 11:18:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; thanks for comments. &lt;br/&gt;
I added jira for that: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12016&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-12016&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14139093" author="virag" created="Thu, 18 Sep 2014 15:58:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;1. Meta should keep 1 version (with atomic row updates we definitely need no more then one actual version), even better make that configurable.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;2. Meta HLogs should be archived for debugging&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For 1., making configurable sounds good. I don&apos;t feel a strong need for 2. and again we need to have a separate config for cleanup of archived meta WALs.&lt;/p&gt;</comment>
                            <comment id="14139101" author="octo47" created="Thu, 18 Sep 2014 16:02:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; I agree, 1. is enough for most cases. In &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12016&quot; title=&quot;Reduce number of versions in Meta table. Make it configurable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12016&quot;&gt;&lt;del&gt;HBASE-12016&lt;/del&gt;&lt;/a&gt; I made number of versions configurable.&lt;/p&gt;</comment>
                            <comment id="14149778" author="eclark" created="Fri, 26 Sep 2014 18:38:27 +0000"  >&lt;p&gt;Had some discussions last night and people were asking about read vs write numbers for meta.&lt;br/&gt;
So currently on a stable cluster meta is getting a read to write request ratio of 50,000 reads for every one write request. To me that really suggest that adding read replicas is the next thing that we should look at before splitting meta or master. I think that could get us quite a way.&lt;/p&gt;</comment>
                            <comment id="14149823" author="stack" created="Fri, 26 Sep 2014 19:12:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; asked last night if stripped compactions could help w/ compaction problem in meta?&lt;/p&gt;

&lt;p&gt;Regards the 50k reads/1 write, what is the ratio across a restart do you think?&lt;/p&gt;</comment>
                            <comment id="14149859" author="apurtell" created="Fri, 26 Sep 2014 19:36:41 +0000"  >&lt;p&gt;Might also be useful to track the ratio when bringing a new table online and scaling it up from 0 to a few billion rows.&lt;/p&gt;</comment>
                            <comment id="14342847" author="stack" created="Mon, 2 Mar 2015 06:54:08 +0000"  >&lt;p&gt;Some notes on having a scalable meta here: &lt;a href=&quot;https://docs.google.com/document/d/1eCuqf7i2dkWHL0PxcE1HE1nLRQ_tCyXI4JsOB6TAk60/edit?usp=sharing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1eCuqf7i2dkWHL0PxcE1HE1nLRQ_tCyXI4JsOB6TAk60/edit?usp=sharing&lt;/a&gt;  Let me attach a PDF version.&lt;/p&gt;</comment>
                            <comment id="14342903" author="octo47" created="Mon, 2 Mar 2015 08:14:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; so, we are going splitmeta?&lt;/p&gt;</comment>
                            <comment id="14343691" author="stack" created="Mon, 2 Mar 2015 20:19:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=octo47&quot; class=&quot;user-hover&quot; rel=&quot;octo47&quot;&gt;Andrey Stepachev&lt;/a&gt; IMO, yeah. Tried getting consensus on that a while back and seemed to have it but didn&apos;t nail it down... Will be back.  Doc. above needs work too.&lt;/p&gt;</comment>
                            <comment id="14343705" author="apurtell" created="Mon, 2 Mar 2015 20:28:43 +0000"  >&lt;p&gt;See above on this issue where &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; wanted to split meta. Since his shop is running 0.98 I offered to maintain a branch of 0.98 that includes that change in ASF Git for their use, figuring that positive results there would inform on when/if we should do that on all branches. Not sure what the current status of this discussion is. No patch yet, for 0.98 or any other branch.&lt;/p&gt;</comment>
                            <comment id="14343882" author="lhofhansl" created="Mon, 2 Mar 2015 22:18:06 +0000"  >&lt;p&gt;Lemme step back... The fundamental conflict is that for assignment we to assign in large enough chunks (i.e. a region size), but for other parts of the system smaller chunks would be better (compactions, input split for M/R), etc. Right?&lt;/p&gt;

&lt;p&gt;The unit of failure is always going to be a region server, whether we have 2000 1GB regions or 20 100GB region makes no difference from that angle. Assuming server with 16TB of disk space or so, the granularity of assignment is also not at issue as long as we keep in that ball park (i.e. 1GB - 1TB or so).&lt;/p&gt;

&lt;p&gt;So what are the exact problems with large regions:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;compactions and write amplification&lt;/li&gt;
	&lt;li&gt;input split calculation for M/R&lt;/li&gt;
	&lt;li&gt;log replay upon recovery (is that an issue, i.e. is it worse replaying 1 large log compared to replaying 100 small ones)&lt;/li&gt;
	&lt;li&gt;(more?)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Can we &lt;b&gt;only&lt;/b&gt; solve these three with many small regions? (or do stripe compactions, simple width stats for M/R, etc)&lt;br/&gt;
I&apos;m trying to get from a statement about an implementation (that might just shift complexity from one part of HBase to another) to an exact problem statement.&lt;/p&gt;</comment>
                            <comment id="14344011" author="mantonov" created="Mon, 2 Mar 2015 23:36:05 +0000"  >&lt;p&gt;I also remember some time ago it was discovered that we&apos;re keeping too many versions of cells in meta, and the patch making is configurable was committed. So I suppose &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; or others had a chance to play with it and see if their problem was alleviated and to what extent? Curious to know the results.&lt;/p&gt;</comment>
                            <comment id="14344250" author="stack" created="Tue, 3 Mar 2015 01:25:30 +0000"  >&lt;p&gt;In your list, IMO, 1. is reason enough to do small regions.&lt;/p&gt;

&lt;p&gt;4. Smaller regions mean less of the keyspace is offline when balancing; also more even balance is possible when regions are smaller.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Can we only solve these three with many small regions?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we just did small regions, without introducing anything new (other than doing what we already do &apos;better&apos;/&apos;faster&apos;), we could improve on your list without need to add custom compaction policy(-ies) and the recording of interstices at 100MB intervals in metadata (which we&apos;d have to teach clients to read), etc.&lt;/p&gt;

&lt;p&gt;Regards a problem statement, you want one on why we should tend down toward small rather than continue our current trajectory of larger and larger regions, or do you want a problem statement for the subject of this JIRA? Regards this JIRA, we have users who are headed toward 1M now (Flurry reported being at 300k afraid to go up from there and Francis has &apos;larger&apos; clusters) so we have to deal.  You thinking we should explore going up from 10/20G toward 100G or 1TB? (With stripe compactions++ and means of apportioning out the 1TB region, etc., to address the 1-4 list above?).&lt;/p&gt;</comment>
                            <comment id="14344384" author="lhofhansl" created="Tue, 3 Mar 2015 02:46:22 +0000"  >&lt;p&gt;Just saying we should do what solves the problem with least amount of effort.&lt;br/&gt;
I think Flurry had abnormally small regions (1g), and that&apos;s why they have so many (with 20g region they&apos;d have 5.8pb &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) &lt;br/&gt;
If having many regions and fixing all the issues from that is easiest we should do that. If the other ways are easier we should do those.&lt;br/&gt;
There have been other ideas floating (have regions share a memstore, groups of regions for assignment, etc).&lt;/p&gt;

&lt;p&gt;I&apos;m not against this. Exploring what new problems we&apos;re exchanging for the old problems:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;splittable META&lt;/li&gt;
	&lt;li&gt;scalable assignment manager&lt;/li&gt;
	&lt;li&gt;handle many memstores&lt;/li&gt;
	&lt;li&gt;multi-master&lt;/li&gt;
	&lt;li&gt;potential NN scaling issues&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;If those are easier to solve than those and the previous comment, we know what we should do &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14344491" author="stack" created="Tue, 3 Mar 2015 04:27:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Just saying we should do what solves the problem with least amount of effort.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree. Let me do a better job writing up what we&apos;ve all spewed across tens of JIRAs and in offline emails and go from there. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14344512" author="lhofhansl" created="Tue, 3 Mar 2015 04:55:43 +0000"  >&lt;p&gt;Thanks. Sorry for being a pain in the a**.&lt;/p&gt;</comment>
                            <comment id="14711716" author="stack" created="Tue, 25 Aug 2015 18:16:16 +0000"  >&lt;p&gt;Notes on scalable meta&lt;/p&gt;</comment>
                            <comment id="14711736" author="eclark" created="Tue, 25 Aug 2015 18:26:23 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; some good notes there.&lt;/p&gt;</comment>
                            <comment id="15111617" author="toffer" created="Thu, 21 Jan 2016 23:44:00 +0000"  >&lt;p&gt;Just an update, we&apos;ve been running a production cluster which has 16 meta regions and around 260k regions for the last 2 months or so. We&apos;ll start getting the supporting changes in zkless (if any), &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11290&quot; title=&quot;Unlock RegionStates&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11290&quot;&gt;HBASE-11290&lt;/a&gt;. Then the big patch, tho will try to break it up into smaller pieces if possible.&lt;/p&gt;</comment>
                            <comment id="15111626" author="stack" created="Thu, 21 Jan 2016 23:46:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=toffer&quot; class=&quot;user-hover&quot; rel=&quot;toffer&quot;&gt;Francis Liu&lt;/a&gt; Sweet&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12717150">HBASE-11267</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12645929" name="HBASE-11165.zip" size="29877" author="apurtell" created="Wed, 21 May 2014 02:54:31 +0000"/>
                            <attachment id="12648064" name="Region Scalability test.pdf" size="127450" author="toffer" created="Tue, 3 Jun 2014 03:33:58 +0000"/>
                            <attachment id="12701804" name="ScalableMeta.pdf" size="218763" author="stack" created="Mon, 2 Mar 2015 06:55:11 +0000"/>
                            <attachment id="12662118" name="zk_less_assignment_comparison_2.pdf" size="46090" author="virag" created="Fri, 15 Aug 2014 18:37:11 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12718058">HBASE-11286</subtask>
                            <subtask id="12718059">HBASE-11287</subtask>
                            <subtask id="12718060">HBASE-11288</subtask>
                            <subtask id="12718061">HBASE-11289</subtask>
                            <subtask id="12718069">HBASE-11290</subtask>
                            <subtask id="12718070">HBASE-11291</subtask>
                            <subtask id="12730639">HBASE-11610</subtask>
                            <subtask id="12710152">HBASE-11059</subtask>
                            <subtask id="12728295">HBASE-11546</subtask>
                            <subtask id="12734137">HBASE-11747</subtask>
                            <subtask id="12733957">HBASE-11740</subtask>
                            <subtask id="12734483">HBASE-11758</subtask>
                            <subtask id="12734489">HBASE-11759</subtask>
                            <subtask id="12747455">HBASE-12233</subtask>
                            <subtask id="12841405">HBASE-13991</subtask>
                            <subtask id="12845303">HBASE-14090</subtask>
                            <subtask id="12986277">HBASE-16169</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 15 May 2014 18:29:41 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>392415</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            47 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1vkvj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>392600</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>