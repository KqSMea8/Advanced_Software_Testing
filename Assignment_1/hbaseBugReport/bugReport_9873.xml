<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:08:23 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-9873/HBASE-9873.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-9873] Some improvements in hlog and hlog split</title>
                <link>https://issues.apache.org/jira/browse/HBASE-9873</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Some improvements in hlog and hlog split&lt;/p&gt;

&lt;p&gt;1) Try to clean old hlog after each memstore flush to avoid unnecessary hlogs split in failover.  Now hlogs cleaning only be run in rolling hlog writer. &lt;/p&gt;

&lt;p&gt;2) Add a background hlog compaction thread to compaction the hlog: remove the hlog entries whose data have been flushed to hfile. The scenario is that in a share cluster, write requests of a table may very little and periodical,  a lots of hlogs can not be cleaned for entries of this table in those hlogs.&lt;/p&gt;

&lt;p&gt;3) Rely on the smallest of all biggest hfile&apos;s seqId of previous served regions to ignore some entries.  Facebook have implemented this in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; and we backport it to hbase 0.94 in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9568&quot; title=&quot;backport HBASE-6508 to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9568&quot;&gt;HBASE-9568&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;4) Support running multiple hlog splitters on a single RS and on master(latter can boost split efficiency for tiny cluster)&lt;/p&gt;

&lt;p&gt;5) Enable multiple splitters on &apos;big&apos; hlog file by splitting(logically) hlog to slices(configurable size, eg hdfs trunk size 64M)&lt;br/&gt;
support concurrent multiple split tasks on a single hlog file slice &lt;/p&gt;

&lt;p&gt;6) Do not cancel the timeout split task until one task reports it succeeds (avoids scenario where split for a hlog file fails due to no one task can succeed within the timeout period ), and and reschedule a same split task to reduce split time ( to avoid some straggler in hlog split)&lt;/p&gt;

&lt;p&gt;7) Consider the hlog data locality when schedule the hlog split task.  Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;

&lt;p&gt;8) Support multi hlog writers and switching to another hlog writer when long write latency to current hlog due to possible temporary network spike? &lt;/p&gt;

&lt;p&gt;This is a draft which lists the improvements about hlog we try to implement in the near future. Comments and discussions are welcomed.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12677018">HBASE-9873</key>
            <summary>Some improvements in hlog and hlog split</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="liushaohui">Liu Shaohui</reporter>
                        <labels>
                            <label>failover</label>
                            <label>hlog</label>
                    </labels>
                <created>Fri, 1 Nov 2013 07:10:23 +0000</created>
                <updated>Sat, 23 Nov 2013 21:19:16 +0000</updated>
                                                                            <component>MTTR</component>
                    <component>wal</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>17</watches>
                                                                <comments>
                            <comment id="13811449" author="stack" created="Fri, 1 Nov 2013 17:01:32 +0000"  >&lt;p&gt;Thanks for looking into important issue &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=liushaohui&quot; class=&quot;user-hover&quot; rel=&quot;liushaohui&quot;&gt;Liu Shaohui&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1) Try to clean old hlog after each memstore flush to avoid unnecessary hlogs split in failover. Now hlogs cleaning only be run in rolling hlog writer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Are we just scheduling more checks?  Is that the idea?  Doing it at flush time is a good idea as juncture for WAL-clean-up.  Do you observe us lagging the cleanup by just doing it on log roll?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;2) Add a background hlog compaction thread to compaction the hlog: remove the hlog entries whose data have been flushed to hfile. The scenario is that in a share cluster, write requests of a table may very little and periodical,  a lots of hlogs can not be cleaned for entries of this table in those hlogs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you think this will help?   You will have to do a bunch of reading and rewriting, right?  You will only rewrite WALs that have at least some percentage of flushed edits?  Would it be better to work on making it so we are better at flushing the memstores that have edits holding up our letting go of old WALs?  Just asking.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;4) Support running multiple hlog splitters on a single RS and on master(latter can boost split efficiency for tiny cluster)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree we need more slots on smaller clusters; most of the time log splitting is just waiting on a slot to open.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt; has opinion on this; his thought is that there would need to be lots of spare i/o on a cluster for more slots to make a difference (I like the idea of master hosting a splitlogworker &amp;#8211; on a small cluster 5 nodes or so, it&apos;d make a significant difference). &lt;/p&gt;

&lt;p&gt;I have also wondered if we can&apos;t speed read/write of splits.  I was studying log splitting a while back and it seemed to run slow on a small cluster.  Chatting w/ &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt;, he suggested that concentrating on speeding up writing of the splits should be where we should concentrate since this is writing out three replicas whereas we are reading from one only.   Is there anything we could do to speed writing/reading of WALs around split time (I was seeing 30-60 seconds to do a 128M WAL).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;5) Enable multiple splitters on &apos;big&apos; hlog file by splitting(logically) hlog to slices(configurable size, eg hdfs trunk size 64M)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;support concurrent multiple split tasks on a single hlog file slice&lt;/p&gt;

&lt;p&gt;You think this one necessary?  If enough slots?  We&apos;ll start to have issues where edits come out of order &amp;#8211; something that we need to address for multiwal case but likely not something that will be fixed in 0.94 (I see FB do this right?)&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;7) Consider the hlog data locality when schedule the hlog split task. Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This would be great.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;8) Support multi hlog writers and switching to another hlog writer when long write latency to current hlog due to possible temporary network spike?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This effort is starting up (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=himanshu%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;himanshu@cloudera.com&quot;&gt;Himanshu Vashishtha&lt;/a&gt; &amp;#8211; you have an issue for the multiwal work?).  Let me add it as a link to this one.&lt;/p&gt;

&lt;p&gt;Does lease recovery work reliably for you fellas? You have &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt;, etc., patched into your hadoop and the &apos;stale&apos; dn detection enabled?   &lt;/p&gt;</comment>
                            <comment id="13811458" author="stack" created="Fri, 1 Nov 2013 17:09:12 +0000"  >&lt;p&gt;Made this critical since its about MTTR&lt;/p&gt;</comment>
                            <comment id="13811575" author="jeffreyz" created="Fri, 1 Nov 2013 19:16:30 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Support running multiple hlog splitters on a single RS&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We can make this configurable(HBase-9736). In many cases, recovery is happening while a cluster is serving live traffic so you normally don&apos;t want recovery traffic to affect other live traffic too much. Making the number of log splitter configurable normally helps when the cluster has free IO capacity(SSD clusters) or in distributedLogReplay mode where no extra small random writes from recovery.edits operations. &lt;/p&gt;

&lt;p&gt;When a WAL splitting takes about 30+ seconds, I guess opening more splitters may have counter effect because log splitting normally slows at writing side and reader idles to wait for writing to finish. Opening more splitter basically add more write load in the cluster so it could even drag current split task.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Try to clean old hlog after each memstore flush to avoid unnecessary hlogs split in failover. Now hlogs cleaning only be run in rolling hlog writer.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I have a different idea in this area: we could be smart on the log cleaning such as we can maintain last flushed sequence number of each region and regions for each wal in memory so a log cleaner can out of order clean a wal instead of checking global smallest flushed sequence number.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;5) Enable multiple splitters on &apos;big&apos; hlog file by splitting(logically) hlog to slices(configurable size, eg hdfs trunk size 64M)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;d wait for our multiple wal solution. Because it basically assumes we have the IO capacity but less worker slots while with the multiple splitter per RS and limiting wal size, the suggestion seems not needed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;7) Consider the hlog data locality when schedule the hlog split task. Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We have a JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt; on this.&lt;/p&gt;

&lt;p&gt;In general, RS failure recovery spends huge percentage in detection time. It&apos;d be better if we can look into that as well.  Thanks.&lt;/p&gt;


</comment>
                            <comment id="13811585" author="v.himanshu" created="Fri, 1 Nov 2013 19:33:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;I have a different idea in this area: we could be smart on the log cleaning such as we can maintain last flushed sequence number of each region and regions for each wal in memory so a log cleaner can out of order clean a wal instead of checking global smallest flushed sequence number.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt;. Yep, I agree. And, this is exactly what the &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8741&quot; title=&quot;Scope sequenceid to the region rather than regionserver (WAS: Mutations on Regions in recovery mode might have same sequenceIDs)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8741&quot;&gt;&lt;del&gt;HBASE-8741&lt;/del&gt;&lt;/a&gt; patch does.&lt;/p&gt;</comment>
                            <comment id="13811602" author="v.himanshu" created="Fri, 1 Nov 2013 19:54:12 +0000"  >&lt;p&gt;1) Try to clean old hlog after each memstore flush to avoid unnecessary hlogs split in failover. Now hlogs cleaning only be run in rolling hlog writer.&lt;br/&gt;
On write heavy clusters, we flush almost every 3-4 seconds. I think having a region-map per WAL and then referring to it at time of roll could get archiving old wals some.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;7) Consider the hlog data locality when schedule the hlog split task. Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, this would help. &lt;br/&gt;
Another thought I had while looking at this code was: Currently, a split log worker just picks log files randomly. I think it would be better to pick older files first, and pick the latest (which most probably, is incomplete) file last. This reduces the chances of a split log worker landing into lease-recovery loop to an extant.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In general, RS failure recovery spends huge percentage in detection time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sorry, which detection phase we are talking about here? &lt;/p&gt;</comment>
                            <comment id="13811616" author="yuzhihong@gmail.com" created="Fri, 1 Nov 2013 20:20:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;then referring to it at time of roll could get archiving old wals some.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you elaborate some more ?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;it would be better to pick older files first, and pick the latest (which most probably, is incomplete) file last.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13811663" author="jeffreyz" created="Fri, 1 Nov 2013 21:09:10 +0000"  >&lt;blockquote&gt;
&lt;p&gt;In general, RS failure recovery spends huge percentage in detection time.&lt;br/&gt;
Sorry, which detection phase we are talking about here?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Because an end-to-end RS failure includes: failed region server detection and recovery. Currently ZK session time out is 30 secs(by default) to detect a RS failure, if we can detect a failure much earlier. For example, if all clients have issues to talk to a RS, a master can close the ZK session of RS and then immediately trigger recovery without waiting for ZK session time out.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Currently, a split log worker just picks log files randomly. I think it would be better to pick older files first, and pick the latest (which most probably, is incomplete) file last. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In other threads, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; and I have an idea to send recover lease calls async to all log files and then put them in ZK one by one after recoverlease is done. Therefore, we don&apos;t need to enforce pick order in SplitLogWorker.&lt;/p&gt;


</comment>
                            <comment id="13812701" author="nkeywal" created="Mon, 4 Nov 2013 09:25:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;2) Add a background hlog compaction thread to compaction the hlog: remove the hlog entries whose data have been flushed to hfile. The scenario is that in a share cluster, write requests of a table may very little and periodical, a lots of hlogs can not be cleaned for entries of this table in those hlogs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;hbase.regionserver.optionalcacheflushinterval can be used to limit the effect of such tables. In my mind, this should be set to something like 10 minutes max, the default (1 hour) is very conservative.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;6) Do not cancel the timeout split task until one task reports it succeeds (avoids scenario where split for a hlog file fails due to no one task can succeed within the timeout period ), and and reschedule a same split task to reduce split time ( to avoid some straggler in hlog split)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That&apos;s not &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;7) Consider the hlog data locality when schedule the hlog split task. Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We have a JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt; on this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve got it partly done, actually. I need to finish it and test it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;8) Support multi hlog writers and switching to another hlog writer when long write latency to current hlog due to possible temporary network spike?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As in the original big table paper you mean? I agree. &lt;/p&gt;</comment>
                            <comment id="13814809" author="liushaohui" created="Wed, 6 Nov 2013 11:39:10 +0000"  >&lt;p&gt;Sorry for late reply.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;~liochon&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;    2) Add a background hlog compaction thread to compaction the hlog: remove the hlog entries whose data have been flushed to hfile. The scenario is that in a share cluster, write requests of a table may very little and periodical, a lots of hlogs can not be cleaned for entries of this table in those hlogs.&lt;/p&gt;

&lt;p&gt;hbase.regionserver.optionalcacheflushinterval can be used to limit the effect of such tables. In my mind, this should be set to something like 10 minutes max, the default (1 hour) is very conservative.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, this config does help. But I think small optionalcacheflushinterval will bring small hfiles, which sacrifices read latency and brings more compacts.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    7) Consider the hlog data locality when schedule the hlog split task. Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;

&lt;p&gt;    We have a JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt; on this.&lt;/p&gt;

&lt;p&gt;I&apos;ve got it partly done, actually. I need to finish it and test it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Great.  I can help to test and review it if needed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    8) Support multi hlog writers and switching to another hlog writer when long write latency to current hlog due to possible temporary network spike?&lt;/p&gt;

&lt;p&gt;As in the original big table paper you mean? I agree. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes.&lt;/p&gt;</comment>
                            <comment id="13814843" author="nkeywal" created="Wed, 6 Nov 2013 12:16:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yes, this config does help. But I think small optionalcacheflushinterval will bring small hfiles, which sacrifices read latency and brings more compacts&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Well, if the writes are &quot;very little and periodical&quot;, it should not be an issue, no?. If we&apos;re speaking about table heavily written, the regions should be flushed by the standard flush.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Great. I can help to test and review it if needed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks a lot, I will send you the patch when it&apos;s ready.&lt;/p&gt;</comment>
                            <comment id="13814844" author="liushaohui" created="Wed, 6 Nov 2013 12:17:07 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;~liochon&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;    6) Do not cancel the timeout split task until one task reports it succeeds (avoids scenario where split for a hlog file fails due to no one task can succeed within the timeout period ), and and reschedule a same split task to reduce split time ( to avoid some straggler in hlog split)&lt;/p&gt;

&lt;p&gt;That&apos;s not &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Part of this suggestion is &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt;.  Actually, we want to intro a speculative scheduler for hlog tasks, as the speculative scheduler for map/reduce tasks in mapreduce. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt; resubmits the task after a configurable timeout, and interrupt the old task. But we want to resubmit it early if there is an idle split worker and we think it may finish split task earlier than the old one.  There is no timeout config and &quot;resubmittion&quot; do not interrupt the old task.&lt;/p&gt;
</comment>
                            <comment id="13814864" author="liushaohui" created="Wed, 6 Nov 2013 13:10:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;    5) Enable multiple splitters on &apos;big&apos; hlog file by splitting(logically) hlog to slices(configurable size, eg hdfs trunk size 64M)&lt;br/&gt;
I&apos;d wait for our multiple wal solution. Because it basically assumes we have the IO capacity but less worker slots while with the multiple splitter per RS and limiting wal size, the suggestion seems not needed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OK. We can consider this suggestion after multiple wal solution&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    7) Consider the hlog data locality when schedule the hlog split task. Schedule the hlog to a splitter which is near to hlog data.&lt;/p&gt;

&lt;p&gt;We have a JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6772&quot; title=&quot;Make the Distributed Split HDFS Location aware&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6772&quot;&gt;HBASE-6772&lt;/a&gt; on this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks a lot. I will follow this JIRA.&lt;/p&gt;</comment>
                            <comment id="13814868" author="liushaohui" created="Wed, 6 Nov 2013 13:21:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;    1) Try to clean old hlog after each memstore flush to avoid unnecessary hlogs split in failover. Now hlogs cleaning only be run in rolling hlog writer.&lt;/p&gt;

&lt;p&gt;Are we just scheduling more checks? Is that the idea? Doing it at flush time is a good idea as juncture for WAL-clean-up. Do you observe us lagging the cleanup by just doing it on log roll?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, It just schedules more checks for old hlogs. &lt;br/&gt;
I will add some logs to check there are hlog lagging cleanups.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    2) Add a background hlog compaction thread to compaction the hlog: remove the hlog entries whose data have been flushed to hfile. The scenario is that in a share cluster, write requests of a table may very little and periodical, a lots of hlogs can not be cleaned for entries of this table in those hlogs.&lt;/p&gt;

&lt;p&gt;Do you think this will help? You will have to do a bunch of reading and rewriting, right? You will only rewrite WALs that have at least some percentage of flushed edits? Would it be better to work on making it so we are better at flushing the memstores that have edits holding up our letting go of old WALs? Just asking.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, exactly.&lt;/p&gt;</comment>
                            <comment id="13814875" author="liushaohui" created="Wed, 6 Nov 2013 13:30:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;~liochon&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;3) Rely on the smallest of all biggest hfile&apos;s seqId of previous served regions to ignore some entries. Facebook have implemented this in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; and we backport it to hbase 0.94 in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9568&quot; title=&quot;backport HBASE-6508 to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9568&quot;&gt;HBASE-9568&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What about this? I think &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; is useful. &lt;br/&gt;
Could any one help to review &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9568&quot; title=&quot;backport HBASE-6508 to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9568&quot;&gt;HBASE-9568&lt;/a&gt;(The backport of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; to 0.94) ? &lt;br/&gt;
We may backport &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; to trunk later.&lt;/p&gt;</comment>
                            <comment id="13815039" author="nkeywal" created="Wed, 6 Nov 2013 17:07:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;Actually, we want to intro a speculative scheduler for hlog tasks, as the speculative scheduler for map/reduce tasks in mapreduce.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Note that there is a new algo implemented in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7006&quot; title=&quot;[MTTR] Improve Region Server Recovery Time - Distributed Log Replay&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7006&quot;&gt;&lt;del&gt;HBASE-7006&lt;/del&gt;&lt;/a&gt;, allows to have writes during the recovery. This algo is not really suitable for speculative execution, because the writes are always executed on the same machines, so adding executions would likely slow down the process. Ok that&apos;s not for 0.94&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Rely on the smallest of all biggest hfile&apos;s seqId of previous served regions to ignore some entries. Facebook have implemented this in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; and we backport it to hbase 0.94 in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9568&quot; title=&quot;backport HBASE-6508 to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9568&quot;&gt;HBASE-9568&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep, this would be useful for sure (my understanding is that 0.96+ has it)&lt;/p&gt;</comment>
                            <comment id="13815593" author="liushaohui" created="Thu, 7 Nov 2013 03:02:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;    Actually, we want to intro a speculative scheduler for hlog tasks, as the speculative scheduler for map/reduce tasks in mapreduce.&lt;/p&gt;

&lt;p&gt;Note that there is a new algo implemented in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7006&quot; title=&quot;[MTTR] Improve Region Server Recovery Time - Distributed Log Replay&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7006&quot;&gt;&lt;del&gt;HBASE-7006&lt;/del&gt;&lt;/a&gt;, allows to have writes during the recovery. This algo is not really suitable for speculative execution, because the writes are always executed on the same machines, so adding executions would likely slow down the process. Ok that&apos;s not for 0.94&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I will take a deep look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7006&quot; title=&quot;[MTTR] Improve Region Server Recovery Time - Distributed Log Replay&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7006&quot;&gt;&lt;del&gt;HBASE-7006&lt;/del&gt;&lt;/a&gt; first. Thanks.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    Rely on the smallest of all biggest hfile&apos;s seqId of previous served regions to ignore some entries. Facebook have implemented this in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6508&quot; title=&quot;[0.89-fb] Filter out edits at log split time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6508&quot;&gt;&lt;del&gt;HBASE-6508&lt;/del&gt;&lt;/a&gt; and we backport it to hbase 0.94 in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9568&quot; title=&quot;backport HBASE-6508 to 0.94&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9568&quot;&gt;HBASE-9568&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Yep, this would be useful for sure (my understanding is that 0.96+ has it)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8573&quot; title=&quot;Store last flushed sequence id for each store of region for Distributed Log Replay&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8573&quot;&gt;&lt;del&gt;HBASE-8573&lt;/del&gt;&lt;/a&gt; has done it in 0.96. Sorry for not noticing it. &lt;br/&gt;
As many companies still use 0.94, I think backporting is needed.&lt;/p&gt;
</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 1 Nov 2013 17:01:32 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>356394</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 6 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1pfun:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>356682</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>