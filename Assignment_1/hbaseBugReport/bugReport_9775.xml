<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:07:28 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-9775/HBASE-9775.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-9775] Client write path perf issues</title>
                <link>https://issues.apache.org/jira/browse/HBASE-9775</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Testing on larger clusters has not had the desired throughput increases.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12674011">HBASE-9775</key>
            <summary>Client write path perf issues</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="eclark">Elliott Clark</reporter>
                        <labels>
                    </labels>
                <created>Tue, 15 Oct 2013 23:17:50 +0000</created>
                <updated>Mon, 11 Nov 2013 21:29:47 +0000</updated>
                                            <version>0.96.0</version>
                                                    <component>Client</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>17</watches>
                                                                                                            <comments>
                            <comment id="13795787" author="eclark" created="Tue, 15 Oct 2013 23:20:04 +0000"  >&lt;p&gt;Here&apos;s what I&apos;m seeing on a largish test cluster.  77 Nodes  (the pinkish red) has about the same throughput as the 31 node cluster (blue).  They are the exact same machines, I just took away two racks.&lt;/p&gt;

&lt;p&gt;This is YCSB loads.&lt;/p&gt;</comment>
                            <comment id="13796250" author="stack" created="Wed, 16 Oct 2013 00:30:51 +0000"  >&lt;p&gt;What you suspect the issue Mr &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt;?  Client.  The way we are batching up edits in client and sending them across?&lt;/p&gt;</comment>
                            <comment id="13796264" author="eclark" created="Wed, 16 Oct 2013 00:47:20 +0000"  >&lt;p&gt;Here&apos;s the network.  The client barely reaches 1.2 gbit of a possible 10 gbit.&lt;/p&gt;

&lt;p&gt;We&apos;re also seeing a pretty huge slow down vs 0.94.  Something in the order of 15% slower on the write path (the write path is better or the same).&lt;/p&gt;

&lt;p&gt;So I&apos;m thinking that it&apos;s got to be something in the AsyncProcess code.  Batching seems suspect but I&apos;m running more tests to check.  For me that huge a pref loss and a scaling loss are pretty huge issues.&lt;/p&gt;</comment>
                            <comment id="13796294" author="eclark" created="Wed, 16 Oct 2013 01:25:23 +0000"  >&lt;p&gt;YCSB with 33 nodes fails completely. with:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
com.yahoo.ycsb.DBException: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 8608 actions: IOException: 8608 times, 
	at com.yahoo.ycsb.db.HBaseClient.cleanup(HBaseClient.java:113)
	at com.yahoo.ycsb.DBWrapper.cleanup(DBWrapper.java:73)
	at com.yahoo.ycsb.ClientThread.run(Client.java:307)
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 8608 actions: IOException: 8608 times, 
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:185)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$500(AsyncProcess.java:169)
	at org.apache.hadoop.hbase.client.AsyncProcess.getErrors(AsyncProcess.java:782)
	at org.apache.hadoop.hbase.client.HTable.backgroundFlushCommits(HTable.java:934)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1193)
	at com.yahoo.ycsb.db.HBaseClient.cleanup(HBaseClient.java:108)
	... 2 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;running:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
#!/bin/bash
OPERTATIONS=20000000
RECORDS=$(($OPERTATIONS*10))

echo &lt;span class=&quot;code-quote&quot;&gt;&quot;disable &apos;usertable&apos;; drop &apos;usertable&apos;&quot;&lt;/span&gt; | hbase shell
echo &lt;span class=&quot;code-quote&quot;&gt;&quot;create &apos;usertable&apos;,{NAME =&amp;gt; &apos;d&apos;, VERSIONS =&amp;gt; 1,COMPRESSION =&amp;gt; &apos;lz4&apos;, BLOCKCACHE =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;}&quot;&lt;/span&gt; | hbase shell
echo &lt;span class=&quot;code-quote&quot;&gt;&quot;balancer&quot;&lt;/span&gt; | hbase shell
sleep 10
cd /home/eclark/RC5/ycsb/
bin/ycsb load hbase -P workloads/workloada -p columnfamily=d -s -threads 32 -p recordcount=$RECORDS 2&amp;gt; /tmp/ycsb-workload-a-load-output.log
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;I&apos;ve also seen the same stack and exception when trying to run ITBLL on this cluster.&lt;/p&gt;</comment>
                            <comment id="13796798" author="nkeywal" created="Wed, 16 Oct 2013 13:49:23 +0000"  >&lt;p&gt;On the perfs, beyond possible bugs, I see:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;with a max.total.tasks of 100 and max.perserver.tasks of 5, the client might not use all the server. May be a default of 2 for max.perserver.tasks would be better&lt;/li&gt;
	&lt;li&gt;with 32 clients and 77 servers, the client can have 32*77 threads (~2400) but in 0.96 we have a default limit of 256. May be we should increase it to something like 1024? We&apos;re not async on the client side, so...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For the failure, I don&quot;t know. Something I&apos;m not totally sure about is &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9467&quot; title=&quot;write can be totally blocked temporarily by a write-heavy region&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9467&quot;&gt;&lt;del&gt;HBASE-9467&lt;/del&gt;&lt;/a&gt;: we&apos;re now rejecting the client when we&apos;re busy, instead of keeping it in the queue. A possible side effect when the system is heavily loaded with a lot of client is that the rejected client is always unlucky, and its next attempts are rejected as well: wedon&apos;t have any priority between the new callers and the old callers that were rejected.&lt;/p&gt;</comment>
                            <comment id="13796938" author="eclark" created="Wed, 16 Oct 2013 16:23:30 +0000"  >&lt;p&gt;I ran a 94 vs 96 comparison.  Here are the results.  You can see that 0.94 handily beats 96 until compaction become the limiting factor.&lt;/p&gt;</comment>
                            <comment id="13796947" author="eclark" created="Wed, 16 Oct 2013 16:34:21 +0000"  >&lt;p&gt;For me the ITBLL failures (with no chaos monkey) are the most concerning.   I would pretty much consider this a blocker.  If we can&apos;t scale up correctly then 0.96 isn&apos;t deployable.&lt;/p&gt;

&lt;p&gt;I&apos;ll deploy with the max.perserver.tasks set to 2 and see what the results are like.  &lt;/p&gt;

&lt;p&gt;1024 threads on a client just seems excessive especially when there are already exceptions about regions being overloaded.&lt;/p&gt;</comment>
                            <comment id="13796955" author="nkeywal" created="Wed, 16 Oct 2013 16:39:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;1024 threads on a client just seems excessive&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I fully agree, but do you know how many threads you have on the 0.94 during the test? we&apos;re not bounded there iirc, so as the workload is random i would expect all batch calls to go on all servers.&lt;/p&gt;</comment>
                            <comment id="13796959" author="nkeywal" created="Wed, 16 Oct 2013 16:42:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;For me the ITBLL failures (with no chaos monkey) are the most concerning.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Could you share the logs for this one?&lt;/p&gt;</comment>
                            <comment id="13797011" author="eclark" created="Wed, 16 Oct 2013 17:16:18 +0000"  >&lt;p&gt;Sure starting an ITBLL run right now.&lt;/p&gt;</comment>
                            <comment id="13797102" author="eclark" created="Wed, 16 Oct 2013 18:39:55 +0000"  >&lt;p&gt;here&apos;s the logs.  The RS&apos;s are running G1GC so there should be no issues with GC pausing.&lt;/p&gt;

&lt;p&gt;I&apos;m seeing this as the pause times:&lt;br/&gt;
2013-10-16T10:19:23.182-0700: &lt;span class=&quot;error&quot;&gt;&amp;#91;GC pause (young), 0.10152600 secs&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;All of the boxes are on 10 gig.&lt;/p&gt;

&lt;p&gt;I ran:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hbase org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList --monkey calm Loop 2 154 25000000 IntegrationTestBigLinkedList 77 &amp;gt; job_run.log  2&amp;gt;&amp;amp;1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So there should be 2 clients per region server.&lt;/p&gt;</comment>
                            <comment id="13797104" author="eclark" created="Wed, 16 Oct 2013 18:40:22 +0000"  >&lt;p&gt;Here&apos;s what the network looked like at the time.&lt;/p&gt;</comment>
                            <comment id="13797116" author="ndimiduk" created="Wed, 16 Oct 2013 18:48:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;The RS&apos;s are running G1GC so there should be no issues with GC pausing&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Any chance of running with old CMS settings? Let&apos;s eliminate as many variables as we can.&lt;/p&gt;

&lt;p&gt;Along those lines, is there we can isolate the impact of other major changes, such as old vs new RPC?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I fully agree, but do you know how many threads you have on the 0.94 during the test? we&apos;re not bounded there iirc, so as the workload is random i would expect all batch calls to go on all servers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m also curious about these numbers. What we should be interested in here are the active threads sending data (vs. those from 94 holding open/pending connections and in 96 which are in their RTBE retry loop)&lt;/p&gt;</comment>
                            <comment id="13797124" author="eclark" created="Wed, 16 Oct 2013 18:54:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any chance of running with old CMS settings? Let&apos;s eliminate as many variables as we can.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is an exact replica of the settings that we&apos;re using on our IT test rig.  Same heap space, same gc, and the same number of map tasks per server.  The only difference between the clusters is that our IT test cluster has 6 region servers this has 77.&lt;/p&gt;
</comment>
                            <comment id="13797182" author="nkeywal" created="Wed, 16 Oct 2013 19:55:35 +0000"  >&lt;p&gt;Thanks, Elliot.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So there should be 2 clients per region server.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That something that would work fine with the 0.94 out of the box, right?&lt;/p&gt;

&lt;p&gt;Is there anything on the server that could explain the server timeout (SocketTimeoutException)?&lt;/p&gt;

&lt;p&gt;With 150 clients, and a client being able to send 2 queries per server, a server can receive 300 queries simultaneously.&lt;br/&gt;
On average it should be less: a client can have only 100 tasks, so it will be 200 (but it&apos;s an average: a server can be unlucky and receives these 300 requests). The limit on the threads don&apos;t hold here: there should be less then 250 threads per client.&lt;/p&gt;

&lt;p&gt;Here are the differences I see between the 0.94 and 0.96 that could be related. I may be wrong, I&apos;m not sure about all backports.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;with the settings above, a server would have received 150 queries max (1 per client), instead of 300 now worse, 150 average.&lt;/li&gt;
	&lt;li&gt;the server reject the client when it&apos;s busy (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9467&quot; title=&quot;write can be totally blocked temporarily by a write-heavy region&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9467&quot;&gt;&lt;del&gt;HBASE-9467&lt;/del&gt;&lt;/a&gt;). That increases the number of retries to do, and, on an heavy load, can lead us to fail on something that would have worked before.&lt;/li&gt;
	&lt;li&gt;we&apos;re much more aggressive on the time before retrying (100ms vs 1000ms), the backoff is different. It was  
{ 1, 1, 1, 2, 2, 4, 4, 8, 16, 32, 64 }, it&apos;s now { 1, 2, 3, 5, 10, 100 }. The number of retries was 10 it&apos;s now 31. But we increase the server load as we&apos;re retrying more aggressively. For example, the new settings will make the client to send 4 queries in 1 second when they fail. If the servers can handle the load, it&apos;s great. If there are 150 clients like this may be not.&lt;br/&gt;
 - we now stop after ~5 minutes (calculated from the number of retries &amp;amp; back off time), this whatever the number of retries actually made. I&apos;m not sure that the point here (I would need the debug logs to know), but I&apos;ve seen it on this tests on other  clusters (we were not doing all the retries).&lt;br/&gt;
&lt;br/&gt;
Is there anything that I forgot?&lt;br/&gt;
&lt;br/&gt;
If we want to compare 0.94 and 0.96, may be we should use the same settings, i.e.&lt;br/&gt;
pause: 1000ms&lt;br/&gt;
backoff:  { 1, 1, 1, 2, 2, 4, 4, 8, 16, 32, 64 }
&lt;p&gt;hbase.client.max.perserver.tasks: 1&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This does not match exactly (the 0.96 will still send more tasks at peaks, as it will always sends data to all servers for example, and there is still the time limit and the effect of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9467&quot; title=&quot;write can be totally blocked temporarily by a write-heavy region&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9467&quot;&gt;&lt;del&gt;HBASE-9467&lt;/del&gt;&lt;/a&gt; that makes me more comfortable with more retries), but we&apos;re not too far hopefully. We can use hbase.client.max.total.tasks if we need to control the clients more.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure it should be the default (at least for the backoff, the strategy was to improve latency vs. server load). But it could be recommended for upgrade and/or map reduce tasks.&lt;/p&gt;

&lt;p&gt;Lastly, what&apos;s the configuration of the box?&lt;/p&gt;</comment>
                            <comment id="13797227" author="sershe" created="Wed, 16 Oct 2013 20:32:12 +0000"  >&lt;p&gt;Some comment on the retry time limit, we may need to fix it.&lt;br/&gt;
It was introduced for server-specific retry fallback, which I hope is not broken by recent changes to HCM. That is the logic where we go to one server, retry, wait, retry, wait more, retry, wait more, then we learn that region went to different server. Here, we don&apos;t need to wait, because we can assume by default the different server is healthy; but the old code would carry on with wait sequence.&lt;br/&gt;
However, if region moves around (which is common in aggressive CM IT tests), retry count can quickly be exhausted as we go to each new server a few times and never reach higher multipliers. It was especially pronounced w/10 retries, where some request could fail in just a few seconds in case of double server failure where region is recovered twice; w/31-35 now it&apos;s probably less pronounced but still possible.&lt;br/&gt;
So, the time limit based on original retries is supposed to prevent these fast failures, by allowing the retries to go on for as long as we would have retried &quot;as if&quot; we were just using the multiplier sequence to its &quot;full potential&quot;.&lt;br/&gt;
It should not serve as lower limit, we might want to change code to check that both time AND count are exhaused, in this case.&lt;br/&gt;
Do you want me to file a bug?&lt;/p&gt;

&lt;p&gt;Btw, I filed a jira to remove retry count altogether and just have time limit, because from user perspective retry count doesn&apos;t make any sense, &quot;desired&quot; time between retries can be zero when region moved, or large when region is being opened and we just have to wait. User cannot predict that; he should just specify acceptable retry time for the cluster and/or each request. Perhaps we can do that in 98?&lt;/p&gt;</comment>
                            <comment id="13797251" author="enis" created="Wed, 16 Oct 2013 21:02:15 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;deleted comment, meant for HBASE-5487&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="13797263" author="nkeywal" created="Wed, 16 Oct 2013 21:12:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you want me to file a bug?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes please. It would be great to have a test case, then we will be sure that it won&apos;t be broken later.&lt;/p&gt;</comment>
                            <comment id="13798685" author="jeffreyz" created="Fri, 18 Oct 2013 01:01:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; Have you seen some region servers have much high incoming request than others in your test? Thanks.&lt;/p&gt;</comment>
                            <comment id="13798695" author="eclark" created="Fri, 18 Oct 2013 01:22:43 +0000"  >&lt;p&gt;Nope all region servers are getting about the same rate.&lt;/p&gt;</comment>
                            <comment id="13798765" author="jeffreyz" created="Fri, 18 Oct 2013 04:03:40 +0000"  >&lt;p&gt;I think I found one bug in the AsyncProcess hurts performance. Below is the code snippet:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      incTaskCounters(multiAction.getRegions(), loc.getServerName());
      &lt;span class=&quot;code-object&quot;&gt;Runnable&lt;/span&gt; runnable = Trace.wrap(&lt;span class=&quot;code-quote&quot;&gt;&quot;AsyncProcess.sendMultiAction&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Runnable&lt;/span&gt;() {
            ....
            receiveMultiAction(initialActions, multiAction, loc, res, numAttempt, errorsByServer);
          } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
            decTaskCounters(multiAction.getRegions(), loc.getServerName());
          }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Because receiveMultiAction use recursive way to resubmit failure edits. Therefore, we double bump up the TaskCounter when error happens and the overlap timing is a retry internal which is quite long time for client operations.&lt;/p&gt;

&lt;p&gt;I attached a patch for your reference.&lt;/p&gt;</comment>
                            <comment id="13799187" author="nkeywal" created="Fri, 18 Oct 2013 15:19:45 +0000"  >&lt;p&gt;On failure, receiveMultiAction finishes when it submit another task, so it&apos;s not really recursive. It is recursive if we receive an exception rather than a response; but this path is actually not used (I&apos;ve got a fix for this, but I prefer to wait for the post RC).&lt;br/&gt;
I&apos;ve done a small test with 3 nodes, kill-9ing nodes, the number of task stayed as expected.&lt;/p&gt;</comment>
                            <comment id="13799222" author="jeffreyz" created="Fri, 18 Oct 2013 15:54:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; You&apos;re right. It&apos;s not real recursive. The task counter double bump up time is very short depends on how long the runnable are resubmitted. &lt;br/&gt;
While we still have issues, when there are failures which will stay in the retry queue and still use task counters even during the sleep interval for retry. In Elliot testis, there exists retry failures so I think that blocked some new edits to be sent out. In the posted patch, we only bump up task counter during the time when tasks are being sent out and waiting for response.&lt;/p&gt;</comment>
                            <comment id="13799503" author="nkeywal" created="Fri, 18 Oct 2013 20:52:28 +0000"  >&lt;p&gt;I&apos;ve done various tests, on a much smaller cluster than Elliott&apos;s.&lt;br/&gt;
I observed better write performances on the 0.96 than 0.94, by about 20% when inserting 100m of rows from an empty cluster.  There are  around 18 regions at this stage IIRC, so the cluster size should not matter that much when we start from an empty table. I&apos;ve inserted around 1b w/o issue on 0.96.&lt;/p&gt;

&lt;p&gt;I haven&apos;t compared the number of thread. What I see in .96 is that the actual limit is the limit per region: there is one thread per client and per server. Once one multi operation on this server is done, another starts. For this reason, there are little operations on multiple server: they are not synchronized. In theory this gives better performances; Elliott tests says the opposite, at least at large scales. At least, I&apos;ve seen that adding a YCSB client increase the throughput. It would not be the case if the client was maxing the cluster or client physical capacity. As well, increasing the max per region helped (by about the same ratio: 50%). So there is for sure room for improvement here.&lt;/p&gt;

&lt;p&gt;I will do the comparison with 0.94 beginning of next week for these points (#thread,  impact of more clients).  I will as well look at the pure CPU performances of the client. From the tests so far it seems that we can play with the limits parameters to increase / limit the throughput. This does not explain the ITBLL failure at all.&lt;/p&gt;

&lt;p&gt;BTW, I observed better performances when having 2 YCSB instances vs. a single YCSB with 2 threads. I&apos;ve seen this as well with the .96 before the AsyncProcess implementation. On a 10 nodes cluster the difference was 30%. I&apos;ve never done this test w/ the .94.&lt;/p&gt;

&lt;p&gt;For the ITBLL I would be interested to see the servers &amp;amp; client logs. The SocketTimeoutException was strange.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; It would be great if you could redo the same tests as the ones you&apos;ve done a while ago on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6295&quot; title=&quot;Possible performance improvement in client batch operations: presplit and send in background&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6295&quot;&gt;&lt;del&gt;HBASE-6295&lt;/del&gt;&lt;/a&gt;: it could help to see if we have a regression of if it&apos;s only a matter a medium / large cluster... &lt;/p&gt;</comment>
                            <comment id="13799583" author="eclark" created="Fri, 18 Oct 2013 22:14:24 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I observed better write performances on the 0.96 than 0.94, by about 20% when inserting 100m of rows from an empty cluster. There are around 18 regions at this stage IIRC, so the cluster size should not matter that much when we start from an empty table. I&apos;ve inserted around 1b w/o issue on 0.96.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Our performance team independently ran some ycsb tests vs HBase 0.94.6. Here&apos;s the graph that they generated.  Blue is 94.  Orange is 0.96.&lt;/p&gt;

&lt;p&gt;X axis is target throughput&lt;br/&gt;
Y axis is actual throughput&lt;/p&gt;</comment>
                            <comment id="13799671" author="jmspaggi" created="Fri, 18 Oct 2013 23:59:31 +0000"  >&lt;p&gt;Sure I can! Do you have a patch for 0.94? Or only trunk so far?&lt;/p&gt;</comment>
                            <comment id="13799681" author="eclark" created="Sat, 19 Oct 2013 00:17:57 +0000"  >&lt;p&gt;Here&apos;s what I&apos;m seeing in the logs.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
13-10-18 17:14:02,641 WARN  [RpcServer.responder] ipc.RpcServer: Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; changing ops : java.nio.channels.CancelledKeyException
2013-10-18 17:14:02,642 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {&lt;span class=&quot;code-quote&quot;&gt;&quot;processingtimems&quot;&lt;/span&gt;:13904,&lt;span class=&quot;code-quote&quot;&gt;&quot;call&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;client&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;10.20.130.107:39862&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;starttimems&quot;&lt;/span&gt;:1382141628563,&lt;span class=&quot;code-quote&quot;&gt;&quot;queuetimems&quot;&lt;/span&gt;:30966,&lt;span class=&quot;code-quote&quot;&gt;&quot;class&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;HRegionServer&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;responsesize&quot;&lt;/span&gt;:3330303,&lt;span class=&quot;code-quote&quot;&gt;&quot;method&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Multi&quot;&lt;/span&gt;}
2013-10-18 17:14:03,576 INFO  [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-14] regionserver.HStore: Added hdfs:&lt;span class=&quot;code-comment&quot;&gt;//e1101.halxg.cloudera.com:8020/hbase/data/&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;/IntegrationTestBigLinkedList/663b4cb3fa350cd61c9f3317eb6c577a/meta/dcec7a6ef36e437da110aef12b507d72, entries=1271646, sequenceid=2223534, filesize=94.8 M
&lt;/span&gt;2013-10-18 17:14:03,693 INFO  [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-14] regionserver.HRegion: Finished memstore flush of ~262.0 M/274675536, currentsize=0/0 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region IntegrationTestBigLinkedList,\xB6\x1B\xE0|0+!\xA1\xC0\xB5\x0F\xAB\x85\x9C\xA6\xB0,1382141290187.663b4cb3fa350cd61c9f3317eb6c577a. in 198100ms, sequenceid=2223534, compaction requested=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
2013-10-18 17:14:03,746 DEBUG [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-14] regionserver.HStore: Cannot split region due to reference files being there
2013-10-18 17:14:03,789 DEBUG [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-14] regionserver.HRegion: Started memstore flush &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; IntegrationTestBigLinkedList,\xB4\x1D\xFF\x1A,1382141290187.2845b99ca6c5c8c86f150b1cb17b7063., current region memstore size 264.7 M
2013-10-18 17:14:10,220 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {&lt;span class=&quot;code-quote&quot;&gt;&quot;processingtimems&quot;&lt;/span&gt;:10663,&lt;span class=&quot;code-quote&quot;&gt;&quot;call&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;client&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;10.20.132.117:35031&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;starttimems&quot;&lt;/span&gt;:1382141639521,&lt;span class=&quot;code-quote&quot;&gt;&quot;queuetimems&quot;&lt;/span&gt;:31152,&lt;span class=&quot;code-quote&quot;&gt;&quot;class&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;HRegionServer&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;responsesize&quot;&lt;/span&gt;:1596044,&lt;span class=&quot;code-quote&quot;&gt;&quot;method&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;Multi&quot;&lt;/span&gt;}
2013-10-18 17:14:11,777 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1023ms
GC pool &apos;ParNew&apos; had collection(s): count=1 time=1102ms
2013-10-18 17:14:14,620 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1300ms
GC pool &apos;ParNew&apos; had collection(s): count=1 time=1383ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13799687" author="eclark" created="Sat, 19 Oct 2013 00:24:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; One of the things you asked for earlier was running this with CMS.  Just did it with a much smaller heap (19g vs 29.9g) it still fails at about the same place, though it takes longer to get to the same spot.&lt;/p&gt;</comment>
                            <comment id="13799705" author="jeffreyz" created="Sat, 19 Oct 2013 00:38:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; Is the client config setting &quot;hbase.client.pause&quot; as default 100ms? 0.96 retry is much more aggressively than 0.94 such as the longest retry interval is 1/6 of that of 0.94. If using 200ms seems not overwhelm RS. This retries may combined the factor I mentioned above that failure retries still take task quota may hurt performance significantly.  &lt;/p&gt;</comment>
                            <comment id="13799715" author="eclark" created="Sat, 19 Oct 2013 01:03:54 +0000"  >&lt;p&gt;Yep almost all settings are default.&lt;/p&gt;</comment>
                            <comment id="13799926" author="nkeywal" created="Sat, 19 Oct 2013 15:40:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;Sure I can! Do you have a patch for 0.94? Or only trunk so far?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I would love to see how 96 RC 5 behaves on the test suite you ran a few months ago on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6295&quot; title=&quot;Possible performance improvement in client batch operations: presplit and send in background&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6295&quot;&gt;&lt;del&gt;HBASE-6295&lt;/del&gt;&lt;/a&gt;, as well as 0.94.12: when you ran the tests back them, we were seeing a 50% perf improvement on the write path. I had something similar on a ~10 nodes cluster. Doing the test again would help to see if we have a perf problem or a scalability problem.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Nicolas Liochon One of the things you asked for earlier was running this with CMS. Just did it with a much smaller heap (19g vs 29.9g) it still fails at about the same place, though it takes longer to get to the same spot.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It was actually Nick who asked, but the result is very interesting. As well, in the logs you have ~1s stop-the-world if I understand correctly? Our defaults sockets  timeouts are usually around 30 th seconds, but I wonder if we don&apos;t have here the impact of hbase.rpc.timeout (default to 60s): we lower the timeout during the retries iirc. If it&apos;s possible, I would love to see the full logs of one RS. The responseTooSlow is strange too compared to the stop-the-world time.&lt;/p&gt;</comment>
                            <comment id="13800320" author="jmspaggi" created="Mon, 21 Oct 2013 02:34:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; to make sure I understand, I just run my test suite again 0.96.0 RC5 and 0.94.12? I don&apos;t apply any patch on top of that? I can do that for sure. I have a server which is doing nothing, just waiting for that &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Also, next week-end I will be able to run the same test on a 4 nodes cluster if required. Will wait for your confirmation to start.&lt;/p&gt;</comment>
                            <comment id="13800672" author="nkeywal" created="Mon, 21 Oct 2013 14:04:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;I just run my test suite again 0.96.0 RC5 and 0.94.12?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;yes, if possible on the same HW as the one used for the tests done on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6295&quot; title=&quot;Possible performance improvement in client batch operations: presplit and send in background&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6295&quot;&gt;&lt;del&gt;HBASE-6295&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13800703" author="jmspaggi" created="Mon, 21 Oct 2013 14:47:50 +0000"  >&lt;p&gt;Hum. I still have all those servers since all have the same CPU, motherboard and memory (brand, model, etc.) but I changed from SATA to SSD on the one I used for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6295&quot; title=&quot;Possible performance improvement in client batch operations: presplit and send in background&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6295&quot;&gt;&lt;del&gt;HBASE-6295&lt;/del&gt;&lt;/a&gt; and I don&apos;t recall if I did that before the tests, or after &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; So I don&apos;t think we will be able to compare the results.&lt;/p&gt;

&lt;p&gt;I can add a 3rd version to be run if you want. Just tell me which revision number and I will run all the tests on the 3 versions.&lt;/p&gt;

&lt;p&gt;As soon as I get your confirmation I start all of that.&lt;/p&gt;</comment>
                            <comment id="13801900" author="nkeywal" created="Tue, 22 Oct 2013 15:16:49 +0000"  >&lt;p&gt;Hum, the tests were on the 21/Jun/13 but we didn&apos;t write the exact revision.... May be we should try the 96.RC0 as well to see if one of our recent fix broke something?&lt;/p&gt;</comment>
                            <comment id="13802133" author="stack" created="Tue, 22 Oct 2013 18:56:38 +0000"  >&lt;p&gt;We lost our big rig for a while.  Can we rig a harness so we can inspect the client in isolation?&lt;/p&gt;</comment>
                            <comment id="13808759" author="stack" created="Wed, 30 Oct 2013 05:17:07 +0000"  >&lt;p&gt;I&apos;m trying to write a rig that the client can run in so we can inspect it.  Attached is a bit of code that mocks a cluster of 1k servers and 100k regions.  Currently it runs w/o throwing exceptions of failures.&lt;/p&gt;

&lt;p&gt;When I put it under the profiler, we spin up 7 threads and that seems to keep us running nicely; we never go beyond 7.&lt;/p&gt;

&lt;p&gt;If I add some friction by adding pause to the mock Put handler so it &quot;takes time&quot; to process the puts, thread count spins up and tops out at 100 which looks like it is: AsyncProcess#maxTotalConcurrentTasks whose config is hbase.client.max.total.tasks.&lt;/p&gt;

&lt;p&gt;I suppose I should randomize up the way I put &amp;#8211; it is sort of ordered at the moment but even then, it looks like I&apos;d be doing 1/10th of the servers at a time.&lt;/p&gt;

&lt;p&gt;Let me update and see what the &lt;span class=&quot;error&quot;&gt;&amp;#91;~liochon&amp;#93;&lt;/span&gt; recent changes do.&lt;/p&gt;</comment>
                            <comment id="13808789" author="stack" created="Wed, 30 Oct 2013 06:26:33 +0000"  >&lt;p&gt;Rebase for 0.96.&lt;/p&gt;

&lt;p&gt;You just run the main on TestClientNoCluster.&lt;/p&gt;

&lt;p&gt;After updating, no noticeable difference.  We run up to 100 threads and stay there w/ near all in wait mode.&lt;/p&gt;</comment>
                            <comment id="13814650" author="stack" created="Wed, 6 Nov 2013 06:23:02 +0000"  >&lt;p&gt;v3 dumps mockito.  Mockito keeps references to each invocation so can keep running counts.  I could not figure how to disable this facility.  Patch is better w/o it anyways.&lt;/p&gt;

&lt;p&gt;v3 no longer has heap issues, at least at current &apos;scales&apos;.&lt;/p&gt;

&lt;p&gt;The patch as is is configured to do the inverse of the previous patch.  Now I have a single &apos;server&apos; and I have ten clients beating up on it.  It doesn&apos;t take long for the clients to &apos;overrun&apos; the server.  The server cannot respond in time so we just keep throwing RegionBusyException more and more frequently &amp;#8211; which simulates I think what E was seeing on the &apos;big&apos; cluster.&lt;/p&gt;

&lt;p&gt;Will dig in tomorrow on what we can do when RBE &amp;#8211; how to better back off (Elliott had ideas in here).&lt;/p&gt;</comment>
                            <comment id="13815284" author="nkeywal" created="Wed, 6 Nov 2013 20:59:51 +0000"  >&lt;p&gt;Seems ok. The patch touches many different parts...&lt;/p&gt;

&lt;p&gt;For stuff like:&lt;br/&gt;
+    builder.setMutation(ProtobufUtil.toMutation(MutationType.PUT, put, MutationProto.newBuilder()));&lt;/p&gt;

&lt;p&gt;I&apos;ve tried it myself (exactly the same approach), but I didn&apos;t see a real difference. Do you see one in your tests?&lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13815341" author="stack" created="Wed, 6 Nov 2013 21:59:44 +0000"  >&lt;p&gt;bq, I&apos;ve tried it myself (exactly the same approach), but I didn&apos;t see a real difference. Do you see one in your tests?&lt;/p&gt;

&lt;p&gt;Minor (Certain allocation hotspots went from 3% to 2.4% in my extreme allocation test which probably means close to zero diff).  I left it in since on the face of it there are less allocations.&lt;/p&gt;

&lt;p&gt;I&apos;ll commit this since the rig can be useful.  Want to do some comment/javadoc first though. &lt;/p&gt;</comment>
                            <comment id="13815493" author="stack" created="Thu, 7 Nov 2013 00:26:14 +0000"  >&lt;p&gt;Back to the root discussion on this issue:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;with a max.total.tasks of 100 and max.perserver.tasks of 5, the client might not use all the server. May be a default of 2 for max.perserver.tasks would be better&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;ll work if many servers right but will be a constraint if only a few servers and a few clients. In that we will only schedule two tasks at most to each server when it could take much more.&lt;/p&gt;

&lt;p&gt;Ideally we want something like what you had before &amp;#8211; 5 or 1/2 the CPUs on the local server as guesstimate of how many CPUs the server has, which ever is greater-- and then soon as we get indications that server is struggling, go down from this max per server and slowly ramp back up as we have successful ops against said server (How drastic the drop in tasks-per-server should be would depend on the exception we&apos;d gotten from the server).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the server reject the client when it&apos;s busy (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9467&quot; title=&quot;write can be totally blocked temporarily by a write-heavy region&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9467&quot;&gt;&lt;del&gt;HBASE-9467&lt;/del&gt;&lt;/a&gt;). That increases the number of retries to do, and, on an heavy load, can lead us to fail on something that would have worked before.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We only reject as &apos;busy&apos; when we can&apos;t obtain lock after an amount of time and if we are trying to flush because we are up against the global mem limit.  Regards retries, if we get one of these RegionTooBusyExceptions, rather than back off for a 100ms or so, should we back off more (an Elliott suggestion)?  And drop the number of tasks to throw at this server at any one time.   It&apos;d be hard to do as things are now given backoff is calculated based off retry count only.&lt;/p&gt;

&lt;p&gt;Give the two items above, we should keep more stats per server than just count of tasks?  We should keep a history of success/error and do backoffs &amp;#8211; both amount of time and how many tasks to send the server &amp;#8211; based on this?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;....For example, the new settings will make the client to send 4 queries in 1 second....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, that is not going to help anyone.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we want to compare 0.94 and 0.96, may be we should use the same settings, i.e. pause: 1000ms backoff: { 1, 1, 1, 2, 2, 4, 4, 8, 16, 32, 64 } hbase.client.max.perserver.tasks: 1&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Seems like good idea.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nkeywal&quot; class=&quot;user-hover&quot; rel=&quot;nkeywal&quot;&gt;Nicolas Liochon&lt;/a&gt; What you think of the &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffreyz&quot; class=&quot;user-hover&quot; rel=&quot;jeffreyz&quot;&gt;Jeffrey Zhong&lt;/a&gt; patch?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; Any luck run perf test?&lt;/p&gt;

&lt;p&gt;We got our big cluster back so we&apos;ll start in on this one again.&lt;/p&gt;

&lt;p&gt;In single client, if many regions, I see the client threads blocked waiting to do locateRegionInMeta (I don&apos;t understand this regionLockObject... it locks everyone out while a lookup is going on rather than threads contending on the same region location).  If there are few regions, we are doing softvaluemap operations all the time.&lt;/p&gt;





</comment>
                            <comment id="13815545" author="jmspaggi" created="Thu, 7 Nov 2013 01:57:58 +0000"  >&lt;p&gt;I will try to start the tests this evening, else will be tomorrow morning. Might take about 24h. I will run the last 0.96 and the last 0.96+9775 and compare. I will run in standalone but I can also run in pseudo-dist if you want (6 disks).&lt;/p&gt;</comment>
                            <comment id="13815591" author="stack" created="Thu, 7 Nov 2013 03:00:59 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt;.  Don&apos;t mind the patch in here.  I think Nicolas had a prescription above for you comparing 0.94 and 0.96?&lt;/p&gt;</comment>
                            <comment id="13815768" author="stack" created="Thu, 7 Nov 2013 08:42:17 +0000"  >&lt;p&gt;Looking at YCSB, it makes a static HBaseConfiguration in the head of HBaseClient.  Each &apos;client&apos; then does new HTable(conf, name).  Internally this will do a getConnection so all threads/clients will use the same underlying rpc connection/HConnection instance.  Let me try amending the YCSB HBaseClient so each has its own connection to see if that ups the amount a single client/YCSB-thread can drive.&lt;/p&gt;

&lt;p&gt;Each HTable has an AsyncProcess.  An AsyncProcess can have 100 tasks outstanding at a time; an arbitrary number.  There is an upper bound of 256 threads per connection IFF the connection was let create its own pool. In YCSB hbase client, the HTable creates the pool and it is an unbounded pool contained only by the 100 outstanding tasks AsyncProcess so I&apos;d guess that when Elliott was running, he had &quot;...32*77 threads (~2400)&quot; rather than an upper bound of 256.  To be checked.&lt;/p&gt;</comment>
                            <comment id="13815782" author="nkeywal" created="Thu, 7 Nov 2013 09:02:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;To be checked.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I confirm. That&apos;s by design (in YCSB, and in a way, in HBase as well, 0.94 included...). It does not really match a real use case, an application server would have a single connection.&lt;/p&gt;

&lt;p&gt;32 clients on a single machine takes some CPU (whatever the number of thread), I guess with all the changes we&apos;re likely better, but we could be CPU bound.&lt;/p&gt;

&lt;p&gt;A test with a single YCSB client on each machine would be interesting imho.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Ideally we want something like what you had before &#8211; 5 or 1/2 the CPUs on the local server as guesstimate of how many CPUs the server has, which ever is greater&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;IIRC, I forgot to change hbase-site.xml, so the default has not changed at the end &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;br/&gt;
But it&apos;s a per AsyncProcess limit. With the 32 clients, a server can have 64 writes  queries in parallel. That&apos;s already a lot. (with 0.94, it would have been 32 max, and much likely half of less of it, as we wait for all the queries to be finished before sending a new set of queries). &lt;/p&gt;</comment>
                            <comment id="13815864" author="jmspaggi" created="Thu, 7 Nov 2013 11:46:20 +0000"  >&lt;p&gt;Anyway I&apos;m not able to apply this patch on 0.96:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
jmspaggi@hbasetest1:~/hbase/hbase-0.96-9775$ patch -p0 &amp;lt; 9775.rig.v3.patch 
patching file hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncProcess.java
patching file hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
Reversed (or previously applied) patch detected!  Assume -R? [n] 
Apply anyway? [n] y
Hunk #1 FAILED at 60.
Hunk #2 succeeded at 288 (offset 1 line).
Hunk #3 FAILED at 1025.
2 out of 3 hunks FAILED -- saving rejects to file hbase-client/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java.rej
patching file hbase-client/src/main/java/org/apache/hadoop/hbase/client/MultiServerCallable.java
patching file hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
patching file hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
patching file hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java
patching file hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
Reversed (or previously applied) patch detected!  Assume -R? [n] 
Apply anyway? [n] y
Hunk #1 FAILED at 1632.
1 out of 1 hunk FAILED -- saving rejects to file hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java.rej
patching file hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java
Reversed (or previously applied) patch detected!  Assume -R? [n] 
Apply anyway? [n] y
Hunk #1 FAILED at 34.
Hunk #2 FAILED at 616.
2 out of 2 hunks FAILED -- saving rejects to file hbase-server/src/main/java/org/apache/hadoop/hbase/tool/Canary.java.rej
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I will run on 0.94 instead of 0.96+patch, but I don&apos;t think we can really compare the 2. The default parameters are very different, so just because of the config the results will be different too.&lt;/p&gt;</comment>
                            <comment id="13815875" author="nkeywal" created="Thu, 7 Nov 2013 12:07:37 +0000"  >&lt;p&gt;The patch is not important, this jira is a kind of umbrella, the real meat has been done on other patches, already committed (or not committable, which leads to the same point: there is no patch to apply &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;br/&gt;
What we&apos;re testing here is the baseline, like the graphs you were doing a while ago: how .96 behaves compared to .94 with the default settings in both cases. &lt;/p&gt;

&lt;p&gt;Thanks, Jean-Marc.&lt;/p&gt;

</comment>
                            <comment id="13815915" author="jmspaggi" created="Thu, 7 Nov 2013 12:34:40 +0000"  >&lt;p&gt;Then it&apos;s perfect. 0.96 is running since yesterday, and I have 0.94 ready in the pipe. Seems to take more than 12h per run, so results might come tomorrow only.&lt;/p&gt;

&lt;p&gt;Also, it&apos;s running in standalone, but I have 6 drives in this server (1 SSD + 5 SATA) so I can run it again in pseudo dist over the week-emd if we want...&lt;/p&gt;</comment>
                            <comment id="13816067" author="stack" created="Thu, 7 Nov 2013 15:48:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;I confirm. That&apos;s by design (in YCSB, and in a way, in HBase as well, 0.94 included...). It does not really match a real use case, an application server would have a single connection.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is a little odd.  We are &apos;constraining&apos; all to go via the one connection.  Even if you up the threads because you want more throughput, we will constrain you to run over the one connection.  Let me try and see if connection per thread makes a difference.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;IIRC, I forgot to change hbase-site.xml, so the default has not changed at the end .&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You fixing boss?  Apparently &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; made the change in this test anyways?&lt;/p&gt;
</comment>
                            <comment id="13816075" author="nkeywal" created="Thu, 7 Nov 2013 16:00:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;You fixing boss? Apparently Elliott Clark made the change in this test anyways?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I plan to do a jira once we&apos;re clear on the values we want, client pause &amp;amp; back-off included...&lt;/p&gt;</comment>
                            <comment id="13816831" author="stack" created="Fri, 8 Nov 2013 00:03:49 +0000"  >&lt;p&gt;Playing w/ YCSB, I see that as is,  we are pretty well-behaved now.  A single client will grow its threads to just under two per server and would hold there roughly.  Sometimes it will expand a little beyond this but these are threads that are just waiting to be dropped by the pool.   On my small cluster of 5 nodes, upping the clients to 8 on a 16 core CPU, I was doing about 600% burn and the throughput was at just over twice the single thread.  To be continued.&lt;/p&gt;

&lt;p&gt;I made this change in YCSB to see if more connections would get me more throughput:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
@@ -117,7 +122,8 @@ &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class HBaseClient &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; com.yahoo.ycsb.DB
     &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void getHTable(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; table) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException
     {
         &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (tableLock) {
-            _hTable = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HTable(config, table);
+            _hConnection = HConnectionManager.createConnection(config);
+            _hTable = _hConnection.getTable(table);
             &lt;span class=&quot;code-comment&quot;&gt;//2 suggestions from http://ryantwopointoh.blogspot.com/2009/01/performance-of-hbase-importing.html
&lt;/span&gt;             _hTable.setAutoFlush(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
             _hTable.setWriteBufferSize(1024*1024*12);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What I saw was that my client now had 2k+ threads running in it.  All but a few were just idle waiting, doing nothing.  The burn was up, around 800%.  Didn&apos;t bother checking throughput.&lt;/p&gt;

&lt;p&gt;So for Elliott test above, he probably had sane number of threads in his test.  But if folks follow our new receipe where they do createConnection().getTable(TableName) then they will have clients w/ at least 256 threads just sitting there hanging out.  Let me fix that one.&lt;/p&gt;
</comment>
                            <comment id="13816984" author="stack" created="Fri, 8 Nov 2013 04:25:05 +0000"  >&lt;p&gt;So far it seems that sharing a connection is better than a connection per thread in YCSB.  With 8 clients into a 5-node cluster, sharing the connection is almost 50% faster.  Will dig in.&lt;/p&gt;
</comment>
                            <comment id="13817632" author="jmspaggi" created="Fri, 8 Nov 2013 19:56:22 +0000"  >&lt;p&gt;Here are the numbers for a standalone test between 0.96 and 0.94 fresh from the branches.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;0.94&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;0.96&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$FilteredScanTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10.67&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10.38&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;97.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomReadTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;840.20&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1013.33&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;120.61%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;25041.10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;35527.26&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;141.88%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomScanWithRange1000Test&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2396.20&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2629.48&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;109.74%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$SequentialReadTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2925.24&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3050.02&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;104.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$SequentialWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;27326.22&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;40190.30&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;147.08%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                            <comment id="13818576" author="jmspaggi" created="Sun, 10 Nov 2013 22:27:15 +0000"  >&lt;p&gt;BTW, numbers are rows/min for the first, and rows/sec for the nexts.&lt;/p&gt;</comment>
                            <comment id="13818806" author="nkeywal" created="Mon, 11 Nov 2013 09:56:47 +0000"  >&lt;p&gt;The results a few month ago were (taken from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6295&quot; title=&quot;Possible performance improvement in client batch operations: presplit and send in background&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6295&quot;&gt;&lt;del&gt;HBASE-6295&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Test&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Trunk&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Nic&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;0.95&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org,apache,hadoop,hbase,PerformanceEvaluation$RandomReadTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1377.08&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1420.14&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1390.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org,apache,hadoop,hbase,PerformanceEvaluation$RandomScanWithRange100Test&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;11243.12&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10992.68&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10971.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org,apache,hadoop,hbase,PerformanceEvaluation$RandomSeekScanTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;304.66&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;296.43&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;305.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org,apache,hadoop,hbase,PerformanceEvaluation$RandomWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9176.07&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13619.59&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9134.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org,apache,hadoop,hbase,PerformanceEvaluation$SequentialWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13592.40&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;42655.52&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13255.12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;The writes seems to be much faster on your new computer, while the reads are slower. But we still have an improvement on 0.96 for the writes...&lt;/p&gt;</comment>
                            <comment id="13818936" author="jmspaggi" created="Mon, 11 Nov 2013 13:17:40 +0000"  >&lt;p&gt;Same computer (CPU/Memory/MB) but new disk... Previous was with an old 100GB drive, the new one is with a new 120G SSD. So I&apos;m not surprised that numbers are better now. We can not really compare the 2 unfortunately.&lt;/p&gt;</comment>
                            <comment id="13819421" author="stack" created="Mon, 11 Nov 2013 21:29:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; Thanks for running the tests.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12674228">HBASE-9787</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12676893">HBASE-9869</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12610988" name="9775.rig.txt" size="27491" author="stack" created="Wed, 30 Oct 2013 05:17:07 +0000"/>
                            <attachment id="12610995" name="9775.rig.v2.patch" size="26822" author="stack" created="Wed, 30 Oct 2013 06:26:33 +0000"/>
                            <attachment id="12612334" name="9775.rig.v3.patch" size="49219" author="stack" created="Wed, 6 Nov 2013 06:23:02 +0000"/>
                            <attachment id="12608772" name="Charts Search   Cloudera Manager - ITBLL.png" size="280194" author="eclark" created="Wed, 16 Oct 2013 18:40:22 +0000"/>
                            <attachment id="12608618" name="Charts Search   Cloudera Manager.png" size="318582" author="eclark" created="Wed, 16 Oct 2013 00:47:20 +0000"/>
                            <attachment id="12609090" name="hbase-9775.patch" size="4358" author="jeffreyz" created="Fri, 18 Oct 2013 04:03:40 +0000"/>
                            <attachment id="12608771" name="job_run.log" size="411242" author="eclark" created="Wed, 16 Oct 2013 18:39:55 +0000"/>
                            <attachment id="12608604" name="short_ycsb.png" size="43051" author="eclark" created="Tue, 15 Oct 2013 23:20:04 +0000"/>
                            <attachment id="12609224" name="ycsb.png" size="59907" author="eclark" created="Fri, 18 Oct 2013 22:14:24 +0000"/>
                            <attachment id="12608729" name="ycsb_insert_94_vs_96.png" size="94549" author="eclark" created="Wed, 16 Oct 2013 16:23:30 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12677882">HBASE-9907</subtask>
                            <subtask id="12678117">HBASE-9917</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 16 Oct 2013 00:30:51 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>353634</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 5 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1oyx3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>353926</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>