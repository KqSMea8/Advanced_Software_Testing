<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:59:44 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-8919/HBASE-8919.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-8919] TestReplicationQueueFailover (and Compressed) can fail because the recovered queue gets stuck on ClosedByInterruptException</title>
                <link>https://issues.apache.org/jira/browse/HBASE-8919</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Looking at this build: &lt;a href=&quot;https://builds.apache.org/job/hbase-0.95-on-hadoop2/173/testReport/org.apache.hadoop.hbase.replication/TestReplicationQueueFailoverCompressed/queueFailover/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/hbase-0.95-on-hadoop2/173/testReport/org.apache.hadoop.hbase.replication/TestReplicationQueueFailoverCompressed/queueFailover/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The only thing I can find that went wrong is that the recovered queue was not completely done because the source fails like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2013-07-10 11:53:51,538 INFO  [Thread-1259] regionserver.ReplicationSource$2(799): Slave cluster looks down: Call to hemera.apache.org/140.211.11.27:38614 failed on local exception: java.nio.channels.ClosedByInterruptException
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And just before that it got:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2013-07-10 11:53:51,290 WARN  [ReplicationExecutor-0.replicationSource,2-hemera.apache.org,43669,1373457208379] regionserver.ReplicationSource(661): Can&apos;t replicate because of an error on the remote cluster: 
org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException): org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1594 actions: FailedServerException: 1594 times, 
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:158)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$500(AsyncProcess.java:146)
	at org.apache.hadoop.hbase.client.AsyncProcess.getErrors(AsyncProcess.java:692)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:2106)
	at org.apache.hadoop.hbase.client.HTable.batchCallback(HTable.java:689)
	at org.apache.hadoop.hbase.client.HTable.batchCallback(HTable.java:697)
	at org.apache.hadoop.hbase.client.HTable.batch(HTable.java:682)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.batch(ReplicationSink.java:239)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.replicateEntries(ReplicationSink.java:161)
	at org.apache.hadoop.hbase.replication.regionserver.Replication.replicateLogEntries(Replication.java:173)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.replicateWALEntry(HRegionServer.java:3735)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:14402)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2122)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1829)

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1369)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1573)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1630)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.replicateWALEntry(AdminProtos.java:15177)
	at org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil.replicateWALEntry(ReplicationProtbufUtil.java:94)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:642)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:376)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I wonder what&apos;s closing the socket with an interrupt, it seems it still needs to replicate more data. I&apos;ll start by adding the stack trace for the message when it fails to replicate on a &quot;local exception&quot;. Also I found a thread that wasn&apos;t shutdown properly that I&apos;m going to fix to help with debugging.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12657133">HBASE-8919</key>
            <summary>TestReplicationQueueFailover (and Compressed) can fail because the recovered queue gets stuck on ClosedByInterruptException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="5">Cannot Reproduce</resolution>
                                        <assignee username="jdcryans">Jean-Daniel Cryans</assignee>
                                    <reporter username="jdcryans">Jean-Daniel Cryans</reporter>
                        <labels>
                    </labels>
                <created>Wed, 10 Jul 2013 19:13:58 +0000</created>
                <updated>Tue, 13 Aug 2013 21:59:58 +0000</updated>
                            <resolved>Tue, 13 Aug 2013 21:59:58 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="13704950" author="jdcryans" created="Wed, 10 Jul 2013 19:27:24 +0000"  >&lt;p&gt;I&apos;m committing the attached patch to get more debug and fix a leaky thread.&lt;/p&gt;</comment>
                            <comment id="13705081" author="jdcryans" created="Wed, 10 Jul 2013 21:22:47 +0000"  >&lt;p&gt;Not sure if related, but here&apos;s a test that fails because the scanner is stuck on ClosedByInterruptException:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/1049/testReport/junit/org.apache.hadoop.hbase.client/TestFromClientSide/testCheckAndPut/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/1049/testReport/junit/org.apache.hadoop.hbase.client/TestFromClientSide/testCheckAndPut/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It seems that we should at least not retry it 10 times!&lt;/p&gt;</comment>
                            <comment id="13705107" author="hudson" created="Wed, 10 Jul 2013 21:37:04 +0000"  >&lt;p&gt;Integrated in hbase-0.95 #310 (See &lt;a href=&quot;https://builds.apache.org/job/hbase-0.95/310/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/hbase-0.95/310/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8919&quot; title=&quot;TestReplicationQueueFailover (and Compressed) can fail because the recovered queue gets stuck on ClosedByInterruptException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8919&quot;&gt;&lt;del&gt;HBASE-8919&lt;/del&gt;&lt;/a&gt;  TestReplicationQueueFailover (and Compressed) can fail because the recovered queue&lt;br/&gt;
            gets stuck on ClosedByInterruptException (first try, getting more debug) (Revision 1501923)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
jdcryans : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13705117" author="hudson" created="Wed, 10 Jul 2013 21:43:35 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #4237 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/4237/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/4237/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8919&quot; title=&quot;TestReplicationQueueFailover (and Compressed) can fail because the recovered queue gets stuck on ClosedByInterruptException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8919&quot;&gt;&lt;del&gt;HBASE-8919&lt;/del&gt;&lt;/a&gt;  TestReplicationQueueFailover (and Compressed) can fail because the recovered queue&lt;br/&gt;
            gets stuck on ClosedByInterruptException (first try, getting more debug) (Revision 1501924)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
jdcryans : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13705294" author="hudson" created="Thu, 11 Jul 2013 00:53:47 +0000"  >&lt;p&gt;Integrated in hbase-0.95-on-hadoop2 #174 (See &lt;a href=&quot;https://builds.apache.org/job/hbase-0.95-on-hadoop2/174/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/hbase-0.95-on-hadoop2/174/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8919&quot; title=&quot;TestReplicationQueueFailover (and Compressed) can fail because the recovered queue gets stuck on ClosedByInterruptException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8919&quot;&gt;&lt;del&gt;HBASE-8919&lt;/del&gt;&lt;/a&gt;  TestReplicationQueueFailover (and Compressed) can fail because the recovered queue&lt;br/&gt;
            gets stuck on ClosedByInterruptException (first try, getting more debug) (Revision 1501923)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
jdcryans : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.95/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13705308" author="hudson" created="Thu, 11 Jul 2013 00:59:46 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #608 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/608/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/608/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8919&quot; title=&quot;TestReplicationQueueFailover (and Compressed) can fail because the recovered queue gets stuck on ClosedByInterruptException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8919&quot;&gt;&lt;del&gt;HBASE-8919&lt;/del&gt;&lt;/a&gt;  TestReplicationQueueFailover (and Compressed) can fail because the recovered queue&lt;br/&gt;
            gets stuck on ClosedByInterruptException (first try, getting more debug) (Revision 1501924)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
jdcryans : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13709216" author="jdcryans" created="Tue, 16 Jul 2013 00:01:04 +0000"  >&lt;p&gt;Finally got another test that failed with the stack trace, here it is (from &lt;a href=&quot;http://54.241.6.143/job/HBase-0.95/org.apache.hbase$hbase-server/610/testReport/org.apache.hadoop.hbase.replication/TestReplicationQueueFailover/queueFailover/):&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://54.241.6.143/job/HBase-0.95/org.apache.hbase$hbase-server/610/testReport/org.apache.hadoop.hbase.replication/TestReplicationQueueFailover/queueFailover/):&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2013-07-12 21:17:00,382 INFO  [Thread-962] regionserver.ReplicationSource$2(799): Slave cluster looks down: Call to ip-10-196-81-100.us-west-1.compute.internal/10.196.81.100:39599 failed on local exception: java.nio.channels.ClosedByInterruptException
java.io.IOException: Call to ip-10-196-81-100.us-west-1.compute.internal/10.196.81.100:39599 failed on local exception: java.nio.channels.ClosedByInterruptException
	at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1401)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1373)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1573)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1630)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.getServerInfo(AdminProtos.java:15213)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getServerInfo(ProtobufUtil.java:1466)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$2.run(ReplicationSource.java:793)
Caused by: java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:343)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
	at java.io.DataOutputStream.flush(DataOutputStream.java:106)
	at org.apache.hadoop.hbase.ipc.IPCUtil.write(IPCUtil.java:231)
	at org.apache.hadoop.hbase.ipc.IPCUtil.write(IPCUtil.java:220)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1014)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1349)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13709313" author="jdcryans" created="Tue, 16 Jul 2013 01:00:05 +0000"  >&lt;p&gt;Some more understanding of this issue.&lt;/p&gt;

&lt;p&gt;ReplicationSource is the one that interrupts the thread, what happens is this:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Try to replicate&lt;/li&gt;
	&lt;li&gt;Fail&lt;/li&gt;
	&lt;li&gt;Try to see if the slave cluster is down&lt;/li&gt;
	&lt;li&gt;Interrupt the thread since it took more than 100ms to do getServerInfo&lt;/li&gt;
	&lt;li&gt;Sleep&lt;/li&gt;
	&lt;li&gt;Try to check if it&apos;s down again, but it still takes 100ms...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I don&apos;t understand why we get into situations where it takes 100ms to do a simple operation... forever.&lt;/p&gt;

&lt;p&gt;Another weird thing I&apos;m seeing is that this exception is thrown before the thread is interrupted. Could it be that I&apos;m reusing a channel that&apos;s already interrupted and that&apos;s what it sends back?&lt;/p&gt;</comment>
                            <comment id="13709982" author="stack" created="Tue, 16 Jul 2013 17:46:25 +0000"  >&lt;p&gt;New one &lt;a href=&quot;http://54.241.6.143/job/HBase-0.95-Hadoop-2/635/org.apache.hbase$hbase-server/testReport/junit/org.apache.hadoop.hbase.replication/TestReplicationQueueFailoverCompressed/queueFailover/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://54.241.6.143/job/HBase-0.95-Hadoop-2/635/org.apache.hbase$hbase-server/testReport/junit/org.apache.hadoop.hbase.replication/TestReplicationQueueFailoverCompressed/queueFailover/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13709993" author="jdcryans" created="Tue, 16 Jul 2013 17:52:34 +0000"  >&lt;p&gt;This is a different failure mode, the source cluster became completely unavailable.&lt;/p&gt;</comment>
                            <comment id="13713211" author="jdcryans" created="Fri, 19 Jul 2013 00:55:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; So I dug into that failure you pasted on Jul 16th and it wasn&apos;t the source cluster that was bad but the destination that was. The RS that was killed there was never fully recovered, one log was still being replayed when the test timed out. It could be related to this (THIS IS NOT SUPPOSED TO HAPPEN it says):&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2013-07-16 17:14:25,933 INFO  [IPC Server handler 2 on 56710] blockmanagement.BlockInfoUnderConstruction(248): BLOCK* blk_4297992342878601848_1031{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[127.0.0.1:47006|RBW], ReplicaUnderConstruction[127.0.0.1:51438|RBW]]} recovery started, primary=127.0.0.1:47006
2013-07-16 17:14:25,933 WARN  [IPC Server handler 2 on 56710] namenode.FSNamesystem(3135): DIR* NameSystem.internalReleaseLease: File /user/ec2-user/hbase/.logs/ip-10-197-55-49.us-west-1.compute.internal,39939,1373994850314-splitting/ip-10-197-55-49.us-west-1.compute.internal%2C39939%2C1373994850314.1373994862658 has not been closed. Lease recovery is in progress. RecoveryId = 1040 for block blk_4297992342878601848_1031{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[127.0.0.1:47006|RBW], ReplicaUnderConstruction[127.0.0.1:51438|RBW]]}
...
2013-07-16 17:14:26,666 INFO  [IPC Server handler 5 on 56710] blockmanagement.BlockManager(2174): BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:51438 is added to blk_4297992342878601848_1031{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[127.0.0.1:47006|RBW], ReplicaUnderConstruction[127.0.0.1:51438|RBW]]} size 0
2013-07-16 17:14:26,666 INFO  [IPC Server handler 4 on 56710] blockmanagement.BlockManager(2174): BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:47006 is added to blk_4297992342878601848_1031{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[127.0.0.1:47006|RBW], ReplicaUnderConstruction[127.0.0.1:51438|RBW]]} size 0
...
2013-07-16 17:14:30,338 INFO  [IPC Server handler 8 on 56710] blockmanagement.BlockInfoUnderConstruction(248): BLOCK* blk_4297992342878601848_1031{blockUCState=UNDER_RECOVERY, primaryNodeIndex=1, replicas=[ReplicaUnderConstruction[127.0.0.1:47006|RBW], ReplicaUnderConstruction[127.0.0.1:51438|RBW]]} recovery started, primary=127.0.0.1:51438
2013-07-16 17:14:30,339 WARN  [IPC Server handler 8 on 56710] namenode.FSNamesystem(3135): DIR* NameSystem.internalReleaseLease: File /user/ec2-user/hbase/.logs/ip-10-197-55-49.us-west-1.compute.internal,39939,1373994850314-splitting/ip-10-197-55-49.us-west-1.compute.internal%2C39939%2C1373994850314.1373994862658 has not been closed. Lease recovery is in progress. RecoveryId = 1041 for block blk_4297992342878601848_1031{blockUCState=UNDER_RECOVERY, primaryNodeIndex=1, replicas=[ReplicaUnderConstruction[127.0.0.1:47006|RBW], ReplicaUnderConstruction[127.0.0.1:51438|RBW]]}
...
2013-07-16 17:14:33,340 ERROR [IPC Server handler 7 on 35081] security.UserGroupInformation(1481): PriviledgedActionException as:ec2-user (auth:SIMPLE) cause:java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() &amp;gt;= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
2013-07-16 17:14:33,341 WARN  [org.apache.hadoop.hdfs.server.datanode.DataNode$2@64a1fcba] datanode.DataNode(1894): Failed to obtain replica info for block (=BP-1477359609-10.197.55.49-1373994849464:blk_4297992342878601848_1041) from datanode (=127.0.0.1:47006)
java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() &amp;gt;= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1462)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:1801)
	at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB.initReplicaRecovery(InterDatanodeProtocolServerSideTranslatorPB.java:55)
	at org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(InterDatanodeProtocolProtos.java:2198)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1741)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1737)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1478)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1735)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:1814)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1880)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:215)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$2.run(DataNode.java:1786)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() &amp;gt;= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1462)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:1801)
	at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolServerSideTranslatorPB.initReplicaRecovery(InterDatanodeProtocolServerSideTranslatorPB.java:55)
	at org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InterDatanodeProtocolService$2.callBlockingMethod(InterDatanodeProtocolProtos.java:2198)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1741)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1737)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1478)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1735)

	at org.apache.hadoop.ipc.Client.call(Client.java:1235)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
	at com.sun.proxy.$Proxy24.initReplicaRecovery(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolTranslatorPB.initReplicaRecovery(InterDatanodeProtocolTranslatorPB.java:83)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:1812)
	... 4 more
2013-07-16 17:14:33,342 WARN  [org.apache.hadoop.hdfs.server.datanode.DataNode$2@64a1fcba] datanode.DataNode(1894): Failed to obtain replica info for block (=BP-1477359609-10.197.55.49-1373994849464:blk_4297992342878601848_1041) from datanode (=127.0.0.1:51438)
java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() &amp;gt;= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data1/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data1/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1462)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:1422)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:1801)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:1812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1880)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:215)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$2.run(DataNode.java:1786)
	at java.lang.Thread.run(Thread.java:662)
2013-07-16 17:14:33,342 WARN  [org.apache.hadoop.hdfs.server.datanode.DataNode$2@64a1fcba] datanode.DataNode$2(1788): recoverBlocks FAILED: RecoveringBlock{BP-1477359609-10.197.55.49-1373994849464:blk_4297992342878601848_1041; getBlockSize()=794; corrupt=false; offset=-1; locs=[127.0.0.1:47006, 127.0.0.1:51438]}
java.io.IOException: All datanodes failed: block=BP-1477359609-10.197.55.49-1373994849464:blk_4297992342878601848_1041, datanodeids=[127.0.0.1:47006, 127.0.0.1:51438]
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:1901)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:215)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$2.run(DataNode.java:1786)
	at java.lang.Thread.run(Thread.java:662)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13713321" author="jdcryans" created="Fri, 19 Jul 2013 04:12:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt; is telling me that the page for build 635 is giving 404s and I still had the open so here I&apos;m attaching the whole page that was linked to by Stack.&lt;/p&gt;</comment>
                            <comment id="13738892" author="jdcryans" created="Tue, 13 Aug 2013 21:59:58 +0000"  >&lt;p&gt;I&apos;m closing as Cannot Repro because I haven&apos;t seen this failure in weeks since tweaking the tests.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12591706" name="HBASE-8919.patch" size="1274" author="jdcryans" created="Wed, 10 Jul 2013 19:27:24 +0000"/>
                            <attachment id="12593123" name="HBase-0.95-Hadoop-2 build 635.txt" size="3948518" author="jdcryans" created="Fri, 19 Jul 2013 04:12:41 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 10 Jul 2013 21:37:04 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>337356</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 18 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1m6tr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>337679</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>