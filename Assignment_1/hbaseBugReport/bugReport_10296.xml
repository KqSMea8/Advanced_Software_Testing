<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:12:24 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-10296/HBASE-10296.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-10296] Replace ZK with a consensus lib(paxos,zab or raft) running within master processes to provide better master failover performance and state consistency</title>
                <link>https://issues.apache.org/jira/browse/HBASE-10296</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently master relies on ZK to elect active master, monitor liveness and store almost all of its states, such as region states, table info, replication info and so on. And zk also plays as a channel for master-regionserver communication(such as in region assigning) and client-regionserver communication(such as replication state/behavior change). &lt;br/&gt;
But zk as a communication channel is fragile due to its one-time watch and asynchronous notification mechanism which together can leads to missed events(hence missed messages), for example the master must rely on the state transition logic&apos;s idempotence to maintain the region assigning state machine&apos;s correctness, actually almost all of the most tricky inconsistency issues can trace back their root cause to the fragility of zk as a communication channel.&lt;br/&gt;
Replace zk with paxos running within master processes have following benefits:&lt;br/&gt;
1. better master failover performance: all master, either the active or the standby ones, have the same latest states in memory(except lag ones but which can eventually catch up later on). whenever the active master dies, the newly elected active master can immediately play its role without such failover work as building its in-memory states by consulting meta-table and zk.&lt;br/&gt;
2. better state consistency: master&apos;s in-memory states are the only truth about the system,which can eliminate inconsistency from the very beginning. and though the states are contained by all masters, paxos guarantees they are identical at any time.&lt;br/&gt;
3. more direct and simple communication pattern: client changes state by sending requests to master, master and regionserver talk directly to each other by sending request and response...all don&apos;t bother to using a third-party storage like zk which can introduce more uncertainty, worse latency and more complexity.&lt;br/&gt;
4. zk can only be used as liveness monitoring for determining if a regionserver is dead, and later on we can eliminate zk totally when we build heartbeat between master and regionserver.&lt;/p&gt;

&lt;p&gt;I know this might looks like a very crazy re-architect, but it deserves deep thinking and serious discussion for it, right?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12687801">HBASE-10296</key>
            <summary>Replace ZK with a consensus lib(paxos,zab or raft) running within master processes to provide better master failover performance and state consistency</summary>
                <type id="13" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Brainstorming</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="fenghh">Honghua Feng</reporter>
                        <labels>
                    </labels>
                <created>Wed, 8 Jan 2014 04:40:15 +0000</created>
                <updated>Fri, 18 Apr 2014 21:17:57 +0000</updated>
                                                                            <component>master</component>
                    <component>Region Assignment</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>50</watches>
                                                                <comments>
                            <comment id="13865156" author="lhofhansl" created="Wed, 8 Jan 2014 07:16:56 +0000"  >&lt;p&gt;If we do this, let&apos;s use RAFT: &lt;a href=&quot;https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that we used to have heartbeat between the various services (before my time, though), which was all removed in favor of ZK.&lt;/p&gt;</comment>
                            <comment id="13865230" author="stevel@apache.org" created="Wed, 8 Jan 2014 09:20:42 +0000"  >&lt;p&gt;One aspect of ZK that is worth remembering is that it lets other apps keep an eye on what is going on&lt;/p&gt;</comment>
                            <comment id="13866125" author="apurtell" created="Thu, 9 Jan 2014 00:42:53 +0000"  >&lt;p&gt;Use of ZK has issues but what we had before was much worse. We had heartbeating and partially desynchronized state in a bunch of places. Rather than implement our own consensus protocol we used the specialist component ZK. Engineering distributed consensus protocols is a long term endeavor full of corner cases and hard to debug problems. It is worth consideration, but maybe only as a last resort. Does something about our use of ZK or ZK itself have fatal issues?&lt;/p&gt;</comment>
                            <comment id="13866467" author="stevel@apache.org" created="Thu, 9 Jan 2014 09:28:53 +0000"  >&lt;p&gt;The google chubby paper goes into some detail about why they implemented a Paxos Service and not a paxos library.&lt;/p&gt;

&lt;p&gt;yet perhaps you could persuade the ZK team to rework the code enough that you could reuse it independently of ZK.&lt;/p&gt;

&lt;p&gt;Implementing a consensus protocol is surprisingly hard as you have to &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;understand Paxos&lt;/li&gt;
	&lt;li&gt;implement it&lt;/li&gt;
	&lt;li&gt;prove that your implementation is correct&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Unit tests are not enough -talk to the ZK team about what they had to do to show that it works&lt;/p&gt;</comment>
                            <comment id="13866524" author="eric@apache.org" created="Thu, 9 Jan 2014 10:33:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;Steve Loughran&lt;/a&gt; zk will only tell you a part of the hbase status, so it the usage of zk by a third-app should be seen as a temp workaround (or a nice side-effect). I would be more tempted to prohibit access from third app to hbase-zk (even in read) and enhance hbase to provide monitoring/status api. Just want to point this so the &apos;status&apos; api of zk should not come as an argument in the choice of the consensus mean.&lt;/p&gt;

&lt;p&gt;About the choice, removing a component to maintain (zk) is a good step forward, but there are certainly arguments to keep it.&lt;/p&gt;</comment>
                            <comment id="13866846" author="stevel@apache.org" created="Thu, 9 Jan 2014 17:51:27 +0000"  >&lt;p&gt;..but that ZK path is used to find the hbase master even if it moves round a cluster -what would happen there?&lt;/p&gt;</comment>
                            <comment id="13866891" author="apurtell" created="Thu, 9 Jan 2014 18:39:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;yet perhaps you could persuade the ZK team to rework the code enough that you could reuse it independently of ZK.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;AFAIK, the topic comes up once a year or so and there isn&apos;t sufficient interest. &lt;/p&gt;

&lt;p&gt;The Harvey Mudd Clinic tried extracting ZAB once, circa 2008. See &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-30&quot; title=&quot;Hooks for atomic broadcast protocol&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-30&quot;&gt;ZOOKEEPER-30&lt;/a&gt; and &lt;a href=&quot;http://wiki.apache.org/hadoop/ZooKeeper/ZabProtocol&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/hadoop/ZooKeeper/ZabProtocol&lt;/a&gt; . The contribution failed for lack of a code grant required by the Hadoop PMC chair at the time. ZooKeeper was still a contrib project of Hadoop then. There is different ZooKeeper project leadership now but the code is way out of date. Perhaps it could serve as a template for (re)extraction.&lt;/p&gt;</comment>
                            <comment id="13866993" author="eric@apache.org" created="Thu, 9 Jan 2014 20:16:51 +0000"  >
&lt;p&gt;mmh, no answer here... I have read the description of this jira and &lt;br/&gt;
didn&apos;t find any clue either. The best would be to have a draft design to &lt;br/&gt;
see if it is really doable.&lt;/p&gt;</comment>
                            <comment id="13867014" author="eric@apache.org" created="Thu, 9 Jan 2014 20:30:00 +0000"  >&lt;p&gt;(commented via mail, but context has been removed, so replaying my comment via web ui)&lt;/p&gt;

&lt;p&gt;&amp;gt; ..but that ZK path is used to find the hbase master even if it moves round a cluster -what would happen there?&lt;/p&gt;

&lt;p&gt;mmh, no answer here... I have read the description of this jira and didn&apos;t find any clue either. The best would be to have a draft design to see if it is really doable.&lt;/p&gt;</comment>
                            <comment id="13868787" author="fenghh" created="Sat, 11 Jan 2014 13:51:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;but that ZK path is used to find the hbase master even if it moves round a cluster -what would happen there?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Typically we adopt master-based paxos in practice, so naturally the master process hosting the master paxos replica is the active master. the active master is elected by paxos protocal, not by zk. and each standby master knows who is the current active master. when the active master moves around(for instance when active master dies or its lease timeout), the client or app who attempts to talk with the old active master will fail in two ways: fail to connect if active master dies, or fail by knowing it&apos;s now not the active master and the current new active master info. for the former the client/app will try randomly other alive master instance and that master will accept its request if it&apos;s the new active master, or tell it the current active master info if it&apos;s not the current active master. for the latter it can now talk to the active master...and like how to access a zk, client/app should know the master assemble addresses to access a  HBase cluster. (assuming you&apos;re saying finding the active master, correct me if I&apos;m wrong)&lt;/p&gt;</comment>
                            <comment id="13868792" author="fenghh" created="Sat, 11 Jan 2014 14:18:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;One aspect of ZK that is worth remembering is that it lets other apps keep an eye on what is going on&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, this is a good question. ZK&apos;s watch/notification pattern can be viewed as a communication mechanism: each ZK node represents a piece of data, app A updates this ZK node when it updates the data, then app B which has a watch on it will receives a notification when the data is updated.&lt;br/&gt;
If we use paxos to replace ZK, the data represented by each ZK node now is hosted within each master process&apos; memory as an data structure, updated via the paxos replicated state machine triggered by client/regionserver requests. Now the watch/notification center is moved from ZK to master, and we can still use the node-&amp;gt;watch-list mechanism for implementation which&apos;s used by ZK.&lt;br/&gt;
The above &apos;keep an eye on what is going on&apos;(or watch/notify) now is changed in two ways:&lt;br/&gt;
1. master &amp;lt;&lt;del&gt;&amp;gt; zk &amp;lt;&lt;/del&gt;&amp;gt; regionserver communication now is replaced by master&amp;lt;-&amp;gt;regionserver direct communication&lt;br/&gt;
2. client&amp;lt;&lt;del&gt;&amp;gt;zk&amp;lt;&lt;/del&gt;&amp;gt;regionserver communication now is replaced by client&amp;lt;&lt;del&gt;&amp;gt;master&amp;lt;&lt;/del&gt;&amp;gt;regionserver communication (master plays the role of original ZK)&lt;br/&gt;
a note: we can now provide more flexible options by exposing sync/async notification and one-time/permanent watch. by ZK only one-time async watch is provided.&lt;/p&gt;</comment>
                            <comment id="13868800" author="fenghh" created="Sat, 11 Jan 2014 14:31:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;The google chubby paper goes into some detail about why they implemented a Paxos Service and not a paxos library.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I believe google should have a paxos library, which is used in megastore and spanner, right? And this fact is mentioned in a google paper &lt;b&gt;paxos made live&lt;/b&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Implementing paxos as a standalone/shared service or a library has their own benefits and drawbacks.&lt;br/&gt;
A service: simple API and simple for app to use, can be shared by multiple apps; but abuse by one app can negatively influence other apps using the same paxos service (we ever encountered several times such cases before &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;)&lt;br/&gt;
A library: more difficult for app to use, but have better isolation level(won&apos;t be affected by possible abuse from other app), and have more primitives and more flexibility.&lt;/p&gt;</comment>
                            <comment id="13868802" author="fenghh" created="Sat, 11 Jan 2014 14:43:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;Steve Loughran&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eric%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;eric@apache.org&quot;&gt;Eric Charles&lt;/a&gt; : &lt;br/&gt;
1. paxos / raft / zab library extracted from ZK are all good candidates &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
2. I agree that implementing a &lt;b&gt;correct&lt;/b&gt; consensus protocal for production usage is extremely hard, that&apos;s why I tagged this jira&apos;s type as brainstorming. my intention is to raise it to discuss what&apos;s a better / more reasonable architect would look like.&lt;br/&gt;
3. If we finally all agree on a better architect/design after analysis/discussion/proof, we can approach it in an conservative and incremental way, maybe eventually someday we make it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13868803" author="fenghh" created="Sat, 11 Jan 2014 14:45:44 +0000"  >&lt;p&gt;thanks all guys for the questions/directing/material/history-notes, really appreciated &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13868890" author="lhofhansl" created="Sat, 11 Jan 2014 21:56:08 +0000"  >&lt;p&gt;I always thought that having processes participate in the coordination process directly (as group members) rather than using an external group membership would be better, which I was very disappointed when I first looked at ZK that ZAB was buried to deeply in with the rest of ZK.&lt;/p&gt;

&lt;p&gt;ZK on the other hand is simple (because somebody else solved the hard problems for us). So I can see this go both ways.&lt;/p&gt;

&lt;p&gt;On some level that ties into the discussion as to why we have master and regionserver roles. Cannot all servers serve both roles as needed?&lt;/p&gt;</comment>
                            <comment id="13868947" author="eric@apache.org" created="Sun, 12 Jan 2014 05:18:14 +0000"  >&lt;p&gt;I think the issue is that zk is just a half solution. It is a coordination util, but the job is still to be done. For now, it the coordination logic mainly done in the hbase code (a bit everywhere I think, there is no &apos;coordination&apos; package, no separation of concern)&lt;br/&gt;
To evolve, there are 2 directions:&lt;br/&gt;
1. Embed the coordination with a protocol where the coordination is built-in (zab, p axos or whatever).&lt;br/&gt;
2. Move the coordination out of the hbase code to an external layer. Zk is not enough, would Helix (which relies on Zk) be a good fit ?&lt;/p&gt;</comment>
                            <comment id="13868984" author="fenghh" created="Sun, 12 Jan 2014 09:44:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;I always thought that having processes participate in the coordination process directly (as group members) rather than using an external group membership would be better&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, me too, I hold the same thought all the time &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;On some level that ties into the discussion as to why we have master and regionserver roles. Cannot all servers serve both roles as needed?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This will leads to a totally decentralized architect, such as Dynamo?...it is a much more aggressive re-architect and almost a complete overhaul. Most current code can remain untouched if we only incorporate the zk functionality into master process, right? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13869112" author="apurtell" created="Sun, 12 Jan 2014 18:57:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;This will leads to a totally decentralized architect, such as Dynamo?.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If I understand Lars correctly, not quite. There would still be master and regionserver roles, just that any HBase process could perform those roles, presumably determined by running elections. With Dynamo every process has a homogeneous set of behaviors IIRC. The refactoring wouldn&apos;t be total, but instead of HRegionServer and HMaster as &quot;main&quot; classes, there would be a new main class participating in role elections spawning threads/instances to perform those roles according to outcome. Or something like that. That what you are thinking &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="13869117" author="apurtell" created="Sun, 12 Jan 2014 19:12:32 +0000"  >&lt;p&gt;Also if this conversation morphs to a &quot;let&apos;s P2P HBase&quot;, that would be out of scope for this JIRA which is &quot;Replace ZK with a paxos running within master processes to provide better master failover performance and state consistency&quot;. Perhaps create or use another JIRA for that discussion? &lt;/p&gt;

&lt;p&gt;I will say this though: One crucial difference between a Dynamo or Cassandra install versus HBase is they do their own persistence. Maybe a HBase with a fully homogeneous set of behaviors would be useful, but unless running on an equivalently distributed persistence layer, we would still operationally be subject to HDFS&apos;s master-slave architecture. And a master-slave architecture has some distinct advantages over a P2P one without central control: Under failure conditions, it is easier to get things under control with one commander giving orders rather than relying on the convergence of emergent behaviors.&lt;/p&gt;</comment>
                            <comment id="13869144" author="lhofhansl" created="Sun, 12 Jan 2014 21:03:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;There would still be master and regionserver roles, just that any HBase process could perform those roles, presumably determined by running elections.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Exactly. Now that we have the logic that all HMasters know who is the currently active master (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5083&quot; title=&quot;Backup HMaster should have http infoport open with link to the active master&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5083&quot;&gt;&lt;del&gt;HBASE-5083&lt;/del&gt;&lt;/a&gt;) we could just run the HMaster tasks as part of every server and get rid of the distinct HMaster role (something that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jesse_yates&quot; class=&quot;user-hover&quot; rel=&quot;jesse_yates&quot;&gt;Jesse Yates&lt;/a&gt; had wanted to do a while ago). But I agree with Andy, we are digressing.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Under failure conditions, it is easier to get things under control with one commander&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep, I cannot stress this enough. This discussion comes up at work all the time and I keep making exactly this point, but somehow it is always lost &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
All larger organizations (that I know) favor (multi) master designs over a decentralized approach.&lt;/p&gt;</comment>
                            <comment id="13882197" author="stack" created="Sun, 26 Jan 2014 06:49:58 +0000"  >&lt;p&gt;What would the steps involved moving off zk to a group of masters keeping consensus look like?&lt;/p&gt;</comment>
                            <comment id="13882237" author="fenghh" created="Sun, 26 Jan 2014 09:07:01 +0000"  >&lt;p&gt;sorry for the late reply &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; : thanks for the clarifying to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;&apos;s proposal that &apos;all servers serve both master and regionserver roles as needed&apos;, I can now understand what he really meant, really interesting&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. But if we eventually decide to replace ZK with a consensus algorithm(paxos, zab or raft) running within masters, it is less appealing to make all servers run both master and regionserver roles as needed:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;by this replacing, we reduced the total machines from X masters(typically 3?) plus Y zookeepers(typically 3 or 5) to only Y masters (as zookeeper, typically 3 or 5) by incorporating the master logic (create/remove tables, failover, balance...) and the zookeeper logic (replicating states) to the same set of processes/servers. after this replacing and incorporating, the standby masters are not that &apos;standby&apos; as previous master which just stay idle to wait for the active master to die and then compete to take it over, wasting the standby masters&apos; machine resource most of the time, these standby masters now participate in consensus making, state replicating/persisting, making snapshot and etc. we don&apos;t mind schedule some separate machines for these tasks, just as we don&apos;t mind schedule separate machines for followers within a zookeeper assemble and don&apos;t ever think about to reuse the follower/standby zookeeper servers to serve as regionserver, right?&lt;/li&gt;
	&lt;li&gt;it&apos;s preferred to keep the membership of the consensus algorithm to be fixed, within a fixed small set of predesignated machines. it can noticeably complicate the total design/implementation to support dynamic the consensus algorithm&apos;s membership, if we permit all servers of a HBase cluster to be able to play the role of master&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I also agree with you two on that a total P2P architect like Dynamo/Cassandra will have a much more difficult time when handling failover.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What would the steps involved moving off zk to a group of masters keeping consensus look like?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; : the rough steps I can think about for now is as below:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;implement a robust/reliable consensus lib(paxos/zab/raft)&lt;/li&gt;
	&lt;li&gt;redesign the master based on this consensus lib. now we don&apos;t need to write out the HBase/master states such as region-assign-status, replication info, table info to an far-away/outside persistent/reliable storage such as zookeeper or another HBase system table, we just replicate them among the masters, master itself is the only truth about these states.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5487&quot; title=&quot;Generic framework for Master-coordinated tasks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5487&quot;&gt;HBASE-5487&lt;/a&gt; aims for a master redesign to store the states to a system table, though can avoid the state maintenance problem derived from missed event by zookeeper&apos;s watch/notify mechanism, it still will have the problem of keeping/maintaining truth in two different sites(master memory and the system table) and we still need to be very careful at implementing. it&apos;s always a headache when the state reader/maintainer(master) and state persistence layer are not in the same server;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10295&quot; title=&quot;Refactor the replication  implementation to eliminate permanent zk node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10295&quot;&gt;HBASE-10295&lt;/a&gt; aims for moving replication info from zookeeper to another system table. if we achieve using consensus lib within master, we can represent replication info as just another in-memory data structure, but not a different system table. (personally I don&apos;t think using zookeeper node to store replication info is an as severe problem as region-assign-status, since replication-aware logic has the inherent idempotence: it cares about  only what&apos;s the final state of some replication info when it&apos;s changed, but not how it changes to the final state(the state transition process). on the contrary region assignment logic is more like a state machine, it does care about a state is transitioned from a valid previous state(it looks like transition from an invalid state when some event/state missed) otherwise the code can be pretty tricky and hard to understand/maintain. another concern for moving replication info from zookeeper to another system table is that it&apos;s hard for HBase table to represent a deep tree-like hierarchical structure (HBase table can only naturally represent not more than 3-layer structure (via row + cf + qualifier),  zookeeper and in-memory data structures don&apos;t have such limited hierarchical layer problem)&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13884979" author="liyin" created="Wed, 29 Jan 2014 04:11:30 +0000"  >&lt;p&gt;Speaking of RAFT implementation, we, the FB hbase team, are very close to open source a Raft implementation as a library. And there are multiple potentials to integrate Raft protocol into HBase/HDFS software stack.&lt;/p&gt;
</comment>
                            <comment id="13885002" author="fenghh" created="Wed, 29 Jan 2014 04:56:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;Speaking of RAFT implementation, we, the FB hbase team, are very close to open source a Raft implementation as a library. And there are multiple potentials to integrate Raft protocol into HBase/HDFS software stack.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=liyin&quot; class=&quot;user-hover&quot; rel=&quot;liyin&quot;&gt;Liyin Tang&lt;/a&gt;, sounds really great.&lt;/p&gt;</comment>
                            <comment id="13885014" author="stack" created="Wed, 29 Jan 2014 05:15:14 +0000"  >&lt;p&gt;Sweet &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=liyin&quot; class=&quot;user-hover&quot; rel=&quot;liyin&quot;&gt;Liyin Tang&lt;/a&gt;.  I was going to write that 1. would be fun but looks like it&apos;d be a bit of work going by this list &lt;a href=&quot;http://raftconsensus.github.io/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://raftconsensus.github.io/&lt;/a&gt;.  There does not seem to be a complete easy-to-integrate implementation as yet, not unless, we go JNI it.  Varying group membership and compacting the logs we&apos;d have to contrib.  I suppose we&apos;d write the logs to the local filesystem if we want edits persisted.&lt;/p&gt;

&lt;p&gt;On 2. above &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;, We&apos;d have no callback/notification mechanism so it will take a little more effort replacing the zk-based mechanism.  We&apos;d have to add being able to pass state messages for say when a region has opened on a regionserver or is closing...&lt;br/&gt;
On 3. and 4., agree.&lt;/p&gt;

&lt;p&gt;On undoing the notion of a &apos;master&apos;, we&apos;d also be simplifying hbase packaging and deploy.  There would be no more need to set aside machines for this special role.  Lets keep chatting on this one.&lt;/p&gt;</comment>
                            <comment id="13887841" author="fenghh" created="Fri, 31 Jan 2014 15:34:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;Varying group membership and compacting the logs we&apos;d have to contrib. I suppose we&apos;d write the logs to the local filesystem if we want edits persisted.&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol&gt;
	&lt;li&gt;Yes we need the snapshot feature provided from within the lib which is used by the app/user(here is our HMaster) to reduce log files, otherwise the number of log files can increase immensely over time(a bit like the motivation of flush in HBase)--I assume you meant snapshot when you said &apos;compacting the logs&apos;. and the snapshot functionality should be as a callback function implemented by user logic, and after it&apos;s done the consensus lib perform removing according log files, right? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/li&gt;
	&lt;li&gt;Agree with you on that we&apos;d write the logs to the local filesystem for persistence, no doubt here.&lt;/li&gt;
	&lt;li&gt;Varying group membership has relatively lower priority than others, can safely set as a nice-to-have feature in the beginning in the light that we almost always use a pre-configured fixed set of machines as HMaster, right?&lt;/li&gt;
&lt;/ol&gt;


&lt;blockquote&gt;&lt;p&gt;We&apos;d have no callback/notification mechanism so it will take a little more effort replacing the zk-based mechanism. We&apos;d have to add being able to pass state messages for say when a region has opened on a regionserver or is closing...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I would propose to replace zk in an incremental fashion: &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;for region assignment status info, we move them out of zk to the embedded in-memory consensus lib instance.&lt;/li&gt;
	&lt;li&gt;zk can still serve as the central truth-holder storage for the &apos;configuration&apos;-like data such as replication info, since zk does it job well for such use scenario(we have analysed it more comprehensively in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1755&quot; title=&quot;Putting &amp;#39;Meta&amp;#39; table into ZooKeeper&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1755&quot;&gt;&lt;del&gt;HBASE-1755&lt;/del&gt;&lt;/a&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;/li&gt;
	&lt;li&gt;zk also remain as the liveness monitor for regionservers(but not for HMaster&apos;s healthy which is now handled by the consensus lib instance itself) before we implement heartbeat directly between HMaster and regionservers.&lt;/li&gt;
	&lt;li&gt;for region assignment status info, since HMaster and regionservers now talk directly by sending request/response messages between HMaster and regionservers after we use in-memory consensus lib, it&apos;s natural that &apos;they are able to pass state messages for say when a region has opened on a regionserver or is closing&apos;&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13888308" author="enis" created="Fri, 31 Jan 2014 23:32:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Speaking of RAFT implementation, we, the FB hbase team, are very close to open source a Raft implementation as a library. And there are multiple potentials to integrate Raft protocol into HBase/HDFS software stack.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Nice. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I would propose to replace zk in an incremental fashion:&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I like the incremental approach. The only thing is that, this would require multi master setup unless we keep the logs in hdfs and be able to use a single node RAFT quorum right? &lt;/p&gt;</comment>
                            <comment id="13894362" author="fenghh" created="Fri, 7 Feb 2014 10:02:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;The only thing is that, this would require multi master setup unless we keep the logs in hdfs and be able to use a single node RAFT quorum right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do you mean master back-compatibility issue? &lt;/p&gt;

&lt;p&gt;No master back-compatibility when replacing zk with embedded consensus lib, nor between different incremental phases.&lt;/p&gt;

&lt;p&gt;My point when proposing increment approach is that the data/functionalities provided by zk have different urgency level to be moved to inside master processes, data such as region assignment info has top urgency level to be moved inside master processes, configuration-like data has less urgent level(which needs additional watch/notify feature), liveness monitoring functionality has the least urgent level(which needs additional heart-beat feature)...we can remarkably improve master failover performance and eliminate inconsistency by replicating region assignment info among master processes&apos; memory, the foremost concern/goal of this jira.&lt;/p&gt;

&lt;p&gt;To eliminate ZK as a whole to reduce deploying and machine number/roles, as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; said &quot;...simplifying hbase packaging and deploy. There would be no more need to set aside machines for this special role&quot;, we also need to implement liveness monitoring(for regionservers) and watch/notify features within master processes...&lt;/p&gt;

&lt;p&gt;Above features are independent and can be implemented in an incremental fashion, that&apos;s what I meant by &apos;incremental&apos;, but certainly we can implemented them as a whole.&lt;/p&gt;

&lt;p&gt;Not sure I understand your question correctly, and wonder whether I answer your question, any further clarification is welcome &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13894862" author="enis" created="Fri, 7 Feb 2014 19:01:21 +0000"  >&lt;p&gt;Agreed that the most urgent thing is zk assignment. There has been at least 3 proposals so far for a master + assignment rewrite in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5487&quot; title=&quot;Generic framework for Master-coordinated tasks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5487&quot;&gt;HBASE-5487&lt;/a&gt;, and all want to get rid of zk and fix assignment.&lt;br/&gt;
What I was trying to understand is about the deployment. I was assuming the RAFT quorum servers will be master processes as well. &lt;br/&gt;
Currently it is sufficient to have 1 master, 1 backup and 3 zk servers for HA. With some master  functionality implemented with RAFT but still using zk, we would need at least 3 zk servers, and 3 master servers for full HA, which is a change in the requirement for minimum HA setup. &lt;/p&gt;

&lt;p&gt;However, with the incremental approach, we might even implement RAFT quorum inside region server processes, so that we gradually get rid of the master role as well, and have only 1 type of server, where (2n+1) of them would act like masters (while still serving data). &lt;/p&gt;

&lt;p&gt;How do you imagine the typical small / medium sized deployment will be? &lt;/p&gt;</comment>
                            <comment id="13895363" author="fenghh" created="Sat, 8 Feb 2014 01:50:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;There has been at least 3 proposals so far for a master + assignment rewrite in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5487&quot; title=&quot;Generic framework for Master-coordinated tasks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5487&quot;&gt;HBASE-5487&lt;/a&gt;, and all want to get rid of zk and fix assignment.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree, but all those proposals still use third-party storage(from zk to auxiliary system table) outside of master processes/machines for persisting data such as assign status information, so:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;the new active master needs to read those data from outside third-party storage before serving as active master after the previous master dies, hence with suboptimal master failover performance.&lt;/li&gt;
	&lt;li&gt;the same data/information still be maintained in two different locations: master memory and outside third-party storage, hence with potential consistency issues&lt;/li&gt;
&lt;/ol&gt;


&lt;blockquote&gt;&lt;p&gt;What I was trying to understand is about the deployment...with the incremental approach, we might even implement RAFT quorum inside region server processes, so that we gradually get rid of the master role as well, and have only 1 type of server, where (2n+1) of them would act like masters (while still serving data).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Now I can understand what you meant&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. If we take incremental approach then 3 zk , 3 master and N regionservers, yes it&apos;s a suboptimal setup&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;br/&gt;
If we implement all functionalities that zk provides for HBase such as data replicating, master election, liveness monitor and watch/notify and  eliminate zk totally, the deployment of a HBase is (3 master + N regionserver)&lt;br/&gt;
Though it&apos;s workable eventually to concurrently run master and regionserver roles within a single server, I&apos;m not a fan of this deployment:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;master and regionserver roles can affect each other, it&apos;s hard to debug/diagnose when issue arises&lt;/li&gt;
	&lt;li&gt;master and regionserver are both memory-consuming, for servers concurrently running both roles we need to balance the memory usage, and for servers running only regionserver role we need regionserver memory/heap configuration different from running both roles to take full advantage of the available memory&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13900045" author="clehene" created="Thu, 13 Feb 2014 06:06:39 +0000"  >&lt;p&gt;Zookeeper is used also by HDFS, Kafka, Storm as well as several other systems. Is it realistic (or desirable) to assume it would go away (from an operations standpoint)? (With raft-go there&apos;s etcd, for example).&lt;/p&gt;

&lt;p&gt;Will a library based approach simplify the code overall or make it easier to understand? it seems that it will make at least some parts more complex.&lt;br/&gt;
What aspects of the system will be improved by the lower latencies? I&apos;m not really clear on the faster master failover benefit. Will this improve region reassignment in a manner that could not be achieved without it?&lt;/p&gt;</comment>
                            <comment id="13900208" author="fenghh" created="Thu, 13 Feb 2014 10:46:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Zookeeper is used also by HDFS, Kafka, Storm as well as several other systems. Is it realistic (or desirable) to assume it would go away (from an operations standpoint)? (With raft-go there&apos;s etcd, for example).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not quite. For applications which just need a simple/reliable storage for storing small amount of configuration or meta like data with sparse access, Zookeeper still has advantage over raft-based solution:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Economical: Zookeeper can be shared among a big number of applications with such simple storage requirement, but for raft-based solution each application need to allocate its own separate 3-5 nodes for replication purpose.&lt;/li&gt;
	&lt;li&gt;Simple: Application code is simple by just calling Zookeeper API to create/read/write node/data to/from Zookeeper, while raft-based solution need to write more complex interacting code between application code and raft library such as passing/converting raft write/log to in-memory data structure with application-specific meaning, and snapshot making and log truncate, etc.&lt;/li&gt;
	&lt;li&gt;Convenient: Zookeeper&apos;s tree-like hierarchical structure for organizing data and watch/notify mechanism is convenient for application to represent data and organize code, as long as watch/notify mechanism is not used to implement state-machine-like logic with the &apos;A process changes a znode, B process watches that znode and then reads the znode value to trigger its state-machine&apos; pattern&lt;br/&gt;
In short, raft-based solution is somewhat an overkill for such applications with simple, small and sparse-access storage requirement.&lt;/li&gt;
&lt;/ol&gt;


&lt;blockquote&gt;&lt;p&gt;Will a library based approach simplify the code overall or make it easier to understand? it seems that it will make at least some parts more complex. What aspects of the system will be improved by the lower latencies? I&apos;m not really clear on the faster master failover benefit. Will this improve region reassignment in a manner that could not be achieved without it?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For HMaster, raft-based approach has below benefits:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;For assign(split/merge) state machine logic, raft-based approach eliminates the potentials for state inconsistency. HMaster&apos;s current implementation suffers from two facts which can result in consistency issues: 1) Zookeeper&apos;s watch/notify mechanism is used to maintain the assign state machine; 2) assign status is stored in multiple places(master&apos;s memory, Zookeeper), so it always has the headache to guarantee the data consistency among those different places&lt;/li&gt;
	&lt;li&gt;Better master failover performance. New master can immediately play as active master after previous active one dies, without first reading from external storage to rebuild in-memory state(current HBase&apos;s approach) or querying from regionservers and rebuild the in-memory state about the cluster(Bigtable&apos;s approach, personally I think Bigtable&apos;s master startup code should be even more complicated than HBase since it needs to reason out the correct &apos;cluster state&apos; by response from regionservers, not say regionservers can fail during master startup process...)&lt;/li&gt;
	&lt;li&gt;Better whole-cluster restart performance. For cluster with big number of regions(say 10K-100K), during the cluster restart master need to do assignment for all the regions, hence result in access to Zookeeper in a very frequent fashion, due to the fact that only a single IO thread and a single event thread are used by master to communicate with Zookeeper, the interaction with Zookeeper can be an obvious bottleneck for the cluster restart, while raft-based approach can perform much better here.&lt;/li&gt;
	&lt;li&gt;Simpler deployment. HBase with raft-based approach&apos;s deployment is &apos;3 master + n regionserver&apos;, while Zookeeper solution is &apos; 3 Zookeeper + 2+ master + n regionserver&apos;. We can&apos;t assume applications running HBase can always find a shared Zookeeper to use.&lt;/li&gt;
	&lt;li&gt;Isolation. Zookeeper-approach HBase cluster can be affected by other applications which may slow down or even turn down by abusing or misusing the shared Zookeeper that our HBase relies on, while raft-based doesn&apos;t need to worry about this.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13900266" author="fanster.z" created="Thu, 13 Feb 2014 12:08:13 +0000"  >&lt;p&gt;ZK is not good enough, but do it by your own will make things worse.&lt;br/&gt;
The only real problem I can see is that ZK is not strong consistent.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But zk as a communication channel is fragile due to its one-time watch and asynchronous notification mechanism which together can leads to missed events(hence missed messages), &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This can be done with the existed API (but performance is much inefficient than chubby).&lt;/p&gt;</comment>
                            <comment id="13902319" author="fenghh" created="Sat, 15 Feb 2014 04:12:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Zookeeper is used also by HDFS, Kafka, Storm as well as several other systems. Is it realistic (or desirable) to assume it would go away (from an operations standpoint)?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Maybe my above answer to this question is a bit too general&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. To be specific for HDFS, personally I think name node also can apply the idea of this jira: the meta data maintained by name nodes can be replicated among all name nodes using consensus lib, removing ZKFC and JournalNodes. I was surprised by finding so many roles/processes introduced to accomplish HDFS HA for the first time, it would be due to some historical reason, right? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13902322" author="fenghh" created="Sat, 15 Feb 2014 04:27:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;ZK is not good enough, but do it by your own will make things worse.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Would you list the detailed reasons for this statement? Do you mean the coding complexity and correctness risk when implementing our own consensus lib when saying &apos;will make things worse&apos;? Or anything else? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The only real problem I can see is that ZK is not strong consistent.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ZK itself should be strong consistent, right? But our ZK usage of &apos;A process changes a znode, B process watches that znode and then reads the znode value to trigger its state-machine&apos; pattern for maintaining the state-machine logic(especially assign state-machine) results in the inconsistency problem in HMaster...but the data/states we put in ZK still have consistency, right?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This can be done with the existed API (but performance is much inefficient than chubby).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Actually if we make HMaster the arbitrator and only HMaster can write to ZK, ZK acts as the only truth holder, regionservers can&apos;t write/update the states directly to ZK but talk to HMaster and HMaster updates to ZK for them...this way the current inconsistency issue of HMaster can be remarkably alleviated. But still need careful treatment/handling for maintaining the consistency between ZK and HMaster&apos;s in-memory data...&lt;/p&gt;</comment>
                            <comment id="13961549" author="pablomedina85" created="Sun, 6 Apr 2014 22:12:59 +0000"  >&lt;p&gt;Hi all,&lt;br/&gt;
I&apos;m the author of CKite (&lt;a href=&quot;https://github.com/pablosmedina/ckite&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/pablosmedina/ckite&lt;/a&gt;) a JVM library implementation of Raft. It covers all the Raft consensus protocol functionality and it has an easy to use API for both Java and Scala. I think it can be easily integrated into HBase designing a good StateMachine and its respective Commands. CKite can provide some listeners about elections and others specific listeners can be implemented in the StateMachine itself as the reception of Commands. I would be glad to start working on a draft integration to demonstrate its capabilities. I have experience working with HBase in production so It would be exciting to collaborate on this. If you were to start this in an incremental way, which step would be the better one to start with?&lt;/p&gt;</comment>
                            <comment id="13961554" author="lhofhansl" created="Sun, 6 Apr 2014 22:36:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pablomedina85&quot; class=&quot;user-hover&quot; rel=&quot;pablomedina85&quot;&gt;Pablo Medina&lt;/a&gt;, great. I&apos;d be reluctant pulling a Scala dependency into HBase, but it would be great to have access to an easy Raft API in HBase. I imagine we can use that even to have multiple replicas of regions in multiple RegionServers eventually.&lt;/p&gt;</comment>
                            <comment id="13961719" author="mantonov" created="Mon, 7 Apr 2014 08:56:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pablomedina85&quot; class=&quot;user-hover&quot; rel=&quot;pablomedina85&quot;&gt;Pablo Medina&lt;/a&gt; you may want to take a look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10909&quot; title=&quot;Abstract out ZooKeeper usage in HBase - phase 1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10909&quot;&gt;&lt;del&gt;HBASE-10909&lt;/del&gt;&lt;/a&gt; (and pdf attached to it). As ZooKeeper is used in many places throughout, integration with other consensus libs to replace it would require certain refactoring of current codebase.&lt;/p&gt;
</comment>
                            <comment id="13961721" author="mantonov" created="Mon, 7 Apr 2014 08:57:41 +0000"  >&lt;p&gt;Shall we update the title of this jira to reflect the fact that this consensus lib thing is broader than just master process (and failover performance specifically)?&lt;/p&gt;</comment>
                            <comment id="13961723" author="mantonov" created="Mon, 7 Apr 2014 09:00:18 +0000"  >&lt;p&gt;Linked to hbase-10909 as it seems like a prerequisite&lt;/p&gt;</comment>
                            <comment id="13962368" author="pablomedina85" created="Mon, 7 Apr 2014 23:09:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; Great. I&apos;ll take a look at your patches and see If I can help. It is a great start.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12601403">ACCUMULO-715</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12687786">HBASE-10295</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12706580">HBASE-10909</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 8 Jan 2014 07:16:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>366805</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 36 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1r86v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>367116</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>