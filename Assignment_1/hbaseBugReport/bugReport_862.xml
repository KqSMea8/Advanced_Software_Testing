<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:48:12 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-862/HBASE-862.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-862] region balancing is clumsy</title>
                <link>https://issues.apache.org/jira/browse/HBASE-862</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Daniel Leffel has an install of 500 regions on 4 nodes.  He&apos;s running 0.2.0.&lt;/p&gt;

&lt;p&gt;On restart, load balancing is running while the 600 regions are being initially opened.  Makes for churn.  Load balancing should wait before it cuts in.&lt;/p&gt;

&lt;p&gt;Have also seen on occasion that it will not find equilibrium after a restart.&lt;/p&gt;

&lt;p&gt;Adding a node is catastrophic.  &amp;gt;20% of the regions were closed and were taking the longest time to show up on the new server.  I would think that the region balancing would work in more sophisticated and gradual manner.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12403560">HBASE-862</key>
            <summary>region balancing is clumsy</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Tue, 2 Sep 2008 21:08:01 +0000</created>
                <updated>Wed, 13 Jul 2011 23:32:27 +0000</updated>
                            <resolved>Fri, 21 Aug 2009 23:01:33 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12627812" author="jdcryans" created="Tue, 2 Sep 2008 21:11:56 +0000"  >&lt;p&gt;+1, I see it too.&lt;/p&gt;</comment>
                            <comment id="12627907" author="viper799" created="Wed, 3 Sep 2008 05:16:02 +0000"  >&lt;p&gt;+1 I see this also. &lt;/p&gt;

&lt;p&gt;I also see MR jobs fail often if I add a region server to the cluster while the job is running.&lt;br/&gt;
I thank this is sometimes from closing regions that are running a timely compaction and will not close for a while to be redeployed.&lt;/p&gt;

&lt;p&gt;What about when we send the request to close a region make it different from normal close call and give the region server a option to decline the request&lt;br/&gt;
example say the master sends a request to close a small group of regions to redeploy and the region server have 1 or more of the regions queued up for compaction&lt;br/&gt;
let the region server send a request back to the master declining the regions that are in the compaction queue or if they have a open scanner on them etc...&lt;/p&gt;

&lt;p&gt;also I would slow down the redeploy of the regions to 1-3 in a cycle where we wait until all the regions are open again before moving more.&lt;br/&gt;
We also might build in some give in the numbers per server to make it less likely to move a region if one of the servers is 1-3 regions or 1-5%  out of balance.&lt;br/&gt;
I would like to see the balancer keep everything even but I would be ok with it leavening it a little out of balance.&lt;br/&gt;
Maybe we can use something like the lease timeout var from the config to define how often the balancer runs a cycle.&lt;/p&gt;

&lt;p&gt;My down the road wish list is one day be able report back to the master in the heartbeat the load on the regions that a region server has and generate a read/write load numbers per region/table/server/cluster/etc..&lt;br/&gt;
With this data we could be more sophisticated on what regions to move and when.&lt;/p&gt;</comment>
                            <comment id="12628285" author="apurtell" created="Thu, 4 Sep 2008 09:24:05 +0000"  >&lt;p&gt;Let me take a look at this one if nobody else is working on it. Will incorporate stack and Billy&apos;s thoughts into more sophisticated strategy that balances balancing against regionserver load and region availability considerations, ready for 0.2.2. We&apos;ll be deploying 10 additional nodes into a running 15 node cluster with 1000s of regions running a continuous cycle of TableMap/TableReduce tasks so there will be ample opportunity for testing. &lt;/p&gt;</comment>
                            <comment id="12628366" author="jimk" created="Thu, 4 Sep 2008 16:10:40 +0000"  >&lt;p&gt;If you factor in load, you&apos;ll need a more sophisticated algorithm than we had in the past. Because the META region gets so much traffic, the previous algorithm would never assign any other regions to the server handling META (or rarely).&lt;/p&gt;

&lt;p&gt;Since we&apos;ve already discussed not doing balancing until all regions are loaded, maybe factoring (a more sophisticated) load at that point would be ok.&lt;/p&gt;</comment>
                            <comment id="12628940" author="viper799" created="Sun, 7 Sep 2008 07:33:07 +0000"  >&lt;p&gt;I thank a simple way to do startups is using a round ribbon assignment of regions&lt;br/&gt;
Just a thought.&lt;/p&gt;</comment>
                            <comment id="12629008" author="apurtell" created="Sun, 7 Sep 2008 20:33:45 +0000"  >&lt;p&gt;One problem with simple RR is that not all regionservers may come up at the same time. Still need to consider region counts (but not load) in the beginning to try to smooth assignments out. &lt;/p&gt;

&lt;p&gt;Also I will try modifying bin/hbase-daemons.sh to issue commands via SSH in parallel rather than sequentially, and have start-hbase.sh bring up the regionservers ahead of the master. &lt;/p&gt;</comment>
                            <comment id="12629038" author="jimk" created="Mon, 8 Sep 2008 01:48:17 +0000"  >&lt;p&gt;When a cluster starts, it might help if the master knew how many region servers to expect to report in. It could then divide the number of regions by n servers, and if n servers didn&apos;t report in by the time the root and meta were assigned (for example), or after some configurable interval, the master could assume that the servers were not going to report in and assign the remaining regions over the n - (missing servers)&lt;/p&gt;</comment>
                            <comment id="12629060" author="apurtell" created="Mon, 8 Sep 2008 03:37:07 +0000"  >&lt;p&gt;For cluster startup, how about:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Master waits a short interval for regionserver start messages to arrive.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;After the initial waiting period has elapsed, begin assigning regions based on the current count of announced regionservers.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;If additional servers report in, try to get their region count up to the current average before assigning additional regions to earlier reporters.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Once all regions have been assigned, wait a dampening period before starting the balancer.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Combine the above with changes to start-hbase.sh and hbase-daemons.sh to start the regionservers ahead of the master and start the regionservers in parallel rather than serially, and I think the startup behavior will improve. &lt;/p&gt;

&lt;p&gt;Balancing regions in steady state, especially if there are late or new arrivals to the cluster, is a different proposition I think. Billy had some ideas about that in an earlier comment. Also some special case handling of META is in order.  &lt;/p&gt;

&lt;p&gt;Concerning balancing in steady state, &apos;load balancing&apos; attempts to ensure that the workload on each host is within a small degree of the workload present on every other host in the system. &apos;Load leveling&apos; on the other hand is a more relaxed approach that only seeks to avoid congestion on any one host. Balancing is proactive. Leveling is reactive. I think both achieve the same end over time (with balancing &quot;trying harder&quot;) and since leveling is simpler and requires little work or coordination on the part of the master, I thought I&apos;d try that first.&lt;/p&gt;</comment>
                            <comment id="12629071" author="viper799" created="Mon, 8 Sep 2008 05:02:48 +0000"  >&lt;p&gt;I added some ideas to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-678&quot; title=&quot;hbase needs a &amp;#39;safe-mode&amp;#39;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-678&quot;&gt;&lt;del&gt;HBASE-678&lt;/del&gt;&lt;/a&gt; that might be helpful for balancing on start up later down the road when we get a safe mode.&lt;/p&gt;</comment>
                            <comment id="12629213" author="stack" created="Mon, 8 Sep 2008 17:08:41 +0000"  >&lt;p&gt;Sounds great Andrew.&lt;/p&gt;</comment>
                            <comment id="12630035" author="osm" created="Wed, 10 Sep 2008 23:33:42 +0000"  >&lt;p&gt;+1 &lt;br/&gt;
Ive seen this when starting up my test cluster with about 300 regions on 4 nodes.  It usually subsides after a few minutes.&lt;/p&gt;</comment>
                            <comment id="12659849" author="apurtell" created="Tue, 30 Dec 2008 16:59:20 +0000"  >&lt;p&gt;Seeing this on our cluster now. Master starts up in safe mode. All HRS start up and check in. Then initial assignments start. Assignment is lumpy &amp;#8211; average load should be e.g. 40, some HRS get e.g. 80 initial assignments. They report as overloaded, so then regions are closed and moved almost as soon as they are opened. Compaction/split load is increased unnecessarily. &lt;/p&gt;

&lt;p&gt;Also seeing asserts about illegal state transitions from the Master.&lt;/p&gt;

&lt;p&gt;All regions do come up assigned and HRS are evenly loaded after a while, unless compaction/split load overwhelms DFS. Then usually there are a few regions that did not deploy correctly that have to be manually (re)deployed with close_region from the shell. &lt;/p&gt;</comment>
                            <comment id="12664881" author="stack" created="Sat, 17 Jan 2009 23:12:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1124&quot; title=&quot;Balancer kicks in way too early&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1124&quot;&gt;&lt;del&gt;HBASE-1124&lt;/del&gt;&lt;/a&gt; should have helped this issue as well as others that made balancing sloppier.&lt;/p&gt;</comment>
                            <comment id="12746338" author="stack" created="Fri, 21 Aug 2009 23:01:32 +0000"  >&lt;p&gt;Resolving as old; region balancing is not too bad.  Ryan opened a new issue for new balancing issues seen with bigger numbers of regions.&lt;/p&gt;</comment>
                            <comment id="13064940" author="nspiegelberg" created="Wed, 13 Jul 2011 23:11:16 +0000"  >&lt;p&gt;We got bit by this in our 89 branch a couple weeks back.  Looking at the trunk and I don&apos;t see &quot;hbase.regions.close.max&quot; anywhere?  Is this still an issue?  What are we doing to limit regionsToMove.size()?&lt;/p&gt;</comment>
                            <comment id="13064949" author="nspiegelberg" created="Wed, 13 Jul 2011 23:28:48 +0000"  >&lt;p&gt;Sorry, our internal issue is slightly different than this one.  The basic gist I&apos;m thinking is that we need a region rebalancing threshold.  If something goes wrong with the load balancer, don&apos;t have more than X% of a cluster&apos;s regions being moved/unassigned at a time.&lt;/p&gt;</comment>
                            <comment id="13064951" author="yuzhihong@gmail.com" created="Wed, 13 Jul 2011 23:32:27 +0000"  >&lt;p&gt;@Nicholas:&lt;br/&gt;
You can use hbase.balancer.max.balancing to specify the total amount of time each balance() call can take.&lt;br/&gt;
Since that is a new parameter, we expect it not to be set normally. In that case, we use half of hbase.balancer.period for this limit.&lt;br/&gt;
If balancing is limited, you would see the following in master log:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;            LOG.debug(&quot;No more balancing till next balance run; maximumBalanceTime=&quot; +
              maximumBalanceTime);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12406228">HBASE-920</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12406182">HBASE-918</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12409044">HBASE-1017</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 2 Sep 2008 21:11:56 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25442</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 23 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h9wn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98867</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>