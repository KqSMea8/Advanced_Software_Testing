<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:01:20 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2375/HBASE-2375.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2375] Revisit compaction configuration parameters</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2375</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently we will make the decision to split a region when a single StoreFile in a single family exceeds the maximum region size.  This issue is about changing the decision to split to be based on the aggregate size of all StoreFiles in a single family (but still not aggregating across families).  This would move a check to split after flushes rather than after compactions.  This issue should also deal with revisiting our default values for some related configuration parameters.&lt;/p&gt;

&lt;p&gt;The motivating factor for this change comes from watching the behavior of RegionServers during heavy write scenarios.&lt;/p&gt;

&lt;p&gt;Today the default behavior goes like this:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We fill up regions, and as long as you are not under global RS heap pressure, you will write out 64MB (hbase.hregion.memstore.flush.size) StoreFiles.&lt;/li&gt;
	&lt;li&gt;After we get 3 StoreFiles (hbase.hstore.compactionThreshold) we trigger a compaction on this region.&lt;/li&gt;
	&lt;li&gt;Compaction queues notwithstanding, this will create a 192MB file, not triggering a split based on max region size (hbase.hregion.max.filesize).&lt;/li&gt;
	&lt;li&gt;You&apos;ll then flush two more 64MB MemStores and hit the compactionThreshold and trigger a compaction.&lt;/li&gt;
	&lt;li&gt;You end up with 192 + 64 + 64 in a single compaction.  This will create a single 320MB and will trigger a split.&lt;/li&gt;
	&lt;li&gt;While you are performing the compaction (which now writes out 64MB more than the split size, so is about 5X slower than the time it takes to do a single flush), you are still taking on additional writes into MemStore.&lt;/li&gt;
	&lt;li&gt;Compaction finishes, decision to split is made, region is closed.  The region now has to flush whichever edits made it to MemStore while the compaction ran.  This flushing, in our tests, is by far the dominating factor in how long data is unavailable during a split.  We measured about 1 second to do the region closing, master assignment, reopening.  Flushing could take 5-6 seconds, during which time the region is unavailable.&lt;/li&gt;
	&lt;li&gt;The daughter regions re-open on the same RS.  Immediately when the StoreFiles are opened, a compaction is triggered across all of their StoreFiles because they contain references.  Since we cannot currently split a split, we need to not hang on to these references for long.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This described behavior is really bad because of how often we have to rewrite data onto HDFS.  Imports are usually just IO bound as the RS waits to flush and compact.  In the above example, the first cell to be inserted into this region ends up being written to HDFS 4 times (initial flush, first compaction w/ no split decision, second compaction w/ split decision, third compaction on daughter region).  In addition, we leave a large window where we take on edits (during the second compaction of 320MB) and then must make the region unavailable as we flush it.&lt;/p&gt;


&lt;p&gt;If we increased the compactionThreshold to be 5 and determined splits based on aggregate size, the behavior becomes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We fill up regions, and as long as you are not under global RS heap pressure, you will write out 64MB (hbase.hregion.memstore.flush.size) StoreFiles.&lt;/li&gt;
	&lt;li&gt;After each MemStore flush, we calculate the aggregate size of all StoreFiles.  We can also check the compactionThreshold.  For the first three flushes, both would not hit the limit.  On the fourth flush, we would see total aggregate size = 256MB and determine to make a split.&lt;/li&gt;
	&lt;li&gt;Decision to split is made, region is closed.  This time, the region just has to flush out whichever edits made it to the MemStore during the snapshot/flush of the previous MemStore.  So this time window has shrunk by more than 75% as it was the time to write 64MB from memory not 320MB from aggregating 5 hdfs files.  This will greatly reduce the time data is unavailable during splits.&lt;/li&gt;
	&lt;li&gt;The daughter regions re-open on the same RS.  Immediately when the StoreFiles are opened, a compaction is triggered across all of their StoreFiles because they contain references.  This would stay the same.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In this example, we only write a given cell twice (instead of 4 times) while drastically reducing data unavailability during splits.  On the original flush, and post-split to remove references.  The other benefit of post-split compaction (which doesn&apos;t change) is that we then get good data locality as the resulting StoreFile will be written to the local DataNode.  In another jira, we should deal with opening up one of the daughter regions on a different RS to distribute load better, but that&apos;s outside the scope of this one.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12460243">HBASE-2375</key>
            <summary>Revisit compaction configuration parameters</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="streamy">Jonathan Gray</assignee>
                                    <reporter username="streamy">Jonathan Gray</reporter>
                        <labels>
                            <label>moved_from_0_20_5</label>
                    </labels>
                <created>Thu, 25 Mar 2010 15:23:13 +0000</created>
                <updated>Wed, 16 Jul 2014 20:47:43 +0000</updated>
                            <resolved>Wed, 16 Jul 2014 20:47:43 +0000</resolved>
                                    <version>0.20.3</version>
                                                    <component>regionserver</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>15</watches>
                                                                <comments>
                            <comment id="12849750" author="jdcryans" created="Thu, 25 Mar 2010 16:23:51 +0000"  >&lt;p&gt;This is only in the case of rapid uploads right? In a &quot;normal&quot; production system you shouldn&apos;t be splitting thousands of times per day, you will flush some old regions because you hit the maximum number of hlogs or it took more than 24h to fill a region so it was major compacted, etc. &lt;/p&gt;

&lt;p&gt;But wrt uploads I like the idea but I have an issue applying that to .META., I&apos;m at the point where I think that we should major compact it at every flush, so compacting it only at 5 files would be killing your performance more than the current situation is.&lt;/p&gt;

&lt;p&gt;Also I don&apos;t think this should be in 0.20, the branch is disrupted enough as it is.&lt;/p&gt;</comment>
                            <comment id="12849765" author="streamy" created="Thu, 25 Mar 2010 16:36:24 +0000"  >&lt;p&gt;It&apos;s not just in the case of rapid uploads, but makes the biggest difference in that circumstance.  I don&apos;t really see the upside to requiring compactions on both sides of a split.  If more flushes happen because of heap pressure, hlogs, majors, etc... that&apos;s fine.  If we hit the compactionThreshold but aren&apos;t bigger than the max region size, we will still compact.  Doesn&apos;t impact that behavior.  We would just do it later (5 files not 3) because of configuration changes.&lt;/p&gt;

&lt;p&gt;I&apos;m +1 on treating META differently.  We should also investigate doing a flush + major compact as a single operation which shouldn&apos;t be so bad to implement.&lt;/p&gt;

&lt;p&gt;As for where to put it, I think we want to get it working here at fb, so whether it makes it into branch or not, if we get the time to implement it we&apos;ll be applying it to our internal 0.20 branch.  Understand the concern though, branch is packed.  In any case, we&apos;ll be doing heavy load testing on it and comparing performance numbers so we&apos;ll see if it&apos;s a big win or not.  We&apos;re totally io bound so I suspect this will make a difference.&lt;/p&gt;</comment>
                            <comment id="12849766" author="jdcryans" created="Thu, 25 Mar 2010 16:39:46 +0000"  >&lt;p&gt;Right I see your point... I&apos;m definitely interested in numbers! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12849767" author="streamy" created="Thu, 25 Mar 2010 16:41:34 +0000"  >&lt;p&gt;Actually if you think about how things behave with lots of smaller flushes, it may be equally as ugly.  If we imagine 16MB flushes, we&apos;ll do 3 * 16 -&amp;gt; 48, +16 +16 -&amp;gt; 80, +16 +16 -&amp;gt; 112, +16 +16 -&amp;gt; 146, etc...  The cost of being compaction-happy is that we end up rewriting data an inordinate amount of times.   The cells in the first flushed file get rewritten over and over again.  Bumping from 3 to 5 reduces rewrites by a significant percentage (what is that, 40% reduction?).&lt;/p&gt;</comment>
                            <comment id="12849769" author="streamy" created="Thu, 25 Mar 2010 16:42:13 +0000"  >&lt;p&gt;We&apos;ll work on a patch for this and try to get numbers soon.  Stay tuned.&lt;/p&gt;</comment>
                            <comment id="12851963" author="stack" created="Wed, 31 Mar 2010 17:08:06 +0000"  >&lt;p&gt;Thanks for doing the detective work Jon.  The original flush/compaction/split configuration &apos;balance&apos; was set in place a very long time ago when things were very different.  Thanks for taking the time to reexamine.&lt;/p&gt;

&lt;p&gt;I see nothing wrong w/ your narrative above.&lt;/p&gt;

&lt;p&gt;To avoid 5 store files &amp;#8211; it&apos;d be better if we never got to this point as J-D hints because looking in many storefiles at once hurts performance &amp;#8211; maybe we could set the threshold down, to 3 again or 4 even, but so 3 files trigger a split, we might need to change flushsize. Up it?  64MB is probably bad since by time it is written out, its going to be &amp;gt; 64MB and so the flush file will span more than on hdfs block anyways.  So we should embrace this fact and set flush up to 96MB?  (This paragraph conflates a few ideas &amp;#8211; flushing and compacting thresholds &amp;#8211; but base idea should come across).&lt;/p&gt;

&lt;p&gt;Chatting up on IRC, this issue is related to hbase-1892 though it does the dbl-flush you mention over there implicitly (as you noted on IRC).&lt;/p&gt;

&lt;p&gt;.bq Also I don&apos;t think this should be in 0.20, the branch is disrupted enough as it is.&lt;/p&gt;

&lt;p&gt;I&apos;d say it depends on how disruptive the patch.  We could get big change in our i/o pattern for small code change.&lt;/p&gt;


</comment>
                            <comment id="12852035" author="apurtell" created="Wed, 31 Mar 2010 18:40:47 +0000"  >&lt;p&gt;Going from .3 to .4, this is really a fork, and we&apos;ve already committed to related disruption.&lt;/p&gt;

&lt;p&gt;We should maintain the philosophy that API and RPC (client and server interfaces) can support rolling restart upgrade on branch, everything else is open for change, especially performance improvements. &lt;/p&gt;

&lt;p&gt;Just my 0.02.&lt;/p&gt;</comment>
                            <comment id="12853026" author="streamy" created="Sat, 3 Apr 2010 00:28:19 +0000"  >&lt;p&gt;Please review.  Tests pass except TestTableIndex is flakey for some reason (seems unrelated).&lt;/p&gt;</comment>
                            <comment id="12853081" author="stack" created="Sat, 3 Apr 2010 05:31:28 +0000"  >&lt;p&gt;In &apos;testForceSplitMultiFamily&apos;, do you intend filling two families or just the second family?  Is the bug that you we don&apos;t split if second family is over split limit?&lt;/p&gt;

&lt;p&gt;In Store#compact, where we do &apos; if(forceSplit) {   &apos;... is this ok if multiple families?  If multiple families, one of the families will have the best midkey... is that what we split on or is it just the first?  (I see later in MemStoreFlusher that it may have the same problem in that it will split on the first family that meets the criteria).  Maybe this is ok for now.  Perhaps file improvement to improve on this behavior.&lt;/p&gt;

&lt;p&gt;I&apos;m do not follow this bit:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-comment&quot;&gt;// Do not trigger any splits otherwise, so always &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; references=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&lt;/span&gt;+      &lt;span class=&quot;code-comment&quot;&gt;// which will prevent splitting.
&lt;/span&gt;+       
       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fs.exists(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.regionCompactionDir) &amp;amp;&amp;amp;
           !fs.mkdirs(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.regionCompactionDir)) {
         LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Mkdir on &quot;&lt;/span&gt; + &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.regionCompactionDir.toString() + &lt;span class=&quot;code-quote&quot;&gt;&quot; failed&quot;&lt;/span&gt;);
-        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; checkSplit(forceSplit);
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; DO_NOT_SPLIT;
       }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we falied make the compactiondir, we used to check for split, now we don&apos;t split.  Whats up here?&lt;/p&gt;

&lt;p&gt;Same later in the file at #238 in patch and at #247.&lt;/p&gt;

&lt;p&gt;Should we change this data member name and the configuration that feeds it, its description at least to explain now we are doing size of Store rather then maximum file size.&lt;/p&gt;

&lt;p&gt;Thinking about it, we&apos;ll now split more often &amp;#8211; because we split sooner (see your first narrative above Jon and no need to wait on compaction to finish before we split &amp;#8211; this takes time).  Also, split will make more references than in past because usually in past we&apos;d split one file after big compaction.  Now we split before compaction so the 3 or 4 files in family will all be split using References.  Ain&apos;t sure if this will make any difference.  Just stating what will happen.&lt;/p&gt;

&lt;p&gt;This change in default needs to make it out to hbase-default.xml: &lt;/p&gt;

&lt;p&gt;484 +        conf.getInt(&quot;hbase.hstore.compactionThreshold&quot;, 5);&lt;/p&gt;

&lt;p&gt;Otherwise, patch looks good.  Did you try it?&lt;/p&gt;


</comment>
                            <comment id="12853149" author="stack" created="Sat, 3 Apr 2010 15:24:42 +0000"  >&lt;p&gt;FYI, this patch does not fail the TestTableIndex over here.&lt;/p&gt;</comment>
                            <comment id="12853158" author="apurtell" created="Sat, 3 Apr 2010 16:42:28 +0000"  >&lt;p&gt;I find some e.g. IHBase tests intermittently fail. The prior test does not always shut down in time (or at all), leading to initialization problems of the subsequent test. Is it like this? Check the exceptions in the log of the failed test. Or run the failed test alone to confirm the above effect.&lt;/p&gt;
</comment>
                            <comment id="12853169" author="kannanm" created="Sat, 3 Apr 2010 17:34:47 +0000"  >&lt;p&gt;Jonathan wrote: &amp;lt;&amp;lt;&amp;lt;Tests pass except TestTableIndex is flakey for some reason (seems unrelated).&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;Yes, this started happening in our internal branch when we refreshed from our internal hadoop branch. Karthik was looking into this yesterday... seemed like some env/setup issue... but it is unrelated to your stuff.&lt;/p&gt;</comment>
                            <comment id="12853211" author="stack" created="Sun, 4 Apr 2010 00:17:28 +0000"  >&lt;p&gt;I ran the patch a while.  Seems to work basically.  Lets fix them ugly DEBUG NotServingRegion messages &amp;#8211; make them one liners and not exceptions on server?&lt;/p&gt;

&lt;p&gt;Can you make this message better detailed:&lt;/p&gt;

&lt;p&gt;2010-04-03 16:10:14,872 INFO org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Split triggered on region TestTable,0115344204,1270339622626 because Store info exceeds max size&lt;/p&gt;

&lt;p&gt;What is max size and what is our current size and file count might be of use.&lt;/p&gt;

&lt;p&gt;Is this right?  Its confusing at least:&lt;/p&gt;

&lt;p&gt;The above line was followed by this:&lt;/p&gt;

&lt;p&gt;2010-04-03 16:10:14,872 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region TestTable,0115344204,1270339622626/81593204 because: store size &amp;gt; max size&lt;/p&gt;

&lt;p&gt;I thought we&apos;d just said we were going to split and here we are now compacting?&lt;/p&gt;

&lt;p&gt;Here is another example:&lt;/p&gt;

&lt;p&gt;2010-04-03 16:10:27,725 INFO org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Split triggered on region TestTable,0186659666,1270339709264 because Store info exceeds max size&lt;br/&gt;
2010-04-03 16:10:27,725 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region TestTable,0186659666,1270339709264/814994791 because: store size &amp;gt; max size&lt;/p&gt;

&lt;p&gt;Good stuff Jon.&lt;/p&gt;</comment>
                            <comment id="12853218" author="streamy" created="Sun, 4 Apr 2010 03:54:29 +0000"  >&lt;p&gt;Thanks stack.  I will run full cluster tests on Monday and work on  &lt;br/&gt;
cleaning up log messages.&lt;/p&gt;</comment>
                            <comment id="12854728" author="stack" created="Wed, 7 Apr 2010 23:33:48 +0000"  >&lt;p&gt;I did some more testing on this patch and we seem to be running into the &apos;too many store files&apos; issue more often so as said up on IRC, maybe a flush that melds the memstore with a compaction would be needed now we compact less.&lt;/p&gt;</comment>
                            <comment id="12857194" author="kannanm" created="Thu, 15 Apr 2010 05:02:49 +0000"  >&lt;p&gt;I was seeing the  &apos;too many store files issue&apos; recently too, but that was even without this patch. And after &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2439&quot; title=&quot;HBase can get stuck if updates to META are blocked&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2439&quot;&gt;&lt;del&gt;HBASE-2439&lt;/del&gt;&lt;/a&gt; fix, I don&apos;t see this that much.&lt;/p&gt;

&lt;p&gt;One data point: Without the patch, a import test I ran with 1M keys, ~1000 columns each of ~512bytes took about 15 hrs without the patch, and 8 hours with the patch! Basically the patch halfs the number of split related compactions-- and the import test was mostly busy doing split related compactions.&lt;/p&gt;

&lt;p&gt;We did notice some unevenly sized regions at the end of the run... but not sure if that is related to the patch. Will look more into it tomorrow.&lt;/p&gt;</comment>
                            <comment id="12857344" author="stack" created="Thu, 15 Apr 2010 14:54:03 +0000"  >&lt;p&gt;We need this patch (smile)&lt;/p&gt;</comment>
                            <comment id="12866810" author="stack" created="Wed, 12 May 2010 23:48:23 +0000"  >&lt;p&gt;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.&lt;/p&gt;</comment>
                            <comment id="12874199" author="whitingj" created="Tue, 1 Jun 2010 19:46:34 +0000"  >&lt;p&gt;The optimizations here look great.  It seems like an additional optimization could still be made.  Looking at the patch, there doesn&apos;t seem to be any prioritization of compaction requests.  So if you have a region server that is in charge of a large number of regions the compaction queue can still get quite large and prevent more important compactions from happening in a timely manner.    I implemented a priority queue for compactions that may make a lot of sense to include with these optimizations (see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2646&quot; title=&quot;Compaction requests should be prioritized to prevent blocking&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2646&quot;&gt;&lt;del&gt;HBASE-2646&lt;/del&gt;&lt;/a&gt;).  &lt;/p&gt;</comment>
                            <comment id="12902178" author="jdcryans" created="Wed, 25 Aug 2010 00:03:02 +0000"  >&lt;p&gt;Do we have enough time do this if 0.90.0 is due for HW? Punt?&lt;/p&gt;</comment>
                            <comment id="12918180" author="streamy" created="Tue, 5 Oct 2010 20:59:02 +0000"  >&lt;p&gt;Punting to 0.92 for now.  The bigger compaction/flush improvements should happen in that version.&lt;/p&gt;</comment>
                            <comment id="12970319" author="stack" created="Fri, 10 Dec 2010 21:36:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3312&quot; title=&quot;Region fails to split&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3312&quot;&gt;&lt;del&gt;HBASE-3312&lt;/del&gt;&lt;/a&gt; has an interesting pathological case that this issue is supposed to fix.  Linking to it so we don&apos;t forget.&lt;/p&gt;</comment>
                            <comment id="13009431" author="streamy" created="Mon, 21 Mar 2011 22:03:59 +0000"  >&lt;p&gt;punting from 0.92.  still needs to be done but should not be tied to a version until work is being actively done&lt;/p&gt;</comment>
                            <comment id="13204701" author="jdcryans" created="Thu, 9 Feb 2012 18:08:17 +0000"  >&lt;p&gt;A bunch of things changed since this jira was created: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;we now split based on the store size&lt;/li&gt;
	&lt;li&gt;regions split at 1GB&lt;/li&gt;
	&lt;li&gt;memstores flush at 128MB&lt;/li&gt;
	&lt;li&gt;there&apos;s been a lot of work on tuning the store file selection algorithm&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;My understanding of this jira is that it aims at making the &quot;out of the box mass import&quot; experience better. Now that we have bulk loads and pre-splitting this use case is becoming less and less important... although we still see people trying to benchmark it (hi hypertable). &lt;/p&gt;

&lt;p&gt;I see three things we could do:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Trigger splits after flushes, I hacked a patch and it works awesomely&lt;/li&gt;
	&lt;li&gt;Have a lower split size for newly created tables. Hypertable does this with a soft limit that gets doubled every time the table splits until it reaches the normal split size&lt;/li&gt;
	&lt;li&gt;Have multi-way splits (Todd&apos;s idea), so that if you have enough data that you know you&apos;re going to be splitting after the current split then just spawn as many daughters as you need.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; I&apos;m planning on just fixing the first bullet point in the context of this jira. Maybe there&apos;s another stuff from the patch in this jira that we could fit in.&lt;/p&gt;</comment>
                            <comment id="13204779" author="stack" created="Thu, 9 Feb 2012 19:48:44 +0000"  >&lt;p&gt;Doing first bullet point only sounds good.  Lets file issues for the split other suggestions.&lt;/p&gt;

&lt;p&gt;What about the other recommendations made up in the issue regards compactionThreshold.&lt;/p&gt;

&lt;p&gt;Upping compactionThreshold from 3 to 5 where 5 is &amp;gt; than the number of flushes it would take to make us splittable; i.e. the intent is no compaction before first split.&lt;/p&gt;

&lt;p&gt;Should we do this too as part of this issue?  We could make our flush size 256M and compactionThreshold 5.  Or perhaps thats too rad (thats a big Map to be carrying around)?  Instead up the compactionThreshold and down the default regionsize from 1G to 512M and keep flush at 128M?&lt;/p&gt;

&lt;p&gt;I took a look at patch and its pretty stale now given changes that have gone in since.&lt;/p&gt;
</comment>
                            <comment id="13204797" author="jdcryans" created="Thu, 9 Feb 2012 20:00:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;Doing first bullet point only sounds good. Lets file issues for the split other suggestions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Kewl.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Upping compactionThreshold from 3 to 5 where 5 is &amp;gt; than the number of flushes it would take to make us splittable; i.e. the intent is no compaction before first split.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds like a change that can have a bigger impact but that mostly helps this specific use case...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Instead up the compactionThreshold and down the default regionsize from 1G to 512M and keep flush at 128M?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d rather split earlier for the first regions.&lt;/p&gt;</comment>
                            <comment id="13204820" author="stack" created="Thu, 9 Feb 2012 20:25:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;Upping compactionThreshold from 3 to 5... Sounds like a change that can have a bigger impact but that mostly helps this specific use case...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Dunno.  3 strikes me as one of those decisions that made sense long time ago but a bunch has changed since...  We should test it I suppose.&lt;/p&gt;

&lt;p&gt;On changing flush/regionsize, you&apos;d rather have us split faster then slow as count of regions goes up.  Ok.&lt;/p&gt;</comment>
                            <comment id="13207058" author="jdcryans" created="Mon, 13 Feb 2012 18:43:56 +0000"  >&lt;p&gt;This patch adds a check for split after flushing, I&apos;ve been running this with write heavy tests and it makes it makes you skip at least one compaction before splitting.&lt;/p&gt;</comment>
                            <comment id="13207063" author="stack" created="Mon, 13 Feb 2012 18:48:02 +0000"  >&lt;p&gt;+1 on patch&lt;/p&gt;

&lt;p&gt;Make new issue to change compactionThreshod to at least 4 I&apos;d say and then you can close out this one?&lt;/p&gt;</comment>
                            <comment id="13207079" author="jdcryans" created="Mon, 13 Feb 2012 19:01:19 +0000"  >&lt;p&gt;Changing this jira&apos;s title to reflect that the first part of it is already done and the only work remaining is revisiting the compaction configurations. I created &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5393&quot; title=&quot;Consider splitting after flushing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5393&quot;&gt;&lt;del&gt;HBASE-5393&lt;/del&gt;&lt;/a&gt; for the split after flush patch.&lt;/p&gt;</comment>
                            <comment id="13216164" author="stack" created="Sat, 25 Feb 2012 00:27:25 +0000"  >&lt;p&gt;Split early has been committed too.&lt;/p&gt;

&lt;p&gt;All that remains of this issue is upping default compaction threshold.&lt;/p&gt;</comment>
                            <comment id="14064064" author="stack" created="Wed, 16 Jul 2014 20:47:43 +0000"  >&lt;p&gt;stale&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12460959">HBASE-2399</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12437541">HBASE-1892</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12492100">HBASE-3312</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12468742">HBASE-2821</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12465893">HBASE-2646</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12461337">HBASE-2415</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12514384" name="HBASE-2375-flush-split.patch" size="968" author="jdcryans" created="Mon, 13 Feb 2012 18:43:56 +0000"/>
                            <attachment id="12440661" name="HBASE-2375-v8.patch" size="22974" author="streamy" created="Sat, 3 Apr 2010 00:28:19 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 25 Mar 2010 16:23:51 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32546</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 22 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02ffz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12103</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>