<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:12:15 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-10278/HBASE-10278.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-10278] Provide better write predictability</title>
                <link>https://issues.apache.org/jira/browse/HBASE-10278</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently, HBase has one WAL per region server. &lt;br/&gt;
Whenever there is any latency in the write pipeline (due to whatever reasons such as n/w blip, a node in the pipeline having a bad disk, etc), the overall write latency suffers. &lt;/p&gt;

&lt;p&gt;Jonathan Hsieh and I analyzed various approaches to tackle this issue. We also looked at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt;, which talks about adding concurrent multi WALs. Along with performance numbers, we also focussed on design simplicity, minimum impact on MTTR &amp;amp; Replication, and compatibility with 0.96 and 0.98. Considering all these parameters, we propose a new HLog implementation with WAL Switching functionality.&lt;/p&gt;

&lt;p&gt;Please find attached the design doc for the same. It introduces the WAL Switching feature, and experiments/results of a prototype implementation, showing the benefits of this feature.&lt;br/&gt;
The second goal of this work is to serve as a building block for concurrent multiple WALs feature.&lt;/p&gt;

&lt;p&gt;Please review the doc.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12687287">HBASE-10278</key>
            <summary>Provide better write predictability</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="v.himanshu">Himanshu Vashishtha</assignee>
                                    <reporter username="v.himanshu">Himanshu Vashishtha</reporter>
                        <labels>
                    </labels>
                <created>Sat, 4 Jan 2014 00:58:45 +0000</created>
                <updated>Tue, 16 Dec 2014 23:54:39 +0000</updated>
                                                                            <component>wal</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>27</watches>
                                                                                                            <comments>
                            <comment id="13863843" author="sershe" created="Tue, 7 Jan 2014 02:37:39 +0000"  >&lt;p&gt;Skimmed the doc, looks really nice. I do think that out-of-order WAL should eventually become ok (we will get per-region mvcc from seqId-mvcc merge, and mvcc in WAL from this or several other jiras). One thing I might have missed - since it currently requires log rolling, would it need throttling for switching? If there&apos;s a long sequence of network hiccups from the machine (i.e. to both files), it might roll lots of tiny logs.&lt;/p&gt;</comment>
                            <comment id="13863921" author="ram_krish" created="Tue, 7 Jan 2014 05:27:02 +0000"  >&lt;p&gt;I read this document.  Looks nice.&lt;br/&gt;
Few questions, to clarify if my understanding is right,&lt;br/&gt;
when there is log switch happening say edits from 1 ... 10 are in WAL A.  Due to switch the edits 11 .. 13 are in WAL B.  &lt;br/&gt;
Now if this above mentioned thing is to happen then the log roll for WAL A has to be completed by blocking all writes?  Will this be costly ? How costly will this be.&lt;br/&gt;
If the rollwriter happens and at the same time we start taking writes on WAL B the above mentioned scenario happens.  so in that case we may have out of order edits during log split if this RS crashes right ?.&lt;br/&gt;
Currently the assumption is there are 2 WALs per RS and only one of them is active.  So how do you plan to make the interface for this, in the sense do you have plans to extend this number 2 to something more than 2 ? If so how many of them will be active?&lt;br/&gt;
the reason am asking this is, the doc says this implementation will form the basis for other multi log implementations.  So if that is true, then if is say RS.getLog() how many logs should it return?  currently in testcases and in the HRS.rollWriter() the rolling happens only on one HLog.  But with multiWAL this may change.&lt;br/&gt;
I tried out some interfaces for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8610&quot; title=&quot;Introduce interfaces to support MultiWAL&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8610&quot;&gt;&lt;del&gt;HBASE-8610&lt;/del&gt;&lt;/a&gt; inorder to introduce interfaces for multi WAL.  A very general use case would be to have MultiWAL per table.  If that model needs to fit in here how easy would it be with these interfaces introduced in this JIRA.&lt;/p&gt;

</comment>
                            <comment id="13864017" author="nkeywal" created="Tue, 7 Jan 2014 09:00:40 +0000"  >&lt;p&gt;Great doc. I&apos;m personally very comfortable with the idea of a WAL Switching functionality.&lt;/p&gt;</comment>
                            <comment id="13864711" author="v.himanshu" created="Tue, 7 Jan 2014 21:29:12 +0000"  >&lt;p&gt;Thanks a lot for the reviews and comments guys.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; Yes, merging of seqid-mvcc would help in relaxing the limitation but I think it would be good to have WAL switches compatible with 0.96/0.98 without any other dependency.&lt;br/&gt;
Yes, the switch happens when the current WAL becomes slow. It makes the new WAL active (doesn&apos;t need rolling of the new WAL). The new WAL takes the inflight edits and then starts taking newer edits. Meanwhile the slow old WAL is rolled in parallel. Its not taking any writes, so no throttling is required. If the hiccup stays for long, the new WAL might switch too. In future, we could use some heuristic to monitor switches (current WAL size, last switch time, etc).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;the log roll for WAL A has to be completed by blocking all writes?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;WAL A is not taking any new writes at this moment, as WAL B is the active one. I don&apos;t see any writes blocked by A&apos;s rolling. Read the above explanation and let me know if I am missing anything in your question.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If the rollwriter happens and at the same time we start taking writes on WAL B the above mentioned scenario happens. so in that case we may have out of order edits during log split if this RS crashes right ?.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So, the situation is RS crashes while switching? &lt;br/&gt;
I do see duplicate edits in two WALs (as in-flight edits has to be appended on every switch), but I don&apos;t see out-of-order edits even in this case. Could you please explain how you see out-of-order edits?&lt;/p&gt;

&lt;p&gt;Re: Other implementations:&lt;br/&gt;
The goal is to implement WAL Switching such that other HLog implementations  (such as per table MultiWAL) can re-use it. &lt;br/&gt;
Yes, there is some refactoring required to make test classes use HLog as an interface (currently, they are calling 14 non-interface methods in FSHLog). There are methods which are implementation specific: such as rollLog, getNumberOfWALs, etc. A HLog client (such as Regionserver) shouldn&apos;t really care about that, but implementors do. I plan to do this refactoring here.&lt;/p&gt;</comment>
                            <comment id="13865014" author="xieliang007" created="Wed, 8 Jan 2014 03:08:20 +0000"  >&lt;p&gt;nice design, especially considering MTTR!  +100 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  very impressive about the table under &quot;Switch Threshold vs N/W Hiccup&quot; section!&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Its not taking any writes, so no throttling is required. If the hiccup stays for long, the new WAL might switch too. In future, we could use some heuristic to monitor switches (current WAL size, last switch time, etc).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To me, i think it would be better if we have a throttling config. e.g. there is a long rack level n/w outage, then lots of RS&apos;s log switchings will put not low pressure to NN. and those tiny logs seems unhappy to every hdfs ops&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In the section &quot;Cost of HLog Rolling&#8217;s Open / Close with data&quot;, i can&apos;t understand the results for &quot;Open/1k write/Close 1000 files concurrently: ~300ms&quot;&lt;br/&gt;
Results:&lt;br/&gt;
&#9679; Open/1k write/Close 1 file: ~340 ms (avg).&lt;br/&gt;
&#9679; Open/1k write/Close 1000 files concurrently: ~300ms&lt;br/&gt;
          &#9675; 4 sec; 568ops takes &amp;gt; 1sec (2,3,4 sec)&lt;br/&gt;
          &#9675; 56.8%tile is &amp;gt;1sec&lt;br/&gt;
the &quot;~300ms&quot; is avg ??? why it&apos;s smaller than &quot;1 file&quot; scenario ?&lt;/p&gt;

&lt;p&gt;could you guys add more detailed stuff about how to handle the 2+ opening log files on replication path ?&lt;/p&gt;</comment>
                            <comment id="13868551" author="v.himanshu" created="Sat, 11 Jan 2014 00:58:03 +0000"  >&lt;p&gt;Thanks for reviewing the doc Liang (and sorry about this delay in replying).&lt;/p&gt;

&lt;p&gt;True, to handle longer outages (rack down, for e.g.), we could tune the switching policy to avoid tiny log files (for e.g., take number of append ops since last switched, etc).&lt;/p&gt;

&lt;p&gt;Yes, 300ms is the avg time (total time for 1k ops was about 30sec). I didn&apos;t really dig into it to know the why it is better than as compared to 1 file scenario, but for me the interesting bit was about 568/1000 ops took more than a sec.&lt;/p&gt;

&lt;p&gt;Yes, The replication needs to handle two opened files. To get minimal impact on Replication, I am thinking of adding a separate ReplicationSource thread for the second WAL. But, I still need to look into it more if there is a better way to achieve this.&lt;/p&gt;</comment>
                            <comment id="13868556" author="v.himanshu" created="Sat, 11 Jan 2014 01:02:31 +0000"  >&lt;p&gt;As mentioned in the doc, I will work on this feature on a different branch and merge it in the trunk when it is ready. &lt;br/&gt;
I have created branch at my github (&lt;a href=&quot;https://github.com/HimanshuVashishtha/hbase/tree/HBASE-10278&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/HimanshuVashishtha/hbase/tree/HBASE-10278&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="13924043" author="v.himanshu" created="Fri, 7 Mar 2014 16:49:56 +0000"  >&lt;p&gt;Attached is the trunk-based first cut of providing a Writer-switch functionality. &lt;/p&gt;

&lt;p&gt;Here is a brief description of what this patch adds :&lt;br/&gt;
a) An additional writer, which is used in case the current writer becomes slow and WALSwitchPolicy agrees to kick off the switch.&lt;br/&gt;
b) A WALSwitchPolicy interface. A concrete policy would tell when to do the switch, etc based on passed params. For a start, there is one impl, AggressiveWALSwitchPolicy (which switches when even one sync op took more than the threshold time). I find it very good to test this feature (actually, it acts as a &quot;chaos-monkey&quot; for this feature, where it is switching a lot). I plan to have a less aggressive one (where it also takes into account last time we switch, and last few ops which took more than threshold time after switch).&lt;br/&gt;
c) A thread pool for sync ops. The SyncRunners submits a callable for the sync call and wait on the returned Future. &lt;br/&gt;
d) SyncLatencyWatcher thread to monitor sync ops latency, and send input to the WALSwitchPolicy to make decision.&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;Howdoesitwork%3F&quot;&gt;&lt;/a&gt;How does it work ?&lt;/h4&gt;
&lt;p&gt;SyncRunner submits a sync call to the writer. SyncLatencyWatcher monitors the call duration and send input to WALSwitchPolicy. If the later decides to make the switch, the following sequence of events happen:&lt;br/&gt;
1) Set FSHLog#switching true. This blocks the RingBufferEventHandler thread in its onEvent method.&lt;br/&gt;
2) Interrupt the SyncRunner threads to unblock them from their current sync call, and wait till they reach a safe point.&lt;br/&gt;
3) Grab their Append lists (i.e., whatever they were trying to sync). Consolidate, and sort it. These are the &quot;in-flight&quot; edits we need to append to the new Writer.&lt;br/&gt;
4) Get the max SyncFuture object, and note its sequenceId. We ought to unblock all handlers that are waiting for all sequence &amp;lt;= max_syncedSequenceId, after switching.&lt;br/&gt;
5) Take the &quot;other&quot; writer, and append-sync these &quot;in-flight&quot; edits. Set the current writer to this writer.&lt;br/&gt;
6) Tell SyncRunners that switch is done, and let them take new writes (complete the latch)&lt;br/&gt;
7) Set FSHLog#switching true. &lt;br/&gt;
8) Roll the old writer.&lt;/p&gt;

&lt;p&gt;It is worthy to note that in case the sync op delay is due to a concurrent log roll, it doesn&apos;t switch. This avoids un-necessary switches.&lt;/p&gt;

&lt;p&gt;I intend to add metrics for number of in-flight edits used, etc. But, the above patch is good for giving a sense of how it looks.&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;Testing%3A&quot;&gt;&lt;/a&gt;Testing:&lt;/h4&gt;
&lt;p&gt;I tested it on trunk and compared it with WAL switch enable and disable mode. I also tested by introducing hiccups (similar approach used in the above doc).&lt;/p&gt;
&lt;h5&gt;&lt;a name=&quot;Nohiccups%3A&quot;&gt;&lt;/a&gt;No hiccups:&lt;/h5&gt;
&lt;p&gt;1.) Trunk :&lt;br/&gt;
2014-03-07 06:37:45,049 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(413)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 212.120s 47143.129ops/s&lt;br/&gt;
2014-03-07 06:42:38,271 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(413)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 214.548s 46609.617ops/s&lt;br/&gt;
2014-03-07 06:47:43,457 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(413)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 223.635s 44715.723ops/s&lt;/p&gt;

&lt;p&gt;2. Trunk + patch, but with switch disabled:&lt;br/&gt;
2014-03-07 04:54:50,451 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 218.036s 45863.988ops/s&lt;br/&gt;
2014-03-07 04:59:55,640 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 223.940s 44654.816ops/s&lt;br/&gt;
2014-03-07 05:04:56,496 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 219.976s 45459.504ops/s&lt;/p&gt;

&lt;p&gt;3. Trunk + patch, switch enabled:&lt;br/&gt;
2014-03-07 06:12:04,946 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 214.938s 46525.043ops/s&lt;br/&gt;
2014-03-07 06:16:59,603 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 217.718s 45930.973ops/s&lt;br/&gt;
2014-03-07 06:21:48,768 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 216.949s 46093.781ops/s&lt;/p&gt;

&lt;h5&gt;&lt;a name=&quot;Withasleepof2secafterevery2ksyncops%28ThisinvolvedsomeinstrumentationinProtobufLogWriter%29.&quot;&gt;&lt;/a&gt;With a sleep of 2 sec after every 2k sync ops (This involved some instrumentation in ProtobufLogWriter).&lt;/h5&gt;
&lt;p&gt;1. Trunk:&lt;br/&gt;
2014-03-06 20:52:03,212 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(413)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 406.600s 24594.195ops/s&lt;br/&gt;
2014-03-06 21:00:03,974 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(413)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 398.953s 25065.609ops/s&lt;br/&gt;
2014-03-06 21:08:21,323 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(413)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 416.390s 24015.945ops/s&lt;/p&gt;


&lt;p&gt;2. Trunk  + patch:&lt;br/&gt;
2014-03-06 21:15:53,566 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 307.909s 32477.129ops/s&lt;br/&gt;
2014-03-06 21:22:13,517 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 303.185s 32983.164ops/s&lt;br/&gt;
2014-03-06 21:28:42,993 INFO  wal.HLogPerformanceEvaluation (HLogPerformanceEvaluation.java:logBenchmarkResult(438)) - Summary: threads=10, iterations=1000000, syncInterval=10 took 314.436s 31802.975ops/s&lt;/p&gt;

&lt;p&gt; Note: It is the later set I wanted to fix&#8230;i.e.,  how we perform when we are having a non-optimal sync performance.&lt;br/&gt;
Also note that these numbers are from AggressiveSwitchPolicy which involves number of switches (switch is a costly affair). &lt;br/&gt;
I think these numbers would be better if we have somewhat less aggressive policy. That would come later. &lt;/p&gt;

&lt;p&gt;Attaching the patch for your review. Thanks.&lt;/p&gt;</comment>
                            <comment id="13924058" author="v.himanshu" created="Fri, 7 Mar 2014 17:05:44 +0000"  >&lt;p&gt;The last attachment has some unwanted changes creeped in&#8230; I will remove it.&lt;/p&gt;</comment>
                            <comment id="13924064" author="v.himanshu" created="Fri, 7 Mar 2014 17:12:13 +0000"  >&lt;p&gt;mvn clean test -Dtest=Test*WAL*,&lt;b&gt;Log&lt;/b&gt; passes on local. &lt;br/&gt;
Running the QA to see how other tests behave.&lt;/p&gt;</comment>
                            <comment id="13924211" author="hadoopqa" created="Fri, 7 Mar 2014 18:58:44 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12633401/10278-wip-1.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12633401/10278-wip-1.1.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12633401&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 9 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.0&lt;/font&gt;.  The patch compiles against the hadoop 1.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.1&lt;/font&gt;.  The patch compiles against the hadoop 1.1 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8924//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13924256" author="yuzhihong@gmail.com" created="Fri, 7 Mar 2014 19:38:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;7) Set FSHLog#switching true. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Did you mean set to false ?&lt;/p&gt;

&lt;p&gt;Can you put the patch on review board ?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13924527" author="v.himanshu" created="Sat, 8 Mar 2014 00:05:37 +0000"  >&lt;p&gt;Thanks Ted, and yes you are right. &lt;br/&gt;
Working more on error handling; will make a rb request it once that&apos;s done. Stay tuned.&lt;/p&gt;</comment>
                            <comment id="13926171" author="jmhsieh" created="Mon, 10 Mar 2014 20:35:10 +0000"  >&lt;p&gt;It has been a few days, can you post current wip on review board? (noting that it is WIP?)&lt;/p&gt;</comment>
                            <comment id="13926213" author="v.himanshu" created="Mon, 10 Mar 2014 21:05:01 +0000"  >&lt;p&gt;Sure, I have created a WIP rb request:  &lt;a href=&quot;https://reviews.apache.org/r/18983/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/18983/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13926453" author="stack" created="Mon, 10 Mar 2014 23:42:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Interrupt the SyncRunner threads to unblock them from their current sync call, and wait till they reach a safe point.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Any issues interrupting?  I&apos;ve found interrupting hdfs a PITA or rather, the variety of exceptions that can come up are many... its tricky figuring which can be caught and which not.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Set FSHLog#switching true.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Every new append or is it sync must run over this new volatile?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;3) Grab their Append lists (i.e., whatever they were trying to sync). Consolidate, and sort it. These are the &quot;in-flight&quot; edits we need to append to the new Writer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&apos;sort&apos;?  We&apos;ve given these items their seqid at this stage, right?  Will the sort mess this up?&lt;/p&gt;

&lt;p&gt;More comments over in rb&lt;/p&gt;


</comment>
                            <comment id="13932433" author="stack" created="Wed, 12 Mar 2014 21:46:09 +0000"  >&lt;p&gt;Also was wondering what can we do to prevent a pipeline being set up on same set of disks as that of the faulty writer that we are switching off?  Anything we can do to hint dfsclient?&lt;/p&gt;</comment>
                            <comment id="13933558" author="v.himanshu" created="Thu, 13 Mar 2014 17:11:04 +0000"  >&lt;p&gt;I have attached the flow diagram of the switching process. In this, I summarize the current trunk behaviour, what does the Switching add, and the steps involved and invariants maintained while doing the switching.&lt;/p&gt;

&lt;p&gt;Chatting with Stack, he suggested to measure the costs of adding one level of indirection b/w SyncRunners (SR) and writer.sync() (this patch adds a thread pool in order to monitor SR and interrupt them to release them from current sync() call.&lt;/p&gt;

&lt;p&gt;Attached are perf stat numbers on 5 node cluster (hadoop2.2) with trunk and patch  + trunk.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hbase-0.99.0-SNAPSHOT/bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -iterations 1000000 -threads 10 ; done

Trunk:
Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &apos;/home/himanshu/dists/hbase-0.99.0-SNAPSHOT/bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -iterations 1000000 -threads 10&apos;:

    1891960.295558 task-clock                #    2.396 CPUs utilized
        55,076,890 context-switches          #    0.029 M/sec
         1,770,901 CPU-migrations            #    0.936 K/sec
            73,650 page-faults               #    0.039 K/sec
 2,853,602,378,588 cycles                    #    1.508 GHz                     [83.32%]
 2,126,410,331,760 stalled-cycles-frontend   #   74.52% frontend cycles idle    [83.31%]
 1,274,582,986,073 stalled-cycles-backend    #   44.67% backend  cycles idle    [66.72%]
 1,511,777,502,744 instructions              #    0.53  insns per cycle
                                             #    1.41  stalled cycles per insn [83.37%]
   264,303,859,957 branches                  #  139.698 M/sec                   [83.33%]
     7,946,652,758 branch-misses             #    3.01% of all branches         [83.33%]

     789.767027189 seconds time elapsed

WITH PATCH:
Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &apos;/home/himanshu/10278-patch/hbase-0.99.0-SNAPSHOT/bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -iterations 1000000 -threads 10&apos;:

    2184799.924959 task-clock                #    2.465 CPUs utilized
        67,056,548 context-switches          #    0.031 M/sec
         5,879,054 CPU-migrations            #    0.003 M/sec
            71,844 page-faults               #    0.033 K/sec
 3,293,173,733,811 cycles                    #    1.507 GHz                     [83.33%]
 2,402,602,947,823 stalled-cycles-frontend   #   72.96% frontend cycles idle    [83.33%]
 1,476,790,256,434 stalled-cycles-backend    #   44.84% backend  cycles idle    [66.70%]
 1,878,777,337,255 instructions              #    0.57  insns per cycle
                                             #    1.28  stalled cycles per insn [83.38%]
   331,265,703,652 branches                  #  151.623 M/sec                   [83.30%]
    10,449,872,625 branch-misses             #    3.15% of all branches         [83.34%]

     886.148976683 seconds time elapsed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are more context switches going on here. &lt;br/&gt;
I am working on how to remove this one level of indirection so we have lesser number of threads, but still have SR interruptible so as to unblock them from ongoing problematic sync call. &lt;br/&gt;
May be merging the syncPool with SRs (worker in pool are actual SRs).&lt;/p&gt;</comment>
                            <comment id="13934624" author="jmhsieh" created="Fri, 14 Mar 2014 05:12:22 +0000"  >&lt;p&gt;The slides help a lot.  Ideally this implementation intent/design would be integrated into the code as comments similar to the description of how the disruptor patterned wal works.&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;Added a thread pool which executes the writer.sync() call. SR submits a callable &amp;amp; waits on the returned Future&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;So one concern is now that the common case has to go through an extra thread context switch.  Can we make extra context switches only happen on the rarer switching case?&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;Interrupt SyncRunners to unblock them from current sync call, and wait till all of them reach a &#8220;safepoint&#8221;.&lt;/li&gt;
	&lt;li&gt;When all SRs are interrupted, they signal switch that they have reached the safepoint, and Switch process can swap the writer&lt;/li&gt;
	&lt;li&gt;Take all the in-flight edits from all SR(s); append-sync them in same order using the &#8220;reserve&#8221; writer&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;So with this approach, we need to wait for interrupts to handle before we can start making progress using the reserve writer.&lt;/p&gt;

&lt;p&gt;Couldn&apos;t we just take the roll writer lock, block incoming appends, take the list and start using the reserve writer when we&apos;ve initiated the switch process?  We could avoid the extra thread context switches in the common case.  We may end up with potential duplicate edits in the old log and new reserve log but that is ok.  instead of interrupting the new sync thread pool threads, we&apos;d interrupt the syncrunner threads after we&apos;ve moved to the reserve writer potentially after we have unblocked the ring buffer handler and even after the roll writer lock.&lt;/p&gt;</comment>
                            <comment id="13934926" author="jmspaggi" created="Fri, 14 Mar 2014 12:02:12 +0000"  >&lt;p&gt;Is there any metrics to say &quot;we have switched&quot; that number of times, &quot;we have waiting an average of&quot; before switching, in &quot;total operation took x ms&quot;, and &quot;in total it will have taken x ms&quot; (based on the duration of the first thread)? Or we do not record that for now?&lt;/p&gt;</comment>
                            <comment id="13942083" author="stack" created="Thu, 20 Mar 2014 18:20:15 +0000"  >&lt;p&gt;Would be cool if we didn&apos;t have to do 20% more context switches for the &apos;normal&apos; case.&lt;/p&gt;</comment>
                            <comment id="13942104" author="hv.csuoa@gmail.com" created="Thu, 20 Mar 2014 18:39:44 +0000"  >&lt;p&gt;Yes, I agree. Adding even one level of indirection increases the number of&lt;br/&gt;
context switches, as seen in the last set of experiments.&lt;/p&gt;

&lt;p&gt;I changed the model so the SRs does the syncing directly as it does now.&lt;br/&gt;
(No extra  pool). So, there is no extra context switch for &apos;normal&apos; case.&lt;/p&gt;

&lt;p&gt;When things go bad for the current pipeline, SwitchMonitor does the syncing&lt;br/&gt;
and interrupt these existing SR&apos;s and replaces them with new SR&apos;s.&lt;/p&gt;

&lt;p&gt;We switch when things go bad, and it is OK to add little cost when we are&lt;br/&gt;
switching because 1) it is rare case in comparison to regular hdfs-sync&lt;br/&gt;
calls, 2) it occurs when things are bad from HDFS pipeline point of view,&lt;br/&gt;
and cost of adding new SRs is very less as compare to bad pipeline.&lt;/p&gt;

&lt;p&gt;On other hand, things become interesting since SFs are reusable objects,&lt;br/&gt;
(it is tied to a rs-handler), AND, both SwitchMonitor and SyncRunner race&lt;br/&gt;
to mark the SF as done. And, as soon as it is done, the handler put them&lt;br/&gt;
back in the RB. I am now working on to handle this exact case, rest looks&lt;br/&gt;
ok.&lt;/p&gt;


</comment>
                            <comment id="13942257" author="jmhsieh" created="Thu, 20 Mar 2014 20:50:22 +0000"  >&lt;p&gt;Metrics would be great and could be done in a critica/mustdo follow-on patch &lt;/p&gt;</comment>
                            <comment id="13943504" author="v.himanshu" created="Fri, 21 Mar 2014 20:29:01 +0000"  >&lt;p&gt;Attached is a patch based on the new model I mentioned in my last comment.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;Overalldesign%3A&quot;&gt;&lt;/a&gt;Overall design:&lt;/h3&gt;
&lt;p&gt;1. During FSHLog instantiation, open a reserved writer.&lt;br/&gt;
2. The SyncRunners does the sync to the file system as they do now. They registers to a map before calling the &apos;inflight-sync-ops&apos; map, and unregisters themselves when done.&lt;br/&gt;
3. A monitoring thread, SyncOpsMonitor periodically iterates over the inflight-sync-ops map, and feed the start time of a sync op to the configured WALSwitchPolicy.&lt;br/&gt;
4. If switch policy decides to make the switch, it goes through steps mentioned in &quot;WAL Switch workflow&quot;.&lt;br/&gt;
5. If there is a concurrent log roll going on, then we ignore the switch request.&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;WALSwitchworkflow&quot;&gt;&lt;/a&gt;WAL Switch workflow&lt;/h4&gt;
&lt;p&gt;1. Grab the roll writer lock to ensure there is no concurrent log roll. It would do the roll after switching.&lt;br/&gt;
2. Block the processing of RingBufferHandler and let it reaches a &apos;safe point&apos;. A safe point is just a marker to tell that the RingBuffer is blocked at this sequence Id. Let that sequence Id be &apos;X&apos;.&lt;br/&gt;
3. Take the &apos;inflight&apos; WALEdits (called as Appends ops), and SyncFutures from all the SyncRunners, and also from the RingBufferHandler (the later could be in the process of forming a SyncFuture batch while appending WALEdits). Ensure the ordering of Append ops. Ignore the SyncFutures with sequence Id &amp;gt; &apos;X&apos;.&lt;br/&gt;
4. Use the reserved writer to append-sync these inflight edits.&lt;br/&gt;
5. Swap the writer with the reserved writer. &lt;br/&gt;
6. Release all sync futures (to free up the handlers), and recreate SyncRunners, and interrupt the old SyncRunners.&lt;br/&gt;
7. Release the RingBuffer and starts the normal processing. &lt;br/&gt;
8. Roll the old writer.&lt;br/&gt;
9. Release the rollWriter lock.&lt;/p&gt;


&lt;h3&gt;&lt;a name=&quot;Testing&quot;&gt;&lt;/a&gt;Testing&lt;/h3&gt;
&lt;p&gt;I tested the HLogPE with trunk on a 5 node cluster, running hadoop2.2. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
On trunk:
Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &apos;/home/himanshu/dists/hbase-0.99.0-SNAPSHOT/bin/hbase org.apache.hadoop.hbase.regionserver.w
al.HLogPerformanceEvaluation -iterations 1000000 -threads 10&apos;:

    1891960.295558 task-clock                #    2.396 CPUs utilized
        55,076,890 context-switches          #    0.029 M/sec
         1,770,901 CPU-migrations            #    0.936 K/sec
            73,650 page-faults               #    0.039 K/sec
 2,853,602,378,588 cycles                    #    1.508 GHz                     [83.32%]
 2,126,410,331,760 stalled-cycles-frontend   #   74.52% frontend cycles idle    [83.31%]
 1,274,582,986,073 stalled-cycles-backend    #   44.67% backend  cycles idle    [66.72%]
 1,511,777,502,744 instructions              #    0.53  insns per cycle
                                             #    1.41  stalled cycles per insn [83.37%]
   264,303,859,957 branches                  #  139.698 M/sec                   [83.33%]
     7,946,652,758 branch-misses             #    3.01% of all branches         [83.33%]

     789.767027189 seconds time elapsed

Trunk + patch, with &lt;span class=&quot;code-keyword&quot;&gt;switch&lt;/span&gt; threshold = 1 sec.
 Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &apos;/home/himanshu/10278-patch/hbase-0.99.0-SNAPSHOT/bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -iterations 1000000 -threads 10&apos;:

    1937313.168376 task-clock                #    2.450 CPUs utilized
        54,774,802 context-switches          #    0.028 M/sec
         1,981,573 CPU-migrations            #    0.001 M/sec
            63,150 page-faults               #    0.033 K/sec
 2,967,414,126,620 cycles                    #    1.532 GHz                     [83.33%]
 2,198,851,794,211 stalled-cycles-frontend   #   74.10% frontend cycles idle    [83.33%]
 1,394,951,252,428 stalled-cycles-backend    #   47.01% backend  cycles idle    [66.68%]
 1,627,172,938,178 instructions              #    0.55  insns per cycle
                                             #    1.35  stalled cycles per insn [83.36%]
   279,686,885,670 branches                  #  144.368 M/sec                   [83.34%]
     8,362,175,551 branch-misses             #    2.99% of all branches         [83.32%]

     790.709682812 seconds time elapsed



Trunk  + patch , with &lt;span class=&quot;code-keyword&quot;&gt;switch&lt;/span&gt; threshold 100ms.
 Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &apos;/home/himanshu/10278-patch/hbase-0.99.0-SNAPSHOT/bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -iterations 1000000 -threads 10&apos;:

    1926591.375141 task-clock                #    2.416 CPUs utilized
        55,231,306 context-switches          #    0.029 M/sec
         1,996,458 CPU-migrations            #    0.001 M/sec
            62,600 page-faults               #    0.032 K/sec
 2,938,081,049,913 cycles                    #    1.525 GHz                     [83.34%]
 2,174,078,968,852 stalled-cycles-frontend   #   74.00% frontend cycles idle    [83.31%]
 1,385,993,249,374 stalled-cycles-backend    #   47.17% backend  cycles idle    [66.75%]
 1,615,848,452,958 instructions              #    0.55  insns per cycle
                                             #    1.35  stalled cycles per insn [83.41%]
   277,855,085,701 branches                  #  144.221 M/sec                   [83.29%]
     8,449,913,638 branch-misses             #    3.04% of all branches         [83.31%]

     797.338722847 seconds time elapsed

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With default 1 sec, there is almost 0 extra cost. Note that these tests were done with no n/w hiccup injection. The tests where network hiccups are injected, we see that WAL Switch functionality is very effective in overcoming a bad hdfs pipeline.&lt;/p&gt;

&lt;p&gt;This patch also adds the metrics for WALSwitching, number of WAL switches, and number of inflight Append ops used when switching. I also took care of comments from the review board that were applicable with this approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Any issues interrupting? I&apos;ve found interrupting hdfs a PITA or rather, the variety of exceptions that can come up are many... its tricky figuring which can be caught and which not.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, interrupting the SyncRunners causes variety of exceptions. But, I interrupt them when the SyncFuture they were interrupting are done by the monitor, and handlers are also freed. Also, I let them die when interrupted, so no special handling as such to keep them alive, etc.&lt;br/&gt;
But, some interesting race arises when old SyncRunners resumes itself (or tries to release SyncFutures after interruption in its &apos;finally&apos; block), and the SyncFuture, which was already freed by the SyncMonitor, is also present as &quot;not done&quot; in the RingBuffer (or with some other sync runner) because the regionserver-handler has done some appends and has put it again in the RingBuffer. I fixed that by comparing the last completed Sequence Id of the SyncFuture with the &apos;offered&apos; value of the SyncRunner. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Set FSHLog#switching true. Every new append or is it sync must run over this new volatile?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No longer needed.&lt;/p&gt;

&lt;p&gt;3) Grab their Append lists (i.e., whatever they were trying to sync). Consolidate, and sort it. These are the &quot;in-flight&quot; edits we need to append to the new Writer.&lt;br/&gt;
&apos;sort&apos;? We&apos;ve given these items their seqid at this stage, right? Will the sort mess this up?&lt;br/&gt;
Yes, the sequence Ids are present. &lt;br/&gt;
Nah, I keep a linked list of these inflight edits when they are about to be appended, so that sort would not mess the region sequence Ids sorting. I added a test case with 20 threads inserting 5k entries with switch threshold to 10ms. It verifies that there is no &quot;out-of-order&quot; edits when it is done writing. It is a heavy test (also, takes about 90 sec on my local), but pretty good for testing the correctness of this functionality.&lt;/p&gt;

&lt;p&gt;@jon, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeanmarcc&quot; class=&quot;user-hover&quot; rel=&quot;jeanmarcc&quot;&gt;Jean-Marc&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Metrics would be great and could be done in a critica/mustdo follow-on patch&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I added metrics for number of WAL switches, and total number of in-flight edits that got re-appended while doing WAL switching. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;we have waiting an average of&quot; before switching, in &quot;total operation took x ms&quot;, and &quot;in total it will have taken x ms&quot; (based on the duration of the first thread)? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I added some metrics , but didn&apos;t quite follow the above ones. &lt;/p&gt;</comment>
                            <comment id="13943529" author="v.himanshu" created="Fri, 21 Mar 2014 20:55:58 +0000"  >&lt;p&gt;I missed license in one of the Test class. Adding it.&lt;/p&gt;</comment>
                            <comment id="13943535" author="stack" created="Fri, 21 Mar 2014 21:03:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;Anything we can do to hint dfsclient?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Anything on the above?&lt;/p&gt;</comment>
                            <comment id="13943570" author="v.himanshu" created="Fri, 21 Mar 2014 21:19:27 +0000"  >&lt;p&gt;I missed this comment, sorry. Let me take a look at the nkeywal stuff you mentioned. Thanks.&lt;/p&gt;</comment>
                            <comment id="13943692" author="v.himanshu" created="Fri, 21 Mar 2014 22:56:49 +0000"  >&lt;p&gt;Retrying to let qa bot pick it up.&lt;/p&gt;</comment>
                            <comment id="13943760" author="hadoopqa" created="Sat, 22 Mar 2014 00:07:41 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12636108/10278-trunk-v2.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12636108/10278-trunk-v2.1.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12636108&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 10 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.testLogRollOnDatanodeDeath(TestLogRolling.java:368)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/9071//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13946158" author="jmhsieh" created="Tue, 25 Mar 2014 04:47:21 +0000"  >&lt;p&gt;The numbers look promising and the implementation and docs are much better on this go.  More comments on the review board.&lt;/p&gt;</comment>
                            <comment id="13950135" author="stack" created="Thu, 27 Mar 2014 23:35:11 +0000"  >&lt;p&gt;Is the failure related?&lt;/p&gt;</comment>
                            <comment id="13972910" author="jmhsieh" created="Thu, 17 Apr 2014 13:09:32 +0000"  >&lt;p&gt;I&apos;m going to pickup work on this issue.&lt;/p&gt;</comment>
                            <comment id="13972922" author="jmhsieh" created="Thu, 17 Apr 2014 13:21:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt; I believe it is not related.&lt;/p&gt;
</comment>
                            <comment id="13972937" author="v.himanshu" created="Thu, 17 Apr 2014 13:37:54 +0000"  >&lt;p&gt;Jon, Thanks for chiming in, but I am working on the core functionality here.&lt;br/&gt;
If you are interested in helping, I would appreciate if you can pick on the related tasks (as mentioned in the design doc).&lt;/p&gt;</comment>
                            <comment id="13979439" author="jmhsieh" created="Thu, 24 Apr 2014 08:17:03 +0000"  >&lt;p&gt;Currently trunk has some correctness problems when running the ITMTTR (having to do with killing the rs hosting meta).&lt;/p&gt;

&lt;p&gt;However running trunk and a modified version with this patch applied and on by default we see significantly worse worst case recovery time when the target RS is killed. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
		kill master			kill rs				move regions			
		AVE,STD 99	AVE,STD 99.99	AVE,STD 99	AVE,STD 99.99	AVE,STD 99	AVE,STD 99.99
mhlog	admin	18302.5	122.8	18302.5	122.8	2.1	0.3	51.9	10.4	2.0	0.0	34.2	7.6
	put	5.1	0.3	117.9	95.3	5.1	0.3	37647.2	9888.0	5.6	0.5	169.4	30.3
	scan	2.0	0.0	24.1	15.7	3.7	0.9	36131.2	13245.5	2.0	0.0	45.2	14.7
trunk	admin	18557.3	357.2	18557.3	357.2	2.1	0.3	41.8	7.2	2.0	0.0	31.7	4.7
	put	5.4	0.6	79.4	92.0	5.1	0.3	735.2	673.7	5.0	0.0	130.4	13.9
	scan	2.0	0.0	27.0	15.8	2.4	0.7	165.7	138.3	2.0	0.5	39.9	9.4
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13980454" author="stack" created="Thu, 24 Apr 2014 23:07:32 +0000"  >&lt;p&gt;Tell us more &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;.  Is mhlog this patch?  It gets stuck scanning? Why is 99 and 99.99 &amp;lt; AVE,STD.&lt;/p&gt;</comment>
                            <comment id="13980752" author="jmhsieh" created="Fri, 25 Apr 2014 07:15:40 +0000"  >&lt;p&gt;mhlog is with the multihlog patch in (on a trunk branch from last week) and on by default.  trunk was yesterday&apos;s trunk.&lt;/p&gt;

&lt;p&gt;The two numbers are the average and stdev of 10 of the same consecutive runs. 99 is for the 99%tile, and 99.99 is for the 99.99%tile as reported by ITMTTR.&lt;/p&gt;

&lt;p&gt;So to interpret the most interesting results with the patch has a higher percentage that are adversely affected with high latencies.  The 99%tile latency for a puts when killing an RS using multihlog is 5.1ms, with a stddev of 0.3ms.  The 99.99%tile is 37647.2ms average with a 9888.0ms stddev.  using the old log it is 735.2 and 673.7 ms respetively.  Next time i run this I&apos;ll also get the worst cases, my guess is that they are the same or there is some constant extra latency due to two wals in the multilog case.&lt;/p&gt;

</comment>
                            <comment id="14247385" author="busbey" created="Mon, 15 Dec 2014 22:56:19 +0000"  >&lt;p&gt;Attaching a link to the 0.89-fb approach to the general problem of slow pipelines, which was to request a roll on slow syncs. (ref &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=hbase.git;a=commit;h=ae31cc53050bdf656bf16f094c0b066eb5a0fc0e&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;commit ae31cc53&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14249181" author="stack" created="Tue, 16 Dec 2014 23:54:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=busbey&quot; class=&quot;user-hover&quot; rel=&quot;busbey&quot;&gt;Sean Busbey&lt;/a&gt; Could do the 0.89fb tack first, before this.&lt;/p&gt;

&lt;p&gt;How would you implement this in new regime &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=busbey&quot; class=&quot;user-hover&quot; rel=&quot;busbey&quot;&gt;Sean Busbey&lt;/a&gt;? It changes FSHLog.  You&apos;d do a derivative or decorated FSHLog, SwitchingFSHLog?  Can get rid of stuff like the &apos;enabled&apos; flag and checks.&lt;/p&gt;

&lt;p&gt;Looking at last patch, pity we couldn&apos;t switch to the other writer when rolling log on current writer (given rolling takes a while).  Looks like this was a consideration: &quot;2241	   * NOTE: Don&apos;t switch if there is a ongoing log roll. Most likely, this could be a redundant&lt;br/&gt;
2242	   * step.&quot;&lt;/p&gt;

&lt;p&gt;Patch is worth a study. Pity has to be a syncmonitor but not sure how else you&apos;d do it.&lt;/p&gt;

&lt;p&gt;On keeping around edits, one implementation, rather than append the WAL directly as we do  now, instead, kept the edits in a single list and then did bulk appends (IIRC, no advantage doing bulk append over single appends). Edits stayed in List until syncs came back to say it was ok let them go.  IIRC, it was not that much slower.  It was a little more involved (was easier just doing the WAL append  immediately since then we were done) but it might be worth considering having a single list of all outstanding edits on other side of the ring buffer as store for edits in flight (downside would be extra thread coordination)&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12717943">HBASE-11283</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12549194">HBASE-5699</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12636136" name="10278-trunk-v2.1.patch" size="72668" author="v.himanshu" created="Fri, 21 Mar 2014 22:56:49 +0000"/>
                            <attachment id="12636108" name="10278-trunk-v2.1.patch" size="72668" author="v.himanshu" created="Fri, 21 Mar 2014 20:56:16 +0000"/>
                            <attachment id="12633401" name="10278-wip-1.1.patch" size="54471" author="v.himanshu" created="Fri, 7 Mar 2014 17:05:44 +0000"/>
                            <attachment id="12621426" name="Multiwaldesigndoc.pdf" size="376396" author="v.himanshu" created="Sat, 4 Jan 2014 01:00:20 +0000"/>
                            <attachment id="12634479" name="SwitchWriterFlow.pptx" size="79271" author="v.himanshu" created="Thu, 13 Mar 2014 17:11:04 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12689687">HBASE-10378</subtask>
                            <subtask id="12764994">HBASE-12807</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 7 Jan 2014 02:37:39 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>366285</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1r4zb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>366596</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>