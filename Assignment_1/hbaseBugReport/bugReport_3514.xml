<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:10:50 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3514/HBASE-3514.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3514] Speedup HFile.Writer append</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3514</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Remove double writes when block cache is specified, by using, only, the ByteArrayDataStream.&lt;br/&gt;
baos is flushed with the compress stream on finishBlock.&lt;/p&gt;

&lt;p&gt;On my machines HFilePerformanceEvaluation SequentialWriteBenchmark passes from 4000ms to 2500ms.&lt;/p&gt;

&lt;p&gt;Running SequentialWriteBenchmark for 1000000 rows took 4247ms.&lt;br/&gt;
Running SequentialWriteBenchmark for 1000000 rows took 4512ms.&lt;br/&gt;
Running SequentialWriteBenchmark for 1000000 rows took 4498ms.&lt;/p&gt;

&lt;p&gt;Running SequentialWriteBenchmark for 1000000 rows took 2697ms.&lt;br/&gt;
Running SequentialWriteBenchmark for 1000000 rows took 2770ms.&lt;br/&gt;
Running SequentialWriteBenchmark for 1000000 rows took 2721ms.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12497960">HBASE-3514</key>
            <summary>Speedup HFile.Writer append</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="mbertozzi">Matteo Bertozzi</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Feb 2011 08:19:01 +0000</created>
                <updated>Fri, 20 Nov 2015 12:40:46 +0000</updated>
                            <resolved>Tue, 15 Mar 2011 22:02:24 +0000</resolved>
                                    <version>0.90.0</version>
                                    <fixVersion>0.92.0</fixVersion>
                                    <component>io</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="12991834" author="mbertozzi" created="Tue, 8 Feb 2011 08:19:57 +0000"  >&lt;p&gt;Remove double writes&lt;/p&gt;</comment>
                            <comment id="12991836" author="ryanobjc" created="Tue, 8 Feb 2011 08:30:38 +0000"  >&lt;p&gt;how does the performance numbers look like on a HDFS cluster? I&apos;m looking at the code, I need to look at it more to see where it&apos;s going exactly.&lt;/p&gt;

&lt;p&gt;As for the meta binary search, I&apos;m wondering how you see performance improvement? We only typically have a maximum of 2-3 meta blocks, and typically more like 0.  I&apos;d prefer not to add another (untested) binary search algorithm to our code base.&lt;/p&gt;</comment>
                            <comment id="12991837" author="mbertozzi" created="Tue, 8 Feb 2011 08:42:23 +0000"  >&lt;p&gt;@ryan I&apos;ve only a single node cluster, so I can&apos;t tell you how it performs on a cluster.&lt;/p&gt;

&lt;p&gt;Yep, for the binary search on meta, with 2-3 meta blocks (as StoreFile does) there&apos;s no performance improvements (maybe it&apos;s a bit slower).&lt;/p&gt;</comment>
                            <comment id="12992008" author="stack" created="Tue, 8 Feb 2011 15:41:44 +0000"  >&lt;p&gt;@Matteo  Patch looks great.  The speedup comes from not doing so many reallocs and because we get rid of an indirection writing the BAOS instead?&lt;/p&gt;</comment>
                            <comment id="12992091" author="mbertozzi" created="Tue, 8 Feb 2011 18:34:58 +0000"  >&lt;p&gt;@stack The first patch that I&apos;ve written hasn&apos;t the preallocated ByteArrayOutputStream and has similar  performance if block is small (64k). The speedup comes from writing in memory and flushing everything to the OutputStream on finishBlock().&lt;/p&gt;</comment>
                            <comment id="12992214" author="davelatham" created="Tue, 8 Feb 2011 22:32:38 +0000"  >&lt;blockquote&gt;
&lt;p&gt;// pre-allocates the byte stream to the block size + 25%&lt;br/&gt;
baos = new ByteArrayOutputStream(blocksize + (blocksize / 25));&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The comments says 25%, but it looks like 4%.  Not sure which it&apos;s supposed to be.&lt;/p&gt;</comment>
                            <comment id="12992250" author="streamy" created="Wed, 9 Feb 2011 00:23:02 +0000"  >&lt;p&gt;I didn&apos;t review the code just now, but I recall last time I looked that blocks are cut off before overflowing the block size rather than the first KV after they overflow.  This obviously isn&apos;t the case when a single KV is larger than the block size, but adding an addition X% wouldn&apos;t cover that anyways.  So, probably no need for pre-allocating anything larger than the block size.&lt;/p&gt;</comment>
                            <comment id="12992252" author="ryanobjc" created="Wed, 9 Feb 2011 00:30:57 +0000"  >&lt;p&gt;the hfile writer append code does:&lt;br/&gt;
    private void checkBlockBoundary() throws IOException {&lt;br/&gt;
      if (this.out != null &amp;amp;&amp;amp; this.out.size() &amp;lt; blocksize) return;&lt;br/&gt;
      finishBlock();&lt;br/&gt;
      newBlock();&lt;/p&gt;


&lt;p&gt;If we did stopped at the first key &amp;gt; than a block we&apos;d also have to&lt;br/&gt;
check that there is at least 1 KV, otherwise we&apos;d end up stuck &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Perhaps it might be beneficial to undersize the blocks to the first KV&lt;br/&gt;
&amp;lt; than block size, rather than the first KV &amp;gt; block size?&lt;/p&gt;</comment>
                            <comment id="12992273" author="streamy" created="Wed, 9 Feb 2011 01:24:03 +0000"  >&lt;p&gt;Thanks for looking that up Ryan.  Sorry for spreading misinformation!&lt;/p&gt;

&lt;p&gt;I must have made that change as part of a block cache slab allocator I was working on.  Shouldn&apos;t be too bad of a change and might be desirable.  I&apos;d be +1.&lt;/p&gt;</comment>
                            <comment id="12992278" author="ryanobjc" created="Wed, 9 Feb 2011 01:31:57 +0000"  >&lt;p&gt;if you file a jira and post a patch i&apos;ll review it. i have some&lt;br/&gt;
additional thoughts that belong there not here.&lt;/p&gt;

&lt;p&gt;as for this one, I need to apply the batch and grovel it in my ide, i&lt;br/&gt;
havent had time today so far. I&apos;m thinking part1 is ok, and the binary&lt;br/&gt;
search for meta index might not work because im not sure if the meta&lt;br/&gt;
block names are actually sorted.&lt;/p&gt;</comment>
                            <comment id="12996977" author="stack" created="Sun, 20 Feb 2011 06:09:23 +0000"  >&lt;p&gt;Marking patch available&lt;/p&gt;</comment>
                            <comment id="12998639" author="ryanobjc" created="Thu, 24 Feb 2011 00:58:48 +0000"  >&lt;p&gt;im putting this to trunk, it won&apos;t apply cleanly to 0.90 branch.&lt;/p&gt;

&lt;p&gt;can you produce a patch that applies to 0.90 branch? if so we can include it there too! Thanks!&lt;/p&gt;</comment>
                            <comment id="12998738" author="ryanobjc" created="Thu, 24 Feb 2011 08:04:23 +0000"  >&lt;p&gt;This is breaking the build, here is the output:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://hudson.apache.org/hudson/job/HBase-TRUNK/1749/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/HBase-TRUNK/1749/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Apparently this line:&lt;br/&gt;
+      // pre-allocates the byte stream to the block size + 25%&lt;br/&gt;
+      baos = new ByteArrayOutputStream(blocksize + (int)(blocksize * 0.25));&lt;/p&gt;

&lt;p&gt;isn&apos;t doing the right thing.  The patch had (blocksize / 25) which wasnt giving what the comment said, so I switched to what you see here.  Somehow this stack snippet happens:&lt;/p&gt;


&lt;p&gt;Caused by: java.lang.IllegalArgumentException: Negative initial size: -1610612738&lt;br/&gt;
	at java.io.ByteArrayOutputStream.&amp;lt;init&amp;gt;(ByteArrayOutputStream.java:57)&lt;br/&gt;
	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.newBlock(HFile.java:417)&lt;/p&gt;

&lt;p&gt;Can you poke at it please?&lt;/p&gt;</comment>
                            <comment id="12998790" author="hudson" created="Thu, 24 Feb 2011 10:03:13 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #1750 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/HBase-TRUNK/1750/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/HBase-TRUNK/1750/&lt;/a&gt;)&lt;br/&gt;
    undoing &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3514&quot; title=&quot;Speedup HFile.Writer append&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3514&quot;&gt;&lt;del&gt;HBASE-3514&lt;/del&gt;&lt;/a&gt; due to build breakage&lt;/p&gt;</comment>
                            <comment id="12998810" author="mbertozzi" created="Thu, 24 Feb 2011 12:01:40 +0000"  >&lt;p&gt;@ryan&lt;br/&gt;
Errors are caused by block size greater than 1.5G&lt;br/&gt;
I don&apos;t know if this patch is a good idea with block this large.&lt;br/&gt;
Maybe a bufferedOutputStream will be better.&lt;/p&gt;</comment>
                            <comment id="12999029" author="yuzhihong@gmail.com" created="Thu, 24 Feb 2011 20:39:01 +0000"  >&lt;p&gt;Since the buffer managed by ByteArrayOutputStream can grow, we don&apos;t need to allocate a huge buffer in its ctor.&lt;br/&gt;
We can bound the buffer size by a constant:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; sz = blocksize;
      sz += (blocksize / 4);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (sz &amp;gt; 140000000) sz = 140000000;
      &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.baos = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayOutputStream((&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)sz);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;TestMultipleTimestamps passes with the above change in patch.&lt;/p&gt;</comment>
                            <comment id="12999058" author="mbertozzi" created="Thu, 24 Feb 2011 21:20:47 +0000"  >&lt;p&gt;@Ted&lt;br/&gt;
Yep, I&apos;ve added the 0.90-v2 and trunk-v2 patch, that do this, limiting to 1G of size.&lt;/p&gt;

&lt;p&gt;But the problem is different...&lt;br/&gt;
Without this patch, HFile doesn&apos;t keep all in memory (if block cache is null).&lt;br/&gt;
So, if you&apos;ve a blocksize of 1G or more.. maybe you don&apos;t want to keep it in memory until finishBlock.&lt;/p&gt;</comment>
                            <comment id="12999582" author="yuzhihong@gmail.com" created="Fri, 25 Feb 2011 21:29:35 +0000"  >&lt;p&gt;v2 of the patches produced the following in TestMultipleTimestamps:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2011-02-25 13:23:33,208 ERROR [RS_CLOSE_REGION-10.32.41.42,59119,1298668976212-2] handler.CloseRegionHandler(127): Unrecoverable exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; closing region testReseeksWithOneColumnMiltipleTimestamps,,1298668987716.1f43d31fd29ab39c0ae0e4c176bc0766., still finishing close
org.apache.hadoop.hbase.DroppedSnapshotException: region: testReseeksWithOneColumnMiltipleTimestamps,,1298668987716.1f43d31fd29ab39c0ae0e4c176bc0766.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:989)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:894)
        at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:540)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:493)
        at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:118)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:680)
Caused by: java.lang.OutOfMemoryError: Java heap space
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12999757" author="mbertozzi" created="Sat, 26 Feb 2011 09:53:17 +0000"  >&lt;p&gt;@Ted&lt;br/&gt;
Of course there&apos;s an OutOfMemoryError, you need to increase your heap size because you&apos;re trying to allocate a 2G block when your heap is limited to 1G.&lt;/p&gt;

&lt;p&gt;I&apos;ve added two new patches for trunk and 0.90.&lt;br/&gt;
As I said before, when block cache is null and blocksize is greater then DEFAULT_MAX_BUFSIZE a &quot;direct flush&quot; is used, else everything is flushed on finishBlock.&lt;/p&gt;</comment>
                            <comment id="13000559" author="ryanobjc" created="Mon, 28 Feb 2011 22:07:21 +0000"  >&lt;p&gt;a few comments here:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;why do you use a 256MB buffer in the &quot;direct write&quot; path?  That is a pretty damn big buffer, the default buffer is 32k.  I&apos;m not sure we should have such a huge buffer (and ram sink).&lt;/li&gt;
	&lt;li&gt;finishBlock() timings for writes dont measure anything useful anymore, the long now = ... is too low, it needs to go up higher.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m poking at the unit tests, I&apos;m still not sure why we got that negative number, no unit tests sets the block size to 1GB as far as i know!&lt;/p&gt;</comment>
                            <comment id="13000572" author="ryanobjc" created="Mon, 28 Feb 2011 22:25:21 +0000"  >&lt;p&gt;I looked at the unit test, and it looks like the situation was pretty broken, we were creating column families with BLOCKSIZE = Integer.MAX_VALUE (which is WAY wrong).&lt;/p&gt;

&lt;p&gt;I think we should revert to an earlier version of the patch without the directWrite code, and also using the ByteBufferOutputStream class that is located in hbase.ipc package (feel free to move it to hbase.util).  It does not have the same resizing bugs that the ByteArrayOutputStream does.  &lt;/p&gt;

&lt;p&gt;Another issue with this is what happens when blocks are really big - this will normally be due to really large single KVs, we will require another allocation of a given buffer. Given that there is no size restriction to block caching, I think this isn&apos;t an issue for this patch, and we can think about it elsewhere.&lt;/p&gt;</comment>
                            <comment id="13001219" author="ryanobjc" created="Wed, 2 Mar 2011 01:01:38 +0000"  >&lt;p&gt;I think we should go back to pre-direct write version, and you should also use the ByteBufferOutputStream found in hbase.rpc because it fixes a serious bug that ByteArrayOutputStream with large arrays has.&lt;/p&gt;

&lt;p&gt;Given that I think we can finish this up and call it &quot;done&quot;.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="13001340" author="mbertozzi" created="Wed, 2 Mar 2011 08:48:20 +0000"  >&lt;p&gt;@ryan&lt;br/&gt;
I still don&apos;t have looked tests in deep, but we can&apos;t just replace MAX_VALUE with DEFAULT_BLOCKSIZE?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
Index: src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java	(revision 1076127)
+++ src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java	(working copy)
@@ -547,7 +547,7 @@
           HColumnDescriptor.DEFAULT_COMPRESSION,
           HColumnDescriptor.DEFAULT_IN_MEMORY,
           HColumnDescriptor.DEFAULT_BLOCKCACHE,
-          &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE, HColumnDescriptor.DEFAULT_TTL,
+          HColumnDescriptor.DEFAULT_BLOCKSIZE, HColumnDescriptor.DEFAULT_TTL,
           HColumnDescriptor.DEFAULT_BLOOMFILTER,
           HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);
       desc.addFamily(hcd);
@@ -574,7 +574,7 @@
           HColumnDescriptor.DEFAULT_COMPRESSION,
           HColumnDescriptor.DEFAULT_IN_MEMORY,
           HColumnDescriptor.DEFAULT_BLOCKCACHE,
-          &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE, HColumnDescriptor.DEFAULT_TTL,
+          HColumnDescriptor.DEFAULT_BLOCKSIZE, HColumnDescriptor.DEFAULT_TTL,
           HColumnDescriptor.DEFAULT_BLOOMFILTER,
           HColumnDescriptor.DEFAULT_REPLICATION_SCOPE);
       desc.addFamily(hcd);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13001343" author="ryanobjc" created="Wed, 2 Mar 2011 08:55:37 +0000"  >&lt;p&gt;Yes you are correct, I forgot to commit that, ill do it in the am.&lt;/p&gt;</comment>
                            <comment id="13002364" author="ryanobjc" created="Fri, 4 Mar 2011 00:38:53 +0000"  >&lt;p&gt;a test failure caused me to dig into this, and there are 2 problems:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the handling of the ByteBuffer causes us to cache bad data. (easy fix)&lt;/li&gt;
	&lt;li&gt;the resulting buffer is not being trimmed, which is causing us to over use ram on block caching.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The latter one IS a big deal, it means that our block cache can expect to be 25% larger than necessary, which is NOT good.  We should consider trimming the resulting buffer, but this involves a array copy possibly removing all optimizations added by this patch!&lt;/p&gt;</comment>
                            <comment id="13002366" author="ryanobjc" created="Fri, 4 Mar 2011 00:39:36 +0000"  >&lt;p&gt;not fixed yet causing test failures and has a problem with the design&lt;/p&gt;</comment>
                            <comment id="13002416" author="hudson" created="Fri, 4 Mar 2011 02:55:00 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #1767 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/HBase-TRUNK/1767/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/HBase-TRUNK/1767/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3514&quot; title=&quot;Speedup HFile.Writer append&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3514&quot;&gt;&lt;del&gt;HBASE-3514&lt;/del&gt;&lt;/a&gt; revert&lt;/p&gt;</comment>
                            <comment id="13002805" author="mbertozzi" created="Fri, 4 Mar 2011 20:48:20 +0000"  >&lt;p&gt;@ryan&lt;br/&gt;
trunk-v2c.patch trims the buffer with an array copy, but this is the original behavior.&lt;br/&gt;
If you take a look at the finishBlock() there&apos;s a baos.toByteArray().&lt;/p&gt;</comment>
                            <comment id="13005526" author="stack" created="Fri, 11 Mar 2011 07:00:49 +0000"  >&lt;p&gt;Marking patch available.  Want to take a looksee at Matteo&apos;s latest Ryan?&lt;/p&gt;</comment>
                            <comment id="13005905" author="ryanobjc" created="Fri, 11 Mar 2011 23:51:44 +0000"  >&lt;p&gt;this new patch is showing a modest improvement of about 50%.  Without:&lt;br/&gt;
11/03/11 15:33:45 INFO hbase.HFilePerformanceEvaluation: Running SequentialWriteBenchmark for 1000000 rows took 3260ms.&lt;/p&gt;

&lt;p&gt;with:&lt;br/&gt;
11/03/11 15:46:57 INFO hbase.HFilePerformanceEvaluation: Running SequentialWriteBenchmark for 1000000 rows took 2009ms.&lt;/p&gt;</comment>
                            <comment id="13005991" author="stack" created="Sat, 12 Mar 2011 06:17:24 +0000"  >&lt;p&gt;Thats worth commit Ryan?&lt;/p&gt;</comment>
                            <comment id="13006127" author="ryanobjc" created="Sun, 13 Mar 2011 02:15:47 +0000"  >&lt;p&gt;given the rocky history, lets put it to trunk only.&lt;/p&gt;</comment>
                            <comment id="13006245" author="stack" created="Sun, 13 Mar 2011 18:28:56 +0000"  >&lt;p&gt;Agreed&lt;/p&gt;</comment>
                            <comment id="13007861" author="hudson" created="Thu, 17 Mar 2011 10:32:59 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #1792 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/HBase-TRUNK/1792/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://hudson.apache.org/hudson/job/HBase-TRUNK/1792/&lt;/a&gt;)&lt;/p&gt;
</comment>
                            <comment id="15016795" author="lars_francke" created="Fri, 20 Nov 2015 12:40:46 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12471873" name="HBASE-3514-append-0.90-v2.patch" size="2225" author="mbertozzi" created="Thu, 24 Feb 2011 21:15:43 +0000"/>
                            <attachment id="12472659" name="HBASE-3514-append-0.90-v2b.patch" size="10202" author="mbertozzi" created="Fri, 4 Mar 2011 11:02:37 +0000"/>
                            <attachment id="12472038" name="HBASE-3514-append-0.90-v3.patch" size="4233" author="mbertozzi" created="Sat, 26 Feb 2011 09:53:49 +0000"/>
                            <attachment id="12471827" name="HBASE-3514-append-0.90.patch" size="1954" author="mbertozzi" created="Thu, 24 Feb 2011 11:56:36 +0000"/>
                            <attachment id="12471874" name="HBASE-3514-append-trunk-v2.patch" size="4618" author="mbertozzi" created="Thu, 24 Feb 2011 21:16:02 +0000"/>
                            <attachment id="12472657" name="HBASE-3514-append-trunk-v2c.patch" size="13310" author="mbertozzi" created="Fri, 4 Mar 2011 10:56:11 +0000"/>
                            <attachment id="12472037" name="HBASE-3514-append-trunk-v3.patch" size="7218" author="mbertozzi" created="Sat, 26 Feb 2011 09:53:39 +0000"/>
                            <attachment id="12470567" name="HBASE-3514-append.patch" size="4461" author="mbertozzi" created="Tue, 8 Feb 2011 08:21:00 +0000"/>
                            <attachment id="12470566" name="HBASE-3514-metaBlock-bsearch.patch" size="2734" author="mbertozzi" created="Tue, 8 Feb 2011 08:20:30 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 8 Feb 2011 08:30:38 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>33064</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hml3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100921</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>submitted to trunk!</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>