<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:22:15 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-11339/HBASE-11339.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-11339] HBase MOB</title>
                <link>https://issues.apache.org/jira/browse/HBASE-11339</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;  It&apos;s quite useful to save the medium binary data like images, documents into Apache HBase. Unfortunately directly saving the binary MOB(medium object) to HBase leads to a worse performance since the frequent split and compaction.&lt;br/&gt;
  In this design, the MOB data are stored in an more efficient way, which keeps a high write/read performance and guarantees the data consistency in Apache HBase.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12721032">HBASE-11339</key>
            <summary>HBase MOB</summary>
                <type id="14" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Umbrella</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jingcheng.du@intel.com">Jingcheng Du</assignee>
                                    <reporter username="jingcheng.du@intel.com">Jingcheng Du</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Jun 2014 03:30:27 +0000</created>
                <updated>Tue, 29 Dec 2015 06:40:55 +0000</updated>
                            <resolved>Wed, 22 Jul 2015 20:13:30 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                    <fixVersion>hbase-11339</fixVersion>
                                    <component>regionserver</component>
                    <component>Scanners</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>42</watches>
                                                                                                            <comments>
                            <comment id="14030285" author="jmhsieh" created="Fri, 13 Jun 2014 06:04:07 +0000"  >&lt;p&gt;Nice doc.  I did a quick read and have some design level questions and concerns:&lt;/p&gt;

&lt;p&gt;The core problem we are trying to avoid is write amplification (writing the data in the hlog, then in flush and then over and over again with compactions).&lt;/p&gt;

&lt;p&gt;Does the proposed design write out LOBs to both the HLog and then later LOB files?  As designed, it must write them to the log so that we preserve durability and consistency properties of a row.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/add.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; good that this should just would work with replication&lt;br/&gt;
&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/forbidden.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; in the best case, the data is written at least twice &amp;#8211; once before the ack is sent to the client and then again on flush.  Can we limit this to once?&lt;/p&gt;

&lt;p&gt;We could avoid extra writes by just writing to a separate LOB log/file.  Was this considered?&lt;/p&gt;

&lt;p&gt;Is there any consideration of locality and performance?&lt;/p&gt;

&lt;p&gt;5MB cells are large but aren&apos;t really that big.  Maybe this should just be &quot;blobs&quot; (binary large objects) or &quot;mobs&quot; (medium objects)?  the objects being immutable is important too.&lt;/p&gt;

</comment>
                            <comment id="14031458" author="lhofhansl" created="Sat, 14 Jun 2014 04:37:47 +0000"  >&lt;p&gt;Is it better to store small blobs (let&apos;s say 1mb or less) in HBase (by value) and larger blob directly in files in HDFS with just a reference in HBase? Writing large blobs would be a three step process: (1) add the metadata to HBase (2) stream the actual blob into HDFS (3) set a &quot;written&quot; column in the HBase row to true.&lt;/p&gt;

&lt;p&gt;Just saying... That way it could be handled by client alone.&lt;/p&gt;</comment>
                            <comment id="14032246" author="jingcheng.du@intel.com" created="Mon, 16 Jun 2014 09:15:05 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; for the comments.&lt;/p&gt;

&lt;p&gt;&amp;gt;Does the proposed design write out LOBs to both the HLog and then later LOB files?&lt;br/&gt;
Yes, the Lobs are written in both HLogs and Lob files.&lt;/p&gt;

&lt;p&gt;&amp;gt;in the best case, the data is written at least twice &#8211; once before the ack is sent to the client and then again on flush. Can we limit this to once?&lt;br/&gt;
&amp;gt;We could avoid extra writes by just writing to a separate LOB log/file. Was this considered?&lt;br/&gt;
It was considered. But we didn&apos;t find a good solution for this.&lt;/p&gt;

&lt;p&gt;&amp;gt;Is there any consideration of locality and performance?&lt;br/&gt;
The locality is only retained after the Lobs are flushed from the MemStore. But it&apos;s not guaranteed after the SweepTool runs(Lob compaction) or regions move to other regionservers.&lt;br/&gt;
The write/read performance of HBase is not supposed be be impacted too much, I will provide the details later as soon as the performance testing is done.&lt;/p&gt;

&lt;p&gt;&amp;gt;5MB cells are large but aren&apos;t really that big. Maybe this should just be &quot;blobs&quot; (binary large objects) or &quot;mobs&quot; (medium objects)?  the objects being immutable is important too&lt;br/&gt;
Actually the Lobs could be mutable. The Lobs that are not used anymore will be handled by the Sweep Tool.&lt;/p&gt;</comment>
                            <comment id="14032254" author="jingcheng.du@intel.com" created="Mon, 16 Jun 2014 09:23:00 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; for the comments.&lt;/p&gt;

&lt;p&gt;&amp;gt; Is it better to store small blobs (let&apos;s say 1mb or less) in HBase (by value) and larger blob directly in files in HDFS with just a reference in HBase? Writing large blobs would be a three step process: (1) add the metadata to HBase (2) stream the actual blob into HDFS (3) set a &quot;written&quot; column in the HBase row to true.&lt;br/&gt;
Good idea. But In this way, all the actions occurs in the client, each client writes a new file in HDFS. It&apos;s hard to control the file size which consequently leads to too many small files in HDFS probably.&lt;/p&gt;</comment>
                            <comment id="14032599" author="yuzhihong@gmail.com" created="Mon, 16 Jun 2014 16:10:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;We could avoid extra writes by just writing to a separate LOB log/file.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The above would be a useful enhancement, not just for LOB feature. This would simplify decision making w.r.t. flushing.&lt;/p&gt;</comment>
                            <comment id="14033537" author="jingcheng.du@intel.com" created="Tue, 17 Jun 2014 08:01:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;The above would be a useful enhancement, not just for LOB feature. This would simplify decision making w.r.t. flushing&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;HI, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt;. You mean to write the WAL by stores? If we use the HLog as the Lob files directly, is it efficient to seek a KV in it? I don&apos;t think so.&lt;/p&gt;</comment>
                            <comment id="14034172" author="jmhsieh" created="Tue, 17 Jun 2014 18:27:59 +0000"  >&lt;p&gt;&amp;gt;&amp;gt;We could avoid extra writes by just writing to a separate LOB log/file. Was this considered?&lt;br/&gt;
&amp;gt;It was considered. But we didn&apos;t find a good solution for this.&lt;br/&gt;
...&lt;br/&gt;
&amp;gt;  You mean to write the WAL by stores? If we use the HLog as the Lob files directly, is it efficient to seek a KV in it? I don&apos;t think so.&lt;/p&gt;

&lt;p&gt;I&apos;m not convinced.  The idea I&apos;m suggesting is having a special lob log file that is written once at write time that is essentially the lob store files in the doc, and put a reference to it (file name, and offset) in the normal wal.  This allows the lob to only be written once.  I don&apos;t see how this would be less efficient than an approach that must write the values out at least twice.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;5MB cells are large but aren&apos;t really that big. Maybe this should just be &quot;blobs&quot; (binary large objects) or &quot;mobs&quot; (medium objects)? the objects being immutable is important too&lt;br/&gt;
&amp;gt;Actually the Lobs could be mutable. The Lobs that are not used anymore will be handled by the Sweep Tool.&lt;/p&gt;

&lt;p&gt;When I say mutable I mean that I can modify a particular byte in the lob without having to &quot;overwrite&quot; the previous lob with a whole new lob.  I don&apos;t think the proposed design handles this modify a few bytes in a large blob without doing the rewrite of the entire lob.&lt;/p&gt;

&lt;p&gt;&amp;gt; Good idea. But In this way, all the actions occurs in the client, each client writes a new file in HDFS. It&apos;s hard to control the file size which consequently leads to too many small files in HDFS probably.&lt;/p&gt;

&lt;p&gt;I agree about the hdfs small files problem but I think we need to properly define what a LOB is and the scope of this effort.  (hence my suggestion of Medium Objects &amp;#8211; MOBS).  &lt;/p&gt;

&lt;p&gt;Consider storing and shipping real large objects (say 100MB&apos;s or GB&apos;s).  Here hbase&apos;s api is insufficient.  We&apos;d want a streaming api for that or allow the client to go to the file system directly (which may be a security concern for some users).  &lt;/p&gt;

&lt;p&gt;Consider storing and shipping moderately sized objects (say 100k&apos;s to 10MB&apos;s).  HBase&apos;s API is still sufficient, but we&apos;d want to avoid the write amplification problem.  The proposed design does this, but I think it could go further to avoid a 2x write amplification if we handled it at the logging portion of the write path as opposed to the flushing part of the of the write path.&lt;/p&gt;

&lt;p&gt;I&apos;m under the impression we are solving the latter case here.  Is that correct?&lt;/p&gt;</comment>
                            <comment id="14035448" author="jingcheng.du@intel.com" created="Wed, 18 Jun 2014 09:01:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m not convinced. The idea I&apos;m suggesting is having a special lob log file that is written once at write time that is essentially the lob store files in the doc, and put a reference to it (file name, and offset) in the normal wal. This allows the lob to only be written once. I don&apos;t see how this would be less efficient than an approach that must write the values out at least twice.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;  In this way, we save the Lob files as SequenceFiles, and save the offset and file name back into the put before putting the KV into the MemStore, right?&lt;br/&gt;
 1. If so, we don&apos;t use the MemStore to save the Lob data, right? Then how to read the Lob data that are not sync yet(which are still in the writer buffer)?&lt;br/&gt;
 2. We need add a preSync and preAppend to the HLog so that we could sync the Lob files before the HLogs are sync.&lt;br/&gt;
 3. In order to the get the correct offset, we have synchronized the prePut in the coprocessor, or we could use different Lob files for each thread?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I agree about the hdfs small files problem but I think we need to properly define what a LOB is and the scope of this effort. (hence my suggestion of Medium Objects &#8211; MOBS).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m under the impression we are solving the latter case here. Is that correct?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That&apos;s right.&lt;/p&gt;</comment>
                            <comment id="14035526" author="jingcheng.du@intel.com" created="Wed, 18 Jun 2014 09:49:47 +0000"  >&lt;p&gt;In the current design, the Lob files are saved by date(for example tableName/columnfamily/date/lobFileName), it&apos;s easy to delete the lob files which are expired (by the TTL).&lt;br/&gt;
The date of commit is used as this date in the path.&lt;/p&gt;

&lt;p&gt;1. If using the date of commit in the suggested way, we need to update the reference KVs after the Lob files are committed(rename the file from the temp directory to the date directory). If the MemStore flush fails while the Lob file commits successfully, the date of commit is lost when the WALEdits are replayed. The Lob data and reference KV in HBase could not be connected.&lt;br/&gt;
2. If we don&apos;t save lob files by date, all the lob files for a column family are saved together. Then it&apos;s difficult to delete the expired lob files( could delete them by sweep tool instead).&lt;/p&gt;</comment>
                            <comment id="14035550" author="jingcheng.du@intel.com" created="Wed, 18 Jun 2014 10:17:52 +0000"  >&lt;p&gt;To correct the typo.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I&apos;m not convinced. The idea I&apos;m suggesting is having a special lob log file that is written once at write time that is essentially the lob store files in the doc, and put a reference to it (file name, and offset) in the normal wal. This allows the lob to only be written once. I don&apos;t see how this would be less efficient than an approach that must write the values out at least twice.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In this way, we save the Lob files as SequenceFiles, and save the offset and file name back into the Put before putting the KV into the MemStore, right?&lt;br/&gt;
1. If so, we don&apos;t use the MemStore to save the Lob data, right? Then how to read the Lob data that are not sync yet(which are still in the writer buffer)?&lt;br/&gt;
2. We need add a preSync and preAppend to the HLog so that we could sync the Lob files before the HLogs are sync.&lt;br/&gt;
3. In order to get the correct offset, we have to synchronize the prePut in the coprocessor, or we could use different Lob files for each thread?&lt;/p&gt;</comment>
                            <comment id="14037015" author="jingcheng.du@intel.com" created="Thu, 19 Jun 2014 06:19:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zhihyu%40ebaysf.com&quot; class=&quot;user-hover&quot; rel=&quot;zhihyu@ebaysf.com&quot;&gt;Ted Yu&lt;/a&gt;, thanks for the comments.&lt;br/&gt;
Think about the suggestion carefully, and have some ideas. Share with all of you guys, and please kindly provide comments. According to the suggestion, I&apos;ll name the Lob as Mob from now on.&lt;/p&gt;

&lt;p&gt;We don&apos;t use the MemStore to save the mob data, we directly write the to the mob file and just for once.&lt;/p&gt;

&lt;p&gt;In the prePut of the coprocessor, the KV are split to two KVs, one(KV0) is the offset+path, the other one(KV1) is the lob KV. KV0 is written to the HLog and MemStore, and KV1 is written to the mob file.&lt;br/&gt;
Before the mob data are async to the disk, they are saved in the buffer of the mob writer, these data are not seekable until the buffer is full or sync to the disk.&lt;br/&gt;
In order to avoid this, we have to sync the mob data for each put to the disk (is it ok to sync for the mob in each put? The mob data are usually pictures, the size is around 1-5MB).&lt;/p&gt;

&lt;p&gt;By design, each store has a single mob file for writing. We have to synchronize the operation to increase the offset of KVs within a single mob file. So we have to have a synchronization block(two operations in the block, one is the sync the mob data to disk, the other is to increase the offset) in the prePut method, consequently all the puts are synchronized here. This is not efficient. Instead we could improve it here, to use different mob files for each thread. If so we don&apos;t need synchronization, but we will have too many open files in region server (handler*regionNum). This is a problem.&lt;br/&gt;
Also we have a solution for this, we could define a SynchronousQueue with limited size so that we could have limited open files for each region. All of these occurs in prePut, and the prePut method should have a synchronization block in each thread. It&apos;s improved, but not efficient IMO.&lt;/p&gt;

&lt;p&gt;Before the MemStore flushes(do this in the preFlush of coprocessor), we roll the mob writers and update the KV offset to 0 for new writers. This will block the prePut.&lt;/p&gt;

&lt;p&gt;Usually by the requirements of customers, using the TTL to clean expired mob files are very important, it&apos;s more efficient to clean the mob files than the sweep tool(mob files are hardly updated, but have a fixed life time).&lt;br/&gt;
We need a way to rename the mob files before the MemStore flushes in the store flusher, and save these mob files by date.&lt;br/&gt;
Such a situation probably happens: The MemStore flushing fails while the mob files renaming succeeds. When the WALEdits are replayed, the connection between the edits and mob files are lost. In order to avoid this, we need to add a rename-transaction znode to zk, each renaming transaction has a znode which contains several child znodes(they&apos;re the mapping from the nameBeforeRename to nameAfterRename). The txn znode will be deleted after every successful MemStore flushing and all the txns for each store are exclusive to each other.&lt;/p&gt;

&lt;p&gt;How about this?&lt;/p&gt;</comment>
                            <comment id="14037065" author="jingcheng.du@intel.com" created="Thu, 19 Jun 2014 07:25:13 +0000"  >&lt;p&gt;Correct the typo.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Usually by the requirements of customers, using the TTL to clean expired mob files are very important, it&apos;s more efficient to clean the mob files than the sweep tool(mob files are hardly updated, but have a fixed life time).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;-&amp;gt; Usually by the requirements of customers, using the TTL to clean expired mob files are very important, and it&apos;s more efficient to clean the expired mob files than the sweep tool using MapReduce(mob files are hardly updated, but have a fixed life time).&lt;/p&gt;</comment>
                            <comment id="14037141" author="jingcheng.du@intel.com" created="Thu, 19 Jun 2014 08:51:44 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;, maybe I misunderstood your suggestion.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m not convinced. The idea I&apos;m suggesting is having a special lob log file that is written once at write time that is essentially the lob store files in the doc, and put a reference to it (file name, and offset) in the normal wal. This allows the lob to only be written once. I don&apos;t see how this would be less efficient than an approach that must write the values out at least twice.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You mean we have a new HLog implementation for the mob which write the mob file and wal separately, right? And we still use the MemStore to save the mob data, right? I will draft the design of the mob file and post it later. Thanks.&lt;/p&gt;</comment>
                            <comment id="14037864" author="jmhsieh" created="Thu, 19 Jun 2014 20:51:05 +0000"  >&lt;p&gt;Thanks for following up with good questions! &lt;/p&gt;

&lt;p&gt;You haven&apos;t called it out directly but your questions are leading towards trouble spots in a loblog design.  One has to do with atomicity and the other has to do with reading recent values.  I think the latter effectively disqualifies the loblog idea.  Here&apos;s a writeup.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In this way, we save the Lob files as SequenceFiles, and save the offset and file name back into the Put before putting the KV into the MemStore, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Essentially yes.  They aren&apos;t necessarily sequence files &amp;#8211; they would be synced to complete writing the lob just like the current hlog files does with edits. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1. If so, we don&apos;t use the MemStore to save the Lob data, right? Then how to read the Lob data that are not sync yet(which are still in the writer buffer)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the loblog write and locator write into the hlog both succeed, we&apos;d use the same design/mechanism you currently have to read lobs that aren&apos;t present in the memstore since they were flushed.  &lt;/p&gt;

&lt;p&gt;The difference is that the loblogs are still being written. In HDFS you can read files that are currently being written, however you aren&apos;t guaranteed to read to the most recent end of the file since we have no built in tail in hdfs yet).   Hm.. so we have a problem getting latest data.&lt;/p&gt;

&lt;p&gt;So for the lob log design to be correct, it would need work on hdfs to provide guarantees or a tail operation.  While not out of the question, that would be a ways out from now and disqualifies the lob log for the short term.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;2. We need add a preSync and preAppend to the HLog so that we could sync the Lob files before the HLogs are sync.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Explain why you need presync and preappend? &lt;/p&gt;

&lt;p&gt;I think this is getting at a problem where we are trying to essentially sync writes to two logs atomically. Could we just not issue the locator put until the lob has been synced?  (a lob that is just around won&apos;t hurt anything, but a bad locator would).  Both the lob and the locator would have the same ts/mvcc/seqno.&lt;/p&gt;

&lt;p&gt;In the PDF&apos;s design, this shouldn&apos;t be a problem because it would use the normal write path for atomicity guarantees.  Currently hbase guarantees atomicity of CF&apos;s at flush time, and by having all cf:c&apos;s added to the hlog and memstore atomically.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In order to get the correct offset, we have to synchronize the prePut in the coprocessor, or we could use different Lob files for each thread?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why not just write+sync the lob and then write the locator put?  For lobs we&apos;d use the same mechanism to sync (one loblog for all threads, queued using the disruptor work).  &lt;/p&gt;</comment>
                            <comment id="14037870" author="jmhsieh" created="Thu, 19 Jun 2014 20:55:49 +0000"  >&lt;p&gt;Let&apos;s do one more strawman and try to disqualify it for the MOB case.&lt;/p&gt;

&lt;p&gt;Why not just improve/use existing column family functionality and have use a cf for lob/mob fields? Couldn&apos;t we just do a combination of  per-cf compaction and per-cf flushes (not sure if all or some of those features are in already)  and get to good performance while avoiding write amplification penalties?&lt;/p&gt;</comment>
                            <comment id="14037889" author="ndimiduk" created="Thu, 19 Jun 2014 21:08:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Couldn&apos;t we just do a combination of per-cf compaction and per-cf flushes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1. This strikes me as very well aligned with the design intention of column families.&lt;/p&gt;</comment>
                            <comment id="14038559" author="jingcheng.du@intel.com" created="Fri, 20 Jun 2014 07:33:23 +0000"  >&lt;p&gt;Thanks for the comments! &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Does it mean the mob files are not feasible?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why not just improve/use existing column family functionality and have use a cf for lob/mob fields? Couldn&apos;t we just do a combination of per-cf compaction and per-cf flushes (not sure if all or some of those features are in already) and get to good performance while avoiding write amplification penalties?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You mean directly saving the mob into HBase and using different compaction policy for the mob cf? The compaction on the mob cf in HBase is costly, will probably delay the flushing and block the updates. And a large mob store leads to frequent region split. All of these impact the HBase potentially.&lt;/p&gt;

&lt;p&gt;In the current design (introduced in the pdf), if users are concerned for the write performance rather than the consistency and replication, how about to disable the WAL directly? If users want to enable the WAL and don&apos;t want the twice writing, they could write the mob in the client side ( the way like Lars&apos;s suggestion). The scanner and sweep tool could work as well with this if the locator(reference) column follows the specific format.&lt;/p&gt;</comment>
                            <comment id="14039195" author="jmhsieh" created="Fri, 20 Jun 2014 19:04:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;Does it mean the mob files are not feasibe?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m trying to be convinced that we need a special mechanism to handle MOBs.  We can put the loblog idea to rest for the time being because of the read-recently written issues.&lt;/p&gt;

&lt;p&gt;Let&apos;s see if improving the cf flushes/compactions could achieve the same goal as the pdf.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You mean directly saving the mob into HBase and using different compaction policy for the mob cf? The compaction on the mob cf in HBase is costly, will probably delay the flushing and block the updates. And a large mob store leads to frequent region split. All of these impact the HBase potentially.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes roughly. &lt;/p&gt;

&lt;p&gt;With the algorithms today sure.  However, I was thinking a few things that we could use to avoid excessive write amplification.&lt;br/&gt;
1) compact individual cf&apos;s without compacting others.&lt;br/&gt;
2) having different compaction selection/promotion algorithms per cf.&lt;br/&gt;
3) decided to split only based on certain cf&apos;s&lt;/p&gt;

&lt;p&gt;Even with the pdf design, we still end up flushing fairly frequently (potentially a flush every ~100 objects!) and we&apos;d end up with a lot of hfiles or lob files.  &lt;/p&gt;

&lt;p&gt;How many lob files could be generated per flush?  If I flush a table, would  all regions the relevant regions on a particular RS go to one lob sequence file as opposed to many hfiles in the cf case?   (e.g. similarly to how all edits on an RS go to one hlog) &lt;/p&gt;

&lt;p&gt;I don&apos;t think the pdf design mentions antything about caching mob values.  Would frequently requested mob always hit hdfs?  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In the current design (introduced in the pdf), if users are concerned for the write performance rather than the consistency and replication, how about to disable the WAL directly? If users want to enable the WAL and don&apos;t want the twice writing, they could write the mob in the client side ( the way like Lars&apos;s suggestion). The scanner and sweep tool could work as well with this if the locator(reference) column follows the specific format.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Interesting point but the obvious problem is we lose durability guarantees and isn&apos;t something we can really recommend for normal use.  (in the lob log idea seems pretty obvious that we&apos;d be able to maintain durability guarantees).&lt;/p&gt;</comment>
                            <comment id="14040463" author="jingcheng.du@intel.com" created="Mon, 23 Jun 2014 06:42:27 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; !&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1) compact individual cf&apos;s without compacting others. 2)having different compaction selection/promotion algorithms per cf.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, this could improve the compaction. But this doesn&apos;t reduce the twice writing for the mob file.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;3) decided to split only based on certain cf&apos;s&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We could split the region by a certain cf, but after all the cf of mob will be split. Let&apos;s assume a metadata(description data for the mob, they&apos;re other cfs than the mob cf) is 1KB and a mob is 5MB, when the region is split by the metadata size, the mob data will be very very large. Saving the mob off from the HBase could avoid this. &lt;br/&gt;
When scanning, the mob data is counted in the heap of scanners if saving the mob in the HBase whereas the mob are directly sought in a single file each time if saving them into mob files(We have a mechanism to cache several opened scanners of the mob files). The latter one seems to be more efficient.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How many lob files could be generated per flush? If I flush a table, would all regions the relevant regions on a particular RS go to one lob sequence file as opposed to many hfiles in the cf case? (e.g. similarly to how all edits on an RS go to one hlog)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The files related with the mob are reference(path)HFile + mobFile. The amount of the files is doubled than the one related with mob directly saving them into HBase.&lt;br/&gt;
Saving the mob files by stores than by region server is more efficient to use the TTL to clean the expired mobs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Even with the pdf design, we still end up flushing fairly frequently (potentially a flush every ~100 objects!) and we&apos;d end up with a lot of hfiles or lob files.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The HFiles for metadata are supposed to be small, it&apos;s not so costly as the one in mob files.&lt;br/&gt;
Usually the mob is much larger than the metadata, the mob files are large enough when flushing. And because of the read against a single file, the amount of the mob files won&apos;t impact the read performance.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t think the pdf design mentions antything about caching mob values. Would frequently requested mob always hit hdfs?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We have a MobCacheConfig which extends the CacheConfig for the each mob store, it provides a cache for several opened mob files(only cache the opened reader, the capacity is limited and , use LRU to evict them if the capacity is exceeded.), and this cache had the same global block cache with the one in region server. If saving the mob into HFile, the block cache works with mob files as well.&lt;/p&gt;</comment>
                            <comment id="14042655" author="jmhsieh" created="Tue, 24 Jun 2014 20:44:05 +0000"  >&lt;p&gt;In the pdf design, is there one MobManager per RS or one MobManager per table or one MobManager per region?  Is the mob hfiles kind of like a shared cf that all regions with mobs eventually throw their data into?   &lt;/p&gt;

&lt;p&gt;Can you explain what happens if I have a RS with regions, some belonging to tableA and and some belonging to tableB.  Let&apos;s say all writes to tableA and tableB have Mobs in them. &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;a region gets full and decides to flush.  we generate one mob file.  10 separate flushes, 10 separate mob files.&lt;/li&gt;
	&lt;li&gt;an admin user issues a flush tableA command and there are multiple tableA regions on the rs.  How many mob files are generated?  one mob file per region in tableA on the rs? exactly one because only one table was flushed? exactly one because only one table was flushed?&lt;/li&gt;
	&lt;li&gt;the node goes down cleanly, causing all regions to be flushed.  how many mobfiles are generated.  one mob file per region on the rs, one mob file per table on the rs, or exactly one because there is only one rs?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Where are the mob files written to?  are they in the region dir, the family dir, the table dir or something else? In 98, the dir structure is /hbase/&amp;lt;namespace&amp;gt;/&amp;lt;table&amp;gt;/&amp;lt;region&amp;gt;/&amp;lt;cf&amp;gt;/hfile.  Where do the mob files for region1 of tableA go and where does the mob files for region2 of tableB go to?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yes, this could improve the compaction. But this doesn&apos;t reduce the twice writing for the mob file.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, so this is essentially equal &amp;#8211; both the pdf and the cf approach require a minimum of 2x.writes of mob data &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Saving the mob files by stores than by region server is more efficient to use the TTL to clean the expired mobs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;With this It sounds like new mob file per region, and that mobs would still generate the same number of files as the separate cf&apos;s approach.&lt;/p&gt;

&lt;p&gt;Can&apos;t we (or do we already) have the ttl optimization in our existing cf&apos;s since our hfiles have start and end ts in them?&lt;/p&gt;

&lt;p&gt;... (i think I need to understand the answers to the first section before some of this makes sense to me.)&lt;/p&gt;



</comment>
                            <comment id="14042987" author="jingcheng.du@intel.com" created="Wed, 25 Jun 2014 02:46:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;In the pdf design, is there one MobManager per RS or one MobManager per table or one MobManager per region? Is the mob hfiles kind of like a shared cf that all regions with mobs eventually throw their data into?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The MobManager is per region server, it maintain the mapping between the (tableName,cfName) to mob cf.&lt;br/&gt;
The mob files are saved in the &lt;/p&gt;
{mobRootDir}/{tableNameAsString}/{cfName}/{date}/mobFiles.&lt;br/&gt;
1. A mob file is generated per MemStore flushing.&lt;br/&gt;
2. All the mob files for all regions in a single table of a region server are saved into the same directory {mobRootDir}
&lt;p&gt;/&lt;/p&gt;
{tableNameAsString}/{cfName}/{date}.&lt;br/&gt;
The greatest advantage is using the TTL to clean the whole date directory in one cf.&lt;br/&gt;
&lt;br/&gt;
bq. Can you explain what happens if I have a RS with regions, some belonging to tableA and and some belonging to tableB. Let&apos;s say all writes to tableA and tableB have Mobs in them.&lt;br/&gt;
The mob files are save in the {mobRootDir}/{tableNameAsString}
&lt;p&gt;/&lt;/p&gt;
{cfName}/{date}/mobFiles. So each mob cf should have its own mob file, one new mob file is generated for each cf when a region flushes.&lt;br/&gt;
1. The mob files for tableA and tableB are saved into different directories. The ones for tableA are saved into {mobRootDir}/tableAAsString/{cfName}
&lt;p&gt;/&lt;/p&gt;
{date}
&lt;p&gt;/mobFiles, and the ones for tableB are saved into &lt;/p&gt;
{mobRootDir}/tableBAsString{cfName}/{data}/mobFiles.&lt;br/&gt;
2. Per flushing, a new mob file is generated for each cf, the one for tableA is {mobRootDir}
&lt;p&gt;/tableBAsString&lt;/p&gt;
{cf1}
&lt;p&gt;/&lt;/p&gt;
{data}/{aNewMobFileForTableACf1}, the one for tableB is {mobRootDir}/tableBAsString{cf2}/{data}
&lt;p&gt;/&lt;/p&gt;
{aNewMobFileForTableBCf2}
&lt;p&gt;.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With this It sounds like new mob file per region, and that mobs would still generate the same number of files as the separate cf&apos;s approach.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can&apos;t we (or do we already) have the ttl optimization in our existing cf&apos;s since our hfiles have start and end ts in them?&lt;br/&gt;
The mob files are saved by table/cf instead of table/region/cf.&lt;br/&gt;
If saving the mob into HBase directly, the writing when splitting the mob store are not avoided even if we split the regions by certain cfs.&lt;br/&gt;
If getting the end ts by the last key in the HFile, we have to read all the HFile to know whether it&apos;s expired. In the pdf, we check it by directories which needs less read.&lt;/p&gt;</comment>
                            <comment id="14042990" author="jingcheng.du@intel.com" created="Wed, 25 Jun 2014 02:56:16 +0000"  >&lt;p&gt;Resend it to correct the format.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;In the pdf design, is there one MobManager per RS or one MobManager per table or one MobManager per region? Is the mob hfiles kind of like a shared cf that all regions with mobs eventually throw their data into?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The MobManager is per region server, it maintain the mapping between the (tableName,cfName) to mob cf.&lt;br/&gt;
The mob files are saved in the &amp;lt;i&amp;gt;mobRootDir / tableNameAsString / cfName / date / mobFiles&amp;lt;/i&amp;gt;.&lt;br/&gt;
1.  A mob file is generated per MemStore flushing.&lt;br/&gt;
2.  All the mob files for all regions in a single table of a region server are saved into the same directory &amp;lt;i&amp;gt;mobRootDir / tableNameAsString / cfName /  date&amp;lt;/i&amp;gt;.&lt;br/&gt;
The greatest advantage is using the TTL to clean the whole date directory in one cf.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Can you explain what happens if I have a RS with regions, some belonging to tableA and and some belonging to tableB. Let&apos;s say all writes to tableA and tableB have Mobs in them.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The mob files are save in the &amp;lt;i&amp;gt;mobRootDir / tableNameAsString / cfName / date / mobFiles&amp;lt;/i&amp;gt;. So each mob cf should have its own mob file, one new mob file is generated for each cf when a region flushes.&lt;br/&gt;
1. The mob files for tableA and tableB are saved into different directories. The ones for tableA are saved into &amp;lt;i&amp;gt; mobRootDir / tableAAsString / cfName / date / mobFiles&amp;lt;/i&amp;gt;, and the ones for tableB are saved into &amp;lt;i&amp;gt;mobRootDir / tableBAsString / cfName / data / mobFiles&amp;lt;/i&amp;gt;.&lt;br/&gt;
2. Per flushing, a new mob file is generated for each cf, the one for tableA is &amp;lt;i&amp;gt;mobRootDir / tableBAsString / cf1 / data/ aNewMobFileForTableACf1&amp;lt;/i&amp;gt;, the one for tableB is &amp;lt;i&amp;gt;mobRootDir / tableBAsString / cf2 / data / aNewMobFileForTableBCf2&amp;lt;/i&amp;gt;.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With this It sounds like new mob file per region, and that mobs would still generate the same number of files as the separate cf&apos;s approach.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can&apos;t we (or do we already) have the ttl optimization in our existing cf&apos;s since our hfiles have start and end ts in them?&lt;br/&gt;
The mob files are saved by table/cf instead of table/region/cf.&lt;br/&gt;
If saving the mob into HBase directly, the writing when splitting the mob store are not avoided even if we split the regions by certain cfs.&lt;br/&gt;
If getting the end ts by the last key in the HFile, we have to read all the HFile to know whether it&apos;s expired. In the pdf, we check it by directories which needs less read.&lt;/p&gt;</comment>
                            <comment id="14055601" author="jmhsieh" created="Tue, 8 Jul 2014 22:15:24 +0000"  >&lt;p&gt;Jingcheng and some of his colleagues chatted with me last week. Here&apos;s a quick summary and some follow up questions from the conversation.&lt;/p&gt;

&lt;p&gt;The proposed design essentially adds a special table wide column family/directory where all blobs are written to.  &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This avoids having to rewrite lob data on splits (the problem the cf approach suffers from).&lt;/li&gt;
	&lt;li&gt;Blobs are written to the WAL and the memstore.  Flushes write out a reference in the normal cf dir and the one blob hfile per region into the shared blob dir.   The normal cf write which contains a pointer to the blob hfile/offset while the blob write contains the blob data.  This is the simplest way to preserve atomicity by avoiding read/write race conditions that Could be present if blobs read directly froma &quot;blob log&quot; approach.&lt;/li&gt;
	&lt;li&gt;There is a special sweep tool that uses zk and is used garbage collect deleted or overwritten blobs based upon a garbage threshold.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Follow up questions and tasks from after reviewing the design:&lt;br/&gt;
1) Please write user level documentation on how an operator or application developer would enable and use blobs.  This would be folded into the ref guide and is more useful for most folks that the current approach of focusing on the individual mechanisms.  For example, does one specify that a cf is a blob?  a particular column? a particular cell? A helpful approach would be to write up the life cycle of a single blob.&lt;br/&gt;
2) Instead of using &quot;special&quot; column/ column family names to denote a reference, use the new 0.98 tags feature to tag if a cell is a reference to a value in the blob dir.&lt;br/&gt;
3) Better explain the life cycle of a blob that has a user specified historical timestamp.  where is this written? (into the date dir of the time stamp or of the actual write) how is this deleted?  How does the sweep tool interact with this?&lt;br/&gt;
4) Better explain what if any caching happens when we read values from blob hfiles.&lt;br/&gt;
5) Provide Integration tests that others can use to verify the correctness and robustness of the implementation.&lt;/p&gt;

&lt;p&gt;A new question that came up when thinking about the design:&lt;br/&gt;
1) How do snapshots work with relation to the current design.  Are the HFiles in the Blob dir archived?  Are they needed files tracked when a snapshot is taken?  If this is not handled, is there a plan on how to handle it?  &lt;/p&gt;</comment>
                            <comment id="14058614" author="jingcheng.du@intel.com" created="Fri, 11 Jul 2014 10:19:21 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;!&lt;br/&gt;
I&apos;ve uploaded the latest design document which includes the cache and snapshot. Please review and advise. Thanks.&lt;/p&gt;

&lt;p&gt;The user level document will be uploaded later.&lt;/p&gt;</comment>
                            <comment id="14061795" author="jingcheng.du@intel.com" created="Tue, 15 Jul 2014 07:26:55 +0000"  >&lt;p&gt;Upload the latest design document. Refine the design and description.&lt;/p&gt;</comment>
                            <comment id="14066187" author="jingcheng.du@intel.com" created="Fri, 18 Jul 2014 09:24:01 +0000"  >&lt;p&gt;Upload the latest design document HBase MOB Design-v2.pdf where details the cases in MOB compaction done by sweep tool.&lt;/p&gt;</comment>
                            <comment id="14066196" author="jingcheng.du@intel.com" created="Fri, 18 Jul 2014 09:42:49 +0000"  >&lt;p&gt;Upload the first patch hbase-11339-in-dev.patch which is still under developing for review, and it&apos;s not tested yet. &lt;br/&gt;
You could find this patch in the review board through this link &lt;a href=&quot;https://reviews.apache.org/r/23676/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/23676/&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="14072706" author="jiajia" created="Thu, 24 Jul 2014 02:26:18 +0000"  >&lt;p&gt;add the mob user guide.&lt;/p&gt;</comment>
                            <comment id="14074262" author="ram_krish" created="Fri, 25 Jul 2014 10:21:24 +0000"  >&lt;p&gt;Bulk loading mob files is what was discussed in internal discussions and why use table.put() in the sweep tool.  Using table.put is again flushing the data to the memstore and internally causes the flushes to happen thus affecting the write path of the system.&lt;br/&gt;
Bulk loading mob is possible and it should work fine considering &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6630&quot; title=&quot;Port HBASE-6590 to trunk : Assign sequence number to bulk loaded files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6630&quot;&gt;&lt;del&gt;HBASE-6630&lt;/del&gt;&lt;/a&gt; available where the bulk loaded files are also assigned with a sequence number and the same sequence number can be used to resolve a conflict in case the keyvalueheap finds two cells with same row, ts but different values.  &lt;br/&gt;
In our case of sweep tool one thing to note is that by using this tool we are trying to create a new store file for a same row, ts, cf, cq cell but update it with a new value. Here the new value is that of the new path that we are generating after the sweep tool merges some of the mob data into one single file.&lt;br/&gt;
So consider in our case row1, cf,c1, ts1 = path1.  The above data is written in Storefile 1&lt;br/&gt;
The updated path is path 2 and so we try to bulk load that new info into a new store file row1,cf1,c1,ts1 = path2.  Now the HFile containing the new value is bulk loaded into the system and we try to scan for row1.&lt;br/&gt;
What we would expect is to  get the cell with path2 as the value and that should come from the bulk loaded file.&lt;br/&gt;
&lt;b&gt;Does this happen - Yes in case of 0.96 - No in case of 0.98+&lt;/b&gt; .&lt;br/&gt;
In 0.96 case the compacted file will have kvs with mvcc as 0 if the kvs are smaller than the smallest read point. So in case where a scanner is opened after a set of files have been compacted all the kvs will have mvcc = 0 in it.&lt;br/&gt;
In 0.98+ above that is not the case because &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; oldestHFileTimeStampToKeepMVCC = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis() - 
      (1000L * 60 * 60 * 24 * &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.keepSeqIdPeriod);  

    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (StoreFile file : filesToCompact) {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(allFiles &amp;amp;&amp;amp; (file.getModificationTimeStamp() &amp;lt; oldestHFileTimeStampToKeepMVCC)) {
        &lt;span class=&quot;code-comment&quot;&gt;// when isAllFiles is &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, all files are compacted so we can calculate the smallest 
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// MVCC value to keep
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(fd.minSeqIdToKeep &amp;lt; file.getMaxMemstoreTS()) {
          fd.minSeqIdToKeep = file.getMaxMemstoreTS();
        }
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And so the performCompaction()&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        KeyValue kv = KeyValueUtil.ensureKeyValue(c);
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (cleanSeqId &amp;amp;&amp;amp; kv.getSequenceId() &amp;lt;= smallestReadPoint) {
          kv.setSequenceId(0);
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;is not able to setSeqId to 0 as atleast for 5 days we expect the value to be retained. &lt;br/&gt;
Remember that in the above case we are assigning seq numbers to bulk loaded files also and the case there is that when the scanner starts the bulk loaded file is having the highest seq id and that is ensured by using HFileOutputFormat2 which writes &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    w.appendFileInfo(StoreFile.BULKLOAD_TIME_KEY,
              Bytes.toBytes(&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis()));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So on opening the reader for this bulk loaded store file we are able to get the sequence id.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isBulkLoadResult()){
      &lt;span class=&quot;code-comment&quot;&gt;// generate the sequenceId from the fileName
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// fileName is of the form &amp;lt;randomName&amp;gt;_SeqId_&amp;lt;id-when-loaded&amp;gt;_
&lt;/span&gt;      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; fileName = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.getPath().getName();
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; startPos = fileName.indexOf(&lt;span class=&quot;code-quote&quot;&gt;&quot;SeqId_&quot;&lt;/span&gt;);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (startPos != -1) {
        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.sequenceid = &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;.parseLong(fileName.substring(startPos + 6,
            fileName.indexOf(&apos;_&apos;, startPos + 6)));
        &lt;span class=&quot;code-comment&quot;&gt;// Handle reference files as done above.
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (fileInfo.isTopReference()) {
          &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.sequenceid += 1;
        }
      }
    }
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.reader.setSequenceID(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.sequenceid);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now when the scanner tries to read from the above two files which has same cells in it for row1,cf,c1,ts1 but with path1 and path 2 as the values, the mvcc in the compacted store files that has path 1 (is a non-zero positive value) in 0.98+ and 0 in 0.96 case) and the mvcc for the KV in the store file generated by bulk load will have 0 in it (both 0.98+ and 0.96).&lt;br/&gt;
In KeyValueHeap.java&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; compare(KeyValueScanner left, KeyValueScanner right) {
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; comparison = compare(left.peek(), right.peek());
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (comparison != 0) {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; comparison;
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        &lt;span class=&quot;code-comment&quot;&gt;// Since both the keys are exactly the same, we &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt; the tie in favor
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// of the key which came latest.
&lt;/span&gt;        &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; leftSequenceID = left.getSequenceID();
        &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; rightSequenceID = right.getSequenceID();
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (leftSequenceID &amp;gt; rightSequenceID) {
          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; -1;
        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (leftSequenceID &amp;lt; rightSequenceID) {
          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; 1;
        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; 0;
        }
      }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In 0.96 when the scanner tries to compare the different StoreFileScanner to retrieve from which file the scan has to happen, the if condition will give a &apos;0&apos; because the KV will have all items same -  row1,cf,c1,ts1 and mvcc =0.&lt;br/&gt;
So it tries to get the reader&apos;s sequence id (else part of the code) and in the above case the bulk loaded file has the highest sequence id and so that row1,cf1,c1,ts1 with path2 is the KV that is returned.&lt;/p&gt;

&lt;p&gt;In 0.98 case since the mvcc of the kv in the compacted file is a non-zero value we always tend to return the compacted file and so the result would be row1,cf1,c1,ts1 with path1.&lt;br/&gt;
So this is a behavioral change between 0.96 and 0.98 and also considering that the seq id of the bulk loaded file is higher than the compacted file it makes sense to read from the bulk loaded file than the compacted file as it is the newest value. If this is an issue we can raise a JIRA and find a soln for it. Correct me if am wrong.  Feedback appreciated.&lt;/p&gt;</comment>
                            <comment id="14074874" author="apurtell" created="Fri, 25 Jul 2014 20:35:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;If this is an issue we can raise a JIRA and find a soln for it. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11591&quot; title=&quot;Scanner fails to retrieve KV  from bulk loaded file with highest sequence id than the cell&amp;#39;s mvcc in a non-bulk loaded file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11591&quot;&gt;&lt;del&gt;HBASE-11591&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14075381" author="ram_krish" created="Sat, 26 Jul 2014 13:26:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;Does this happen - Yes in case of 0.96 - No in case of 0.98+ .&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The above stmt is wrong. It should be 0.99+ and not 0.98+.&lt;/p&gt;</comment>
                            <comment id="14088704" author="jingcheng.du@intel.com" created="Thu, 7 Aug 2014 02:36:07 +0000"  >&lt;p&gt;The latest document is uploaded. Please kindly review. Thanks.&lt;br/&gt;
1. Fix mistakes in statement.&lt;br/&gt;
2. Change the format of mob file name.&lt;br/&gt;
3. Add the chapter &quot;handle the mob in HBase compaction&quot;.&lt;/p&gt;</comment>
                            <comment id="14089104" author="jingcheng.du@intel.com" created="Thu, 7 Aug 2014 10:20:00 +0000"  >&lt;p&gt;A new version of design document. In this new version, the value of a cell in the mob column family consists two parts.&lt;br/&gt;
1. The value size of a mob data (first 8 bytes).&lt;br/&gt;
2. The path of a mob file.&lt;br/&gt;
Whereas in the old version, the value only had the path of a mob file.&lt;/p&gt;</comment>
                            <comment id="14101924" author="jiajia" created="Tue, 19 Aug 2014 06:35:42 +0000"  >&lt;p&gt;update the mob user guide. &lt;/p&gt;</comment>
                            <comment id="14102218" author="jmhsieh" created="Tue, 19 Aug 2014 14:00:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jiajia&quot; class=&quot;user-hover&quot; rel=&quot;jiajia&quot;&gt;Jiajia Li&lt;/a&gt;, thanks for the update to the user guide.  I think it has the key details points (the whats) needed for a user who already understands what a MOB is and is for.  We should add some context for users (the why&apos;s and the bigger picture) that aren&apos;t familiar with it thought but adding some background into this user doc. We&apos;ll eventually fold into the ref guide here&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Let me provide a quick draft that we could build off of.&lt;/p&gt;

&lt;p&gt;Before Bullet we should have some info (this is a paraphrased version of the design doc&apos;s intro.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Data comes in many sizes, and it is convenient to save the binary data like images, documents into the HBase. While HBase can handle binary objects with cells that are 1 byte to 10MB long, HBase&apos;s normal read and write paths are optimized for values smaller than 100KB in size.  When HBase deals with large numbers of values &amp;gt; 100kb and up to ~10MB of data, it encounters performance degradations due to write amplification caused by splits and compactions.  HBase 2.0+ has added support for better managing large numbers of &lt;b&gt;Medium Objects&lt;/b&gt; (MOBs) that maintains the same high performance,  strongly consistently characteristics with low operational overhead.&lt;/p&gt;

&lt;p&gt;To enable the feature, one must enable and config the mob components in each region server and enable the mob feature on particular column families during table creation or table alter.  Also in the preview version of this feature, the admin must setup periodic processes that re-optimizes the layout of mob data.&lt;/p&gt;

&lt;p&gt;Section: Enabling and Configuring the mob feature on region servers.&lt;/p&gt;

&lt;p&gt;Need to enable feature in flushes and compactions.  Tuning settings on caches.&lt;/p&gt;

&lt;p&gt;user doc bullet 1. edit hbase-site...&lt;br/&gt;
user doc bullet 7. mob cache&lt;/p&gt;

&lt;p&gt;Would be nice to have an examples of doing this from the shell &amp;#8211; an example of creating a table with mob on a cf, and an example of a table alter that changes a cf to use the mob path.&lt;/p&gt;

&lt;p&gt;Section: Mob management&lt;/p&gt;

&lt;p&gt;The mob feature introduces a new read and write path to hbase and in its current incarnation requires external tools for housekeeping and reoptimization.  There are two tools introduced &amp;#8211; the expiredMobFileCleaner for handling ttls and time based expiry of data, and the sweep tool for coalescing small mob files or mob files with many deletions or updates.&lt;/p&gt;

&lt;p&gt;user doc bullet 8.&lt;/p&gt;

&lt;p&gt;Section: Enabling the mob feature on user tables&lt;/p&gt;

&lt;p&gt;This can be done when creating a table or when altering a table&lt;/p&gt;

&lt;p&gt;user doc bullet 2 (set cf with mob)&lt;br/&gt;
user doc bullet 6 (threshold size)&lt;/p&gt;

&lt;p&gt;To a client, mob cells act just like normal cells.&lt;/p&gt;

&lt;p&gt;user doc bullet 3 put&lt;br/&gt;
user doc bullet 4 scan&lt;/p&gt;

&lt;p&gt;There is a special scanner mode users can use to read the raw values&lt;/p&gt;

&lt;p&gt;user doc bullet 5.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://hbase.apache.org/book.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/book.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14105191" author="jingcheng.du@intel.com" created="Thu, 21 Aug 2014 08:35:34 +0000"  >&lt;p&gt;Dear all, the patches for the sub tasks of HBase MOB have been uploaded. Please help review and comment. Thanks a lot!&lt;/p&gt;</comment>
                            <comment id="14106572" author="jiajia" created="Fri, 22 Aug 2014 07:05:52 +0000"  >&lt;p&gt;update the mob user guide.&lt;/p&gt;</comment>
                            <comment id="14108771" author="jiajia" created="Mon, 25 Aug 2014 05:52:55 +0000"  >&lt;p&gt;update the mob user guide(add the coprocessor master configuration)&lt;/p&gt;</comment>
                            <comment id="14114727" author="jingcheng.du@intel.com" created="Fri, 29 Aug 2014 01:36:48 +0000"  >&lt;p&gt;Now we have made some changes in the design.&lt;br/&gt;
1. Change the checksumHexString(startKey) to md5HexString(startKey) as the mob file prefix. After this, we could avoid the checksum conflict between regions and this might be useful in future.&lt;br/&gt;
2. Add a new tag to the mob cell(its value is the realMobValueLength + fileNameOfMobFile) in HBase. This tag has the table name where the cell is flushed. It&apos;s useful in cloning table and reading from the cloned table.&lt;br/&gt;
These changes will be applied in the design document and upload it later.&lt;/p&gt;</comment>
                            <comment id="14114993" author="jiajia" created="Fri, 29 Aug 2014 07:48:48 +0000"  >&lt;p&gt;update the mob user guide.&lt;/p&gt;</comment>
                            <comment id="14115526" author="jmhsieh" created="Fri, 29 Aug 2014 17:24:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jiajia&quot; class=&quot;user-hover&quot; rel=&quot;jiajia&quot;&gt;Jiajia Li&lt;/a&gt;, new version of docs look good, I think it is done for now unless we make changes to it. &lt;/p&gt;

&lt;p&gt;nits: I found there are two typos,  &quot;provinding&quot; &amp;#45;&amp;gt; &quot;providing&quot; and &quot;handers&quot;&amp;#45;&amp;gt;&quot;handlers&quot;.    Don&apos;t worry about fixing this for now &amp;#8211; we&apos;ll have &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=misty&quot; class=&quot;user-hover&quot; rel=&quot;misty&quot;&gt;Misty Stanley-Jones&lt;/a&gt; convert them into a chapter or section in the ref guide.&lt;/p&gt;

&lt;p&gt;Also, in the future, please do not delete attachments &amp;#8211; just provide a new version with a v2 or some think like that so we can keep track of the evolution.  &lt;/p&gt;
</comment>
                            <comment id="14117774" author="lhofhansl" created="Mon, 1 Sep 2014 23:10:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jon%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;jon@cloudera.com&quot;&gt;Jonathan Hsieh&lt;/a&gt; and I talked about this at the HBase meetup...&lt;/p&gt;

&lt;p&gt;I&apos;m sorry to be the party pooper here, but this complexity and functionality really does not belong into HBase IMHO.&lt;br/&gt;
I still do not get the motivation for this... Here&apos;s why:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;We still cannot stream the mobs. They have to be materialized at both the server and the client (going by the documentation here)&lt;/li&gt;
	&lt;li&gt;As I state above this can be achieved with a HBase/HDFS client alone and better: Store mobs up to a certain size by value in HBase (say 5 or 10mb or so), everything larger goes straight into HDFS with a reference only in HBase. This addresses both the many small files issue in HDFS (only files larger than 5-10mb would end up in HDFS) and the streaming problem for large files in HBase. Also as outlined by me in June we can still make this &quot;transactional&quot; in the HBase sense with a three step protocol: (1) write reference row, (2) stream blob to HDFS, (3) record location in HDFS (that&apos;s the commit). This solution is also missing from the initial PDF in the &quot;Existing Solutions&quot; section.&lt;/li&gt;
	&lt;li&gt;&quot;Replication&quot; here can still happen by the client, after all, each file successfully stored in HDFS has a reference in HBase.&lt;/li&gt;
	&lt;li&gt;We should use the tools what they were intended for. HBase for key value storage, HDFS for streaming large blobs.&lt;/li&gt;
	&lt;li&gt;Just saying using one client API for client convenience is &lt;b&gt;not&lt;/b&gt; a reason to put all of this into HBase. A client can easily speak both HBase and HDFS protocols.&lt;/li&gt;
	&lt;li&gt;(Subjectively) I do not like the complexity of this as seen by the various discussions here. That part is just my $0.02 of course.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This looks to me like solution to a problem that we do not have.&lt;/p&gt;

&lt;p&gt;Again I am sorry about being negative here, but we have to be careful what we put into HBase and for what reasons.&lt;/p&gt;

&lt;p&gt;Especially when there seems to be a &lt;b&gt;better&lt;/b&gt; client only solution (in the sense that it can deal with larger files, and allows for streaming the larger files).&lt;/p&gt;

&lt;p&gt;If we need a solution for this, let&apos;s build one on top of HBase/HDFS. We (Salesforce) are actually building a client only solution for this, it&apos;s not that difficult (I will see whether we can open source this - it might be too entangled with our internals). With an easy protocol we can still allow data locality for all blob reads (as much as the block distribution allows it at least), etc.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jesse_yates&quot; class=&quot;user-hover&quot; rel=&quot;jesse_yates&quot;&gt;Jesse Yates&lt;/a&gt;, maybe you want to add here?&lt;/p&gt;

&lt;p&gt;If we cannot store 10mb Cells in HBase then that&apos;s something to address. The fact that we cannot stream into and out of HBase needs to be addressed, that is the real problem anyway.&lt;/p&gt;</comment>
                            <comment id="14118654" author="apurtell" created="Tue, 2 Sep 2014 20:08:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;we cannot store 10mb Cells in HBase then that&apos;s something to address.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can store 10 MB cells in HBase. It is true that beyond some use-case-dependent threshold we risk OOME under load with very large cells. This is because the complete cell contents are materialized on the server for RPC, as you mention. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The fact that we cannot stream into and out of HBase needs to be addressed, that is the real problem anyway.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Definitely the lack of a streaming API is an issue worth looking at.&lt;/p&gt;

&lt;p&gt;Related, the MOB design also attempts to avoid write amplification of large cells during compaction, by segregating large values into separate files set outside the normal compaction process. Rather than normal compaction, an external MapReduce based tool is used for compacting MOB files. HBase has never &lt;b&gt;required&lt;/b&gt; MapReduce before and we should really think hard before introducing such a change. Are we &lt;b&gt;sure&lt;/b&gt; the desired objectives cannot be met with a pluggable compaction policy?&lt;/p&gt;</comment>
                            <comment id="14118659" author="jmhsieh" created="Tue, 2 Sep 2014 20:09:29 +0000"  >&lt;p&gt;Lars and I chatted back on Thursday, and agree about the solution for truly large objects (larger than default hdfs block) would require a new streaming API.  We also talked about the importance of making configuration and operations simple.  &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;, can you describe the scale and the load for the hybrid MOB storage system you are have or are working on?  It is new to me, and I&apos;d very curious about how things like backups and bulk loads are handled in that system.  &lt;/p&gt;

&lt;p&gt;Details below:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The fundamental problem this MOB solution is addressing is the a balance between is the hdfs small file problem and write amplification and performance variability caused by write amplification.  Objects that have greater than 64MB+ values are out of scope and we are in agreement about needing a streaming api and that a hdfs+hbase solution seems more reasonable.  The goal with the MOB mechanism is to show demonstrable improvements in predictability and scalability when we are too small for where hdfs makes sense and where hbase is non-optimal due to splits and compactions.&lt;/p&gt;

&lt;p&gt;In some workloads I&apos;ve been seeing, (timeseries large sensor dumps, mini indexes, or binary documents or images as blob cells) this feature would potentially be very helpful.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;We still cannot stream the mobs. They have to be materialized at both the server and the client (going by the documentation here)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is true; however this is &lt;b&gt;not&lt;/b&gt; the design point we are trying to solve.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As I state above this can be achieved with a HBase/HDFS client alone and better: Store mobs up to a certain size by value in HBase (say 5 or 10mb or so), everything larger goes straight into HDFS with a reference only in HBase. This addresses both the many small files issue in HDFS (only files larger than 5-10mb would end up in HDFS) and the streaming problem for large files in HBase. Also as outlined by me in June we can still make this &quot;transactional&quot; in the HBase sense with a three step protocol: (1) write reference row, (2) stream blob to HDFS, (3) record location in HDFS (that&apos;s the commit). This solution is also missing from the initial PDF in the &quot;Existing Solutions&quot; section.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Back in June, JingCheng&apos;s response to your comments never got feedback on how you&apos;d manage the small files problem.&lt;/p&gt;

&lt;p&gt;Also, in there are two HDFS blob + HBase metadata solutions are explicitly mentioned in section 4.1.2 (v4 design doc) with pros and cons.  The solution you propose is actually the first described hdfs+hbase approach &amp;#8211; though its pro&apos;s and con&apos;s don&apos;t go into the particulars of the commit protocol (though the two-phase prep and then commit be the commit make sense).  The largest concern was in the doc as well &amp;#8211; the HDFS small files problem.&lt;/p&gt;

&lt;p&gt;Having a separate hdfs file per 100k-10mb value is not a scaleable or long term solution.  Let&apos;s do an example &amp;#8211; lets say we wrote 200M 500KB blobs.  This ends up being 100TB of data.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Using hbase as is, we end up with objects in potentially in 10,000 10GB files.
	&lt;ul&gt;
		&lt;li&gt;Along the way, we&apos;d end up splitting and compacting every 20,000 objects, rewriting large chunks of the 100TB over and over.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Using hdfs+hbase, we&apos;d end up with a 200M files &amp;#8211; a lot more files than the optimal 10000 files that vanilla hbase approach could eventually compact to.
	&lt;ul&gt;
		&lt;li&gt;200M files would consume ~200GB ram for block records in the NN (200M files * 3 block replicas per file * ~300 bytes per hdfs inode+ blockinfo &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;  -&amp;gt;  ~200GB), which is definitely in an  uncharted area for NN&apos;s  &amp;#8211; a place where there would likely be GC problems and other negative affects.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;&quot;Replication&quot; here can still happen by the client, after all, each file successfully stored in HDFS has a reference in HBase.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The design doc approach actually minimizes the operational changes required to store MOBs.  From an operational point of view, a users could just enable the optional feature and potentially take advantage of its potential benefits.&lt;/p&gt;

&lt;p&gt;This hdfs+hbase proposed approach actually pushes more complexity into the replication mechanism.  For replication to work, the source cluster would now need to add mechanisms to open the mobs on the hdfs and ship them off to the other cluster.  The MOB approach is simpler operationally and in code because it can use the normal replication mechanism.&lt;/p&gt;

&lt;p&gt;The hdfs+hbase proposed approach would need updates and a new bulk load mechanism.  The MOB approach is simpler operationally and in code because it can would use normal bulk loads and compactions would push out eventually push the mobs out.  (same IO cost)&lt;/p&gt;

&lt;p&gt;The hdfs+hbase proposed approach would need updates to properly handle table snapshots and restoring table snapshots and backups.  Naively this seems like we&apos;d either have to do a lot of NN operations to backup the mobs. (1 per mob).  Also, we&apos;d need to build new tools to manage export and copy table operations as well.    The MOB approach is simpler because the copy/export table mechanisms remain the same, and we can use the same archiver mechanism to manage mob file snapshots (mobs are essentially stored in a special region).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We should use the tools what they were intended for. HBase for key value storage, HDFS for streaming large blobs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We agree here.&lt;/p&gt;

&lt;p&gt;The case for HDFS is weak: 1MB-5MB blobs are not large enough for HDFS &amp;#8211; in fact for HDFS this is nearly pathological.&lt;/p&gt;

&lt;p&gt;The case for HBase is ok: We are writing key-values that tend to be larger than normal.  However, with constant continuous ingest with 1MB-5MB MOBs will likely cause trigger splits more frequently which will trigger unavoidable major compactions.  This would occur even if bulk load mechanisms were used.&lt;/p&gt;

&lt;p&gt;The case for HBase + MOB is stronger:  We are writing key-values that tend to be large.  The bulk of the 1MB-5MB MOB data is written off to the MOB path.  The metadata for the mob  (let&apos;s say 100 bytes per) is relatively small and thus compactions and splits will be much rarer (10000x less frequent) than the hbase-only approach.  If bulk loads are done, an initial compaction would separate out the mob data and keep the region relatively small.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Just saying using one client API for client convenience is not a reason to put all of this into HBase. A client can easily speak both HBase and HDFS protocols.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The tradeoff being made by the hdfs+hbase approach is opting for more operational complexity vs implementation simplicity.  With the hdfs+hbase approach, we&apos;d also introduce new  security issues &amp;#8211; now users in hbase would have the ability to modify the file system directly, and now manage users and credentials on both hdfs and hbase in sync.  With the MOB approach we just rely on HBase&apos;s security mechanism, its and should be able have per cell ACLs, vis tags, and all the rest.&lt;/p&gt;

&lt;p&gt;Given the choice of 1) making hbase simpler to use by adding some internal complexity or 2) making hbase more operationally difficult by adding new external processes and requiring external integrations to manage parts of its data, we should opt for 1.  Making HBase easier to use by removing knobs or making knobs as simple as possible should be the priority.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(Subjectively) I do not like the complexity of this as seen by the various discussions here. That part is just my $0.02 of course.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We agree about not liking complexity.  However, the discussion process was public and we described and knocked down several strawmen. I actually initially took the side against adding this feature but have been convinced me that when complete, this would have light operator impact and actually less complex than a full solution that uses the hybrid hdfs+hbase approach.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-6658&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HDFS-6658&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12651408/Block-Manager-as-a-Service.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12651408/Block-Manager-as-a-Service.pdf&lt;/a&gt; (see RAM Explosion section)&lt;/p&gt;</comment>
                            <comment id="14118682" author="anoop.hbase" created="Tue, 2 Sep 2014 20:28:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;HBase has never required MapReduce before and we should really think hard before introducing such a change. Are we sure the desired objectives cannot be met with a pluggable compaction policy?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It would be possible for a compaction on MOB files with out needing MR. Subtask &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11861&quot; title=&quot;Native MOB Compaction mechanisms.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11861&quot;&gt;&lt;del&gt;HBASE-11861&lt;/del&gt;&lt;/a&gt; aims for this.&lt;/p&gt;</comment>
                            <comment id="14118911" author="jmhsieh" created="Tue, 2 Sep 2014 22:58:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Related, the MOB design also attempts to avoid write amplification of large cells during compaction, by segregating large values into separate files set outside the normal compaction process. Rather than normal compaction, an external MapReduce based tool is used for compacting MOB files. HBase has never required MapReduce before and we should really think hard before introducing such a change. Are we sure the desired objectives cannot be met with a pluggable compaction policy?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Removing the external processes that perform &quot;mob compaction&quot; is one of the follow up goals and is noted at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11861&quot; title=&quot;Native MOB Compaction mechanisms.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11861&quot;&gt;&lt;del&gt;HBASE-11861&lt;/del&gt;&lt;/a&gt;.  We want to get rid of the MR dependencies because it introduces a new piece of operational complexity and I don&apos;t want that.  I don&apos;t consider the MOB feature to be production ready if it still requires the external process to manage this.  &lt;/p&gt;

&lt;p&gt;The mob feature, like other experimental features that require external tooling, will  be experimental until simplified operationally.  We&apos;ve done this before &amp;#8211; for example,favored nodes &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7932&quot; title=&quot;Do the necessary plumbing for the region locations in META table and send the info to the RegionServers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7932&quot;&gt;&lt;del&gt;HBASE-7932&lt;/del&gt;&lt;/a&gt; is experimental because it is not &quot;set-it-and-forget&quot;; it requires extra processes such as an external balancer.  For MOB, after we get the other blockers in (snapshot support, metrics) we&apos;ll revamp the mob compaction and then remove the experimental tag.  Our goal would be to get this all in by the end of the year.&lt;/p&gt;</comment>
                            <comment id="14119357" author="anoop.hbase" created="Wed, 3 Sep 2014 05:13:34 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Just in case one is not watching the progress in the sub tasks&amp;#93;&lt;/span&gt;&lt;br/&gt;
I don&apos;t think there is any -1 yet.&lt;br/&gt;
The 1st sub task patch (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11643&quot; title=&quot;Read and write MOB in HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11643&quot;&gt;&lt;del&gt;HBASE-11643&lt;/del&gt;&lt;/a&gt;)  I have done 3 rounds of review and my +1 stands.&lt;br/&gt;
We have total 3 +1s for that Jira after many rounds of review rework. Can get it committed tomorrow IST &lt;b&gt;unless objections&lt;/b&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14119366" author="ram_krish" created="Wed, 3 Sep 2014 05:25:05 +0000"  >&lt;p&gt;The very first thought of some one wanting to store a KV that is bigger in size (I mean 100s of KBs to few MBs - 1 or 2 MB) makes one think if HBase could be the ideal choice.  The first think comes to mind is that write the references in HBase and the files in HDFS. But getting making this atomic itself needs some external things to monitor this.  Also the HBase features like snapshot and security may come inbuilt when we go with an approach of using HBase only and leveraging all its features.  If you see the discussion thread there were questions on writing the MOB part in the WAL and again in the HFiles. But all of the arguments had the pros and cons and finally the decision was made just because using HBase and leveraging its feature to support this MOB rather than external process and integrations helped us arrive in this decision.&lt;br/&gt;
I think Jon&apos;s nice write up is pretty much explains it.  &lt;br/&gt;
We had spent good amount of time since Jingcheng had proposed the feature and later in the reviews.  Having an MR tool (external) to control the MOB files came up even in internal discussion. For now we did not have a direct work around for that but &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11861&quot; title=&quot;Native MOB Compaction mechanisms.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11861&quot;&gt;&lt;del&gt;HBASE-11861&lt;/del&gt;&lt;/a&gt; is for solving this problem. &lt;br/&gt;
Another advantage I would see here is that the snapshot feature that would work even with MOB. I think that would make this a clear winner instead of having to write another application that would do this MOB snapshot if HBase+HDFS would be used.&lt;br/&gt;
Adding to Anoop&apos;s comments we have reviewed the core patch &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11643&quot; title=&quot;Read and write MOB in HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11643&quot;&gt;&lt;del&gt;HBASE-11643&lt;/del&gt;&lt;/a&gt; that provides the basic things needed for MOB support and we are ready for a commit with 3 +1s to it.&lt;/p&gt;</comment>
                            <comment id="14119489" author="lhofhansl" created="Wed, 3 Sep 2014 07:15:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Back in June, JingCheng&apos;s response to your comments never got feedback on how you&apos;d manage the small files problem.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To be fair, my comment itself addressed that by saying small blobs are stored by &lt;b&gt;value&lt;/b&gt; in HBase, and only large bloba in HDFS. We can store a lot of 10MB (in the worst case scenario it&apos;s 200m x 10mb = 2pb) in HDFS, if that&apos;s not enough, we can dial up the threshold.&lt;/p&gt;

&lt;p&gt;It seems nobody understood what I am suggesting. Depending on use case and data distribution you pick a threshold X. Blobs with a size of &amp;lt; X are stored directly in HBase as a column value. Blobs &amp;gt;= X are stored in a HDFS with a reference in HBase using the 3-phase approach.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;there are two HDFS blob + HBase metadata solutions are explicitly mentioned in section 4.1.2 (v4 design doc) with pros and cons&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;True, but as I state the &quot;store small blobs by value and only large ones by reference&quot; solution is not mentioned in there.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The solution you propose is actually the first described hdfs+hbase approach&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Not it&apos;s not... It says either all blobs go into HBase or all blobs go into HDFS... See above. Small blobs would be stored directly in HBase, not in HDFS. That&apos;s key, nobody wants to store 100k or 1mb files directly in HDFS.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We have total 3 +1s for that Jira after many rounds of review rework. Can get it committed tomorrow IST unless objections...?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We won&apos;t get this committed until we finish this discussion. So consider this my -1 until we finish.&lt;/p&gt;

&lt;p&gt;Going by the comments the use case is only 1-5mb files (definitely less than 64mb), correct? That changes the discussion, but it looks to me that now the use case is limited to a single scenario and carefully constructed (200m x 500k files) so that this change might be useful. I.e. pick a blob size just right, and pick the size distribution of the files just right and this makes sense.&lt;/p&gt;

&lt;p&gt;In my approach one can dial up/down the threshold of by-value and by-reference storage as needed. And I did not even realize the need for M/R.&lt;/p&gt;

&lt;p&gt;I do agree with all of following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;snapshots are harder&lt;/li&gt;
	&lt;li&gt;bulk load is harder&lt;/li&gt;
	&lt;li&gt;backup/restore/replication is harder&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Yet, all that is possible to do with a client only solution and could be abstracted there.&lt;/p&gt;

&lt;p&gt;I&apos;ll also admit that our blob storage tool is not finished, yet, and that for its use case we don&apos;t need replication or backup as it itself will be the backup solution for another very large data store.&lt;/p&gt;

&lt;p&gt;Are you guys absolutely... 100%... positive that this cannot be done in any other way and has to be done this way? That we cannot store files up to a certain size as values in HBase and larger files in HDFS? And there is not good threshold value for this?&lt;/p&gt;</comment>
                            <comment id="14119741" author="jingcheng.du@intel.com" created="Wed, 3 Sep 2014 10:47:23 +0000"  >&lt;p&gt;Thanks Lars for the comments. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Going by the comments the use case is only 1-5mb files (definitely less than 64mb), correct? That changes the discussion, but it looks to me that now the use case is limited to a single scenario and carefully constructed (200m x 500k files) so that this change might be useful. I.e. pick a blob size just right, and pick the size distribution of the files just right and this makes sense.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;the client solution could work well too in certain cases of bigger size blobs and we could try leveraging the current MOB design approach for smaller values of KVs.&lt;br/&gt;
In some usage scenarios, the value size is almost fixed, for example the pictures taken by camera of the traffic bureau, the contracts between banks and customers, the CT(Computed Tomography) records in hospitals, etc. This might be limited, but it&#8217;s really useful.&lt;br/&gt;
As mentioned the client solution saves the records larger than 10MB to hdfs, and saves others to the HBase directly. To turn down the threshold less will lead to the insufficient using of the hdfs in client solution, instead saving them directly in HBase for this case.&lt;br/&gt;
And even with value size less 10MB, the mob implementation has big improvements in performance than directly saving those records into HBase.&lt;/p&gt;

&lt;p&gt;The mob has a threshold as well, the mob could be saved as either value or reference by this threshold. We have a default value 100KB for it now. Users could change it and we also have a compactor to handle it (move the mob file to hbase, and vice versa).&lt;/p&gt;

&lt;p&gt;As Jon said, we&apos;ll revamp the mob compaction and get rid of the MR dependency.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yet, all that is possible to do with a client only solution and could be abstracted there.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To implement the snapshot, replication things in client solution are harder, it will bring the complexity for the client solution as well. To keep the consistency bwtween HBase and HDFS files during replication is a problem.&lt;br/&gt;
To implement this in server side is a little bit easier, the mob includes the implementation of snapshot, and it supports the replication naturally because the mob data are saved in WAL.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(Subjectively) I do not like the complexity of this as seen by the various discussions here. That part is just my $0.02 of course.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, it&#8217;s complex, but they are meaningful and valuable.&lt;br/&gt;
The patches provide features of read/write, compactions, snapshot and sweep for mob files. Even in the future HBase decides to implement streaming feature, the read, compaction, and snapshot parts would be useful probably.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="14119894" author="apurtell" created="Wed, 3 Sep 2014 14:17:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;As Jon said, we&apos;ll revamp the mob compaction and get rid of the MR dependency.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Please. I don&apos;t think we should ever ship a release with a dependency on MR for core function. Committing this to trunk in stages could be ok, as long as we do not attempt a release including the feature before MOB compaction is handled natively. &lt;/p&gt;</comment>
                            <comment id="14120094" author="jmhsieh" created="Wed, 3 Sep 2014 17:07:26 +0000"  >&lt;p&gt;Re: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;To be fair, my comment itself addressed that by saying small blobs are stored by value in HBase, and only large bloba in HDFS. We can store a lot of 10MB (in the worst case scenario it&apos;s 200m x 10mb = 2pb) in HDFS, if that&apos;s not enough, we can dial up the threshold.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;It seems nobody understood what I am suggesting. Depending on use case and data distribution you pick a threshold X. Blobs with a size of &amp;lt; X are stored directly in HBase as a column value. Blobs &amp;gt;= X are stored in a HDFS with a reference in HBase using the 3-phase approach.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The MOB solution we&apos;re espousing does not preclude the hybrid hdfs+hbase approach - that could be still used with objects that are larger than or approach the hdfs block size.  Our claim is that the mob approach is complementary to a proper streaming api based hdfs+hbase mechanism for large object.  &lt;/p&gt;

&lt;p&gt;Operationally, the MOB design is similar &amp;#8211; Depending on use case and data distribution you pick a threshold X on each column family.  Blobs with a size of &amp;lt; X are stored directly in HBase as a column value.  Blobs &amp;gt;= X are stored in the MOB area with a reference in HBase using the on-flush/on-compaction approach. If the blob is larger than the ~10MB default &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, it is rejected. &lt;/p&gt;

&lt;p&gt;With the MOB design, if the threshold X performs poorly, then you can alter table the X value and the next major compaction will shift values between the MOB area and the normal hbase regions.  With the HDFS+HBase approach, would we need a new mechanism to shift data between hdfs and hbase? Is there a simple tuning/migration story?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;True, but as I state the &quot;store small blobs by value and only large ones by reference&quot; solution is not mentioned in there.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;Not it&apos;s not... It says either all blobs go into HBase or all blobs go into HDFS... See above. Small blobs would be stored directly in HBase, not in HDFS. That&apos;s key, nobody wants to store 100k or 1mb files directly in HDFS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m confused.  Section 4.1.2 part this split was assumed and the different mechanisms were for handling the &quot;large ones&quot;.  The discussions earlier in the jira explicitly added a threshold sizes to separate them when the value or reference implementations are used.&lt;/p&gt;

&lt;p&gt;For people that want to put a lot of 100k or 1mb objects in hbase there are many problems that arise, and this mob feature is an approach to make this valid (according to the defaults) workload work better and more predictably.  The mob design says store small blobs by value,  moderate blobs by reference (with data in to mob area), and maintains that hbase is not for large objects &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; . &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yet, all that is possible to do with a client only solution and could be abstracted there.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I&apos;ll also admit that our blob storage tool is not finished, yet, and that for its use case we don&apos;t need replication or backup as it itself will be the backup solution for another very large data store.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Are you guys absolutely... 100%... positive that this cannot be done in any other way and has to be done this way? That we cannot store files up to a certain size as values in HBase and larger files in HDFS? And there is not good threshold value for this?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think that saying &quot;this is the only way something could be done&quot; is right thing to ask.  There always many ways to get a functionality &amp;#8211; we&apos;ve presented a few other potential solutions, and have chosen and are justifying a design considering many of the tradeoffs.  It presented a need, a design, an early implementation, and evidence of a deployment and other potential use cases.&lt;/p&gt;

&lt;p&gt;The hybrid hdfs-hbase approach is one of the alternatives. I believe we agree that there will be some complexity introduced with that approach dealing with atomicity, bulk load, security, backup, replication and potentially tuning.  We have enough detail from the discussion to handle atomicity, there are open questions with the others.  It is hard to claim a feature is production-ready if we don&apos;t have a relatively simple mechanism for backups and disaster recovery.  In some future, when the hybrid hdfs+hbase system gets open sourced along with operationally internalized tools complexities, I think it would be a fine addition to hbase. &lt;/p&gt;

&lt;p&gt;Rough thresholds would be 0-100k hbase by value, 100k-10MB hbase by mob, 10MB+ hbase by ref to hdfs.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Today the default Cell size max is ~10MB. &lt;a href=&quot;https://github.com/apache/hbase/blob/master/hbase-common/src/main/resources/hbase-default.xml#L530&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hbase/blob/master/hbase-common/src/main/resources/hbase-default.xml#L530&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14120118" author="jmhsieh" created="Wed, 3 Sep 2014 17:27:05 +0000"  >&lt;p&gt;re: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Please. I don&apos;t think we should ever ship a release with a dependency on MR for core function. Committing this to trunk in stages could be ok, as long as we do not attempt a release including the feature before MOB compaction is handled natively.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I  agree &amp;#8211; moreover, ideally hbase should not need external processes except for hdfs/zk.  &lt;/p&gt;

&lt;p&gt;However, there is what should be and what has happened and what does happen.  In these cases we have ended up marking features experimental.  There are many examples of features in core hbase that shipped in &quot;stable&quot; releases and that still require external processes and may have no demonstrated users.  You&apos;d have to go back a bit to get one that explicitly depended on MR but they did exist.  (e.g. pre dist log splitting we had a MR based log replay &amp;#8211; useful in avoiding 10 hr recovery downtimes).  This would be a good discussion topic for an upcoming PMC meeting.&lt;/p&gt;

&lt;p&gt;What is your definition of stages? &amp;#8211; do you mean patch a time or something more like: stage one with external compactions, stage 2 with internal compactions?  For this MOB feature, we would have the experimental tag while we had external compactions and it would remain until we remove external dependencies and this compaction harden with fault testing.  Give our current cadence, we should be able have this completed as part of hbase 1.99/2.0 line&apos;s timeframe.&lt;/p&gt;</comment>
                            <comment id="14120174" author="apurtell" created="Wed, 3 Sep 2014 18:04:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;You&apos;d have to go back a bit to get one that explicitly depended on MR but they did exist. (e.g. pre dist log splitting we had a MR based log replay &#8211; useful in avoiding 10 hr recovery downtimes).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The master&apos;s built in splitting was still available even if there was no MR runtime that could run the replay tool.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What is your definition of stages? &amp;#8211; do you mean patch a time or something more like: stage one with external compactions, stage 2 with internal compactions? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Stage = JIRA issue.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For this MOB feature, we would have the experimental tag while we had external compactions and it would remain until we remove external dependencies and this compaction harden with fault testing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Whether or not the feature is tagged as experimental seems orthogonal to the compaction implementation question (at least to me).&lt;/p&gt;

&lt;p&gt;If I read the above correctly we are looking at 2.0 as a possible release for shipping this feature? I suggest we communicate the feature status as experimental for the whole release line, i.e. until 2.1, like what we have done with the cell security features in the 0.98 line. &lt;/p&gt;</comment>
                            <comment id="14120250" author="jmhsieh" created="Wed, 3 Sep 2014 18:50:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;The master&apos;s built in splitting was still available even if there was no MR runtime that could run the replay tool.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you were ok with 10 hr downtimes due to recovery (back then no meta first recovery), the sure.  For large deployments that MR for this was critical and not really optional.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Stage = JIRA issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;sgtm.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If I read the above correctly we are looking at 2.0 as a possible release for shipping this feature? I suggest we communicate the feature status as experimental for the whole release line, i.e. until 2.1, like what we have done with the cell security features in the 0.98 line.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes &amp;#8211; trunk is 2.0 and new features should only land in trunk and yes, we would note it as experimental until all pieces are in and some hardening as taken place. .  Ideally, all major features would be experimental in their first release. If we follow through with having 2.0 &amp;#45;&amp;gt; 2.1 be like will be like 0.92 &amp;#45;&amp;gt; 0.94 or 0.96 &amp;#45;&amp;gt;0.98, then following the cell security approach for experimental status sounds good to me.&lt;/p&gt;

&lt;p&gt;(edit fixed some formatting with accidental &lt;del&gt;strikethroughs&lt;/del&gt;)&lt;/p&gt;</comment>
                            <comment id="14120841" author="jiajia" created="Thu, 4 Sep 2014 02:06:18 +0000"  >&lt;p&gt;update the mob user guid(add the options in integration test)&lt;/p&gt;</comment>
                            <comment id="14120899" author="lhofhansl" created="Thu, 4 Sep 2014 03:21:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m confused. Section 4.1.2 part this split was assumed and the different mechanisms were for handling the &quot;large ones&quot;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Let&apos;s not ride that point. To me it was not clear that that was implied.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Rough thresholds would be 0-100k hbase by value, 100k-10MB hbase by mob, 10MB+ hbase by ref to hdfs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Still not happy to introduce all of this for this &quot;small&quot; band of size.&lt;/p&gt;

&lt;p&gt;In any case, thanks for indulging me. I realize it&apos;s frustrating. Let me change my vote to -0 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I do strongly prefer if we could build the entire thing out (including non-MR compactions, and tests, etc) in a feature branch, so it&apos;s complete before it&apos;s checked in. Any objections to that? Overkill?&lt;/p&gt;</comment>
                            <comment id="14120908" author="jmhsieh" created="Thu, 4 Sep 2014 03:37:59 +0000"  >&lt;p&gt;Thanks Lars.  Justifying features and implementations is a worthwhile exercise  especially since it leaves a record of alternatives considered.  &lt;/p&gt;

&lt;p&gt;Feature branch sounds good to me &amp;#8211; i&apos;ve been a general fan of these.  We&apos;ll call it the hbase-11339 branch.  Along the way we&apos;ll likely commit the mr managed code, but refactor/remove it with the new mechanism and have metrics and snapshots support before we call a merge vote.&lt;/p&gt;
</comment>
                            <comment id="14120918" author="anoop.hbase" created="Thu, 4 Sep 2014 03:53:23 +0000"  >&lt;p&gt;+1. Thanks LarsH&lt;/p&gt;</comment>
                            <comment id="14120923" author="jingcheng.du@intel.com" created="Thu, 4 Sep 2014 04:02:10 +0000"  >&lt;p&gt;Thanks Lars!&lt;/p&gt;</comment>
                            <comment id="14120943" author="apurtell" created="Thu, 4 Sep 2014 04:29:53 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;The master&apos;s built in splitting was still available even if there was no MR runtime that could run the replay tool.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If you were ok with 10 hr downtimes due to recovery (back then no meta first recovery), the sure.  For large deployments that MR for this was critical and not really optional.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It was possible if (perhaps deeply) suboptimal. We should expect the same with MOB compaction, perhaps the first cut it&apos;s better to use the MR tool, but we should not mandate the presence of the MR runtime for core HBase function.&lt;/p&gt;</comment>
                            <comment id="14120948" author="ram_krish" created="Thu, 4 Sep 2014 04:40:37 +0000"  >&lt;p&gt;Thanks Lars.  Making the compaction run without MR whould be the prime focus next so that this feature can be merged to the trunk.&lt;/p&gt;</comment>
                            <comment id="14121312" author="jmhsieh" created="Thu, 4 Sep 2014 12:53:30 +0000"  >&lt;p&gt;I have created a new version in the jira, hbase-11339, and a new branch with the same name off of master in the repo. We will commit changes under the &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11339&quot; title=&quot;HBase MOB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11339&quot;&gt;&lt;del&gt;HBASE-11339&lt;/del&gt;&lt;/a&gt; umbrella to this branch. The last commit before this branch has this hash bcfc6d65af.&lt;/p&gt;</comment>
                            <comment id="14142895" author="jiajia" created="Mon, 22 Sep 2014 05:00:03 +0000"  >&lt;p&gt;update some properties.&lt;/p&gt;</comment>
                            <comment id="14150962" author="jiajia" created="Sun, 28 Sep 2014 04:47:50 +0000"  >&lt;p&gt;change the method to enable mob feature.&lt;/p&gt;</comment>
                            <comment id="14154243" author="misty" created="Wed, 1 Oct 2014 02:22:18 +0000"  >&lt;p&gt;Please don&apos;t make changes to the DOCX anymore as I can&apos;t diff it and I have already integrated the changes (as of version 3) to the Ref Guide in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11986&quot; title=&quot;Document MOB in Ref Guide&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11986&quot;&gt;&lt;del&gt;HBASE-11986&lt;/del&gt;&lt;/a&gt;, and committed them to the hbase-11339 branch. I will try to figure out the changes in v4 and v5, but it will be easier for you to just list the changes here if possible in future.&lt;/p&gt;</comment>
                            <comment id="14156027" author="misty" created="Thu, 2 Oct 2014 03:07:57 +0000"  >&lt;p&gt;Made the changes from the docx files and committed to hbase-11339 branch.&lt;/p&gt;</comment>
                            <comment id="14162934" author="jingcheng.du@intel.com" created="Wed, 8 Oct 2014 01:55:49 +0000"  >&lt;p&gt;Thanks Misty &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=misty&quot; class=&quot;user-hover&quot; rel=&quot;misty&quot;&gt;Misty Stanley-Jones&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14217255" author="jiajia" created="Wed, 19 Nov 2014 01:56:11 +0000"  >&lt;p&gt;thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoopsamjohn&quot; class=&quot;user-hover&quot; rel=&quot;anoopsamjohn&quot;&gt;Anoop Sam John&lt;/a&gt;&lt;br/&gt;
Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;, is this patch ok?&lt;/p&gt;</comment>
                            <comment id="14319169" author="jmhsieh" created="Thu, 12 Feb 2015 22:56:47 +0000"  >&lt;p&gt;Attached a patch to attempt a run in hadoopqa that merges to trunk.&lt;/p&gt;</comment>
                            <comment id="14319328" author="hadoopqa" created="Fri, 13 Feb 2015 00:46:33 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12698561/merge-150212.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12698561/merge-150212.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 7561ae6d1257b51c0bb1ef46e52d8ede2c7c926f.&lt;br/&gt;
  ATTACHMENT ID: 12698561&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 80 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12811//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12811//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14319434" author="jmhsieh" created="Fri, 13 Feb 2015 02:01:04 +0000"  >&lt;p&gt;trying again, this time with --no-prefix&lt;/p&gt;</comment>
                            <comment id="14319442" author="jmhsieh" created="Fri, 13 Feb 2015 02:05:26 +0000"  >&lt;p&gt;version c removes the conflict.&lt;/p&gt;</comment>
                            <comment id="14319552" author="hadoopqa" created="Fri, 13 Feb 2015 04:23:28 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12698600/merge.150212c.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12698600/merge.150212c.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 7561ae6d1257b51c0bb1ef46e52d8ede2c7c926f.&lt;br/&gt;
  ATTACHMENT ID: 12698600&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 77 new or modified tests.&lt;br/&gt;
    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.1 2.5.2 2.6.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javadoc&lt;/font&gt;.  The javadoc tool appears to have generated 4 warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1961 checkstyle errors (more than the master&apos;s current 1937 errors).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +          .addCounter(Interns.info(MOB_COMPACTED_FROM_MOB_CELLS_COUNT, MOB_COMPACTED_FROM_MOB_CELLS_COUNT_DESC),&lt;br/&gt;
+          .addCounter(Interns.info(MOB_COMPACTED_INTO_MOB_CELLS_COUNT, MOB_COMPACTED_INTO_MOB_CELLS_COUNT_DESC),&lt;br/&gt;
+          .addCounter(Interns.info(MOB_COMPACTED_FROM_MOB_CELLS_SIZE, MOB_COMPACTED_FROM_MOB_CELLS_SIZE_DESC),&lt;br/&gt;
+          .addCounter(Interns.info(MOB_COMPACTED_INTO_MOB_CELLS_SIZE, MOB_COMPACTED_INTO_MOB_CELLS_SIZE_DESC),&lt;br/&gt;
+  public StoreFile.Writer createWriterInTmp(MobFileName mobFileName, Path basePath, long maxKeyCount,&lt;br/&gt;
+            performCompaction(fd, scanner, writer, smallestReadPoint, cleanSeqId, throughputController,&lt;br/&gt;
+        stats.getStoreFilesCount(), stats.getArchivedStoreFilesCount(), stats.getMobStoreFilesCount(),&lt;br/&gt;
+    TEST_UTIL.getConfiguration().setInt(&quot;hbase.hstore.compaction.min&quot;, 15); // avoid major compactions&lt;br/&gt;
+    TEST_UTIL.getConfiguration().setInt(&quot;hbase.hstore.compaction.max&quot;, 30); // avoid major compactions&lt;br/&gt;
+      assertTrue(refPath.getName() + &quot; should be a HFileLink&quot;, HFileLink.isHFileLink(refPath.getName()));&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles&lt;br/&gt;
                  org.apache.hadoop.hbase.util.TestProcessBasedCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestImportExport&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): 	at org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification.testSingleAppKillInvalidState(TestRMWebServicesAppsModification.java:441)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-rest.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-annotations.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Javadoc warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/patchJavadocWarnings.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//artifact/patchprocess/patchJavadocWarnings.txt&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/12814//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14320380" author="jmhsieh" created="Fri, 13 Feb 2015 16:51:09 +0000"  >&lt;p&gt;The last patch was the delta from the merged hbase-11339/master branch.   All Test*Mob* tests pass. Of the list that failed, org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad is legit.  The others pass locally for me and are likely flakey&lt;/p&gt;

&lt;p&gt;On the failing test, I spent an hour or two and didn&apos;t find anything obvious.  I&apos;ll give it another chunk of time today, and I can&apos;t find it, I&apos;d like to merge/commit it to the hbase-11339 branch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jingchengdu&quot; class=&quot;user-hover&quot; rel=&quot;jingchengdu&quot;&gt;JingchengDu&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt;, you guys want to take a look?  Here&apos;s a link on my personal github.  It is a little bit rough if you use the web interface &amp;#8211; you could check it out to see the first merge, and then the breakdown of fixes after I got it to compile. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jmhsieh/hbase/commits/hbase-11339-trunk&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jmhsieh/hbase/commits/hbase-11339-trunk&lt;/a&gt;&lt;/p&gt;

</comment>
                            <comment id="14321838" author="jingcheng.du@intel.com" created="Sun, 15 Feb 2015 06:51:26 +0000"  >&lt;p&gt;Thanks a lot &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;, great work! I went through the merges and I am +1 with it.&lt;/p&gt;</comment>
                            <comment id="14324553" author="jmhsieh" created="Tue, 17 Feb 2015 17:52:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jingchengdu&quot; class=&quot;user-hover&quot; rel=&quot;jingchengdu&quot;&gt;JingchengDu&lt;/a&gt;, thanks for taking a look.&lt;/p&gt;

&lt;p&gt;I&apos;ve isolated the lines of code that cause the test failures &amp;#8211; fixing it one way breaks TestHRegionServerBulkLoad and the other breaks a bulk loading mob test.  I&apos;m digging in to figure this out before i push the merge.&lt;/p&gt;

&lt;p&gt;Here&apos;s the code in DefaultCompactor (args are backwards in trunk!)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
writer = store.createWriterInTmp(fd.maxKeyCount, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.compactionCompression, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;,
-            &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, fd.maxTagsLength &amp;gt; 0);
+            fd.maxTagsLength &amp;gt; 0, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
         &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; finished =

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14332366" author="jmhsieh" created="Sun, 22 Feb 2015 20:59:58 +0000"  >&lt;p&gt;I found the problem, go a clean test suite run, and merged master from 2/11/15 into hbase-11339.  There problem came from a place where inheritance was used and where composition may have made it easier to track.  (e.g. there was a createTmpWriter method added to DefaultCompactor and DefaultMobCompactor and it was not obvious that the usage of the derived method was required via inspection)&lt;/p&gt;</comment>
                            <comment id="14348357" author="wilm" created="Thu, 5 Mar 2015 08:10:46 +0000"  >&lt;p&gt;love that feature! But I have two newbie feature requests:&lt;/p&gt;

&lt;p&gt;1.) When I run my hbase with the mob feature everything works fine. Except that I get &lt;br/&gt;
java.lang.illegalargumentexception key value size too large&lt;br/&gt;
when the data i load up gets to large. One solution would be to set the limit to 0, but I think that the limitation is kind of a usefull feature. Perhaps it would be a nice feature to ignore the limit for the families which have the is_mob =&amp;gt; &apos;true&apos;.&lt;/p&gt;

&lt;p&gt;2.) in the documentation there is &quot;MOB&quot; and &quot;LOB&quot; defined. However, in the corpus of the text only MOBs are discussed. By the design explaination I cannot see why LOBs would be more problematic (except the client upload) to save than MOBs. Are there any reasons to avoid &quot;LOBs&quot; (up to several hundreds of MBs) from the database site of view?&lt;/p&gt;</comment>
                            <comment id="14348362" author="jmhsieh" created="Thu, 5 Mar 2015 08:18:09 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Wilm&quot; class=&quot;user-hover&quot; rel=&quot;Wilm&quot;&gt;Wilm Schumacher&lt;/a&gt;.  This feature is called MOB &amp;#8211; focused on cells that are 100k-10MB in size (possibly slightly larger than than).  Currently large objects (we&apos;ll define those to be &amp;gt;10MB) are problematic because we lack a streaming api to handle  breaking rpc requests up to efficiently ship data from the server side to the client side.  While they are hypothetically possible with the current api, the will cause large memory allocations that will stress the memory systems of both the servers and the clients.&lt;/p&gt;

&lt;p&gt;We may try to address cases with larger blobs in the future, but for now we&apos;re limiting our scope.&lt;/p&gt;</comment>
                            <comment id="14348381" author="Wilm" created="Thu, 5 Mar 2015 08:26:55 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;thx for the fast answer. At the moment I catch the &quot;memory allocation problem&quot; (what I meant by &quot;client upload&quot;) in my application directly. &quot;Only file upload up to 20 MB&quot; etc. By now I limit it to 20-30 MB, so a little larger than your rule of thumb.&lt;/p&gt;

&lt;p&gt;However, my take away is that there is no intrinsic problem with LOBs, except the client upload/download problem. Thx for the anwwer.&lt;/p&gt;</comment>
                            <comment id="14352335" author="misty" created="Sun, 8 Mar 2015 23:17:59 +0000"  >&lt;p&gt;Just pushed an addendum to put the M/R sweeper docs back for now, as per &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14356138" author="jingcheng.du@intel.com" created="Wed, 11 Mar 2015 02:56:33 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Wilm&quot; class=&quot;user-hover&quot; rel=&quot;Wilm&quot;&gt;Wilm Schumacher&lt;/a&gt;, if you want to enlarge the KeyValue size in your put, you could try to change the conf(&quot;hbase.client.keyvalue.maxsize&quot;) used by the HTable, which allows you have a larger KeyValue.&lt;br/&gt;
As Jon mentioned, LOBs in the current API will cause large memory allocations that will stress the memory systems of both servers and the clients. You need to pay attentions on this.&lt;/p&gt;</comment>
                            <comment id="14364750" author="hudson" created="Tue, 17 Mar 2015 08:13:27 +0000"  >&lt;p&gt;ABORTED: Integrated in HBase-TRUNK #6269 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/6269/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/6269/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13233&quot; title=&quot;add hbase-11339 branch to the patch testing script&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13233&quot;&gt;&lt;del&gt;HBASE-13233&lt;/del&gt;&lt;/a&gt; add hbase-11339 branch to the patch testing script (jmhsieh: rev e192f5ed39911d180287730315db51f18f0e5018)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;dev-support/test-patch.properties&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14487320" author="jmhsieh" created="Thu, 9 Apr 2015 13:01:31 +0000"  >&lt;p&gt;I am in the process of  merging with master in order to call a merge in the next week or so.   Currently I&apos;m working through are some unit test problems.&lt;/p&gt;</comment>
                            <comment id="14488954" author="jingcheng.du@intel.com" created="Fri, 10 Apr 2015 05:35:30 +0000"  >&lt;p&gt;Upload the latest design document.&lt;/p&gt;</comment>
                            <comment id="14505591" author="jmhsieh" created="Tue, 21 Apr 2015 19:40:39 +0000"  >&lt;p&gt;attached hbase-11339.150417.patch.  Have been running for a few days and outside of likely unrelated flakey tests, I&apos;ve been encountering a new  occasional failures of TestAcidGurantees.testMobScanAtomicity 1 out or 10 times.  &lt;/p&gt;

&lt;p&gt;Would like to merge master in to hbase-11339, hunt down the atomicity violation before calling merge to master.&lt;/p&gt;

&lt;p&gt;For reviewing the merge, it will  e easier to look at this merge into hbase-11339 &amp;#8211; the majority of of changes are in the last set of patches found here. &lt;a href=&quot;https://github.com/jmhsieh/hbase/commits/hbase-11339&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jmhsieh/hbase/commits/hbase-11339&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14506735" author="jingcheng.du@intel.com" created="Wed, 22 Apr 2015 09:49:16 +0000"  >&lt;p&gt;Hi Jon &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;.&lt;br/&gt;
Here followings are my findings when I ran TestAcidGurantees.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Each single case can pass if running them separately.&lt;/li&gt;
	&lt;li&gt;When I tried to run all the cases in TestAcidGurantees, the exception was thrown in the last case by the running order which was caused by &quot;IOException: Too many open files&quot;.&lt;/li&gt;
	&lt;li&gt;I commented one method (the number of running cases was decreased by 1. Totally 6 cases, I just ran 5 in this step), all the methods could pass.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So I guess, it is not a logic issue. I think it was because the file handlers were not closed properly in each case.&lt;/p&gt;</comment>
                            <comment id="14506736" author="jingcheng.du@intel.com" created="Wed, 22 Apr 2015 09:49:25 +0000"  >&lt;p&gt;Hi Jon &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;.&lt;br/&gt;
Here followings are my findings when I ran TestAcidGurantees.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Each single case can pass if running them separately.&lt;/li&gt;
	&lt;li&gt;When I tried to run all the cases in TestAcidGurantees, the exception was thrown in the last case by the running order which was caused by &quot;IOException: Too many open files&quot;.&lt;/li&gt;
	&lt;li&gt;I commented one method (the number of running cases was decreased by 1. Totally 6 cases, I just ran 5 in this step), all the methods could pass.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So I guess, it is not a logic issue. I think it was because the file handlers were not closed properly in each case.&lt;/p&gt;</comment>
                            <comment id="14508744" author="jingcheng.du@intel.com" created="Thu, 23 Apr 2015 09:37:38 +0000"  >&lt;p&gt;Thanks Jon for the patch  hbase-11339.150417.patch, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;&lt;br/&gt;
It seems this patch doesn&apos;t include the commit by Anoop on Apr 10. This commit id is eba8a708a578e47a3fad1b1c0dbae4937c536bb9.&lt;br/&gt;
Other part of the patch looks good to me.&lt;br/&gt;
I will be +1 after that patch is applied. Thanks a lot!&lt;/p&gt;</comment>
                            <comment id="14513229" author="ndimiduk" created="Sun, 26 Apr 2015 20:07:51 +0000"  >&lt;p&gt;Is there a branch/tag/sha that looks roughly like what you&apos;d want to commit to branch-1.1? I&apos;d like to add it to my jenkins rotations. Thanks.&lt;/p&gt;</comment>
                            <comment id="14520759" author="jmhsieh" created="Thu, 30 Apr 2015 03:04:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt;, at the moment, I haven&apos;t tried to backport to the 1.1 part of branch-1 yet.  I do have a the complete mob codeline version ported to hbase 1.0.0 branch here&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.  Not ideal I realize, but I want to make sure I get this into trunk before backporting to the apache 1.x lines.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://github.com/cloudera/hbase/commits/cdh5-1.0.0_5.4.0?page=1&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/cloudera/hbase/commits/cdh5-1.0.0_5.4.0?page=1&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14520771" author="jmhsieh" created="Thu, 30 Apr 2015 03:10:27 +0000"  >&lt;p&gt;Also, this upstream branch hash is much closer to the 1.1 line &amp;#8211; &lt;/p&gt;

&lt;p&gt;fe389d1f194c47742fba91e5e3424bb2c0eb0fce&lt;/p&gt;

&lt;p&gt;I planned on backporting this, and then adding some new patches and test fixes from the other line .&lt;/p&gt;</comment>
                            <comment id="14523329" author="jmhsieh" created="Fri, 1 May 2015 15:30:28 +0000"  >&lt;p&gt;I squashed and pushed the 4/15/15 merge.  Will try to do another one today.  Still trying to hunt down the acid problem in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13531&quot; title=&quot;Flakey failures of TestAcidGuarantees#testMobScanAtomicity&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13531&quot;&gt;&lt;del&gt;HBASE-13531&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="14551466" author="jmhsieh" created="Tue, 19 May 2015 23:36:13 +0000"  >&lt;p&gt;I&quot;ve attached a merge with trunk from today 19 May 2015.  I believe we have addressed the acid violation problem found in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13531&quot; title=&quot;Flakey failures of TestAcidGuarantees#testMobScanAtomicity&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13531&quot;&gt;&lt;del&gt;HBASE-13531&lt;/del&gt;&lt;/a&gt; acid violation  problem.  &lt;/p&gt;

&lt;p&gt;Would like a quick review to merge the 19/May 2015 master into hbase-11339 branch, and we will probably call for a vote to merge to master.&lt;/p&gt;
</comment>
                            <comment id="14551473" author="jmhsieh" created="Tue, 19 May 2015 23:38:29 +0000"  >&lt;p&gt;the merge into hbase-11339 branch  would have the following commit message:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Merge remote-tracking branch &apos;apache/master&apos; (5/19/15) into hbase-11339

Patches that caused deltas:
HBASE-10810 - around HColumnDescriptor &apos;should&apos; vs &apos;is&apos; api.
HBASE-11677 - LOG was made &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;
HBASE-11927 - Checksum constant changed
HBASE-10800 - CellComparator instead of KVComparator

Conflicts:
	hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
	hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreEngine.java
	hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor.java
	hbase-server/src/test/java/org/apache/hadoop/hbase/util/LoadTestTool.java
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14551868" author="ram_krish" created="Wed, 20 May 2015 05:57:48 +0000"  >&lt;p&gt;+1 on merge vote.&lt;/p&gt;</comment>
                            <comment id="14606019" author="anilgupta84" created="Mon, 29 Jun 2015 18:05:43 +0000"  >&lt;p&gt;Is there any ETA on this feature? Is it possible to get this in 1.1.x? All the tickets related to this jira are done.&lt;/p&gt;</comment>
                            <comment id="14622035" author="jingcheng.du@intel.com" created="Fri, 10 Jul 2015 09:26:27 +0000"  >&lt;p&gt;Upload the patch for merging hbase-11339 to trunk.&lt;br/&gt;
The patches for integrity check of MOB are also included.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13806&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-13806&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13932&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-13932&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The code is uploaded to RB too, you could read it by the link &lt;a href=&quot;https://reviews.apache.org/r/36391/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/36391/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks a lot!&lt;/p&gt;</comment>
                            <comment id="14622056" author="jingcheng.du@intel.com" created="Fri, 10 Jul 2015 10:02:41 +0000"  >&lt;p&gt;Supplement more information, the latest patch name is merge.150710.patch.&lt;/p&gt;</comment>
                            <comment id="14622313" author="hadoopqa" created="Fri, 10 Jul 2015 13:48:44 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12744683/merge.150710.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12744683/merge.150710.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit bff911a8e894f59f6efe6a24f39a7aef5d689882.&lt;br/&gt;
  ATTACHMENT ID: 12744683&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javac&lt;/font&gt;.  The patch appears to cause mvn compile goal to fail with Hadoop version 2.4.0.&lt;/p&gt;

&lt;p&gt;    Compilation errors resume:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; COMPILATION ERROR : &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMobSnapshotCloneIndependence.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;236,12&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;470,8&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:testCompile (default-testCompile) on project hbase-server: Compilation failure: Compilation failure:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMobSnapshotCloneIndependence.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;236,12&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   method getRegionLocations()&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: variable t of type org.apache.hadoop.hbase.client.HTable&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestHBaseFsck.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;470,8&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   method flushCommits()&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: variable tbl of type org.apache.hadoop.hbase.client.Table&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; -&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; To see the full stack trace of the errors, re-run Maven with the -e switch.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Re-run Maven using the -X switch to enable full debug logging.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; For more information about the errors and possible solutions, please read the following articles:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; After correcting the problems, you can resume the build with the command&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;   mvn &amp;lt;goals&amp;gt; -rf :hbase-server&lt;/p&gt;


&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14735//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14735//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14622478" author="yuzhihong@gmail.com" created="Fri, 10 Jul 2015 15:37:31 +0000"  >&lt;p&gt;For Compactor.java :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-comment&quot;&gt;// TODO mob introduced the fd parameter; can we make &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; cleaner and easier to extend in &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt;?&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Edit: DefaultMobStoreCompactor uses fd parameter.&lt;br/&gt;
We can leave the API change as is.&lt;/p&gt;</comment>
                            <comment id="14622584" author="yuzhihong@gmail.com" created="Fri, 10 Jul 2015 16:56:31 +0000"  >&lt;p&gt;Patch based on Jingcheng&apos;s mega patch with compilation errors fixed.&lt;br/&gt;
Previous patches didn&apos;t carry rev number so I picked an arbitrary one - v3&lt;/p&gt;</comment>
                            <comment id="14622800" author="hadoopqa" created="Fri, 10 Jul 2015 19:33:21 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12744746/11339-master-v3.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12744746/11339-master-v3.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit bff911a8e894f59f6efe6a24f39a7aef5d689882.&lt;br/&gt;
  ATTACHMENT ID: 12744746&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1921 checkstyle errors (more than the master&apos;s current 1896 errors).&lt;/p&gt;

&lt;p&gt;		    &lt;font color=&quot;red&quot;&gt;-1 InterfaceAudience&lt;/font&gt;.  The patch appears to contain InterfaceAudience from hadoop rather than hbase:&lt;br/&gt;
             +import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceStability;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;&lt;br/&gt;
+import org.apache.hadoop.classification.InterfaceAudience;.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +  public static void mergeDelimitedFrom(Message.Builder builder, InputStream in) throws IOException {&lt;br/&gt;
+          .addCounter(Interns.info(CELLS_COUNT_COMPACTED_FROM_MOB, CELLS_COUNT_COMPACTED_FROM_MOB_DESC),&lt;br/&gt;
+          .addCounter(Interns.info(CELLS_SIZE_COMPACTED_FROM_MOB, CELLS_SIZE_COMPACTED_FROM_MOB_DESC),&lt;br/&gt;
+            performCompaction(fd, scanner, writer, smallestReadPoint, cleanSeqId, throughputController,&lt;br/&gt;
+  protected StoreFile.Writer createTmpWriter(FileDetails fd, long smallestReadPoint) throws IOException {&lt;br/&gt;
+        stats.getStoreFilesCount(), stats.getArchivedStoreFilesCount(), stats.getMobStoreFilesCount(),&lt;br/&gt;
+      family.setMobEnabled(JBoolean.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB)&lt;br/&gt;
+      family.setMobThreshold(JLong.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD)&lt;br/&gt;
+        @admin.compactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;br/&gt;
+        @admin.majorCompactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.master.TestRollingRestart&lt;br/&gt;
                  org.apache.hadoop.hbase.util.TestMiniClusterLoadSequential&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestDeleteMobTable&lt;br/&gt;
                  org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS&lt;br/&gt;
                  org.apache.hadoop.hbase.namespace.TestNamespaceAuditor&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterFailover&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestDistributedLogSplitting&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 10 zombie test(s): 	at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.testMasterRestartAtRegionSplitPendingCatalogJanitor(TestSplitTransactionOnCluster.java:592)&lt;br/&gt;
	at org.apache.phoenix.end2end.index.BaseMutableIndexIT.testCoveredColumns(BaseMutableIndexIT.java:476)&lt;br/&gt;
	at org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas.testCreateTableWithMultipleReplicas(TestMasterOperationsForRegionReplicas.java:159)&lt;br/&gt;
	at org.apache.hadoop.hbase.client.TestFromClientSide.testUnmanagedHConnectionReconnect(TestFromClientSide.java:4079)&lt;br/&gt;
	at org.apache.hadoop.hbase.client.TestFromClientSide.testUnmanagedHConnectionReconnect(TestFromClientSide.java:4079)&lt;br/&gt;
	at org.apache.hadoop.hbase.client.TestMetaWithReplicas.testShutdownHandling(TestMetaWithReplicas.java:141)&lt;br/&gt;
	at org.apache.phoenix.end2end.index.IndexExpressionIT.testMutableLocalIndexUpdate(IndexExpressionIT.java:212)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover.testLotsOfRegionReplicas(TestRegionReplicaFailover.java:372)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14740//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14622845" author="yuzhihong@gmail.com" created="Fri, 10 Jul 2015 20:16:41 +0000"  >&lt;p&gt;Fixes test failure in TestDeleteMobTable&lt;/p&gt;

&lt;p&gt;Fixes import of org.apache.hadoop.hbase.classification.InterfaceAudience&lt;/p&gt;</comment>
                            <comment id="14623107" author="hadoopqa" created="Sat, 11 Jul 2015 00:12:54 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12744838/11339-master-v4.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12744838/11339-master-v4.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit c16bbf47cbb1017b92960e15edfaa81cfd104b1d.&lt;br/&gt;
  ATTACHMENT ID: 12744838&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1921 checkstyle errors (more than the master&apos;s current 1896 errors).&lt;/p&gt;

&lt;p&gt;		    &lt;font color=&quot;red&quot;&gt;-1 InterfaceAudience&lt;/font&gt;.  The patch appears to contain InterfaceAudience from hadoop rather than hbase:&lt;br/&gt;
             +import org.apache.hadoop.classification.InterfaceStability;.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +            new MoveRandomRegionOfTableAction(MonkeyConstants.DEFAULT_RESTART_ACTIVE_MASTER_SLEEP_TIME,&lt;br/&gt;
+            new TwoConcurrentActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION1_PERIOD, actions1, actions2),&lt;br/&gt;
+            new PeriodicRandomActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION2_PERIOD,actions3),&lt;br/&gt;
+            new PeriodicRandomActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION4_PERIOD,actions4));&lt;br/&gt;
+    byte[] readEmptyValueOnMobCellMiss = scan.getAttribute(MobConstants.EMPTY_VALUE_ON_MOBCELL_MISS);&lt;br/&gt;
+  public StoreFile.Writer createWriterInTmp(MobFileName mobFileName, Path basePath, long maxKeyCount,&lt;br/&gt;
+    Path mobFamilyDir = new Path(tableDir, new Path(mobRegionInfo.getEncodedName(), Bytes.toString(FAMILY)));&lt;br/&gt;
+      conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName()), cfKey));&lt;br/&gt;
+    // Put some data 5 10, 15, 20  mb ok  (this would be right below protobuf default max size of 64MB.&lt;br/&gt;
+      assertTrue(refPath.getName() + &quot; should be a HFileLink&quot;, HFileLink.isHFileLink(refPath.getName()));&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 site&lt;/font&gt;.  The patch appears to cause mvn post-site goal to fail.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14746//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14623185" author="hadoopqa" created="Sat, 11 Jul 2015 02:01:53 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12744838/11339-master-v4.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12744838/11339-master-v4.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 5e708746b8d301c2fb22a85b8756129147012374.&lt;br/&gt;
  ATTACHMENT ID: 12744838&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1921 checkstyle errors (more than the master&apos;s current 1896 errors).&lt;/p&gt;

&lt;p&gt;		    &lt;font color=&quot;red&quot;&gt;-1 InterfaceAudience&lt;/font&gt;.  The patch appears to contain InterfaceAudience from hadoop rather than hbase:&lt;br/&gt;
             +import org.apache.hadoop.classification.InterfaceStability;.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +            new MoveRandomRegionOfTableAction(MonkeyConstants.DEFAULT_RESTART_ACTIVE_MASTER_SLEEP_TIME,&lt;br/&gt;
+            new TwoConcurrentActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION1_PERIOD, actions1, actions2),&lt;br/&gt;
+            new PeriodicRandomActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION2_PERIOD,actions3),&lt;br/&gt;
+            new PeriodicRandomActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION4_PERIOD,actions4));&lt;br/&gt;
+    byte[] readEmptyValueOnMobCellMiss = scan.getAttribute(MobConstants.EMPTY_VALUE_ON_MOBCELL_MISS);&lt;br/&gt;
+  public StoreFile.Writer createWriterInTmp(MobFileName mobFileName, Path basePath, long maxKeyCount,&lt;br/&gt;
+    Path mobFamilyDir = new Path(tableDir, new Path(mobRegionInfo.getEncodedName(), Bytes.toString(FAMILY)));&lt;br/&gt;
+      conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName()), cfKey));&lt;br/&gt;
+    // Put some data 5 10, 15, 20  mb ok  (this would be right below protobuf default max size of 64MB.&lt;br/&gt;
+      assertTrue(refPath.getName() + &quot; should be a HFileLink&quot;, HFileLink.isHFileLink(refPath.getName()));&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.master.TestRollingRestart&lt;br/&gt;
                  org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures&lt;br/&gt;
                  org.apache.hadoop.hbase.util.TestProcessBasedCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestImportExport&lt;br/&gt;
                  org.apache.hadoop.hbase.namespace.TestNamespaceAuditor&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterFailover&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestDistributedLogSplitting&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 2 zombie test(s): 	at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles(TestSplitTransactionOnCluster.java:483)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.testMasterRestartAtRegionSplitPendingCatalogJanitor(TestSplitTransactionOnCluster.java:592)&lt;br/&gt;
	at org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover.testSecondaryRegionWithNonEmptyRegion(TestRegionReplicaFailover.java:159)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14747//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14625215" author="hadoopqa" created="Mon, 13 Jul 2015 19:45:45 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12745100/11339-master-v5.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12745100/11339-master-v5.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit a3d30892b41f604ab5a62d4f612fa7c230267dfe.&lt;br/&gt;
  ATTACHMENT ID: 12745100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javac&lt;/font&gt;.  The patch appears to cause mvn compile goal to fail with Hadoop version 2.4.0.&lt;/p&gt;

&lt;p&gt;    Compilation errors resume:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; COMPILATION ERROR : &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreScanner.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;198,54&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreScanner.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;203,54&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:testCompile (default-testCompile) on project hbase-server: Compilation failure: Compilation failure:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreScanner.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;198,54&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   method getValue()&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: variable cell of type org.apache.hadoop.hbase.Cell&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMobStoreScanner.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;203,54&amp;#93;&lt;/span&gt; cannot find symbol&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; symbol:   method getValue()&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; location: variable cell of type org.apache.hadoop.hbase.Cell&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; -&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; To see the full stack trace of the errors, re-run Maven with the -e switch.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Re-run Maven using the -X switch to enable full debug logging.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; For more information about the errors and possible solutions, please read the following articles:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; After correcting the problems, you can resume the build with the command&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;   mvn &amp;lt;goals&amp;gt; -rf :hbase-server&lt;/p&gt;


&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14758//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14758//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14625230" author="yuzhihong@gmail.com" created="Mon, 13 Jul 2015 19:54:03 +0000"  >&lt;p&gt;Patch v6 compiles against latest master branch.&lt;/p&gt;</comment>
                            <comment id="14625499" author="yuzhihong@gmail.com" created="Mon, 13 Jul 2015 22:29:30 +0000"  >&lt;p&gt;Wrapped long lines in patch v7.&lt;/p&gt;</comment>
                            <comment id="14625541" author="hadoopqa" created="Mon, 13 Jul 2015 23:05:16 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12745103/11339-master-v6.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12745103/11339-master-v6.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit a3d30892b41f604ab5a62d4f612fa7c230267dfe.&lt;br/&gt;
  ATTACHMENT ID: 12745103&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1898 checkstyle errors (more than the master&apos;s current 1873 errors).&lt;/p&gt;

&lt;p&gt;		    &lt;font color=&quot;red&quot;&gt;-1 InterfaceAudience&lt;/font&gt;.  The patch appears to contain InterfaceAudience from hadoop rather than hbase:&lt;br/&gt;
             +import org.apache.hadoop.classification.InterfaceStability;.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +            new MoveRandomRegionOfTableAction(MonkeyConstants.DEFAULT_RESTART_ACTIVE_MASTER_SLEEP_TIME,&lt;br/&gt;
+            new TwoConcurrentActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION1_PERIOD, actions1, actions2),&lt;br/&gt;
+            new PeriodicRandomActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION2_PERIOD,actions3),&lt;br/&gt;
+            new PeriodicRandomActionPolicy(MonkeyConstants.DEFAULT_PERIODIC_ACTION4_PERIOD,actions4));&lt;br/&gt;
+    byte[] readEmptyValueOnMobCellMiss = scan.getAttribute(MobConstants.EMPTY_VALUE_ON_MOBCELL_MISS);&lt;br/&gt;
+  public StoreFile.Writer createWriterInTmp(MobFileName mobFileName, Path basePath, long maxKeyCount,&lt;br/&gt;
+    Path mobFamilyDir = new Path(tableDir, new Path(mobRegionInfo.getEncodedName(), Bytes.toString(FAMILY)));&lt;br/&gt;
+      conf.get(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, User.getCurrent().getShortName()), cfKey));&lt;br/&gt;
+    // Put some data 5 10, 15, 20  mb ok  (this would be right below protobuf default max size of 64MB.&lt;br/&gt;
+      assertTrue(refPath.getName() + &quot; should be a HFileLink&quot;, HFileLink.isHFileLink(refPath.getName()));&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.rest.client.TestRemoteTable&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14760//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14625655" author="hadoopqa" created="Tue, 14 Jul 2015 01:20:03 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12745138/11339-master-v7.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12745138/11339-master-v7.txt&lt;/a&gt;&lt;br/&gt;
  against master branch at commit a3d30892b41f604ab5a62d4f612fa7c230267dfe.&lt;br/&gt;
  ATTACHMENT ID: 12745138&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1896 checkstyle errors (more than the master&apos;s current 1873 errors).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +    // Put some data 5 10, 15, 20  mb ok  (this would be right below protobuf default max size of 64MB.&lt;br/&gt;
+The utility `org.apache.hadoop.hbase.IntegrationTestIngestMOB` is provided to assist with testing the MOB feature. The utility is run as follows:&lt;br/&gt;
+* `&lt;b&gt;threshold&lt;/b&gt;` is the threshold at which cells are considered to be MOBs. The default is 1 kB, expressed in bytes.&lt;br/&gt;
+* `&lt;b&gt;minMobDataSize&lt;/b&gt;` is the minimum value for the size of MOB data. The default is 512 B, expressed in bytes.&lt;br/&gt;
+* `&lt;b&gt;maxMobDataSize&lt;/b&gt;` is the maximum value for the size of MOB data. The default is 5 kB, expressed in bytes.&lt;br/&gt;
+Because there can be a large number of MOB files at any time, as compared to the number of HFiles, MOB files are not always kept open. The MOB file reader cache is a LRU cache which keeps the most recently used MOB files open. To configure the MOB file reader&apos;s cache on each RegionServer, add the following properties to the RegionServer&apos;s `hbase-site.xml`, customize the configuration to suit your environment, and restart or rolling restart the RegionServer.&lt;br/&gt;
+Next, add the HBase install directory, &lt;em&gt;`$HBASE_HOME`/*&lt;/em&gt;, and HBase library directory to &lt;em&gt;yarn-site.xml&lt;/em&gt; Adjust this example to suit your environment.&lt;br/&gt;
+  public static void mergeDelimitedFrom(Message.Builder builder, InputStream in) throws IOException {&lt;br/&gt;
+          .addCounter(Interns.info(CELLS_COUNT_COMPACTED_FROM_MOB, CELLS_COUNT_COMPACTED_FROM_MOB_DESC),&lt;br/&gt;
+          .addCounter(Interns.info(CELLS_SIZE_COMPACTED_FROM_MOB, CELLS_SIZE_COMPACTED_FROM_MOB_DESC),&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.util.TestProcessBasedCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestImportExport&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14761//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14626030" author="jingcheng.du@intel.com" created="Tue, 14 Jul 2015 08:27:33 +0000"  >&lt;p&gt;The patch V8 is uploaded.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Refine the class import.&lt;/li&gt;
	&lt;li&gt;Shorten the long lines.&lt;/li&gt;
	&lt;li&gt;Some minor code changes.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This patch is uploaded to RB too, you can review it through the link &lt;a href=&quot;https://reviews.apache.org/r/36391/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/36391/&lt;/a&gt;.&lt;br/&gt;
Thanks.&lt;/p&gt;</comment>
                            <comment id="14626037" author="jingcheng.du@intel.com" created="Tue, 14 Jul 2015 08:40:34 +0000"  >&lt;p&gt;Upload a new patch V9 to include a few minor changes in code style.&lt;br/&gt;
This patch is uploaded to RB. You can find it in the link &lt;a href=&quot;https://reviews.apache.org/r/36391/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/36391/&lt;/a&gt;.&lt;br/&gt;
Thanks.&lt;/p&gt;</comment>
                            <comment id="14626130" author="hadoopqa" created="Tue, 14 Jul 2015 09:49:57 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12745215/11339-master-v9.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12745215/11339-master-v9.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit a3d30892b41f604ab5a62d4f612fa7c230267dfe.&lt;br/&gt;
  ATTACHMENT ID: 12745215&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1889 checkstyle errors (more than the master&apos;s current 1873 errors).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +      family.setMobEnabled(JBoolean.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB)&lt;br/&gt;
+      family.setMobThreshold(JLong.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD)&lt;br/&gt;
+        @admin.compactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;br/&gt;
+        @admin.majorCompactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.TestChoreService.testForceTrigger(TestChoreService.java:398)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14765//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14626198" author="hadoopqa" created="Tue, 14 Jul 2015 11:10:50 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12745213/11339-master-v8.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12745213/11339-master-v8.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit a3d30892b41f604ab5a62d4f612fa7c230267dfe.&lt;br/&gt;
  ATTACHMENT ID: 12745213&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;                &lt;font color=&quot;red&quot;&gt;-1 checkstyle&lt;/font&gt;.  The applied patch generated 1889 checkstyle errors (more than the master&apos;s current 1873 errors).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +      family.setMobEnabled(JBoolean.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB)&lt;br/&gt;
+      family.setMobThreshold(JLong.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD)&lt;br/&gt;
+        @admin.compactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;br/&gt;
+        @admin.majorCompactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.rest.client.TestRemoteTable&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;                Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14764//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14626723" author="yuzhihong@gmail.com" created="Tue, 14 Jul 2015 17:32:41 +0000"  >&lt;p&gt;Patch v10 fixes checkstyle warnings.&lt;/p&gt;</comment>
                            <comment id="14626989" author="hadoopqa" created="Tue, 14 Jul 2015 20:21:40 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12745292/11339-master-v10.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12745292/11339-master-v10.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 2f327c911056d02813f642503db9a4383e8b4a2f.&lt;br/&gt;
  ATTACHMENT ID: 12745292&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 102 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 lineLengths&lt;/font&gt;.  The patch introduces the following lines longer than 100:&lt;br/&gt;
    +      family.setMobEnabled(JBoolean.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::IS_MOB)&lt;br/&gt;
+      family.setMobThreshold(JLong.valueOf(arg.delete(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD))) if arg.include?(org.apache.hadoop.hbase.HColumnDescriptor::MOB_THRESHOLD)&lt;br/&gt;
+        @admin.compactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;br/&gt;
+        @admin.majorCompactMob(org.apache.hadoop.hbase.TableName.valueOf(table_name), family.to_java_bytes)&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/14769//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14626991" author="yuzhihong@gmail.com" created="Tue, 14 Jul 2015 20:24:31 +0000"  >&lt;p&gt;The long line warnings all come from .rb file where there are pre-existing long lines.&lt;/p&gt;</comment>
                            <comment id="14627443" author="jingcheng.du@intel.com" created="Wed, 15 Jul 2015 02:44:30 +0000"  >&lt;p&gt;Thanks Ted!&lt;br/&gt;
I&apos;ve uploaded patch v10 to RB. The hbase group members can read it by the link &lt;a href=&quot;https://reviews.apache.org/r/36391/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/36391/&lt;/a&gt;. Thanks.&lt;/p&gt;</comment>
                            <comment id="14637543" author="jmhsieh" created="Wed, 22 Jul 2015 20:13:30 +0000"  >&lt;p&gt;I&apos;ve merged the branch in to master now.  Thanks Jingcheng for all the work, along with ram, anoop, and ted for the reviews.  Also thanks to the folks who participated on the pre-merge discussion thread and votes.&lt;/p&gt;

&lt;p&gt;I committed before I could get a full run of all the unit tests in to avoid commit race (i was about do and then there was another commit that landed) &amp;#8211; we&apos;ll tackle issues if any arise due to this as we&apos;d handle with normal patches.&lt;/p&gt;</comment>
                            <comment id="14637806" author="hudson" created="Wed, 22 Jul 2015 22:48:39 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-TRUNK #6672 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/6672/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/6672/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11339&quot; title=&quot;HBase MOB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11339&quot;&gt;&lt;del&gt;HBASE-11339&lt;/del&gt;&lt;/a&gt; integrated updates made to the MOB Handbook DOCX file (mstanleyjones: rev b72eb7f92eac483e90b460d536166445f84b1de4)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;src/main/docbkx/hbase_mob.xml&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11339&quot; title=&quot;HBase MOB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11339&quot;&gt;&lt;del&gt;HBASE-11339&lt;/del&gt;&lt;/a&gt; Converted hbase_mob.xml to Asciidoc and added it to the Asciidoc TOC (mstanleyjones: rev a1e9ce3d877035a6e21aab6df8eccd8e959e92dc)&lt;/li&gt;
	&lt;li&gt;src/main/docbkx/hbase_mob.xml&lt;/li&gt;
	&lt;li&gt;src/main/asciidoc/book.adoc&lt;/li&gt;
	&lt;li&gt;src/main/asciidoc/_chapters/hbase_mob.adoc&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11339&quot; title=&quot;HBase MOB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11339&quot;&gt;&lt;del&gt;HBASE-11339&lt;/del&gt;&lt;/a&gt; Addendum: Put back the sweeper tool docs for now (mstanleyjones: rev 33a6a819a467e09ce80e7d42362c774e62d35809)&lt;/li&gt;
	&lt;li&gt;src/main/asciidoc/_chapters/hbase_mob.adoc&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14638451" author="hudson" created="Thu, 23 Jul 2015 08:17:04 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-TRUNK #6674 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/6674/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/6674/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14151&quot; title=&quot;Remove the unnecessary file ProtobufUtil.java.rej which is brought in by merging hbase-11339&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14151&quot;&gt;&lt;del&gt;HBASE-14151&lt;/del&gt;&lt;/a&gt; Remove the unnecessary file ProtobufUtil.java.rej which is brought in by merging hbase-11339. (Jingcheng) (anoopsamjohn: rev 4f60d9c28d80472d195637eeff98e19fcdf62af5)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java.rej&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14644879" author="hudson" created="Tue, 28 Jul 2015 19:18:54 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-TRUNK #6682 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/6682/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/6682/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14152&quot; title=&quot;Fix the warnings in Checkstyle and FindBugs brought in by merging hbase-11339&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14152&quot;&gt;&lt;del&gt;HBASE-14152&lt;/del&gt;&lt;/a&gt; Fix the warnings in Checkstyle and FindBugs brought in by merging hbase-11339 (Jingcheng Du) (busbey: rev 6b9b7cb8c729aa15b88c1b91c25a3d5a51bbe3ca)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/mob/mapreduce/SweepJob.java&lt;/li&gt;
	&lt;li&gt;hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java&lt;/li&gt;
	&lt;li&gt;hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                            <outwardlinks description="breaks">
                                        <issuelink>
            <issuekey id="12752560">HBASE-12418</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12729762">HBASE-11591</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12745292" name="11339-master-v10.patch" size="836457" author="yuzhihong@gmail.com" created="Tue, 14 Jul 2015 17:32:41 +0000"/>
                            <attachment id="12744746" name="11339-master-v3.txt" size="830167" author="yuzhihong@gmail.com" created="Fri, 10 Jul 2015 16:56:31 +0000"/>
                            <attachment id="12744838" name="11339-master-v4.txt" size="830397" author="yuzhihong@gmail.com" created="Fri, 10 Jul 2015 22:45:56 +0000"/>
                            <attachment id="12745100" name="11339-master-v5.txt" size="830246" author="yuzhihong@gmail.com" created="Mon, 13 Jul 2015 19:29:15 +0000"/>
                            <attachment id="12745103" name="11339-master-v6.txt" size="830266" author="yuzhihong@gmail.com" created="Mon, 13 Jul 2015 19:54:03 +0000"/>
                            <attachment id="12745138" name="11339-master-v7.txt" size="830171" author="yuzhihong@gmail.com" created="Mon, 13 Jul 2015 22:29:30 +0000"/>
                            <attachment id="12745213" name="11339-master-v8.patch" size="828873" author="jingcheng.du@intel.com" created="Tue, 14 Jul 2015 08:27:33 +0000"/>
                            <attachment id="12745215" name="11339-master-v9.patch" size="828458" author="jingcheng.du@intel.com" created="Tue, 14 Jul 2015 08:40:34 +0000"/>
                            <attachment id="12659066" name="HBase MOB Design-v2.pdf" size="2451964" author="jingcheng.du@intel.com" created="Fri, 1 Aug 2014 01:50:11 +0000"/>
                            <attachment id="12660071" name="HBase MOB Design-v3.pdf" size="2463683" author="jingcheng.du@intel.com" created="Wed, 6 Aug 2014 06:48:33 +0000"/>
                            <attachment id="12660363" name="HBase MOB Design-v4.pdf" size="2482213" author="jingcheng.du@intel.com" created="Thu, 7 Aug 2014 10:20:00 +0000"/>
                            <attachment id="12724468" name="HBase MOB Design-v5.pdf" size="1368231" author="jingcheng.du@intel.com" created="Fri, 10 Apr 2015 05:35:30 +0000"/>
                            <attachment id="12656011" name="HBase MOB Design.pdf" size="2252478" author="jingcheng.du@intel.com" created="Wed, 16 Jul 2014 09:09:51 +0000"/>
                            <attachment id="12665290" name="MOB user guide.docx" size="28008" author="jiajia" created="Fri, 29 Aug 2014 07:48:48 +0000"/>
                            <attachment id="12665728" name="MOB user guide_v2.docx" size="27567" author="jiajia" created="Mon, 1 Sep 2014 07:46:49 +0000"/>
                            <attachment id="12666396" name="MOB user guide_v3.docx" size="27991" author="jiajia" created="Thu, 4 Sep 2014 02:06:18 +0000"/>
                            <attachment id="12670355" name="MOB user guide_v4.docx" size="29925" author="jiajia" created="Mon, 22 Sep 2014 05:00:03 +0000"/>
                            <attachment id="12671690" name="MOB user guide_v5.docx" size="29882" author="jiajia" created="Sun, 28 Sep 2014 04:47:50 +0000"/>
                            <attachment id="12733990" name="hbase-11339-150519.patch" size="775767" author="jmhsieh" created="Tue, 19 May 2015 23:36:13 +0000"/>
                            <attachment id="12656488" name="hbase-11339-in-dev.patch" size="104058" author="jingcheng.du@intel.com" created="Fri, 18 Jul 2014 09:25:28 +0000"/>
                            <attachment id="12726973" name="hbase-11339.150417.patch" size="761165" author="jmhsieh" created="Tue, 21 Apr 2015 19:40:39 +0000"/>
                            <attachment id="12698561" name="merge-150212.patch" size="685910" author="jmhsieh" created="Thu, 12 Feb 2015 22:56:47 +0000"/>
                            <attachment id="12698597" name="merge.150212b.patch" size="685256" author="jmhsieh" created="Fri, 13 Feb 2015 02:01:04 +0000"/>
                            <attachment id="12698600" name="merge.150212c.patch" size="684780" author="jmhsieh" created="Fri, 13 Feb 2015 02:05:26 +0000"/>
                            <attachment id="12744683" name="merge.150710.patch" size="828511" author="jingcheng.du@intel.com" created="Fri, 10 Jul 2015 09:26:27 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12731290">HBASE-11643</subtask>
                            <subtask id="12731291">HBASE-11644</subtask>
                            <subtask id="12731294">HBASE-11646</subtask>
                            <subtask id="12731295">HBASE-11647</subtask>
                            <subtask id="12731292">HBASE-11645</subtask>
                            <subtask id="12732216">HBASE-11683</subtask>
                            <subtask id="12737853">HBASE-11861</subtask>
                            <subtask id="12739309">HBASE-11901</subtask>
                            <subtask id="12741686">HBASE-11986</subtask>
                            <subtask id="12742019">HBASE-12000</subtask>
                            <subtask id="12743537">HBASE-12066</subtask>
                            <subtask id="12743749">HBASE-12080</subtask>
                            <subtask id="12743834">HBASE-12085</subtask>
                            <subtask id="12743973">HBASE-12093</subtask>
                            <subtask id="12743975">HBASE-12094</subtask>
                            <subtask id="12750150">HBASE-12331</subtask>
                            <subtask id="12750152">HBASE-12332</subtask>
                            <subtask id="12751896">HBASE-12391</subtask>
                            <subtask id="12751899">HBASE-12392</subtask>
                            <subtask id="12755584">HBASE-12486</subtask>
                            <subtask id="12755587">HBASE-12487</subtask>
                            <subtask id="12755709">HBASE-12489</subtask>
                            <subtask id="12756624">HBASE-12543</subtask>
                            <subtask id="12758146">HBASE-12591</subtask>
                            <subtask id="12759934">HBASE-12646</subtask>
                            <subtask id="12760838">HBASE-12669</subtask>
                            <subtask id="12760839">HBASE-12670</subtask>
                            <subtask id="12760930">HBASE-12673</subtask>
                            <subtask id="12761732">HBASE-12691</subtask>
                            <subtask id="12761963">HBASE-12698</subtask>
                            <subtask id="12763692">HBASE-12758</subtask>
                            <subtask id="12765557">HBASE-12820</subtask>
                            <subtask id="12774097">HBASE-13012</subtask>
                            <subtask id="12774099">HBASE-13013</subtask>
                            <subtask id="12777727">HBASE-13107</subtask>
                            <subtask id="12777975">HBASE-13117</subtask>
                            <subtask id="12779324">HBASE-13151</subtask>
                            <subtask id="12779332">HBASE-13152</subtask>
                            <subtask id="12779414">HBASE-13154</subtask>
                            <subtask id="12779714">HBASE-13157</subtask>
                            <subtask id="12783033">HBASE-13277</subtask>
                            <subtask id="12781835">HBASE-13230</subtask>
                            <subtask id="12783673">HBASE-13302</subtask>
                            <subtask id="12784665">HBASE-13313</subtask>
                            <subtask id="12823149">HBASE-13531</subtask>
                            <subtask id="12831294">HBASE-13720</subtask>
                            <subtask id="12831607">HBASE-13736</subtask>
                            <subtask id="12831698">HBASE-13739</subtask>
                            <subtask id="12832456">HBASE-13762</subtask>
                            <subtask id="12832461">HBASE-13763</subtask>
                            <subtask id="12833369">HBASE-13790</subtask>
                            <subtask id="12833370">HBASE-13791</subtask>
                            <subtask id="12833691">HBASE-13803</subtask>
                            <subtask id="12833695">HBASE-13804</subtask>
                            <subtask id="12833711">HBASE-13805</subtask>
                            <subtask id="12833722">HBASE-13806</subtask>
                            <subtask id="12835355">HBASE-13836</subtask>
                            <subtask id="12836068">HBASE-13855</subtask>
                            <subtask id="12836092">HBASE-13856</subtask>
                            <subtask id="12836772">HBASE-13886</subtask>
                            <subtask id="12838401">HBASE-13922</subtask>
                            <subtask id="12838715">HBASE-13932</subtask>
                            <subtask id="12742408">HBASE-12015</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 13 Jun 2014 06:04:07 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>399230</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 20 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1wq7z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>399341</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>The Moderate Object Storage (MOB) feature (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11339&quot; title=&quot;HBase MOB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11339&quot;&gt;&lt;strike&gt;HBASE-11339&lt;/strike&gt;&lt;/a&gt;[1]) is modified I/O and compaction path that allows individual moderately sized values (100KB-10MB) to be stored in a way that write amplification is reduced when compared to the normal I/O path. MOB is defined in the column family and it is almost isolated with other components, the features and performance cannot be effected in normal columns.&lt;br/&gt;
&lt;br/&gt;
For more details on how to use the feature please consult the HBase Reference Guide</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>