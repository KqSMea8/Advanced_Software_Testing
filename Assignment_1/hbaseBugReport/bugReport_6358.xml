<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:36:03 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6358/HBASE-6358.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6358] Bulkloading from remote filesystem is problematic</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6358</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Bulk loading hfiles that don&apos;t live on the same filesystem as HBase can cause problems for subtle reasons.&lt;/p&gt;

&lt;p&gt;In Store.bulkLoadHFile(), the regionserver will copy the source hfile to its own filesystem if it&apos;s not already there. Since this can take a long time for large hfiles, it&apos;s likely that the client will timeout and retry. When the client retries repeatedly, there may be several bulkload operations in flight for the same hfile, causing lots of unnecessary IO and tying up handler threads. This can seriously impact performance. In my case, the cluster became unusable and the regionservers had to be kill -9&apos;ed.&lt;/p&gt;

&lt;p&gt;Possible solutions:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Require that hfiles already be on the same filesystem as HBase in order for bulkloading to succeed. The copy could be handled by LoadIncrementalHFiles before the regionserver is called.&lt;/li&gt;
	&lt;li&gt;Others? I&apos;m not familiar with Hadoop IPC so there may be tricks to extend the timeout or something else.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&apos;m willing to write a patch but I&apos;d appreciate recommendations on how to proceed.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12598098">HBASE-6358</key>
            <summary>Bulkloading from remote filesystem is problematic</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="dave_revell">Dave Revell</reporter>
                        <labels>
                    </labels>
                <created>Mon, 9 Jul 2012 21:50:00 +0000</created>
                <updated>Sat, 11 Apr 2015 00:21:21 +0000</updated>
                            <resolved>Sat, 11 Apr 2015 00:21:21 +0000</resolved>
                                    <version>0.94.0</version>
                                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="13409896" author="qwertymaniac" created="Mon, 9 Jul 2012 21:57:31 +0000"  >&lt;p&gt;I sort of agree, except it is also more of a best-practice thing. If you bulk load remotely with only a single or very few requests per source at a time, and with a high RPC timeout at the client (such that it does not retry too often), then it should be more tolerable.&lt;/p&gt;

&lt;p&gt;But in any case, having the RS do FS copies will indeed make it slow.&lt;/p&gt;

&lt;p&gt;I ran into a very similar issue and the tweak I had to suggest was to indeed distcp/cp the data first and bulk load next. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6350&quot; title=&quot;Some logging improvements for RegionServer bulk loading&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6350&quot;&gt;&lt;del&gt;HBASE-6350&lt;/del&gt;&lt;/a&gt; (Logging improvements for ops) and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6339&quot; title=&quot;Bulkload call to RS should begin holding write lock only after the file has been transferred&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6339&quot;&gt;&lt;del&gt;HBASE-6339&lt;/del&gt;&lt;/a&gt; (Possible optimization, negative in the end) came out of it.&lt;/p&gt;</comment>
                            <comment id="13409898" author="tlipcon" created="Mon, 9 Jul 2012 21:59:17 +0000"  >&lt;p&gt;Yea, I originally wrote the code that did the copy, but in hindsight I think it was a mistake. I think we should remove that capability and have the code fail if the filesystem doesn&apos;t match.&lt;/p&gt;</comment>
                            <comment id="13410658" author="apurtell" created="Tue, 10 Jul 2012 18:24:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Require that hfiles already be on the same filesystem as HBase in order for bulkloading to succeed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1. Will avoid surprises like that reported on this issue.&lt;/p&gt;</comment>
                            <comment id="13426680" author="qwertymaniac" created="Wed, 1 Aug 2012 14:58:33 +0000"  >&lt;p&gt;Hey Dave,&lt;/p&gt;

&lt;p&gt;Will you be doing the patch? We can probably mark-deprecate this feature in 0.96, and remove it in the release after that?&lt;/p&gt;</comment>
                            <comment id="13426747" author="dave_revell" created="Wed, 1 Aug 2012 16:49:47 +0000"  >&lt;p&gt;@Harsh,&lt;/p&gt;

&lt;p&gt;Yes I will, sorry for the delay. I can have it within a week.&lt;/p&gt;</comment>
                            <comment id="13427644" author="dave_revell" created="Thu, 2 Aug 2012 21:39:30 +0000"  >&lt;p&gt;Attached &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6358&quot; title=&quot;Bulkloading from remote filesystem is problematic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6358&quot;&gt;&lt;del&gt;HBASE-6358&lt;/del&gt;&lt;/a&gt;-trunk-v1.diff&lt;/p&gt;</comment>
                            <comment id="13427664" author="zhihyu@ebaysf.com" created="Thu, 2 Aug 2012 22:05:55 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(errMsg);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Should DoNotRetryIOException be thrown instead ?&lt;/p&gt;</comment>
                            <comment id="13427711" author="dave_revell" created="Thu, 2 Aug 2012 23:06:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zhihyu%40ebaysf.com&quot; class=&quot;user-hover&quot; rel=&quot;zhihyu@ebaysf.com&quot;&gt;Ted Yu&lt;/a&gt;: yes, I&apos;ll change it.&lt;/p&gt;</comment>
                            <comment id="13427761" author="hadoopqa" created="Fri, 3 Aug 2012 00:14:17 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538970/HBASE-6358-trunk-v2.diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538970/HBASE-6358-trunk-v2.diff&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.io.hfile.TestForceCacheImportantBlocks&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterNoCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2489//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13427769" author="dave_revell" created="Fri, 3 Aug 2012 00:27:52 +0000"  >&lt;p&gt;Cancelling patch because I messed it up. Fix coming soon.&lt;/p&gt;</comment>
                            <comment id="13427772" author="dave_revell" created="Fri, 3 Aug 2012 00:31:05 +0000"  >&lt;p&gt;Patch v3 uploaded&lt;/p&gt;</comment>
                            <comment id="13427786" author="hadoopqa" created="Fri, 3 Aug 2012 01:08:34 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538970/HBASE-6358-trunk-v2.diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538970/HBASE-6358-trunk-v2.diff&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2492//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13427805" author="hadoopqa" created="Fri, 3 Aug 2012 02:00:07 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538985/HBASE-6358-trunk-v3.diff&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538985/HBASE-6358-trunk-v3.diff&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.client.TestAdmin&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterNoCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface&lt;br/&gt;
                  org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2493//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13427810" author="zhihyu@ebaysf.com" created="Fri, 3 Aug 2012 02:24:27 +0000"  >&lt;p&gt;With a little more information added to the log, you would see that the following check is inaccurate:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!srcFs.equals(fs)) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;srcFs is: DFS[DFSClient&lt;span class=&quot;error&quot;&gt;&amp;#91;clientName=DFSClient_hb_rs_192.168.0.13,55152,1343960033351, ugi=zhihyu.hfs.0&amp;#93;&lt;/span&gt;]&lt;br/&gt;
fs is: org.apache.hadoop.hbase.fs.HFileSystem@580a00fd&lt;/p&gt;

&lt;p&gt;I suggest using fs.getHomeDirectory() for comparison: it includes hostname, port number and home path.&lt;/p&gt;</comment>
                            <comment id="13427829" author="hadoopqa" created="Fri, 3 Aug 2012 04:12:10 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538994/6358-suggestion.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538994/6358-suggestion.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.master.TestAssignmentManager&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2495//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13428217" author="dave_revell" created="Fri, 3 Aug 2012 16:30:06 +0000"  >&lt;p&gt;@Todd, can you explain your original rationale for using &quot;srcFs.equals(destFs)&quot; as a way of checking whether the src and dest filesystems are the same? Also, as an HDFS expert, can you suggest the best possible way for checking whether the underlying filesystems are actually the same FS?&lt;/p&gt;</comment>
                            <comment id="13428291" author="tlipcon" created="Fri, 3 Aug 2012 18:47:57 +0000"  >&lt;p&gt;hmm... I don&apos;t know if I thought about it in a huge amount of detail. The original idea was to allow you to run an MR on one cluster, and then &quot;LoadIncrementalHFiles&quot; on your HBase cluster which uses a different HDFS. I was thinking there would be an advantage here over the distcp-then-load approach, because the region server doing the copy would end up with a local replica after the load.&lt;/p&gt;

&lt;p&gt;That said, I didn&apos;t think through the timeout implications, which seems to be the issue discussed in this JIRA.&lt;/p&gt;

&lt;p&gt;As for how to determine if they&apos;re the same, the .equals() call is supposed to do that, but perhaps it&apos;s not working right?&lt;/p&gt;</comment>
                            <comment id="13428295" author="dave_revell" created="Fri, 3 Aug 2012 18:56:21 +0000"  >&lt;p&gt;Thanks Todd. My new plan is to keep the copy, but do it in LoadIncrementalHFiles instead of in Store. This keeps the existing use cases and test cases intact, but works around the timeout/retry problem.&lt;/p&gt;</comment>
                            <comment id="13428337" author="tlipcon" created="Fri, 3 Aug 2012 20:10:50 +0000"  >&lt;p&gt;The problem of doing it automatically in LoadIncrementalHFiles (i.e the client) is that it is going to be very slow for any non-trivial amount of data, to funnel it through this single node.&lt;/p&gt;

&lt;p&gt;Here&apos;s an alternate idea:&lt;br/&gt;
1. In this JIRA, change the RS side to fail if the filesystem doesn&apos;t match&lt;br/&gt;
2. Separately, add a new &quot;DistributedLoadIncrementalHFiles&quot; program which acts as a combination of distcp and LoadIncrementalHFiles. For each RS (or perhaps for each region), it would create one map task, with a locality hint to that server. Then the task would copy the relevant file (achieving a local replica) and make the necessary call to load the file.&lt;/p&gt;

&lt;p&gt;Between step 1 and 2, users would have to use distcp and sacrifice locality. But, with the current scheme, they already don&apos;t get locality for the common case where the MR job runs on the same cluster as HBase.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="13428368" author="dave_revell" created="Fri, 3 Aug 2012 20:48:18 +0000"  >&lt;p&gt;@Todd, that idea seems fine to me overall.&lt;/p&gt;

&lt;p&gt;If we just did the slow copy in LoadIncrementalHFiles as I suggested earlier, users would still have the option of doing distcp before calling LoadIncrementalHFiles if they need performance. This has the benefits of &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;not breaking the current use case of non-local bulk loading when size or speed requirements are modest&lt;/li&gt;
	&lt;li&gt;not requiring a new DistributedLoadIncrementalHFiles utility&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This scheme would not give locality though, so users with serious performance requirements might not be satisfied.&lt;/p&gt;</comment>
                            <comment id="13428508" author="tlipcon" created="Sat, 4 Aug 2012 00:45:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;not breaking the current use case of non-local bulk loading when size or speed requirements are modest&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the size and speed don&apos;t matter, then wouldn&apos;t you have just used a normal (non-bulk-load) MR job to load the data?&lt;/p&gt;

&lt;p&gt;I think funneling the load through one host basically defeats the purpose of bulk load. Perhaps it could be available as an option for people just testing out, but I would prefer the default to be a failure, and you have to enable the copy with a &lt;tt&gt;-copyToCluster&lt;/tt&gt; or something.&lt;/p&gt;</comment>
                            <comment id="13429262" author="dave_revell" created="Mon, 6 Aug 2012 16:57:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;If the size and speed don&apos;t matter, then wouldn&apos;t you have just used a normal (non-bulk-load) MR job to load the data?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There are other reasons to atomically load hfiles even for non-huge datasets, such as ETL and restoring backups. And atomicity could have some benefits for certain use cases. But it&apos;s probably not asking too much for people with these use cases to use a distributed hfile loader that depends on mapreduce, so I&apos;m willing to concede the point.&lt;/p&gt;

&lt;p&gt;@Todd, would you be in favor of adding another JIRA ticket for a distributed bulk loader, and having this ticket be blocked until it&apos;s done? I think it should be blocked so we don&apos;t remove the current &quot;bulkload from remote fs&quot; capability without offering an alternative, though the user does have the option of running distcp themselves.&lt;/p&gt;</comment>
                            <comment id="13429359" author="tlipcon" created="Mon, 6 Aug 2012 19:12:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;@Todd, would you be in favor of adding another JIRA ticket for a distributed bulk loader, and having this ticket be blocked until it&apos;s done? I think it should be blocked so we don&apos;t remove the current &quot;bulkload from remote fs&quot; capability without offering an alternative, though the user does have the option of running distcp themselves.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I could go either way on this. Up to folks who are more actively contributing code than I &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13447790" author="dave_revell" created="Tue, 4 Sep 2012 16:26:28 +0000"  >&lt;p&gt;Unassigning this ticket from myself. I think it will need a new tool, and I&apos;m not able to spend the time to do a good job.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12538994" name="6358-suggestion.txt" size="1636" author="zhihyu@ebaysf.com" created="Fri, 3 Aug 2012 02:28:23 +0000"/>
                            <attachment id="12538959" name="HBASE-6358-trunk-v1.diff" size="1409" author="dave_revell" created="Thu, 2 Aug 2012 21:39:30 +0000"/>
                            <attachment id="12538970" name="HBASE-6358-trunk-v2.diff" size="1409" author="dave_revell" created="Thu, 2 Aug 2012 23:12:23 +0000"/>
                            <attachment id="12538985" name="HBASE-6358-trunk-v3.diff" size="1787" author="dave_revell" created="Fri, 3 Aug 2012 00:30:22 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 9 Jul 2012 21:57:31 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>239664</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 15 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i00s6n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2498</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>