<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:21:56 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-11306/HBASE-11306.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-11306] Client connection starvation issues under high load on Amazon EC2</title>
                <link>https://issues.apache.org/jira/browse/HBASE-11306</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I am using YCSB 0.1.4 with Hadoop 2.2.0 and HBase 0.98.3 RC2 on an EC2 testbed (c3.8xlarge instances, SSD backed, 10 GigE networking). There are five slaves and five separate clients. I start with a prepopulated table of 100M rows over ~20 regions and run 5 YCSB clients concurrently targeting 250,000 ops/sec in aggregate. (Can reproduce this less effectively at 100k/ops/sec aggregate also.) Workload A. Due to how I set up the test, the data is all in one HFile per region and very likely in cache. All writes will fit in the aggregate memstore. No flushes or compactions are observed on any server during the test, only the occasional log roll. Despite these favorable conditions developed over time to isolate this issue, a few of the clients will stop making progress until socket timeouts after 60 seconds, leading to very large op latency outliers. With the above detail plus some added extra logging we can rule out storage layer effects. Turning to the network, this is where things get interesting.&lt;/p&gt;

&lt;p&gt;I used &lt;tt&gt;while true ; do clear ; ss -a -o|grep ESTAB|grep 8120 ; sleep 5 ; done&lt;/tt&gt; (8120 is the configured RS data port) to watch receive and send socket queues and TCP level timers on all of the clients and servers simultaneously during the run. &lt;/p&gt;

&lt;p&gt;I have Nagle disabled on the clients and servers and JVM networking set up to use IPv4 only. The YCSB clients are configured to use 20 threads. These threads are expected to share 5 active connections. one to each RegionServer. When the test starts we see exactly what we&apos;d expect, 5 established TCPv4 connections.&lt;/p&gt;

&lt;p&gt;On all servers usually the recv and send queues were empty when sampled. I never saw more than 10K waiting. The servers occasionally retransmitted, but with timers ~200ms and retry counts ~0.&lt;/p&gt;

&lt;p&gt;The client side is another story. We see serious problems like:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;tcp    ESTAB      0      8733   10.220.15.45:41428   10.220.2.115:8120     timer:(on,38sec,7)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That is about 9K of data still waiting to be sent after 7 TCP level retransmissions. &lt;/p&gt;

&lt;p&gt;There is some unfair queueing and packet drops happening at the network level, but we should be handling this better.&lt;/p&gt;

&lt;p&gt;During the periods when YCSB is not making progress, there is only that one connection to one RS in established state. There should be 5 established connections, one to each RS, but the other 4 have been dropped somehow. The one distressed connection remains established for the duration of the problem, while the retransmission timer count on the connection ticks upward. It is dropped once the socket times out at the app level. Why are the connections to the other RegionServers dropped? Why are all threads blocked waiting on the one connection for the socket timeout interval (60 seconds)? After the socket timeout we see the stuck connection dropped and 5 new connections immediately established. YCSB doesn&apos;t do anything that would lead to this behavior, it is using separate HTable instances for each client thread and not closing the table references until test cleanup. These behaviors seem internal to the HBase client. &lt;/p&gt;

&lt;p&gt;Is maintaining only a single multiplexed connection to each RegionServer the best approach? &lt;/p&gt;

&lt;p&gt;A related issue is we collect zombie sockets in ESTABLISHED state on the server. Also likely not our fault per se. Keepalives are enabled so they will eventually be garbage collected by the OS. On Linux systems this will take 2 hours. We might want to drop connections where we don&apos;t see activity sooner than that. Before &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11277&quot; title=&quot;RPCServer threads can wedge under high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11277&quot;&gt;&lt;del&gt;HBASE-11277&lt;/del&gt;&lt;/a&gt; we were spinning indefinitely on a core for each connection in this state.&lt;/p&gt;

&lt;p&gt;I have tried this using a narrow range of recent Java 7 and Java 8 runtimes and they all produce the same results. I have also launched several separate EC2 based test clusters and they all produce the same results, so this is a generic platform issue.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Amazon EC2&lt;/p&gt;</environment>
        <key id="12718956">HBASE-11306</key>
            <summary>Client connection starvation issues under high load on Amazon EC2</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Fri, 6 Jun 2014 22:43:14 +0000</created>
                <updated>Sat, 21 Jun 2014 03:15:24 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="14020494" author="apurtell" created="Fri, 6 Jun 2014 22:57:59 +0000"  >&lt;p&gt;Server side dmesg is clean but clients show some stress:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[ 3898.001528] xennet: skb rides the rocket: 19 slots
[ 3898.005126] xennet: skb rides the rocket: 22 slots
[ 3951.460940] xennet: skb rides the rocket: 19 slots
[ 3951.786580] xennet: skb rides the rocket: 20 slots
[ 4004.851271] xennet: skb rides the rocket: 20 slots
[ 4005.643567] xennet: skb rides the rocket: 29 slots
[ 4059.344717] xennet: skb rides the rocket: 23 slots
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some Googling turns up some reported Linux issues with EC2 where this leaves the connection in a stalled state until timeout. Disabling TCP offload functionality using &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;ethtool -K eth0 rx off tx off sg off tso off ufo off gso off gro off lro off
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;will work around the problem.&lt;/p&gt;

&lt;p&gt;Despite this I&apos;ve opened this issue for (further) discussion.&lt;/p&gt;</comment>
                            <comment id="14020588" author="vrodionov" created="Sat, 7 Jun 2014 00:42:21 +0000"  >&lt;p&gt;I have never run 250K ops on EC2 but 100K have always been successful (5 RS , 5 YCSB clients with target 20K each), but this is mostly for 0.94.x. I have done just a few 0.96 test runs (all of them successful). This may be 0.98 issue. I am going to run 0.98 tests on EC2 and will try 200-250K insert rates. &lt;/p&gt;</comment>
                            <comment id="14020593" author="vrodionov" created="Sat, 7 Jun 2014 00:46:39 +0000"  >&lt;p&gt;Sorry, I have not run workload A, so may be this is workload specific? &lt;/p&gt;</comment>
                            <comment id="14020597" author="apurtell" created="Sat, 7 Jun 2014 00:48:50 +0000"  >&lt;p&gt;Something about my setup manages to trigger a xenfront bug, that&apos;s for sure&lt;/p&gt;</comment>
                            <comment id="14020599" author="apurtell" created="Sat, 7 Jun 2014 00:51:59 +0000"  >&lt;p&gt;Kernel is &quot;Linux 3.10.40-50.136.amzn1.x86_64 #1 SMP Tue May 13 21:35:08 UTC 2014&quot;&lt;/p&gt;</comment>
                            <comment id="14020673" author="stack" created="Sat, 7 Jun 2014 03:32:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Something about my setup manages to trigger a xenfront bug, that&apos;s for sure&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So, disabling TCP offload fixed starvation &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14020694" author="apurtell" created="Sat, 7 Jun 2014 05:01:41 +0000"  >&lt;p&gt;Disabling offload prevents connections from getting into a bad state yes. Worth looking at if the client can get completely stuck on one stalled connection in other situations I think.&lt;/p&gt;</comment>
                            <comment id="14034919" author="tianq" created="Wed, 18 Jun 2014 07:22:04 +0000"  >&lt;p&gt;Hi  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;,&lt;br/&gt;
we also see user reported the issue: &lt;br/&gt;
a recent case in user mailing list which I am looking at: &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/hbase-user/201406.mbox/%3C001001cf863e$c81a60e0$584f22a0$@com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/hbase-user/201406.mbox/%3C001001cf863e$c81a60e0$584f22a0$@com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;another case:&lt;br/&gt;
&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/hbase-dev/201402.mbox/%3CCAHkeaHXAaJFyC6XNV+cBwupqAKoTmkJ349CPQuk02FGS8qcF1w@mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/hbase-dev/201402.mbox/%3CCAHkeaHXAaJFyC6XNV+cBwupqAKoTmkJ349CPQuk02FGS8qcF1w@mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ps this is really interesting... I just wanted to propose a fix, and when changing the code... just see the code was changed (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11277&quot; title=&quot;RPCServer threads can wedge under high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11277&quot;&gt;&lt;del&gt;HBASE-11277&lt;/del&gt;&lt;/a&gt;).. and get here. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The 11277 fix directly returns. what will happen next?&lt;br/&gt;
One reader could be shared by many connections, it looks readAndProcess is designed to service a complete RPC request(e.g. after processOneRpc call, data buffer is nulled). so my proposal is to start a timeout(user can configure the timeout value) when read count is zero, which is not a normal case for rpc data using non-blocking socket.  after timeout, close the connection.   this could services as a hbase server side keep-alive (as you mentioned, the OS level keep-alive time is 2 hours, and I googled it cannot be changed in java. so actually useless)&lt;/p&gt;

&lt;p&gt;I was thinking the issue is caused by network layer, but do not think about the hbase client side. will look at the client code.&lt;/p&gt;

&lt;p&gt;update for Nagle: it looks both ipc client and ipc server do not follow 1 read--1 write pair..looks nagle is needed in such case.&lt;br/&gt;
cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;thanks.&lt;/p&gt;
</comment>
                            <comment id="14034988" author="tianq" created="Wed, 18 Jun 2014 07:48:44 +0000"  >&lt;p&gt;it looks rpc server connection cleanup(cleanupConnections) is quite strict ( fix of hbase11277 falls into it?)&lt;br/&gt;
1)# of connection threshold for clean up is 4000 by default.&lt;br/&gt;
2)random range scan for connection list in most cases.&lt;br/&gt;
3)rpcCount must be 0. (in our cases, we are at the 3rd channelRead call. the rpcCount is not 0)&lt;/p&gt;

&lt;p&gt;so first time cleanup is better?&lt;br/&gt;
thanks.&lt;/p&gt;</comment>
                            <comment id="14035949" author="stack" created="Wed, 18 Jun 2014 17:02:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is maintaining only a single multiplexed connection to each RegionServer the best approach?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The argument runs something like &amp;#8211; one shared connection scales better.  In issues I could dig up, its reported that connection per HTable instance has better throughput.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...worth looking at if the client can get completely stuck on one stalled connection in other situations I think.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, what is that about?  Stuck trying to do a lookup on meta and all other threads blocked waiting till this resolves?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...when read count is zero, which is not a normal case for rpc data using non-blocking socket. after timeout, close the connection.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we know for sure how the above scenario comes about?  It is connection from client kept-alive?  No data being sent.  OS is keeping it alive for two hours?  Have you done any debug in here &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tianq&quot; class=&quot;user-hover&quot; rel=&quot;tianq&quot;&gt;Qiang Tian&lt;/a&gt;?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;so first time cleanup is better?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Your argument is that we are not doing cleanup of clients that have not sent data in a good while?  Thanks for looking in here Qiang.&lt;/p&gt;
</comment>
                            <comment id="14035994" author="apurtell" created="Wed, 18 Jun 2014 17:28:51 +0000"  >&lt;p&gt;The xennet issue is leaving the TCP FSM in a bad state. On the server this manifests as established connections with no data pending where the client has gone away. They will sit there consuming a file table slot. The TCP keepalive probe will detect eventually that the remote has gone away but on Linux at least the probe interval is fixed at 2 hours. An application keepalive ie idle ping might catch it sooner. I&apos;m on the phone now so can&apos;t check code. Do we still have idle pings in our RPC ? &lt;/p&gt;</comment>
                            <comment id="14035998" author="apurtell" created="Wed, 18 Jun 2014 17:31:40 +0000"  >&lt;p&gt;Not sure why the client gets stuck with only one stalled connection. Not sure why it drops the connections to the 4 other RegionServers. I&apos;m sure I can reproduce this again if you have suggestions for debugging what&apos;s going on there further. &lt;/p&gt;</comment>
                            <comment id="14037213" author="tianq" created="Thu, 19 Jun 2014 10:31:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;...Do we know for sure how the above scenario comes about?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;do not know how at the moment. we are stuck in reading rpc request data or connection header. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;... It is connection from client kept-alive?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;looks not. there is code to handle it. see PING_CALL_ID&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...OS is keeping it alive for two hours?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;it looks for most systems the default timeout is 2 hours. but hbase client socket has much smaller timeout,  which will be triggered first.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...Have you done any debug&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;mostly code analysis based on the 2 occurrences we see in mailing list. I thought it is hard to repro. now Andrew can repro it, I will look if I could (know nothing about ycsb and ec2 at the moment &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...Not sure why the client gets stuck with only one stalled connection.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I suspect it is related to the original 11277 issue.  looks the 11277 fix leaves the bad connection there. at that point client is probably waiting for rpc response and might not send data again. and server side cleanupConnections will not cleanup the connection either.&lt;/p&gt;

&lt;p&gt;read more rpc client code.  it looks client sharing the same connection for each regionserver is not accurate. a connection is identified by 3 tuples: user, RPC method name and server addr.(see getConnection and tracedWriteRequest). so different method call uses different connections(although the same rpcClient instance and same HConnectionImplementation instance).  please correct me if I am wrong. will dig more.&lt;/p&gt;

&lt;p&gt;debug:&lt;br/&gt;
1)from client concurrency aspect,  could we try using different connections for different threads? --create new config objects in each YCSB thread(as Jonathan mentioned in a recent thread &lt;a href=&quot;http://search-hadoop.com/m/DHED4zrOq61/HBase+with+multiple+threads&amp;amp;subj=+Discuss+HBase+with+multiple+threads&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://search-hadoop.com/m/DHED4zrOq61/HBase+with+multiple+threads&amp;amp;subj=+Discuss+HBase+with+multiple+threads&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;2)from server side, I plan to add the timeout(hopefully tomorrow). we can dump diag(such as the phase we are at, some members in Connection class) when read count is 0 and timeout.&lt;/p&gt;

&lt;p&gt;3)another suspect when I did search today --Reader#readSelector. should it be volatile? it is possible that we got stale data and fall into the read incorrectly?&lt;/p&gt;


</comment>
                            <comment id="14038667" author="tianq" created="Fri, 20 Jun 2014 10:36:21 +0000"  >&lt;p&gt;hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;,&lt;br/&gt;
could you take a look the patch? hopefully we could see where it stuck. &lt;br/&gt;
based on my limited observation of client rpc and server rpc code, excellent but complex to analyze&amp;amp;locate such problem... using different connections for different YCSB threads could help isolate as the operations in client connection will be serial.&lt;br/&gt;
thanks.&lt;/p&gt;
</comment>
                            <comment id="14039033" author="apurtell" created="Fri, 20 Jun 2014 17:06:30 +0000"  >&lt;p&gt;This patch effectively reverts the change in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11277&quot; title=&quot;RPCServer threads can wedge under high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11277&quot;&gt;&lt;del&gt;HBASE-11277&lt;/del&gt;&lt;/a&gt; by going around the loop again if count == 0. That would not be correct. After &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11277&quot; title=&quot;RPCServer threads can wedge under high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11277&quot;&gt;&lt;del&gt;HBASE-11277&lt;/del&gt;&lt;/a&gt; we don&apos;t loop around and around after zero length reads. Instead we return from the loop. See the comments on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11277&quot; title=&quot;RPCServer threads can wedge under high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11277&quot;&gt;&lt;del&gt;HBASE-11277&lt;/del&gt;&lt;/a&gt;. I also used an approach where we would count the number of times around the loop after zero length reads, but the patch we decided to commit was a simpler change: The handler does not come back to process the connection again unless it is selectable for reads again.&lt;/p&gt;</comment>
                            <comment id="14039666" author="tianq" created="Sat, 21 Jun 2014 03:15:24 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;! I missed some important info in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11277&quot; title=&quot;RPCServer threads can wedge under high load&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11277&quot;&gt;&lt;del&gt;HBASE-11277&lt;/del&gt;&lt;/a&gt;. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;As you mentioned in your test 10,000 limit has more exceptions hit than 100,000 limit(~20 per server vs 2-3 per server),  so the problem is just network congestion when client sends rpc request?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...The handler does not come back to process the connection again unless it is selectable for reads again.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah with your test this is the best.  cool!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...Not sure why the client gets stuck with only one stalled connection. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;based on what we know so far, I guess it is caused by the shared connection at client side. &lt;br/&gt;
11277 fix relieves the rpc server reader but at client side the IPCUtil.write call in RpcClient.Connection#writeRequest(0.98.3RC2) is blocked IO right? when multiple threads call the same RPC concurrently and there is one thread stuck in it, all threads will be blocked?  the socket out stream has a timeout 60 seconds if the socket has an associated SocketChannel(but I do not see where we open the channel?), after timeout all threads will be released? using different connections in different threads might fix it?&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;...Not sure why it drops the connections to the 4 other RegionServers.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;still not sure about this...&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12651646" name="hbase11306-0.98.3RC2.patch" size="4903" author="tianq" created="Fri, 20 Jun 2014 10:27:33 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 7 Jun 2014 00:42:21 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>397155</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 25 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1wdjj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>397278</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>