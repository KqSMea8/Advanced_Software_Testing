<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:31:02 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-12233/HBASE-12233.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-12233] Bring back root table</title>
                <link>https://issues.apache.org/jira/browse/HBASE-12233</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;First step towards splitting meta.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12747455">HBASE-12233</key>
            <summary>Bring back root table</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12714102">HBASE-11165</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="virag">Virag Kothari</assignee>
                                    <reporter username="virag">Virag Kothari</reporter>
                        <labels>
                    </labels>
                <created>Sat, 11 Oct 2014 00:04:13 +0000</created>
                <updated>Thu, 23 Oct 2014 00:14:34 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                <comments>
                            <comment id="14167827" author="virag" created="Sat, 11 Oct 2014 00:10:38 +0000"  >&lt;p&gt;Attached is the initial patch for review (master branch)&lt;/p&gt;

&lt;p&gt;Primary changes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;meta-region-server znode has additional fields for backward compatibility&lt;br/&gt;
1) isRoot - For branch-1/0.98 whether this znode is holding root or meta location&lt;br/&gt;
2) rootSchema - If we are changing the root comparator/schema&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-Root has its own WAL&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MetaTableLocator and MetaTableAccessor has added/changed few stateless utility methods (I plan to&lt;br/&gt;
remain this to CatalogTableLocator and CatalogTableAccessor. Didn&apos;t refactor in the first patch so that actual changes&lt;br/&gt;
can be easily reviewed) &lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-Public client API&apos;s changed&lt;br/&gt;
1) HRegionInfo adds #isRootRegion()&lt;br/&gt;
2) Registry changes API from #getMetaRegionLocation to #getRootRegionLocation&lt;/p&gt;

&lt;p&gt;-Few assignment related changes (Separate SSH for root)&lt;/p&gt;</comment>
                            <comment id="14167835" author="virag" created="Sat, 11 Oct 2014 00:15:39 +0000"  >&lt;p&gt;Formatting got a bit messed up above. Sorry.&lt;/p&gt;</comment>
                            <comment id="14167837" author="virag" created="Sat, 11 Oct 2014 00:16:12 +0000"  >&lt;p&gt;Patch also on RB: &lt;a href=&quot;https://reviews.apache.org/r/26587&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/26587&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14167893" author="virag" created="Sat, 11 Oct 2014 00:58:27 +0000"  >&lt;p&gt;Things pending in the first draft&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Hbck, UI, shell needs fixing and few test cases are failing&lt;/li&gt;
	&lt;li&gt;Migration/Upgrade from branch-1/0.98&lt;/li&gt;
	&lt;li&gt;Different comparator for root?&lt;/li&gt;
&lt;/ul&gt;

</comment>
                            <comment id="14167900" author="virag" created="Sat, 11 Oct 2014 01:14:58 +0000"  >&lt;p&gt;We already have started deploying 0.98 with Root in Y!, but our internal patch assumes that the cluster is root based. &lt;br/&gt;
We will remove our internal version of patch once we have a good patch here based on the review feedback. Also for 0.98/branch-1, a bit more work needs to be done of hiding root by default.&lt;br/&gt;
We are also working on splitting meta. &lt;/p&gt;</comment>
                            <comment id="14167927" author="eclark" created="Sat, 11 Oct 2014 01:36:40 +0000"  >&lt;p&gt;So right now I don&apos;t see the need for split meta. We found a 5x win on meta versions. This alone means that we can handle lots more regions per cluster with out breaking a sweat. And there&apos;s lots more that can be done without hurting the default case.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We haven&apos;t tried meta region replicas.
	&lt;ul&gt;
		&lt;li&gt;This is where we will see the biggest wins. Right now meta gets 20000x more reads than writes on real production clusters.&lt;/li&gt;
		&lt;li&gt;We already have the region replica stuff being worked on&lt;/li&gt;
		&lt;li&gt;All reads to meta can already be stale so timeline consistent is good enough for &amp;gt;99% of reads that are currently going to meta.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;We haven&apos;t tried allowing a shared cache infront of meta.
	&lt;ul&gt;
		&lt;li&gt;It would be easy to put memcache (or redis if you already have it) in the MetaTableAccessor and that would allow a very fast shared cache that gets warmed by all clients.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;We haven&apos;t tried a more compact meta table representation&lt;/li&gt;
	&lt;li&gt;We haven&apos;t tried picking smaller split keys.
	&lt;ul&gt;
		&lt;li&gt;Right now we just pick a mid point. It would be possible to do something like our index key generation to drastically reduce the size of region names.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;We haven&apos;t tried stripe compactions on meta.&lt;/li&gt;
	&lt;li&gt;We haven&apos;t gotten block encoding working for meta.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Each of those things are largish wins that haven&apos;t been tried, and each of those things helps every user. While split meta helps maybe 0.1% of HBase&apos;s users.&lt;/p&gt;

&lt;p&gt;Until the less invasive more positive options  are tried and shown to not be enough, it just feels like everyone is trying to split meta without a concrete need.&lt;/p&gt;

&lt;p&gt;Splitting meta doesn&apos;t make us able to scale to the sky. The NN is still a bottle neck on cluster size. NameNodes fall over way before we will need a split meta.  Even if someone gets NN to scale to infinity, the network becomes a bottleneck. HDFS clusters can only get so large before they need to be federated (and HBase doesn&apos;t tolerate federation). So making every user and every cluster feel the pain of increased complexity and mttr so that we the HBase developers can perform a thought experiment is just not something I see the benefit in right now.&lt;/p&gt;

&lt;p&gt;I am for sure against bringing root back in for branch-1 ( -1 ). It&apos;s just way too late in the stabilization period to not set us way back.&lt;/p&gt;</comment>
                            <comment id="14168009" author="stack" created="Sat, 11 Oct 2014 05:31:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;We found a 5x win on meta versions. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;5x in what? i/o? memory used?  Ability to cache?  Shrinkage in HDFS size occupied by meta?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We haven&apos;t tried meta region replicas.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;When it lands, a while yet it seems, this will scale read i/o only.  Not write i/o.  It won&apos;t do anything about the size of meta in the fs.  We&apos;ll waste mem caching the same stuff multiple times, once per replica.  Also, IMO regards stale reads of meta I&apos;d suggest we not be cavalier.  Methinks it will make for a new class of problems.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We haven&apos;t tried allowing a shared cache infront of meta.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;At first blush, adding an external system to manage our cache seems way more complicated than adding back an extra tier.  I&apos;d have to hear more.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We haven&apos;t tried a more compact meta table representation&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;d work for the in-memory representation &amp;#8211; being able to represent bigger clusters in-mem &amp;#8211; but it does nothing to address i/o carried by meta-carrying server nor does it address size of meta in HDFS (write-amplification continually rewriting big files).  We&apos;ll also be burning CPU at a higher rate going from compact representation to pb over and over again.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We haven&apos;t tried picking smaller split keys.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fair point.  We should do this for sure.  You think this would make difference when hosting 1M regions or 50M?  How much?  10/20%?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We haven&apos;t tried stripe compactions on meta.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Stripe compactions works best when time-series writes.  It makes the compaction story worse when writes are evenly distributed.  Meta could be of either type in any particular deploy.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We haven&apos;t gotten block encoding working for meta.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Doesn&apos;t address i/o, caching (we can cache encoded but currently we just dumbly decompress each access &amp;#8211; would have to add smarts), and block encoding would probably help some with size-on-disk but what, cut size by 50% max?  Maybe.  3G to 1.5G region when 1M regions at best?  It&apos;d still be too big?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;While split meta helps maybe 0.1% of HBase&apos;s users.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Factor in that we want to pivot and tell folks that small regions is the way to go (compactions): i.e. 10x or 100x what they are carrying currently... so being simplisitic your 0.1% becomes 1% or 10% .  Also this fraction are actually our most critical users, our &apos;fortune&apos; 100 as it were, the reason for hbase, the folks who use us because they need to scale.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;he NN is still a bottle neck on cluster size&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah. But that is another issue we should address separately (it so happens that the folks proposing this patch may be able to help  in this regard).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So making every user and every cluster feel the pain of increased complexity and mttr....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;...lets discuss this.  Its a problem I agree.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;....so that we the HBase developers can perform a thought experiment .....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is not a thought experiment as I understand it.  The lads are up against a scaling problem.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am for sure against bringing root back in for branch-1 &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For sure root will not be back for branch-1. That said, I do not preclude a patch that community brothers and sisters might need that adds root back as a bridge across versions as long as it behind a million safety valves and switches so it does not disturb the mainline code paths in case they need a bridge to branch-1 and beyond. &lt;/p&gt;

&lt;p&gt;This is a tough one.  Should we go to the mailing list with this?  I can rehash the doc I wrote up over on the 1M/50M JIRA or I can paraphrase bits of  the nice job &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mantonov&quot; class=&quot;user-hover&quot; rel=&quot;mantonov&quot;&gt;Mikhail Antonov&lt;/a&gt; did summarizing this issue in his recent meetup talk.&lt;/p&gt;

&lt;p&gt;Good on you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14168226" author="apurtell" created="Sat, 11 Oct 2014 16:05:16 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I am for sure against bringing root back in for branch-1&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;For sure root will not be back for branch-1. &lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;That does make this a thought exercise then, since there&apos;s no credible near term deployment for code checked only into master. My understanding is &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=virag&quot; class=&quot;user-hover&quot; rel=&quot;virag&quot;&gt;Virag Kothari&lt;/a&gt; and crew are actually looking for split meta to become available as an off-by-default option in 0.98. If there are strong feelings about bringing back root for 0.98 and branch-1 let&apos;s veto this and move on. Deploys that would like to try it can maintain custom patches. Or, if that&apos;s an undesirable outcome, let&apos;s not for sure be against anything so soon. If we&apos;re getting patches that implement this feature, let&apos;s put them in a feature branch and compare and contrast in the deployment where there&apos;s a problem to solve. With data in hand and operational feedback about change stability the discussion would be more grounded than current.&lt;/p&gt;</comment>
                            <comment id="14168227" author="apurtell" created="Sat, 11 Oct 2014 16:10:31 +0000"  >&lt;p&gt;Adding to the above. If bringing back root and split meta are off-by-default options, with a rolling upgrade story, in theory it can (and, IMO should) not make releases 1.0.x but could make it into release 1.1.0. That&apos;s different from vetoing a change from branch-1 entirely. &lt;/p&gt;</comment>
                            <comment id="14168325" author="virag" created="Sat, 11 Oct 2014 20:24:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;actually looking for split meta to become available as an off-by-default option in 0.98&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, we are looking for root and split meta as an off-by-default option in 0.98. &lt;br/&gt;
Stack had a suggestion of doing this hiding of root by adding an pbuf entry in meta znode which determines whether it holds the meta location or root location. Depending on this field, the clients will know the presence of root. Master will have a config depending on which it will create root table and set this field in meta znode. For 0.98/branch-1, we can have this config in master to be turned off by default so no root stuff is touched  and the clients wont know the presence of root at all. Also, clients wont need upgrade when the master turns on the switch for root.&lt;/p&gt;

</comment>
                            <comment id="14168328" author="virag" created="Sat, 11 Oct 2014 20:34:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Right now meta gets 20000x more reads than writes on real production clusters&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Write throughput on meta become important during cluster startups, operations like create/enable/disable and during rack failures. Any assignments involved during such activity usually does minimum 2 writes to meta for a single region as each region goes through at least 2 states.&lt;br/&gt;
So even in best case scenarios, there will be 2M writes just for bringing a table with 1M regions online.&lt;br/&gt;
Although, a split meta would help with both reads and writes as discussed over in parent JIRA.&lt;/p&gt;</comment>
                            <comment id="14168402" author="stack" created="Sat, 11 Oct 2014 23:22:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; I like your suggestion of a feature branch.&lt;/p&gt;</comment>
                            <comment id="14168476" author="apurtell" created="Sun, 12 Oct 2014 02:31:40 +0000"  >&lt;p&gt;We could try this just with a 0.98 based feature branch and determine if it&apos;s worth pursuing further and elsewhere. Or do a 0.98 and master based feature branch both. I wouldn&apos;t mind refreshing the 0.98 one occasionally until we can make that determination. &lt;/p&gt;</comment>
                            <comment id="14172123" author="toffer" created="Wed, 15 Oct 2014 08:07:35 +0000"  >&lt;p&gt;Thanks for the feedback guys. Just to reiterate, we do have a real need to scale and we will improve/fix what&apos;s necessary be it on hbase, hdfs, or infra to meet that need. So far based on our experimentation and experience from actual large cluster deployments, it is hbase that is preventing us from scaling to our expected needs and splitting meta is the clear solution. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We could try this just with a 0.98 based feature branch and determine if it&apos;s worth pursuing further and elsewhere.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; Thanks for volunteering. Is this a feature branch used to determine whether it is worthwhile to bring back root (and split meta) or a feature branch to determine whether it is feasible to backport it to 0.98? What would be the measure of success? &lt;/p&gt;

&lt;p&gt;IMHO a 0.98 feature branch should be enough as a proving ground for either scenario.&lt;/p&gt;

</comment>
                            <comment id="14172511" author="apurtell" created="Wed, 15 Oct 2014 16:05:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is this a feature branch used to determine whether it is worthwhile to bring back root (and split meta) or a feature branch to determine whether it is feasible to backport it to 0.98? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I assume used to determine whether it is worthwhile to bring back root (and split meta), but done once where you need it for deployment. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What would be the measure of success?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you end up deploying the feature in your production that would go quite a long way, I think. A compelling before-and-after characterization would also be important, e.g. turn off the feature, deploy a large table, shut down, turn on the feature, redeploy the table, enumerate relevant metrics collected during each table deployment, and report back on the differences here. &lt;/p&gt;</comment>
                            <comment id="14180215" author="toffer" created="Wed, 22 Oct 2014 17:32:34 +0000"  >&lt;blockquote&gt;
&lt;p&gt;If you end up deploying the feature in your production that would go quite a long way, I think. A compelling before-and-after characterization would also be important, e.g. turn off the feature, deploy a large table, shut down, turn on the feature, redeploy the table, enumerate relevant metrics collected during each table deployment, and report back on the differences here. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I see, so in essence if it really does enable scaling and whether it is stable? So how do we get this started?&lt;/p&gt;</comment>
                            <comment id="14180776" author="apurtell" created="Thu, 23 Oct 2014 00:14:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;So how do we get this started?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Presuming you&apos;d like to base the work on 0.98 instead of master, If there is a 0.98 patch, we can make a branch (named &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12233&quot; title=&quot;Bring back root table&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12233&quot;&gt;HBASE-12233&lt;/a&gt;-0.98 I suppose), apply it, push it, and go from there. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12674309" name="HBASE-12233.patch" size="250477" author="virag" created="Sat, 11 Oct 2014 00:10:38 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 11 Oct 2014 01:36:40 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 8 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2124n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>