<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:52:20 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1295/HBASE-1295.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1295] Multi data center replication</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1295</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;HBase should consider supporting a federated deployment where someone might have terascale (or beyond) clusters in more than one geography and would want the system to handle replication between the clusters/regions. It would be sweet if HBase had something on the roadmap to sync between replicas out of the box. &lt;/p&gt;

&lt;p&gt;Consider if rows, columns, or even cells could be scoped: local, or global.&lt;/p&gt;

&lt;p&gt;Then, consider a background task on each cluster that replicates new globally scoped edits to peer clusters. The HBase/Bigtable data model has convenient features (timestamps, multiversioning) such that simple exchange of globally scoped cells would be conflict free and would &quot;just work&quot;. Implementation effort here would be in producing an efficient mechanism for collecting up edits from all the HRS and transmitting the edits over the network to peers where they would then be split out to the HRS there. Holding on to the edit trace and tracking it until the remote commits succeed would also be necessary. So, HLog is probably the right place to set up the tee. This would be filtered log shipping, basically.  &lt;/p&gt;

&lt;p&gt;This proposal does not consider transactional tables. For transactional tables, enforcement of global mutation commit ordering would come into the picture if the user  wants the  transaction to span the federation. This should be an optional feature even with transactional tables themselves being optional because of how slow it would be.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12421518">HBASE-1295</key>
            <summary>Multi data center replication</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Mon, 30 Mar 2009 01:55:52 +0000</created>
                <updated>Fri, 15 Feb 2013 00:02:21 +0000</updated>
                            <resolved>Fri, 15 Feb 2013 00:01:10 +0000</resolved>
                                                                        <due></due>
                            <votes>13</votes>
                                    <watches>46</watches>
                                                                                                            <comments>
                            <comment id="12702411" author="jdcryans" created="Fri, 24 Apr 2009 15:24:26 +0000"  >&lt;p&gt;Glad to see that you already did some work on this issue Andrew, once 0.20 is out I&apos;m sure people will look for a feature like that. Regards the slides:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I like the idea of having nominated gateways.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Using the WALs seems the way to go, especially the way you describe how it should be done.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In the case a cluster is down, I guess that would mean the other clusters would have to keep all the WALs until the it is up again. At that moment, it may receive tons of WALs right?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also I was wondering, if you want to add a new cluster, would the way to go be replicating by hand (MR or else) all the data to the other cluster then telling somehow that the clusters have a new peer?&lt;/p&gt;</comment>
                            <comment id="12702647" author="apurtell" created="Sat, 25 Apr 2009 03:14:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;In the case a cluster is down, I guess that would mean the other clusters would have to keep all the WALs until the it is up again. At that moment, it may receive tons of WALs right? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes the effect of a partition and extended outage is a buildup of WALs on the peer clusters, and then a lot of backlog. Let me think about this case and post a revised slide deck.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also I was wondering, if you want to add a new cluster, would the way to go be replicating by hand (MR or else) all the data to the other cluster then telling somehow that the clusters have a new peer?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was anticipating that the cluster would be advertised as a peer, somehow, and then replication would then start. The replicators should add tables and column families to their local schema on demand as the cells are received, maybe additionally also ask the peer about schema details as necessary. Updates to HTD and HCD attributes can be considered another type of edit. Whether or not to bring over existing data would be a deployment/application concern I think and could be handed by a MR export-import job. &lt;/p&gt;</comment>
                            <comment id="12703389" author="ryanobjc" created="Mon, 27 Apr 2009 21:25:19 +0000"  >&lt;p&gt;We could use ZooKeeper paths as a way for replication endpoints to know about each other.&lt;/p&gt;</comment>
                            <comment id="12704213" author="streamy" created="Wed, 29 Apr 2009 17:25:33 +0000"  >&lt;p&gt;This looks great Andrew!  Some comments...&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;What do we do when there are two identical keys in a KeyValue (row, family, column, timestamp) but different values?  That&apos;s actually going to be possible in 0.20 since you can manually set the stamp, will certainly be possible with multi-master replication.  I&apos;m not sure how it&apos;s handled now.  Would depend on logic in both memcache insertion and more importantly compaction, and then how it&apos;s handled when reading.&lt;/li&gt;
	&lt;li&gt;Everything is now just a KeyValue, so that would be what we send to replicas.&lt;/li&gt;
	&lt;li&gt;Thoughts on network partitioning?  I&apos;m assuming you&apos;re referring to partitioning of replica clusters from one another, not within a cluster right?  If so, I guess you&apos;d hang on to WALs as long as you could, eventually a replicated cluster would go into some secondary mode of needing a full sync (when other cluster(s) could no longer hold all WALs, or should we assume hdfs will not fill and just flush, so can always resync with WALs?).  (note: handling of intra-cluster partitions is virtually impossible because of our strong consistency)&lt;/li&gt;
	&lt;li&gt;Regarding SCOPE and setting things as local or replicated.  What do you suspect the precision/resolution of this would be?  Could i have some tables being replicated to some clusters, other tables to others, some to both?&lt;/li&gt;
	&lt;li&gt;Would replicas of tables &lt;em&gt;always&lt;/em&gt; require identical family settings?  For example, I have a cluster of 5 nodes with lots of memory, I want to just replicate a single high-volume, high-read table from my primary large cluster.  But in the small cluster I want to set a TTL of 1 day and also set as in-memory.  This is kind of advanced and special but the ability to do things like that would be very cool, could definitely see us doing something like it were it possible.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ve got a good bit of experience with database replication, did some work in the postgres world on WAL shipping.  Let me know how I can help your effort.&lt;/p&gt;

&lt;p&gt;I agree on your assessment regarding consistency, etc.  It is clear we should be doing an eventual consistency model for replication.  This is one of my favorite topics!&lt;/p&gt;

&lt;p&gt;One thing that&apos;s a bit special is this would make an HBase cluster of clusters a &quot;read-your-writes&quot;-style eventual consistency distribution model (with our strong consistency, partitioned distribution within each individual cluster a la read-your-writes).  That makes a huge difference for us, internally, on many of our data systems.  This may be obvious as we&apos;re just talking about replication here, but something to keep in mind.&lt;/p&gt;</comment>
                            <comment id="12707924" author="viper799" created="Mon, 11 May 2009 07:40:00 +0000"  >&lt;p&gt;I was thanking on this there is some other thing to consider like table splits will the regions be the same on both because there is no guarantee the compactions will happen at the same time or the split will find the same mid key.&lt;/p&gt;

&lt;p&gt;I would thank the master would be the idea process to pull logs a pass to peer master then it can split the logs in to regions and pass the edits on to the servers hosting the regions.&lt;br/&gt;
I would like to see Sequential process of the edits to the peer so everything is in the same order and that&apos;s the way we store the wal&apos;s now.&lt;/p&gt;

&lt;p&gt;I am not sure what the current status of appends on hdfs right now but if we had that 100% working the master could just remember where in the wal it read up to and pull every x secs to see if there are any updates then we would not have to worry about waiting for a log to roll which could be a while in some cases. Waiting for a log to roll for the updates to get pushed to the peers seams like the wrong way to go with this but might be the only way we have now if append is not working right in hdfs.&lt;/p&gt;

&lt;p&gt;As for a first sync for the peers would be hugh saving if we could do a rolling read only mode on the regions and flush the memcache and copy the needed files unlock the region and start the transfer to the peer this would allow one by one copy of the regions to the remote and  it would only be depending on the site-site bandwidth as the bottleneck in the mean time the peer could be holding edits and waiting for all regions to get copied and then start the replay of the logs skipping any edit that is older the the time stamp of the copy. I thank that could be written in the hfile now I thank as meta data.&lt;/p&gt;

&lt;p&gt;Just some suggestions and/or other thoughts&lt;/p&gt;
</comment>
                            <comment id="12708085" author="apurtell" created="Mon, 11 May 2009 15:49:46 +0000"  >&lt;p&gt;@Billy:&lt;/p&gt;

&lt;p&gt;I don&apos;t follow what you are saying about splits. It won&apos;t matter where a table is split. The replication process does not care about such details. It would send edits from the WALs to peers to be applied as if some local HRS is receiving local batchupdates. &lt;/p&gt;

&lt;p&gt;Also, according to the proposal, the master would not be involved in replication. The proposal considers more than one HRS &amp;#8211; self-elected via ZK &amp;#8211; working in a fault tolerant way to forward edits sent by all the other HRS on to the peer cluster. There is no SPOF in the replication process.&lt;/p&gt;

&lt;p&gt;Also, I disagree that waiting for HLog roll is the wrong way to go. There is no reason a log roll cannot happen once per minute or every five minutes or whatever the configured replication period is. Then, we do not care if append or sync is properly implemented in the underlying FS. Given the state of how those issues are progressing in HDFS, we may have a working replication process before HDFS has a working append. &lt;/p&gt;</comment>
                            <comment id="12708090" author="viper799" created="Mon, 11 May 2009 16:03:47 +0000"  >&lt;p&gt;ok I agree now I reread the pdf the only down side is the lag will be the log roll period for the most part but I can live with that if others can.&lt;/p&gt;</comment>
                            <comment id="12708627" author="apurtell" created="Tue, 12 May 2009 21:09:18 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1411?focusedCommentId=12708624&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12708624&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-1411?focusedCommentId=12708624&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12708624&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12724302" author="viper799" created="Thu, 25 Jun 2009 21:16:02 +0000"  >&lt;p&gt;I assume to get a new peer online we would have to have some kind of a read-lock with a flush or &lt;br/&gt;
some kind of catchup mode that will export all data before x timestamp&lt;/p&gt;

&lt;p&gt;Might include something in the pdf on idea you are thanking of on deploying a new peer&lt;/p&gt;

&lt;p&gt;Also will this be a write one place read anywhere replication or write anywhere read anywhere replication&lt;br/&gt;
Will edits be able to write to any site/cluster and get replication to all the peers?&lt;/p&gt;</comment>
                            <comment id="12724315" author="apurtell" created="Thu, 25 Jun 2009 21:56:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;I assume to get a new peer online we would have to have some kind of a read-lock with a flush or some kind of catchup mode that will export all data before x timestamp &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In my opinion, no. Edits are propagated from HLogs. Existing data would not be replicated, therefore. It would be an application specific consideration, and could be accomplished perhaps of forwarding of existing data at the application level via mapreduce transfer job or a background value fetch-and-refresh strategy.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also will this be a write one place read anywhere replication or write anywhere read anywhere replication&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Write anywhere read anywhere&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Will edits be able to write to any site/cluster and get replication to all the peers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.&lt;br/&gt;
But I know that jgray at least would like for some administrative control over the propagation details. This could be accomplished via local settings stored in each cluster&apos;s peer table, supplied as parameters to ADD PEER commands.&lt;/p&gt;</comment>
                            <comment id="12774544" author="apurtell" created="Sat, 7 Nov 2009 03:55:25 +0000"  >&lt;p&gt;Break out replication into contrib/. &lt;/p&gt;

&lt;p&gt;Put hooks into core for holding onto logs after roll, notify watchers via some mechanism (e.g. ZK), and only allow log delete after all watchers acknowledge it. Everything else, move out.&lt;/p&gt;</comment>
                            <comment id="12875503" author="apurtell" created="Fri, 4 Jun 2010 06:25:51 +0000"  >&lt;p&gt;This issue is an umbrella for ongoing work now, move out from any specific release.&lt;/p&gt;</comment>
                            <comment id="13578570" author="apurtell" created="Thu, 14 Feb 2013 18:55:25 +0000"  >&lt;p&gt;Should we close the umbrella? We&apos;ve had a multi data center replication feature of some capability in several major releases now.&lt;/p&gt;</comment>
                            <comment id="13578827" author="jdcryans" created="Fri, 15 Feb 2013 00:01:10 +0000"  >&lt;p&gt;You are right Andrew, closing this.&lt;/p&gt;</comment>
                            <comment id="13578828" author="apurtell" created="Fri, 15 Feb 2013 00:02:21 +0000"  >&lt;p&gt;Thanks for all the work you put into this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jdcryans&quot; class=&quot;user-hover&quot; rel=&quot;jdcryans&quot;&gt;Jean-Daniel Cryans&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12468179">HBASE-2804</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12445062">HBASE-2098</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12411850" name="hbase_repl.3.odp" size="154532" author="apurtell" created="Thu, 25 Jun 2009 19:33:03 +0000"/>
                            <attachment id="12411851" name="hbase_repl.3.pdf" size="251875" author="apurtell" created="Thu, 25 Jun 2009 19:33:03 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12444139">HBASE-2070</subtask>
                            <subtask id="12445592">HBASE-2129</subtask>
                            <subtask id="12431852">HBASE-1728</subtask>
                            <subtask id="12456177">HBASE-2223</subtask>
                            <subtask id="12465318">HBASE-2611</subtask>
                            <subtask id="12468760">HBASE-2822</subtask>
                            <subtask id="12455800">HBASE-2200</subtask>
                            <subtask id="12455718">HBASE-2197</subtask>
                            <subtask id="12455801">HBASE-2201</subtask>
                            <subtask id="12455717">HBASE-2196</subtask>
                            <subtask id="12455716">HBASE-2195</subtask>
                            <subtask id="12432011">HBASE-1733</subtask>
                            <subtask id="12433768">HBASE-1786</subtask>
                            <subtask id="12435533">HBASE-1834</subtask>
                            <subtask id="12432013">HBASE-1735</subtask>
                            <subtask id="12465677">HBASE-2626</subtask>
                            <subtask id="12469395">HBASE-2838</subtask>
                            <subtask id="12523002">HBASE-4401</subtask>
                            <subtask id="12525253">HBASE-4517</subtask>
                            <subtask id="12534564">HBASE-5002</subtask>
                            <subtask id="12536395">HBASE-5096</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 24 Apr 2009 15:24:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>718</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 44 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hcgn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99281</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>