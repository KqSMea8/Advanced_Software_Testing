<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:49:53 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1008/HBASE-1008.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1008] [performance] The replay of logs on server crash takes way too long</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1008</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Watching recovery from a crash on streamy.com where there were 1048 logs and repay is running at rate of about 20 seconds each.  Meantime these regions are not online.  This is way too long to wait on recovery for a live site.  Marking critical.  Performance related so priority and in 0.20.0.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12408841">HBASE-1008</key>
            <summary>[performance] The replay of logs on server crash takes way too long</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jdcryans">Jean-Daniel Cryans</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Wed, 19 Nov 2008 18:27:51 +0000</created>
                <updated>Sun, 13 Sep 2009 22:24:16 +0000</updated>
                            <resolved>Fri, 15 May 2009 13:46:32 +0000</resolved>
                                                    <fixVersion>0.20.0</fixVersion>
                    <fixVersion>0.19.3</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                <comments>
                            <comment id="12649120" author="stack" created="Wed, 19 Nov 2008 18:28:45 +0000"  >&lt;p&gt;Its looking like this replay could take an hour.&lt;/p&gt;</comment>
                            <comment id="12649125" author="stack" created="Wed, 19 Nov 2008 18:55:40 +0000"  >&lt;p&gt;As is, this replay stuff is unacceptable.  Moving into 0.19.0.&lt;/p&gt;

&lt;p&gt;Replay is slowing down with time.&lt;/p&gt;

&lt;p&gt;Replay is not even multithread &amp;#8211; runs in series.&lt;/p&gt;

&lt;p&gt;If master is shutdown during replay, looks like we lose edits (the region files will not be closed).&lt;/p&gt;</comment>
                            <comment id="12649244" author="stack" created="Thu, 20 Nov 2008 02:09:14 +0000"  >&lt;p&gt;Replay of 1084 files took 1 hour, 30 minutes.  During this time, good part of the cluster was down.&lt;/p&gt;</comment>
                            <comment id="12649518" author="stack" created="Thu, 20 Nov 2008 22:54:31 +0000"  >&lt;p&gt;Looking at this, logging needs to be rethought. In bigtable paper, the split is distributed. If we&apos;re going to have 1000 logs, we need to distribute or at least multithread the splitting.&lt;/p&gt;

&lt;p&gt;1. As is, regions starting up expect to find one reconstruction log only.  Need to make it so pick up a bunch of edit logs and it should be fine that logs are elsewhere in hdfs in an output directory written by all split participants whether multithreaded or a mapreduce-like distributed process (Lets write our distributed sort first as a MR so we learn whats involved; distributed sort, as much as possible should use MR framework pieces).  On startup, regions go to this directory and pick up the files written by split participants deleting and clearing the dir when all have been read in.  Making it so can take multiple logs for input, can also make the split process more robust rather than current tenuous process which loses all edits if it doesn&apos;t make it to the end without error.&lt;br/&gt;
2. Each column family rereads the reconstruction log to find its edits.  Need to fix that.  Split can sort the edits by column family so store only reads its edits.&lt;/p&gt;

&lt;p&gt;Too much work involved here to make it into 0.19.  Moving it out.&lt;/p&gt;</comment>
                            <comment id="12649520" author="stack" created="Thu, 20 Nov 2008 22:55:54 +0000"  >&lt;p&gt;Made it a 0.20.0 blocker since its performance issue &amp;#8211; theme for 0.20.0 &amp;#8211; and because as is its liable to lose data.&lt;/p&gt;</comment>
                            <comment id="12658267" author="stack" created="Sat, 20 Dec 2008 05:58:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1048&quot; title=&quot;HLog: Found 0 logs to remove out of total 1450; oldest outstanding seqnum is 162297053 fr om region -ROOT-,,0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1048&quot;&gt;&lt;del&gt;HBASE-1048&lt;/del&gt;&lt;/a&gt; should help; maximum 64 logs allowed before flush of region with oldest edit is forced.&lt;/p&gt;</comment>
                            <comment id="12683995" author="jdcryans" created="Fri, 20 Mar 2009 19:16:46 +0000"  >&lt;p&gt;A multi-threaded version that I run at openplaces was able to process 33 logs in a &quot;record&quot; time and our job didn&apos;t even failed like it usually does : &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2009-03-20 15:06:24,047 INFO org.apache.hadoop.hbase.regionserver.HLog: Splitting 33 log(s) in hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020&lt;br/&gt;
2009-03-20 15:06:24,047 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 1 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237560687378&lt;br/&gt;
2009-03-20 15:06:24,106 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Adding queue for entities,,1236805004423&lt;br/&gt;
2009-03-20 15:06:25,443 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100006 entries&lt;br/&gt;
2009-03-20 15:06:25,459 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 2 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237560790320&lt;br/&gt;
2009-03-20 15:06:25,879 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Adding queue for hbase_types,426,1225564254435&lt;br/&gt;
2009-03-20 15:06:27,101 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100867 entries&lt;br/&gt;
2009-03-20 15:06:27,103 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 3 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561649939&lt;br/&gt;
2009-03-20 15:06:28,694 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 101754 entries&lt;br/&gt;
2009-03-20 15:06:28,696 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 4 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561658514&lt;br/&gt;
2009-03-20 15:06:33,324 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 332220 entries&lt;br/&gt;
2009-03-20 15:06:33,327 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 5 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561669181&lt;br/&gt;
2009-03-20 15:06:38,707 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 349439 entries&lt;br/&gt;
2009-03-20 15:06:38,711 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 6 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561688463&lt;br/&gt;
2009-03-20 15:06:40,922 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 207909 entries&lt;br/&gt;
2009-03-20 15:06:40,925 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 7 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561698495&lt;br/&gt;
2009-03-20 15:06:42,048 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 102829 entries&lt;br/&gt;
2009-03-20 15:06:42,050 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 8 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561703659&lt;br/&gt;
2009-03-20 15:06:44,199 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 204528 entries&lt;br/&gt;
2009-03-20 15:06:44,201 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 9 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561715064&lt;br/&gt;
2009-03-20 15:06:46,875 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 225964 entries&lt;br/&gt;
2009-03-20 15:06:46,878 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 10 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561726289&lt;br/&gt;
2009-03-20 15:06:47,885 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100645 entries&lt;br/&gt;
2009-03-20 15:06:47,887 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 11 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561793535&lt;br/&gt;
2009-03-20 15:06:49,198 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 105605 entries&lt;br/&gt;
2009-03-20 15:06:49,222 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 12 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561854543&lt;br/&gt;
2009-03-20 15:06:50,227 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100363 entries&lt;br/&gt;
2009-03-20 15:06:50,229 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 13 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561941125&lt;br/&gt;
2009-03-20 15:06:51,305 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100648 entries&lt;br/&gt;
2009-03-20 15:06:51,307 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 14 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561953452&lt;br/&gt;
2009-03-20 15:06:53,111 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 159954 entries&lt;br/&gt;
2009-03-20 15:06:53,113 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 15 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237561986701&lt;br/&gt;
2009-03-20 15:06:54,450 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 101025 entries&lt;br/&gt;
2009-03-20 15:06:54,452 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 16 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562003837&lt;br/&gt;
2009-03-20 15:06:55,717 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100449 entries&lt;br/&gt;
2009-03-20 15:06:55,719 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 17 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562016248&lt;br/&gt;
2009-03-20 15:06:56,682 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 101244 entries&lt;br/&gt;
2009-03-20 15:06:56,699 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 18 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562049500&lt;br/&gt;
2009-03-20 15:06:57,749 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100274 entries&lt;br/&gt;
2009-03-20 15:06:57,751 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 19 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562060231&lt;br/&gt;
2009-03-20 15:06:59,012 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 111015 entries&lt;br/&gt;
2009-03-20 15:06:59,014 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 20 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562127374&lt;br/&gt;
2009-03-20 15:06:59,999 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100373 entries&lt;br/&gt;
2009-03-20 15:07:00,001 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 21 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562177943&lt;br/&gt;
2009-03-20 15:07:01,001 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100213 entries&lt;br/&gt;
2009-03-20 15:07:01,003 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 22 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562277537&lt;br/&gt;
2009-03-20 15:07:04,116 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 192782 entries&lt;br/&gt;
2009-03-20 15:07:04,119 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 23 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562304890&lt;br/&gt;
2009-03-20 15:07:05,774 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 101842 entries&lt;br/&gt;
2009-03-20 15:07:05,776 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 24 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562315409&lt;br/&gt;
2009-03-20 15:07:06,843 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 104371 entries&lt;br/&gt;
2009-03-20 15:07:06,845 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 25 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562321140&lt;br/&gt;
2009-03-20 15:07:08,213 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 102252 entries&lt;br/&gt;
2009-03-20 15:07:08,215 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 26 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562342084&lt;br/&gt;
2009-03-20 15:07:09,371 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100330 entries&lt;br/&gt;
2009-03-20 15:07:09,373 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 27 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562347414&lt;br/&gt;
2009-03-20 15:07:12,583 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 258947 entries&lt;br/&gt;
2009-03-20 15:07:12,585 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 28 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562678929&lt;br/&gt;
2009-03-20 15:07:13,926 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100082 entries&lt;br/&gt;
2009-03-20 15:07:13,928 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 29 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562683228&lt;br/&gt;
2009-03-20 15:07:15,216 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100115 entries&lt;br/&gt;
2009-03-20 15:07:15,218 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 30 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562708153&lt;br/&gt;
2009-03-20 15:07:16,418 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100030 entries&lt;br/&gt;
2009-03-20 15:07:16,420 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 31 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562734111&lt;br/&gt;
2009-03-20 15:07:17,783 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100054 entries&lt;br/&gt;
2009-03-20 15:07:17,785 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 32 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562748277&lt;br/&gt;
2009-03-20 15:07:19,902 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Pushed 100116 entries&lt;br/&gt;
2009-03-20 15:07:19,904 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Splitting 33 of 33: hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020/hlog.dat.1237562763336&lt;br/&gt;
2009-03-20 15:07:36,115 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Thread got 49699 to process&lt;br/&gt;
2009-03-20 15:07:36,114 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Thread got 4357600 to process&lt;br/&gt;
2009-03-20 15:07:36,168 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Creating new log file writer for path hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/entities/1200514131/oldlogfile.log and region entities,,1236805004423&lt;br/&gt;
2009-03-20 15:07:36,199 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Creating new log file writer for path hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/entities/862114639/oldlogfile.log and region hbase_types,426,1225564254435&lt;br/&gt;
2009-03-20 15:07:36,599 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Applied 49699 total edits to hbase_types,426,1225564254435 in 484ms&lt;br/&gt;
2009-03-20 15:07:44,734 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cache hit for row &amp;lt;&amp;gt; in tableName .META.: location server 192.168.1.109:62020, location region name .META.,,1&lt;br/&gt;
2009-03-20 15:07:51,633 DEBUG org.apache.hadoop.hbase.regionserver.HLog: Applied 4357600 total edits to entities,,1236805004423 in 15517ms&lt;br/&gt;
2009-03-20 15:07:51,633 INFO org.apache.hadoop.hbase.regionserver.HLog: Took 87586ms&lt;br/&gt;
2009-03-20 15:07:51,650 INFO org.apache.hadoop.hbase.regionserver.HLog: log file splitting completed for hdfs://factory01.lab.mtl:9200/hbase/amsterdam_factory/log_192.168.1.111_1237511553894_62020&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="12684011" author="jdcryans" created="Fri, 20 Mar 2009 19:43:33 +0000"  >&lt;p&gt;Patch I&apos;m currently using.&lt;/p&gt;</comment>
                            <comment id="12685373" author="streamy" created="Fri, 20 Mar 2009 21:55:47 +0000"  >&lt;p&gt;Great work, JD!  I&apos;ve not tested the patch but read through it and looks good.  One thing though... Might be better to have some default setting of a max thread pool size and farm out to them.  In my case, I had &amp;gt;1000 logs to process... Log reprocessing time is when we least want to run into OOME.  With that many java threads, you run into OOME errors either from running out of stack, heap, or even worse you will cause system problems by surpassing the linux user process limit.  In (recent) experiences, java will keep going fine and go past the soft limits (i had hard limit way up to 65535 on nproc) but a bunch of other stuff will stop working (sometimes even being unable to ssh in to that machine or user).&lt;/p&gt;

&lt;p&gt;There&apos;s a nifty java thing, ThreadPoolExecutor:  &lt;a href=&quot;http://java.sun.com/javase/6/docs/api/java/util/concurrent/ThreadPoolExecutor.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://java.sun.com/javase/6/docs/api/java/util/concurrent/ThreadPoolExecutor.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or more simply, could do it in batches of 50 or so at a time.&lt;/p&gt;</comment>
                            <comment id="12689053" author="stack" created="Wed, 25 Mar 2009 09:53:42 +0000"  >&lt;p&gt;J-D, this patch reads all edits into memory.  I suppose thats OK?  IIRC, the log is rotated after N edits rather than after its grown to a particular size.  If the log individual edits are very large, we could blow out the heap?&lt;/p&gt;

&lt;p&gt;Currently number of threads == number of regions in particular commit log?&lt;/p&gt;

&lt;p&gt;You might try setting bigger buffer on SequenceFile.Reader?  Might make things run faster.&lt;/p&gt;</comment>
                            <comment id="12689459" author="schubertzhang" created="Thu, 26 Mar 2009 12:02:07 +0000"  >&lt;p&gt;I am using 0.19.1 with this patch for 4 days. It is fine.&lt;br/&gt;
I also want ask questions like stack&apos;s.&lt;/p&gt;</comment>
                            <comment id="12704896" author="stack" created="Fri, 1 May 2009 05:01:32 +0000"  >&lt;p&gt;Did you say in the meeting that you were going to test this on openspaces J-D?  Assigning you under that assumption.  Assign to no-one if I have it wrong.&lt;/p&gt;</comment>
                            <comment id="12704974" author="jdcryans" created="Fri, 1 May 2009 12:31:19 +0000"  >&lt;p&gt;This patch was applied on openplaces (not spaces stack &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) main cluster (which runs on a 0.19 branch) the day I posted the v2 patch. Didn&apos;t get any bug. I will commit this patch with a bounded number of threads next week when I come back from vacation.&lt;/p&gt;</comment>
                            <comment id="12706560" author="jdcryans" created="Wed, 6 May 2009 19:30:57 +0000"  >&lt;p&gt;This third version of the patch adds a bounded thread pool.&lt;/p&gt;</comment>
                            <comment id="12708760" author="stack" created="Wed, 13 May 2009 05:28:21 +0000"  >&lt;p&gt;J-D, what you think of suggestion over here: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1394?focusedCommentId=12708663&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12708663?&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-1394?focusedCommentId=12708663&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12708663?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;m not going to bother with pool of writers for 0.20.0 &amp;#8211; logging is back to fast enough and besides, looks like we could do with some friction since its so easy overrunning compactions &amp;#8211; but the bit where we add timestamp to HLogKey and we then run multiple threads in master splitting up the N logs, hows that sounds?  Could cut recover in 3 or 4 or ten even if we ran ten concurrent splitter threads in master?&lt;/p&gt;</comment>
                            <comment id="12708984" author="jdcryans" created="Wed, 13 May 2009 16:06:18 +0000"  >&lt;p&gt;Sounds great. Just to be sure that I understand what you wrote, you basically think that we should reverse the way the latest patch works? Multi threaded reads and a single writer?&lt;/p&gt;</comment>
                            <comment id="12708998" author="stack" created="Wed, 13 May 2009 16:36:11 +0000"  >&lt;p&gt;I&apos;ve changed my mind after reading this patch.  This patch looks great and the amount of splitting processed above &amp;#8211; 3M in ~90seconds &amp;#8211; is good next place to go regards log recovery.&lt;/p&gt;

&lt;p&gt;+1 on commit but make the upper bound on threads a configuration (doesn&apos;t have to be in hadoop-default.xml &amp;#8211; let fellas read code to find it).&lt;/p&gt;

&lt;p&gt;Meantime, I&apos;ll go work elsewhere on bounding size of logs so what shows up in splitlog can be expected to be of reasonable size &amp;#8211; not of a size that will blow out mem.&lt;/p&gt;</comment>
                            <comment id="12709491" author="stack" created="Thu, 14 May 2009 17:51:47 +0000"  >&lt;p&gt;J-D. Any chance of backporting this too for 0.19.3?&lt;/p&gt;</comment>
                            <comment id="12709497" author="jdcryans" created="Thu, 14 May 2009 17:58:13 +0000"  >&lt;p&gt;No problem.&lt;/p&gt;</comment>
                            <comment id="12709585" author="jdcryans" created="Thu, 14 May 2009 20:54:51 +0000"  >&lt;p&gt;Patches for 0.19 and trunk with the number of threads as a constant and with more javadoc comments. Would need a +1 from someone who tested it please.&lt;/p&gt;</comment>
                            <comment id="12709604" author="stack" created="Thu, 14 May 2009 21:48:30 +0000"  >&lt;p&gt;I tested it and it works.&lt;/p&gt;

&lt;p&gt;Please fix following when you apply:&lt;/p&gt;

&lt;p&gt;There are two lines emitted when HLog is done:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2009-05-14 21:40:08,467 [HMaster] INFO org.apache.hadoop.hbase.regionserver.HLog: Took 41393ms
2009-05-14 21:40:09,984 [HMaster] INFO org.apache.hadoop.hbase.regionserver.HLog: log file splitting completed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; hdfs:&lt;span class=&quot;code-comment&quot;&gt;//aa0-000-12.u.powerset.com:9000/hbasetrunk2/.logs/aa0-000-15.u.powerset.com_1242336420277_60021&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can the time taken be added to the &quot;file splitting completed&quot; line?&lt;/p&gt;

&lt;p&gt;I think you can name executor threads..... would help with log lines like this:&lt;/p&gt;

&lt;p&gt;2009-05-14 21:40:02,309 &lt;span class=&quot;error&quot;&gt;&amp;#91;pool-1-thread-2&amp;#93;&lt;/span&gt; DEBUG org.apache.hadoop.hbase.regionserver.HLog: Thread got 62947 to process&lt;/p&gt;

&lt;p&gt;Who are the edits for?  Add in region name I&apos;d say. &lt;/p&gt;

&lt;p&gt;Otherwise, looks good.&lt;/p&gt;

&lt;p&gt;We still need to rewrite it &amp;#8211; if crash during this processing we&apos;re hosed.. but this is a nice speedup.  I&apos;d say up the default number of threads J-D from 3 to 5 or 10 even?&lt;/p&gt;

&lt;p&gt;Good stuff.&lt;/p&gt;

&lt;p&gt;+1 after making above logging hcanges.&lt;/p&gt;
</comment>
                            <comment id="12709837" author="jdcryans" created="Fri, 15 May 2009 13:46:32 +0000"  >&lt;p&gt;Committed in branch and trunk with Stack&apos;s suggestions.&lt;/p&gt;</comment>
                            <comment id="12710074" author="stack" created="Sat, 16 May 2009 05:46:22 +0000"  >&lt;p&gt;J-D, is it true that we read in all the logs before we start splitting?  It looks that way after going back to the patch.  If so, I missed that &amp;#8211; my fault &amp;#8211; and I think this a prob.&lt;/p&gt;

&lt;p&gt;Theoretically, we can have at most 64 logs under a regionserver, each of which has ~64MB of edits.  Thats 4G of edits that we need to pull in before we start processing.&lt;/p&gt;

&lt;p&gt;Can we not run the writer threads every Nth file read, say, every 5 or 10 even?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="12710117" author="jdcryans" created="Sat, 16 May 2009 14:38:23 +0000"  >&lt;p&gt;Stack, we do read all into memory. I guess we can do what you described. I will open a new jira for that.&lt;/p&gt;</comment>
                            <comment id="12710435" author="jimk" created="Mon, 18 May 2009 18:22:42 +0000"  >&lt;p&gt;I&apos;m not a big fan of having to read all the logs into memory.&lt;/p&gt;

&lt;p&gt;My suggestion would be for each unique region in the HLog(s), create a blocking queue and a thread that will&lt;br/&gt;
dequeue entries and write them directly to the log file. Then you have one thread doing the reading and multiple&lt;br/&gt;
threads writing, and the memory footprint is reduced significantly.&lt;/p&gt;

&lt;p&gt;Make sense?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12402687" name="1008-v2.patch" size="11604" author="jdcryans" created="Fri, 20 Mar 2009 19:43:33 +0000"/>
                            <attachment id="12407383" name="hbase-1008-3.patch" size="11920" author="jdcryans" created="Wed, 6 May 2009 19:30:57 +0000"/>
                            <attachment id="12408185" name="hbase-1008-v4-0.19.patch" size="12567" author="jdcryans" created="Thu, 14 May 2009 20:54:51 +0000"/>
                            <attachment id="12408186" name="hbase-1008-v4.patch" size="12056" author="jdcryans" created="Thu, 14 May 2009 20:54:51 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 20 Mar 2009 19:16:46 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31946</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 31 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0has7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99009</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>