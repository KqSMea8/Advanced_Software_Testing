<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:36:09 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-12757/HBASE-12757.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-12757] MR map&apos;s input rowkey out of range of current Region </title>
                <link>https://issues.apache.org/jira/browse/HBASE-12757</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I excute mapreduce scan all table, sometimes map input value of rowkey is out of range on current Region (get from inputsplit ).&lt;br/&gt;
this mabey  lost data or get unused data.&lt;br/&gt;
ps. I want  to use ImportTSV translate table.....&lt;br/&gt;
eg. &lt;br/&gt;
location=datanode11,start_row=D9CB114FD09A82A3_0000000000000000_m_43DAAA689D4AFC86 ,rowkey=D323E1D0A51E5185_0000000000000000_m_75686B8924108044 ,end_row=DB0C4FC44E6D80C1_0000000000000000_m_E956CC65322BA3E5&lt;/p&gt;</description>
                <environment>&lt;p&gt;hadoop 1.1.2, r1440782&lt;br/&gt;
hbase 0.94.7&lt;br/&gt;
linux  2.6.32-279.el6.x86_64&lt;/p&gt;</environment>
        <key id="12763620">HBASE-12757</key>
            <summary>MR map&apos;s input rowkey out of range of current Region </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="6">Invalid</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="pangxiaoxi">pangxiaoxi</reporter>
                        <labels>
                    </labels>
                <created>Wed, 24 Dec 2014 03:42:38 +0000</created>
                <updated>Tue, 6 Jan 2015 16:59:37 +0000</updated>
                            <resolved>Tue, 6 Jan 2015 16:59:37 +0000</resolved>
                                    <version>0.94.7</version>
                                                    <component>Client</component>
                    <component>hbase</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="14257934" author="pangxiaoxi" created="Wed, 24 Dec 2014 04:05:55 +0000"  >&lt;p&gt;protected void setup(Context context){&lt;br/&gt;
               TableSplit split = (TableSplit)context.getInputSplit();&lt;br/&gt;
		if(split != null)&lt;/p&gt;
{
			start_row = split.getStartRow();
			end_row = split.getEndRow();
                       .....
                }
&lt;p&gt;}&lt;br/&gt;
public void map(ImmutableBytesWritable row,Result columns ,Context context) {&lt;br/&gt;
        ...&lt;br/&gt;
	byte[] rowkey = row.get();&lt;br/&gt;
	if(Bytes.compareTo(start_row, rowkey) &amp;gt; 0 || Bytes.compareTo(end_row, rowkey) &amp;lt; 0){&lt;br/&gt;
	    TableSplit split = (TableSplit)context.getInputSplit();&lt;br/&gt;
	    if(split != null)&lt;/p&gt;
{
                System.err.println(String.format(&quot;location=%1$s,start_row=%2$s ,rowkey=%3$s ,end_row=%4$s&quot;,
								split.getRegionLocation() ,HbaseUtil.printRowkey(split.getStartRow()),
								HbaseUtil.printRowkey(rowkey),HbaseUtil.printRowkey(split.getEndRow())));
		}
&lt;p&gt;           }    &lt;br/&gt;
  .................    &lt;/p&gt;</comment>
                            <comment id="14258408" author="apurtell" created="Wed, 24 Dec 2014 17:55:30 +0000"  >&lt;p&gt;Rather than pasting code, please describe what you think is the problem. &lt;/p&gt;</comment>
                            <comment id="14258616" author="pangxiaoxi" created="Thu, 25 Dec 2014 01:37:40 +0000"  >&lt;p&gt;I have a table A, I want translate A to Table B, A &amp;amp; B has different rowkey ,but most of values are same;&lt;br/&gt;
I write a Custom MR like ImportTSV, but my InputFormat is table A.&lt;br/&gt;
When I excute MR , some map() input rowkey is out of range on current Region (get it from inputsplit ).&lt;br/&gt;
this mabey lost data or get unused data. &lt;/p&gt;

&lt;p&gt;code :&lt;br/&gt;
=======================================&lt;br/&gt;
public class RebuildTable{&lt;/p&gt;

&lt;p&gt;		public final static class RebuildMapper extends TableMapper&amp;lt;ImmutableBytesWritable, Writable&amp;gt;{&lt;br/&gt;
		public boolean isOutputRel = true;&lt;br/&gt;
		public boolean isOutputData = true;&lt;br/&gt;
		private static byte[] DOC_FAMILY = Bytes.toBytes(&quot;doc&quot;);&lt;br/&gt;
		private static byte[] URL_QUALIFIER= Bytes.toBytes(&quot;url&quot;);&lt;br/&gt;
		private static byte[] FWDURL_QUALIFIER = Bytes.toBytes(&quot;forward_url&quot;);&lt;br/&gt;
		private static byte[] PKEY_QUALIFIER = Bytes.toBytes(&quot;rel_pkey&quot;);&lt;br/&gt;
		private static byte[] DATAKEY_QUALIFIER = Bytes.toBytes(&quot;data_key&quot;);&lt;br/&gt;
		private static byte[] TN_QUALIFIER = Bytes.toBytes(&quot;table_name&quot;);&lt;br/&gt;
		private static byte[] CURL_QUALIFIER = Bytes.toBytes(&quot;c_url&quot;);&lt;br/&gt;
		private Logger logger = LoggerFactory.getLogger(RebuildMapper.class);&lt;br/&gt;
		protected int type = -1;&lt;br/&gt;
		protected long count = 0;&lt;br/&gt;
		private HTable relTable = null;&lt;br/&gt;
		private String table_name = null;&lt;br/&gt;
		private String table_ng_name = null;&lt;br/&gt;
		private String location = null;&lt;br/&gt;
		private byte[] start_row = null;&lt;br/&gt;
		private byte[] end_row = null;&lt;/p&gt;

&lt;p&gt;		@Override &lt;br/&gt;
		protected void setup(Context context){&lt;br/&gt;
			type = Integer.valueOf(context.getConfiguration().get(&quot;job.hylanda.data_type&quot;));&lt;br/&gt;
			//&#21021;&#22987;&#21270;&#19968;&#20010;xxxx_rel_ng&#34920;&#23545;&#35937;&lt;br/&gt;
			try {&lt;br/&gt;
				System.out.println( table_name + &quot;=&amp;gt;&quot; + table_ng_name);&lt;br/&gt;
				TableSplit split = (TableSplit)context.getInputSplit();&lt;br/&gt;
				if(split != null)&lt;/p&gt;
{
					start_row = split.getStartRow();
					end_row = split.getEndRow();
					System.out.println( split.toString());
					location = split.getRegionLocation();
					System.out.println(String.format(&quot;location=%1$s,start_row=%2$s , end_row=%3$s&quot;,
							location ,HbaseUtil.printRowkey(start_row),HbaseUtil.printRowkey(end_row)));
				}
&lt;p&gt;				isOutputRel = context.getConfiguration().getBoolean(&quot;conf.output_rel&quot;, true);&lt;br/&gt;
				isOutputData= context.getConfiguration().getBoolean(&quot;conf.output_data&quot;, true);&lt;br/&gt;
				table_name = context.getConfiguration().get(&quot;conf.table_name&quot;);&lt;br/&gt;
				table_ng_name = context.getConfiguration().get(&quot;conf.table_ng_name&quot;);&lt;br/&gt;
				if(isOutputRel)&lt;/p&gt;
{
					Configuration conf = new Configuration(context.getConfiguration());
					conf.setLong(&quot;hbase.htable.threads.keepalivetime&quot;, 180);
					relTable = new HTable(conf,context.getConfiguration().get(&quot;conf.reltable_name&quot;));
					relTable.setAutoFlush(false);
				}
&lt;p&gt;			} catch (Exception e) &lt;/p&gt;
{
				// TODO &#33258;&#21160;&#29983;&#25104;&#30340; catch &#22359;
				logger.error(&quot;setup ex:&quot;+e);
				e.printStackTrace();
			}
&lt;p&gt;		}&lt;br/&gt;
		@Override&lt;br/&gt;
		protected void cleanup(Context context){&lt;br/&gt;
			if(relTable != null){&lt;br/&gt;
				try &lt;/p&gt;
{
					relTable.flushCommits();
					relTable.close();
				}
&lt;p&gt; catch (IOException e) &lt;/p&gt;
{
					// TODO &#33258;&#21160;&#29983;&#25104;&#30340; catch &#22359;
					e.printStackTrace();
				}
&lt;p&gt;			}&lt;br/&gt;
		}&lt;br/&gt;
		@Override&lt;br/&gt;
		public void map(ImmutableBytesWritable row,Result columns ,Context context) {&lt;br/&gt;
			try{&lt;br/&gt;
				byte[] rowkey = row.get();&lt;br/&gt;
				if(Bytes.compareTo(start_row, rowkey) &amp;gt; 0 || Bytes.compareTo(end_row, rowkey) &amp;lt; 0){ //test code&lt;br/&gt;
					TableSplit split = (TableSplit)context.getInputSplit();&lt;br/&gt;
					if(split != null)&lt;/p&gt;
{
						SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
						System.err.println(String.format(&quot;%5$s\tlocation=%1$s,start_row=%2$s ,rowkey=%3$s ,end_row=%4$s&quot;,
								split.getRegionLocation() ,HbaseUtil.printRowkey(split.getStartRow()),
								HbaseUtil.printRowkey(rowkey),HbaseUtil.printRowkey(split.getEndRow()),sdf.format(new Date())));
					}
&lt;p&gt;					return;&lt;br/&gt;
				}&lt;/p&gt;

&lt;p&gt;				if(count++ % 10000 == 0) &lt;/p&gt;
{
					logger.info(&quot;Scan=&quot;+ count + &quot; ;rowkey=&quot; + HbaseUtil.printRowkey(rowkey));
				}
&lt;p&gt;				String url = Bytes.toString(columns.getValue(DOC_FAMILY , URL_QUALIFIER));&lt;br/&gt;
				long rcrc = GenUrlCrc64.GenReverseCrc64Long(url); //gen 64-bit crc &lt;br/&gt;
				Bytes.putLong(rowkey, 0, rcrc);&lt;br/&gt;
				Put put = new Put(rowkey); //&#20889;ng&#34920;&#30340;put&lt;br/&gt;
				List&amp;lt;Put&amp;gt; puts = new ArrayList&amp;lt;Put&amp;gt;(); //&#20889;rel&#34920;&#30340;puts&lt;br/&gt;
				if(type == weibo_type  ){&lt;br/&gt;
					for(KeyValue kv :columns.list()){&lt;br/&gt;
						if(Bytes.toString(kv.getQualifier()).equals(&quot;rel_pkey&quot;))&lt;/p&gt;
{
							byte[] pkey = columns.getValue(DOC_FAMILY , PKEY_QUALIFIER);
							String pkurl = Bytes.toString(columns.getValue(DOC_FAMILY , FWDURL_QUALIFIER)); //&#21462;&#21407;&#20018;
							Bytes.putLong(pkey, 0, GenUrlCrc64.GenReverseCrc64Long(pkurl));
							put.add(kv.getFamily(), kv.getQualifier(), kv.getTimestamp(), pkey);
							Put put_rel = new Put(Bytes.toBytes(GenUrlCrc64.GenCrc64Long(pkurl)));
							put_rel.add(DOC_FAMILY , Bytes.add(Bytes.toBytes(&quot;rel_&quot;), rowkey),Bytes.toBytes(table_ng_name));
							puts.add(put_rel);
						}
&lt;p&gt;else&lt;/p&gt;
{
							put.add(kv.getFamily(), kv.getQualifier(), kv.getTimestamp(), kv.getValue());	
						}&lt;br/&gt;
					}&lt;br/&gt;
					&lt;br/&gt;
				}else if(type == ebusiness_type){&lt;br/&gt;
					for(KeyValue kv :columns.list()){&lt;br/&gt;
						if(Bytes.toString(kv.getQualifier()).equals(&quot;rel_pkey&quot;)){
							String pkurl = Bytes.toString(columns.getValue(DOC_FAMILY , CURL_QUALIFIER)); //&#21462;&#21407;&#20018;
							byte[] pkey = columns.getValue(DOC_FAMILY , PKEY_QUALIFIER);
							Bytes.putLong(pkey, 0, GenUrlCrc64.GenReverseCrc64Long(pkurl));
							put.add(kv.getFamily(), kv.getQualifier(), kv.getTimestamp(), pkey);
							//
							Put put_rel = new Put(Bytes.toBytes(GenUrlCrc64.GenCrc64Long(pkurl)));
							put_rel.add(DOC_FAMILY , Bytes.add(Bytes.toBytes(&quot;rel_&quot;), rowkey),Bytes.toBytes(table_ng_name));
							puts.add(put_rel);
						}else{
							put.add(kv.getFamily(), kv.getQualifier(), kv.getTimestamp(), kv.getValue());	
						}
&lt;p&gt;					}&lt;br/&gt;
				}else{&lt;br/&gt;
					for(KeyValue kv :columns.list())&lt;/p&gt;
{
						put.add(kv.getFamily(), kv.getQualifier(), kv.getTimestamp(), kv.getValue());
					}
&lt;p&gt;				}&lt;br/&gt;
				while(isOutputData){&lt;br/&gt;
					try&lt;/p&gt;
{
						context.write(new ImmutableBytesWritable(rowkey), put);
						break;
					}
&lt;p&gt;catch(Exception ex)&lt;/p&gt;
{
						logger.error(&quot;context write ex:&quot;+ex);
					}
&lt;p&gt;				}&lt;br/&gt;
				//&#20889;rel&#34920;&#22522;&#26412;&#20449;&#24687;&lt;br/&gt;
				byte[] urlcrc = Bytes.tail(rowkey, 8);&lt;br/&gt;
				Put putRel = new Put(urlcrc);&lt;br/&gt;
				putRel.add(DOC_FAMILY , DATAKEY_QUALIFIER, rowkey);&lt;br/&gt;
				putRel.add(DOC_FAMILY , TN_QUALIFIER, Bytes.toBytes(table_ng_name));&lt;br/&gt;
				puts.add(putRel);&lt;br/&gt;
				while(isOutputRel &amp;amp;&amp;amp; relTable != null){&lt;br/&gt;
					try&lt;/p&gt;
{
						relTable.put(puts);
						break;
					}
&lt;p&gt;catch(Exception ex)&lt;/p&gt;
{
						logger.error(&quot;put ex:&quot;+ex.toString());
					}
&lt;p&gt;				}&lt;br/&gt;
				context.getCounter(&quot;Rebuild&quot;,&quot;success&quot;).increment(1);&lt;br/&gt;
			}catch(Exception ex)&lt;/p&gt;
{
				System.err.println(&quot;Err:&quot;+ex +&quot;,row:&quot; + Bytes.toStringBinary(row.get()));
				context.getCounter(&quot;Rebuild&quot;,&quot;failed&quot;).increment(1);
			}
&lt;p&gt;		}&lt;br/&gt;
	}&lt;/p&gt;

&lt;p&gt;		public static void main(String[] argv)&lt;/p&gt;
{
			
			String hdfsip = &quot;10.0.5.34&quot;;
			String zkIps = &quot;10.0.5.34&quot;;
			Configuration conf = new Configuration();
			System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);
			Configuration hbaseConfiguration = HBaseConfiguration.create(conf);
			hbaseConfiguration.set(&quot;mapred.job.priority&quot;, JobPriority.HIGH.name());
			hbaseConfiguration.set(&quot;fs.default.name&quot;, &quot;hdfs://&quot; + hdfsip + &quot;:9000&quot;);
			hbaseConfiguration.set(&quot;mapred.job.tracker&quot;, hdfsip + &quot;:9001&quot;);
			hbaseConfiguration.set(&quot;hbase.zookeeper.quorum&quot;, zkIps);
			hbaseConfiguration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);
			hbaseConfiguration.set(&quot;mapred.reduce.tasks.speculative.execution&quot;, &quot;false&quot;);
			hbaseConfiguration.set(&quot;mapred.map.tasks.speculative.execution&quot;, &quot;false&quot;);
			hbaseConfiguration.set(&quot;mapred.job.queue.name&quot;, &quot;default&quot;);
			hbaseConfiguration.set(&quot;mapred.child.java.opts&quot;,&quot;-Xmx1024m&quot;);
			hbaseConfiguration.setLong(HConstants.HBASE_REGIONSERVER_LEASE_PERIOD_KEY, 180000);  
			hbaseConfiguration.setLong(&quot;dfs.socket.timeout&quot;, 180000);
			hbaseConfiguration.setLong(&quot;hbase.htable.threads.keepalivetime&quot;,180);
			
			
			String tablename = &quot;news_201411&quot;;
			String tablename_ng = &quot;news_201411_ng&quot;;
			String tablename_rel_ng = &quot;news_rel_ng&quot;;
			HbaseUtil.createRelTable(hbaseConfiguration, relngName); //create another table
			
			hbaseConfiguration.set(&quot;conf.table_name&quot;, tablename);
			hbaseConfiguration.set(&quot;conf.table_ng_name&quot;, tablename_ng);
			hbaseConfiguration.set(&quot;conf.reltable_name&quot;, tablename_rel_ng);
			
			
			Job job = new Job(hbaseConfiguration,&quot;job_rebuilder_&quot;+tablename);
			job.setJarByClass(RebuildMapper.class);
		
			List&amp;lt;Scan&amp;gt; scans = new ArrayList&amp;lt;Scan&amp;gt;();
			Scan scan = new Scan();
			scan.setCacheBlocks(false);
			scan.setAttribute(Scan.SCAN_ATTRIBUTES_TABLE_NAME, Bytes.toBytes(tablename));
			scan.setCaching(100);
			scans.add(scan);
			TableMapReduceUtil.initTableMapperJob(scans, RebuildMapper.class,ImmutableBytesWritable.class ,Put.class,job);
			job.setReducerClass(PutSortReducer.class);
			String hfileOutPath = &quot;/user/hadoop/&quot;+tablename_ng ;
			Path outputDir = new Path(hfileOutPath);
	    FileOutputFormat.setOutputPath(job, outputDir);
	    job.setMapOutputKeyClass(ImmutableBytesWritable.class);
	    job.setMapOutputValueClass(Put.class);
	    
	    HTable table = new HTable(conf,tablename);
	    HFileOutputFormat.configureIncrementalLoad(job, table);

			System.out.println(&quot;set job begin,&apos;&quot; + tablename + &quot;&apos; =&amp;gt; &apos;&quot; + tablename_ng + &quot;&apos;&quot;);
			boolean bCompleted = job.waitForCompletion(true);

		}
&lt;p&gt;		&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;</comment>
                            <comment id="14266394" author="apurtell" created="Tue, 6 Jan 2015 16:59:37 +0000"  >&lt;p&gt;Thanks. The JIRA system is for tracking project development so I&apos;m going to resolve this issue as Invalid. This doesn&apos;t mean you have done something wrong. For questions / assistance / user support, please email user@hbase.apache.org. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 24 Dec 2014 17:55:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 49 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i23r2n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>