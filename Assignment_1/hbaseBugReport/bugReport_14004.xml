<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:48:48 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-14004/HBASE-14004.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-14004] [Replication] Inconsistency between Memstore and WAL may result in data in remote cluster that is not in the origin</title>
                <link>https://issues.apache.org/jira/browse/HBASE-14004</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Looks like the current write path can cause inconsistency between memstore/hfile and WAL which cause the slave cluster has more data than the master cluster.&lt;/p&gt;

&lt;p&gt;The simplified write path looks like:&lt;br/&gt;
1. insert record into Memstore&lt;br/&gt;
2. write record to WAL&lt;br/&gt;
3. sync WAL&lt;br/&gt;
4. rollback Memstore if 3 fails&lt;/p&gt;

&lt;p&gt;It&apos;s possible that the HDFS sync RPC call fails, but the data is already  (may partially) transported to the DNs which finally get persisted. As a result, the handler will rollback the Memstore and the later flushed HFile will also skip this record.&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12841977">HBASE-14004</key>
            <summary>[Replication] Inconsistency between Memstore and WAL may result in data in remote cluster that is not in the origin</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="heliangliang">He Liangliang</reporter>
                        <labels>
                            <label>replication</label>
                            <label>wal</label>
                    </labels>
                <created>Wed, 1 Jul 2015 12:32:51 +0000</created>
                <updated>Tue, 1 Nov 2016 12:39:50 +0000</updated>
                                                                            <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>21</watches>
                                                                <comments>
                            <comment id="14977691" author="chenheng" created="Wed, 28 Oct 2015 04:18:45 +0000"  >&lt;p&gt;I have a proposal.&lt;br/&gt;
When WAL sync failed, and master has to rollback Memstore.  &lt;br/&gt;
We can record this action in ZK or system table,  meanwhile all slaves should sync this action and modify its memstore.&lt;/p&gt;

&lt;p&gt;Any concerns? &lt;/p&gt;
</comment>
                            <comment id="14977696" author="chenheng" created="Wed, 28 Oct 2015 04:25:00 +0000"  >&lt;blockquote&gt;
&lt;p&gt;When WAL sync failed, and master has to rollback Memstore. &lt;br/&gt;
We can record this action in ZK or system table, meanwhile all slaves should sync this action and modify its memstore.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Of course, before slave rollback memstore, it should check the timestamp of rollback action and WALs to replay.&lt;/p&gt;</comment>
                            <comment id="14977769" author="apache9" created="Wed, 28 Oct 2015 05:35:18 +0000"  >&lt;p&gt;The problem here not only effects replication.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As a result, the handler will rollback the Memstore and the later flushed HFile will also skip this record.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What if the regionserver crashed before flushing HFile? I think the record will come back since it has already been persisted in WAL.&lt;/p&gt;

&lt;p&gt;Add a marker maybe a solution, but you need to check the marker everywhere when replaying WAL, and you still need to deal with the failure when placing marker... I do not think it is easy to do...&lt;/p&gt;

&lt;p&gt;The basic problem here is we may have inconsistency between memstore and WAL when we fail to sync WAL.&lt;br/&gt;
A simple solution is killing the regionserver when we fail to sync WAL which means we will never rollback memstore but reconstruct it using WAL. We can make sure there is no difference between memstore and WAL under this situation.&lt;br/&gt;
If we want to keep regionserver alive when syncing failed, then I think we need to find the real result of the sync operation. Maybe we could close the WAL file and check its length? Of course, if we have lost the connection to namenode, I think there is no simple solution other than killing the regionserver...&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="14977841" author="chenheng" created="Wed, 28 Oct 2015 06:26:54 +0000"  >&lt;blockquote&gt;
&lt;p&gt;What if the regionserver crashed before flushing HFile? I think the record will come back since it has already been persisted in WAL.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Indeed, as current logic, when sync failed, the memstore will rollback,  and client will be told &apos;write failed&apos;.  &lt;br/&gt;
And if RS crash before memstore flush,  the record will came back after WAL replayed.  So client will found write action is not failed, it is inconsistent!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Add a marker maybe a solution, but you need to check the marker everywhere when replaying WAL, and you still need to deal with the failure when placing marker... &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed!&lt;/p&gt;

&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; for your reply!&lt;/p&gt;
</comment>
                            <comment id="14977908" author="anoop.hbase" created="Wed, 28 Oct 2015 07:46:15 +0000"  >&lt;p&gt;In case when RS is killed and so later WAL replay will come in and possibly add back the Mutation, then also the inconsistency with client reply will come in. Client would have told that the mutation failed!&lt;/p&gt;</comment>
                            <comment id="14978004" author="carp84" created="Wed, 28 Oct 2015 09:04:52 +0000"  >&lt;blockquote&gt;
&lt;p&gt;It&apos;s possible that the HDFS sync RPC call fails, but the data is already (may partially) transported to the DNs which finally get persisted&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Does this really happen on your cluster or it&apos;s just an assumption &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=heliangliang&quot; class=&quot;user-hover&quot; rel=&quot;heliangliang&quot;&gt;He Liangliang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt;? &lt;/p&gt;

&lt;p&gt;From what I could see we will get EOFException when trying to read partially written entries (those larger than 64k and split into multiple packets and some packets got lost due to network or dfs error) and these entries will be abandoned during replication.&lt;/p&gt;

&lt;p&gt;If in real case the sync RPC call fails but data somehow persisted, it&apos;s more like a bug in HDFS or error usage in HBase (like not handling all possible exceptions, etc.). I&apos;d suggest to dig deeper into this first before any further discussion.&lt;/p&gt;</comment>
                            <comment id="14978038" author="chenheng" created="Wed, 28 Oct 2015 09:19:20 +0000"  >&lt;p&gt;On the assumption that HDFS sync successfully and do response,  &lt;br/&gt;
but RS NOT get this response because of network disconnect.  As for RS, sync timeout but data persisted on HDFS.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;Yu Li&lt;/a&gt;  &lt;/p&gt;

&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; mentioned, &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A simple solution is killing the regionserver when we fail to sync WAL which means we will never rollback memstore but reconstruct it using WAL.&lt;/p&gt;&lt;/blockquote&gt; 
&lt;p&gt;It may cause two problems.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;we may do mutation twice,  (it seems no problems if mutation is incr/append in current logic?)&lt;/li&gt;
	&lt;li&gt;All RS will be killed if network between RS and NN disconnected shortly.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14978108" author="apache9" created="Wed, 28 Oct 2015 10:00:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;Yu Li&lt;/a&gt; This is an inherent problem of RPC based systems due to temporary network failure. HBase use &lt;tt&gt;hflush&lt;/tt&gt; to sync WAL,  I do not know the details that if hflush will call namenode to update length, but in any case, the last RPC call could fail at client side but succeed at server side(network failure when writing return value back).&lt;/p&gt;

&lt;p&gt;And sure, this should be a bug in HBase. I checked the code, if an exception is thrown from hflush, &lt;tt&gt;FSHLog.SyncRunner&lt;/tt&gt; simply passes it to upper layer. So it could happen that hflush is succeeded at HDFS, but HBase think it is failed and cause inconsistency.&lt;/p&gt;

&lt;p&gt;I think we need to find a way to make sure whether the WAL is actually persisted at HDFS. And if DFSClient already has retry, then I think killing regionserver is enough? Any suggestions &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;Yu Li&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="14999866" author="carp84" created="Wed, 11 Nov 2015 03:10:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt;&lt;br/&gt;
Sorry for the late response, somehow didn&apos;t notice the jira update mail. See your point now, thanks for the explanation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I do not know the details that if hflush will call namenode to update length&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Checking the code, SyncRunner will call ProtobufLogWriter#sync and finally call DataOutputStream#hflush, there we could know it will only call nn to update length when new block created, but won&apos;t while filling one already created one.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think we need to find a way to make sure whether the WAL is actually persisted at HDFS&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree. Just notice you&apos;ve created &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14790&quot; title=&quot;Implement a new DFSOutputStream for logging WAL only&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14790&quot;&gt;HBASE-14790&lt;/a&gt; and let&apos;s discuss more details there. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15042694" author="yangzhe1991" created="Sat, 5 Dec 2015 06:23:14 +0000"  >&lt;p&gt;After discussion in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14790&quot; title=&quot;Implement a new DFSOutputStream for logging WAL only&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14790&quot;&gt;HBASE-14790&lt;/a&gt; , we can move forward now. Let me repost my comment in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14790&quot; title=&quot;Implement a new DFSOutputStream for logging WAL only&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14790&quot;&gt;HBASE-14790&lt;/a&gt; first &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Currently there are two scenarios which may result in inconsistency between two clusters.&lt;/p&gt;

&lt;p&gt;The first is master cluster crashes(for example, power failure) or three DNs and RS crash at the same time and we lost all data that is not flushed to DNs&apos; disks but the data have been already synced to slave cluster.&lt;/p&gt;

&lt;p&gt;The second is we will rollback memstore and response client an error if we get a error on hflush but the log may indeed exists in WAL. This will not only results in inconsistency between two clusters but also gives client a wrong response because the data will &quot;revive&quot; after replaying WAL. This scenario has been discussed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14004&quot; title=&quot;[Replication] Inconsistency between Memstore and WAL may result in data in remote cluster that is not in the origin&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14004&quot;&gt;HBASE-14004&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Comparing to the second, it is easier to solve the first scenario that we can tell ReplicationSource it can only read the logs that is already saved on three disks. We need to know the largest WAL entry id that has been synced. So HDFS&apos;s sync logic for itself may be not helpful for us and we must use hsync to let HBase know the entry id. So we need a configurable periodically hsync here, and if we have only one cluster it is also helpful to reduce data losses because of data center power failure or unluckily crashing three DNs and RS at the same time.&lt;/p&gt;

&lt;p&gt;For the second scenario, it is more complex because we can not rollback memstore and tell client this operation failed unless we are very sure the data will never exist in WAL, and mostly we are not sure... So we have to use a new WAL logic that rewriting the entry to the new file rather than rollback. To implement this we need to handle duplicate entries while replaying WAL.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Therefore, we may have 4 subtasks:&lt;br/&gt;
1: A configurable periodically hsync logic to make sure our data has been saved on disks. It is also helpful for single cluster mode.&lt;br/&gt;
2: ReplicationSource should only read WAL that is hsynced to prevent slave cluster having data that master losses.&lt;br/&gt;
3: WAL reader can handle duplicate entries, in other words, make WAL logging idempotent. &lt;br/&gt;
4: Fixing HBase writing path that we should retry logging WAL in a new file rather than rollback MemStore.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="15042744" author="apache9" created="Sat, 5 Dec 2015 08:02:51 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Fixing HBase writing path that we should retry logging WAL in a new file rather than rollback MemStore.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;To be clear, this means we will hold the &lt;tt&gt;WAL.sync&lt;/tt&gt; request if there are some entries have already been written out but not acked and never return until we successfully write them out and get ack back. And if &lt;tt&gt;WAL.sync&lt;/tt&gt; or &lt;tt&gt;WAL.write&lt;/tt&gt; fails(maybe due to queue full), we will still rollback MemStore since we can confirm that the WAL entries have not been written out. Right?&lt;/p&gt;

&lt;p&gt;And I think there is another task for us. Now the DFSOutputStream does not provide a public method to get acked length. We can open a issue of HDFS project and use reflection first in HBase. But there is still a problem that &lt;tt&gt;hflush&lt;/tt&gt; or &lt;tt&gt;hsync&lt;/tt&gt; does not return the acked length which means get acked length and &lt;tt&gt;hsync&lt;/tt&gt; are two separated operations so it is hard to get the exact acked length after calling &lt;tt&gt;hsync&lt;/tt&gt;. Maybe we could get current total write out bytes first(not acked length) and then call &lt;tt&gt;hsync&lt;/tt&gt;, the acked length after calling &lt;tt&gt;hsync&lt;/tt&gt; must be larger than this value so it is safe to use this value as &quot;acked length&quot;. Any thoughts?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="15042765" author="yangzhe1991" created="Sat, 5 Dec 2015 09:04:22 +0000"  >&lt;p&gt;Is it required to use the size of serialized binary data? I don&apos;t know if there is a sequence increment unique id for each wal log. If so or if we can add this, we can know what is the largest id that has been hsynced, right? And this id can also help us on replaying duplicate entries.&lt;/p&gt;</comment>
                            <comment id="15043246" author="apache9" created="Sat, 5 Dec 2015 11:13:37 +0000"  >&lt;p&gt;Sounds great, an incremental unique id of WAL entry is better since it is managed by ourselves.&lt;/p&gt;</comment>
                            <comment id="15043657" author="stack" created="Sun, 6 Dec 2015 02:26:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;ReplicationSource should only read WAL that is hsynced to prevent slave cluster having data that master losses.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This will require big change in how replication works but for the better and replication will be less resource intense because less NN ops (if crash, we ask NN for file length, not ZK? If so, this would be a task we have been needing to do for a long time; i.e. undo keeping replication position in zk).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;WAL reader can handle duplicate entries, in other words, make WAL logging idempotent. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Might have to add some code to reader to skip an entry it has seen before (this may be there already &amp;#8211; need to check).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Fixing HBase writing path that we should retry logging WAL in a new file rather than rollback MemStore.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is new but has been done before.&lt;/p&gt;

&lt;p&gt;I&apos;d be up for helping w/ WAL changes, stuff like keeping around appends until the sync for them comes in (I&apos;ve messed w/ this before), and would be interested in helping out on replication log length accounting changing it from relying on reopen after it gets EOF and keeping length in zk.&lt;/p&gt;

&lt;p&gt;You fellas are fixing a few fundamental issues here. Sweet.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we will still rollback MemStore since we can confirm that the WAL entries have not been written out. Right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We could try rejiggering the order in which memstore gets updated, putting it off till after the sync. The order we have now came about long time ago when WAL was very different. We might be able to change the order, simplify the write pipeline, and not lose too much perf (or, perhaps, get more perf because we are doing healthier group commits).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe we could get current total write out bytes first(not acked length) and then call hsync, the acked length after calling hsync must be larger than this value so it is safe to use this value as &quot;acked length&quot;. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It would be good if hbase could calculate the written length itself. We could try it. What happens if we want to compress WAL or what about crc tax.... (I suppose this latter would be a constant &amp;#8211; and for the former, maybe we could figure then length... even on compress if per edit or per batch....)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t know if there is a sequence increment unique id for each wal log. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There is such a sequenceid but it is by-region, not global.  Could keep sequence id by region accounts? (We already do this elsewhere).&lt;/p&gt;
</comment>
                            <comment id="15043664" author="apache9" created="Sun, 6 Dec 2015 02:41:00 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This will require big change in how replication works but for the better and replication will be less resource intense because less NN ops (if crash, we ask NN for file length, not ZK? If so, this would be a task we have been needing to do for a long time; i.e. undo keeping replication position in zk).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think we should have two branches to determine how many entries can we read. One is for closed WAL file, one is for the WAL still being written. We can get this information using &lt;tt&gt;DistributedFileSystem.isFileClosed&lt;/tt&gt;. If the file is already closed, then we could use the length that gotten from HDFS. If the file is still opened for writting, then we should ask the rs who is writing it for the safe length. If we can not find the rs(maybe it has already crashed), then we could wait a minute since namenode will finally recover its lease and close the file.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There is such a sequenceid but it is by-region, not global. Could keep sequence id by region accounts? (We already do this elsewhere).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So maybe we still need to use &quot;acked length&quot;, not &quot;acked id&quot;. But this is enough to filter out duplicate WAL entries I think.&lt;/p&gt;</comment>
                            <comment id="15043721" author="carp84" created="Sun, 6 Dec 2015 06:06:57 +0000"  >&lt;p&gt;Nice discussion &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yangzhe1991&quot; class=&quot;user-hover&quot; rel=&quot;yangzhe1991&quot;&gt;Phil Yang&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FWIW, two questions about the Phil&apos;s proposal:&lt;br/&gt;
1. What the logic would be like if the durability is set to ASYNC in table descriptor? Is the following case possible to happen?:&lt;br/&gt;
    1) entry write into memstore&lt;br/&gt;
    2) region reassign to other RS thus content in memstore got flushed into hfile&lt;br/&gt;
    3) wal sync/write failed&lt;br/&gt;
    In this case we might run into another kind of inconsistency, say master cluster has the data but slave doesn&apos;t?&lt;/p&gt;

&lt;p&gt;2. About &lt;tt&gt;WAL logging idempotent&lt;/tt&gt;, maybe we also need to consider the cross-RS case when region assign happens before wal sync acked?&lt;/p&gt;</comment>
                            <comment id="15044308" author="chenheng" created="Mon, 7 Dec 2015 03:07:13 +0000"  >&lt;blockquote&gt;
&lt;p&gt;To be clear, this means we will hold the WAL.sync request if there are some entries have already been written out but not acked and never return until we successfully write them out and get ack back. And if WAL.sync or WAL.write fails(maybe due to queue full), we will still rollback MemStore since we can confirm that the WAL entries have not been written out. Right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I have a big concern about it.  If we not configure hsync every time( hsync  periodically),  it means there are always some entries we make hflush but not hsync.  And as our logical designed, when one hflush failed, we close old wal and open a new one,  the entries which not hsync will be written into new WAL.   &lt;br/&gt;
If RS crashed at this time,  what will happen?  Is it means some entries may be already in place (you have told to client your mutation successed and data was really in place on DN already) will lost.  I think it is a regression.  Because one failed mutation may cause more mutations inconsistency.&lt;br/&gt;
I think it is also &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;Yu Li&lt;/a&gt; concern as his problem.&lt;/p&gt;


</comment>
                            <comment id="15044315" author="yangzhe1991" created="Mon, 7 Dec 2015 03:21:25 +0000"  >&lt;blockquote&gt;
&lt;p&gt;If RS crashed at this time, what will happen?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If only RS crashed, DNs not, there is no data loss because they are hflushed to DNs&apos; memory. Only RS+DNs all crashed causes data loss. But hsync every time will increase latency, so users can configure on this issue.&lt;/p&gt;</comment>
                            <comment id="15044318" author="chenheng" created="Mon, 7 Dec 2015 03:29:37 +0000"  >&lt;blockquote&gt;
&lt;p&gt;If only RS crashed, DNs not, there is no data loss because they are hflushed to DNs&apos; memory.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If i understand correctly,  this is my thought.&lt;br/&gt;
If one hlush failed, we close old WAL and write entries not hsync to new WAL, right?  You close old WAL as old acked length, right?  &apos;Acked length&apos; is updated by hsync, right?  If RS crashed at this time, when WAL replayed by other RS,  the entries not hsynced in Queue (whatever it is) will lost, right?&lt;/p&gt;</comment>
                            <comment id="15044319" author="apache9" created="Mon, 7 Dec 2015 03:29:50 +0000"  >&lt;blockquote&gt;
&lt;p&gt; Is it means some entries may be already in place (you have told to client your mutation successed and data was really in place on DN already) will lost.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If all 3 DNs are all crashed and the RS is also crashed then yes, you have no choice unless hsync every time to prevent this. And this does not related to the scenario you said I think, we do not return SUCESS to client until we got a successful hflush response.&lt;/p&gt;

&lt;p&gt;And what &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=liyu&quot; class=&quot;user-hover&quot; rel=&quot;liyu&quot;&gt;Li Yu&lt;/a&gt; said is another problem. Flush WAL async(or even do not write WAL) is known to have the risk of losing data. We need to determine what we can guarantee when user use these configurations.&lt;/p&gt;</comment>
                            <comment id="15044321" author="apache9" created="Mon, 7 Dec 2015 03:33:49 +0000"  >&lt;blockquote&gt;
&lt;p&gt;If one hlush failed, we close old WAL and write entries not hsync to new WAL, right? You close old WAL as old acked length, right? &apos;Acked length&apos; is updated by hsync, right? If RS crashed at this time, when WAL replayed by other RS, the entries not hsynced in Queue (whatever it is) will lost, right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No. You can close old WAL using any length that larger than the previous succeeded hflushed length, not the hsynced length.&lt;/p&gt;</comment>
                            <comment id="15044327" author="chenheng" created="Mon, 7 Dec 2015 03:47:10 +0000"  >&lt;blockquote&gt;
&lt;p&gt;No. You can close old WAL using any length that larger than the previous succeeded hflushed length, not the hsynced length.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OK,  i got it now.  The &apos;acked length&apos; only be used in Replication.  We need another &apos;hflushed length&apos; to close WAL.  So data only lost when all 3 DNs and RS crashed.&lt;br/&gt;
Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; for your explain. &lt;/p&gt;</comment>
                            <comment id="15044333" author="yangzhe1991" created="Mon, 7 Dec 2015 03:58:14 +0000"  >&lt;p&gt;I have a new concern about the configuration. Now HBase only use hflush, if we add a hsync logic, will there be degradation of performance? Should we still allow users disable hsync just like before? If so, what is the default configure? If user disable hsync, what should we do for ReplicationSource?&lt;/p&gt;</comment>
                            <comment id="15044341" author="chenheng" created="Mon, 7 Dec 2015 04:08:54 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Now HBase only use hflush, if we add a hsync logic, will there be degradation of performance? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think default configuration is hsync periodically. &lt;br/&gt;
In normal path, because of  async hsync, IMO it is OK for performance.  But if hflush failed,  the handler will be hang to process recovery,  if it costs too much time, because retry of client,  maybe all handlers will be exhausted soon. It may be another regression as current logical.&lt;/p&gt;
</comment>
                            <comment id="15044355" author="carp84" created="Mon, 7 Dec 2015 04:43:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;if we add a hsync logic, will there be degradation of performance&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;IMHO for 100% no-data-loss guarantee, we have to sacrifice performance, more or less. This is a trade-off and user should be able to make their choice. However, there&apos;s no real fsync support in HBase yet, although quite some efforts paid like &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt; (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, please correct me if I&apos;ve stated anything wrong here, thanks). Not sure whether I&apos;m off the topic but somehow I feel all these things are related and indeed we are trying to fix a few fundamental issues here just like stack mentioned.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Should we still allow users disable hsync just like before?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think yes, we should leave an option here, user might care more about performance and would like to take the relative low all-3-DN-crashes risk, I guess.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If so, what is the default configure?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I guess this depends on the final perf number of the new design and implementation&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If user disable hsync, what should we do for ReplicationSource?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think we should go back to old logic when user choose to.&lt;/p&gt;

&lt;p&gt;btw, could see quite some discussion here and maybe a doc summarizing the basic design, existing discussion conclusion and left-over questions would be good for understanding and further discussion? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15044368" author="chenheng" created="Mon, 7 Dec 2015 04:52:16 +0000"  >&lt;blockquote&gt;
&lt;p&gt;btw, could see quite some discussion here and maybe a doc summarizing the basic design, existing discussion conclusion and left-over questions would be good for understanding and further discussion?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good suggestion.  We need one design doc,  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yangzhe1991&quot; class=&quot;user-hover&quot; rel=&quot;yangzhe1991&quot;&gt;Phil Yang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; do you have time to prepare a doc for this issue. If you don&apos;t have time, i can write one.&lt;/p&gt;</comment>
                            <comment id="15044380" author="yangzhe1991" created="Mon, 7 Dec 2015 05:02:15 +0000"  >&lt;p&gt;Sure, I&apos;m going to draft a document for summarizing our ideas. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; and I can have more offline discussion so I can post an initial version that satisfies both of us and then ask for your suggestion &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15044489" author="yangzhe1991" created="Mon, 7 Dec 2015 07:16:48 +0000"  >&lt;p&gt;Here is the doc that summarizes our discussion before.&lt;/p&gt;</comment>
                            <comment id="15044563" author="chenheng" created="Mon, 7 Dec 2015 08:30:50 +0000"  >&lt;p&gt;Looks good. Except one thing.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
If we get timeout error &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt;, we should open a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; file and write/hflush all data in buffer to it and use a background thread to close old file using any length that larger than the previous succeeded hflushed length.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;IIRC,  we should ensure seqId of WAL edits increased. Otherwise,  we will failed when replay WALs. &lt;br/&gt;
Some entries in new WAL may have smaller seqid than entries in old WAL as our logical, right?&lt;/p&gt;</comment>
                            <comment id="15044576" author="yangzhe1991" created="Mon, 7 Dec 2015 08:47:45 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Some entries in new WAL may have smaller seqid than entries in old WAL as our logical, right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, that is why we should make sure WAL reader can filter entries that it has seen before. Two entries with same id may not adjacent, maybe two WAL files are just like this:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;1, 2, 3, 4, 5, 6&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;4, 5, 6, 7&amp;#93;&lt;/span&gt;&lt;br/&gt;
If when writing first WAL we only make sure we have hflushed entries no larger than 3&lt;/p&gt;</comment>
                            <comment id="15044603" author="chenheng" created="Mon, 7 Dec 2015 09:04:13 +0000"  >&lt;p&gt;It seems that WAL reader is designed for one WAL,   we can&apos;t filter entries from other WAL in WAL Reader.  &lt;br/&gt;
Maybe we should do something to filter entries when replay.&lt;/p&gt;</comment>
                            <comment id="15044616" author="heliangliang" created="Mon, 7 Dec 2015 09:12:22 +0000"  >&lt;p&gt;Sorry for the late reply. What about this approach:&lt;br/&gt;
1. Keep the current synced sequence id in memory and make sure the replicator does not read over this id when replicating.&lt;br/&gt;
2. When rolling WAL, record the final id in zk replication queue and also mark the file as rolled in-memory so the local replicator can know this file is finished.&lt;br/&gt;
3. When the replication queue is failovered, the new replicator will check the recorded sequence id, if it&apos;s available, it&apos;s successfully rolled, otherwise, read until EOF. For the later case, we need make sure the edits after the last successful sync of that log must be replayed to ensure consistency. Recording the last successful synced sequence id when flushing can guarantee this.&lt;/p&gt;

&lt;p&gt;There overhead is insignificant (just a memory barrier for the volatile sequence id passing to the replicator).&lt;/p&gt;

&lt;p&gt;Guess this may be the original design since there is TODO comments in FSHLog.java:&lt;br/&gt;
TODO:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;replication may pick up these last edits though they have been marked as failed append (Need to&lt;/li&gt;
	&lt;li&gt;keep our own file lengths, not rely on HDFS).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15044643" author="yangzhe1991" created="Mon, 7 Dec 2015 09:32:51 +0000"  >&lt;blockquote&gt;
&lt;p&gt;It seems that WAL reader is designed for one WAL, we can&apos;t filter entries from other WAL in WAL Reader. &lt;br/&gt;
Maybe we should do something to filter entries when replay.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You are right, the logic that ignores logs which are already in HFiles is in HRegion.replayRecoveredEdits, so the logic of filtering seen logs should also be there?&lt;/p&gt;</comment>
                            <comment id="15044666" author="chenheng" created="Mon, 7 Dec 2015 09:46:21 +0000"  >&lt;blockquote&gt;
&lt;p&gt;You are right, the logic that ignores logs which are already in HFiles is in HRegion.replayRecoveredEdits, so the logic of filtering seen logs should also be there?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;yeah, maybe we should store the max seqid of &apos;acked hflushed&apos; wal entries in some places (ZK or system table),   we could use this information to skip entries in new WAL.&lt;/p&gt;
</comment>
                            <comment id="15044685" author="yangzhe1991" created="Mon, 7 Dec 2015 10:00:24 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Guess this may be the original design since there is TODO comments in FSHLog.java:&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If my understanding is right, these comments think we should truncate WAL file according to the position of the last synced log so we can avoid replaying or replicating edits that have been regarded as failed by clients. However, even if RS know where we should truncate. RS may crash after telling clients failing and before truncating. So I think there is a better idea that we do not truncate and only rewrite the logs to the new file and do not telling clients failing after we make WAL logging idempotent.&lt;/p&gt;</comment>
                            <comment id="15044702" author="heliangliang" created="Mon, 7 Dec 2015 10:11:34 +0000"  >&lt;p&gt;We don&apos;t need to guarantee &quot;avoid replaying or replicating edits that have been regarded as failed by clients&quot;. In another word, the client needed&apos;t discriminate between timeout and sync error, right?&lt;/p&gt;

&lt;p&gt;In fact, the most probable case would like this:&lt;br/&gt;
1. DN have problems&lt;br/&gt;
2. HBase client timeout&lt;br/&gt;
3. HDFS client timeout and sync fails&lt;/p&gt;</comment>
                            <comment id="15044835" author="yangzhe1991" created="Mon, 7 Dec 2015 12:14:52 +0000"  >
&lt;blockquote&gt;
&lt;p&gt;In another word, the client needn&apos;t discriminate between timeout and sync error, right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For a HBase client which has retry logic, timeout and other errors result in retrying with no difference, so it seems not important that we must guarantee the non-timeout errors should make sure the data will never exist in database, although I think it is necessary for a reliable database. Different users may have different requirement, what do other fellas think on this question?&lt;/p&gt;

&lt;p&gt;And I think there is not a big difference between your approach and part of my design. There is only difference on implementation.&lt;/p&gt;

&lt;p&gt;If we need the guarantee above that we should differentiate timeout error and non-timeout error, we can save on zk then write to new file (your idea) or just write to new file but skip reading when replaying if repeating (my idea), before acking client success. My idea may be more fast because it needn&apos;t send request to zk.&lt;/p&gt;

&lt;p&gt;If we needn&apos;t the guarantee, we can rollback memstore, ack fail to client and do other work async because we are not afraid of RS crashing right now.&lt;/p&gt;

&lt;p&gt;And I think the major difference between our ideas is you don&apos;t change the logic about WAL&apos;s sync. However, I think the currently logic may be not perfect because hflush only write data to three DNs memory, which is not the real persistence just like users thought. If RS and three DNs down, or the whole cluster down because of some serious issue, the data that is not synced to DNs&apos; disks will be lost. This issue not only affects the inconsistency between two clusters, but also confuses users that actually we don&apos;t save your data on disk. I think it may not be good &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15044970" author="yangzhe1991" created="Mon, 7 Dec 2015 13:52:19 +0000"  >&lt;p&gt;Furthermore, I find another issue that since &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-744&quot; title=&quot;Support hsync in HDFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-744&quot;&gt;&lt;del&gt;HDFS-744&lt;/del&gt;&lt;/a&gt; supported hsync(), it also added CreateFlag.SYNC_BLOCK in FileSystem.create() that &quot;Force closed blocks to disk&quot;. Which means it will send a syncBlock flag in the last DFSPacket in every endBlock(). If we don&apos;t add this just like currently HBase, the files we save on HDFS are not synced to disk immediately. We are having the risk of losing data when we just flush a MemStore into HFile or we just compact some HFiles because we think these data have been saved and we may delete WAL or old HFiles, right?&lt;/p&gt;

&lt;p&gt;If I am not wrong, we can create a new issue to have discussion there since it is independent&lt;/p&gt;</comment>
                            <comment id="15046226" author="carp84" created="Tue, 8 Dec 2015 02:37:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;My idea may be more fast because it needn&apos;t send request to zk&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Considering the fail over case (like RS crash), I guess we need to persist the acked length to somewhere like zk, or else we will still replicate the non-acked data to slave cluster when recover?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We are having the risk of losing data when we just flush a MemStore into HFile&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I believe &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5954&quot; title=&quot;Allow proper fsync support for HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5954&quot;&gt;&lt;del&gt;HBASE-5954&lt;/del&gt;&lt;/a&gt; tried to resolve the same problem, and would suggest to pay a visit there. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15046286" author="yangzhe1991" created="Tue, 8 Dec 2015 03:36:05 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I guess we need to persist the acked length to somewhere like zk, or else we will still replicate the non-acked data to slave cluster when recover?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If we persist the acked length to zk and RS crash before saving to zk, what will happen? There is always a situation that we haven&apos;t done anything after we get a timeout on hflush/hsync or even we crash before getting ack. For hflush, if we hold the request, client will not get any response which means after restarting we can either make it success or fail for this request. For hsync, RS has crashed and it will replay the log after restarting, but we can not make sure the data that is not acked by hsync is on DNs&apos; disks or memories, so ReplicationSource may can only wait until RS restart because we can not make sure if the following visible data will be hsynced to disk. If ReplicationSource read anyway and then three DNs crash, slave will have more data than master...&lt;/p&gt;

&lt;p&gt;And if we add a CreateFlag.SYNC_BLOCK flag when creating WAL file, we can make sure that closed file must be on disks, so ReplicationSource can wait namenode close the file automatically if RS doesn&apos;t recover and read the whole file, right?&lt;/p&gt;</comment>
                            <comment id="15046292" author="chenheng" created="Tue, 8 Dec 2015 03:41:43 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I guess we need to persist the acked length to somewhere like zk, or else we will still replicate the non-acked data to slave cluster when recover?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There is no need to do it.  Replication Queue store current position of WAL in ZK which has been replicated.  If replicator only reads &apos;hsynced&apos; entries,  it will not replicate non hsynced data to slave when recover.&lt;/p&gt;</comment>
                            <comment id="15046371" author="carp84" created="Tue, 8 Dec 2015 05:13:39 +0000"  >&lt;p&gt;It seems to me that replication queue only records where to &lt;b&gt;start&lt;/b&gt; reading but not where to &lt;b&gt;end&lt;/b&gt;, and the acked length exactly records where to end? If e don&apos;t persist this acked length, during failover we still will read until EOF?&lt;/p&gt;</comment>
                            <comment id="15046385" author="carp84" created="Tue, 8 Dec 2015 05:26:58 +0000"  >&lt;p&gt;Assume this situation:&lt;br/&gt;
1. RS received an ack from DFSClient, and record it in memory, not on zk&lt;br/&gt;
2. ReplicationSource read from the last synced length, but RS crashed before it reach the new ack length&lt;br/&gt;
3. Replication queue recovery starts on another RS, where no place to get the acked length for the old-crashed RS&lt;br/&gt;
In this case, where will step 3 ends? If read until it reaches EOF, then the inconsistency issue we try to resolve here happen again, right?&lt;/p&gt;</comment>
                            <comment id="15046395" author="apache9" created="Tue, 8 Dec 2015 05:36:26 +0000"  >&lt;blockquote&gt;
&lt;p&gt; If e don&apos;t persist this acked length, during failover we still will read until EOF?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It depends on the file state. If it is already closed, then we could use the file length get from namenode. If it is still opened for writing, then we should ack for the acked length. You could already get the acked length unless the RS is crashed, but if the RS is crashed, then the file will finally be closed by NN.&lt;/p&gt;</comment>
                            <comment id="15046398" author="apache9" created="Tue, 8 Dec 2015 05:39:57 +0000"  >&lt;blockquote&gt;
&lt;p&gt;3. Replication queue recovery starts on another RS, where no place to get the acked length for the old-crashed RS&lt;br/&gt;
In this case, where will step 3 ends? If read until it reaches EOF, then the inconsistency issue we try to resolve here happen again, right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If the region is assigned to another RS, then the new RS need to replay WAL to reconstruct the memstore, at this time, even unacked WAL entries that appear in the WAL file will also be replayed, right? &lt;/p&gt;</comment>
                            <comment id="15046401" author="carp84" created="Tue, 8 Dec 2015 05:42:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;then the file will finally be closed by NN&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think &lt;b&gt;finally be closed&lt;/b&gt; is enough, since the replication queue recover will start as soon as Master detects RS crash and issue the recovery task. I think we need to handle this case that file not closed but recovery already started, right?&lt;/p&gt;</comment>
                            <comment id="15046405" author="apache9" created="Tue, 8 Dec 2015 05:46:00 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I don&apos;t think finally be closed is enough, since the replication queue recover will start as soon as Master detects RS crash and issue the recovery task.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;HBase uses NN to do fencing which means HMaster will only consider an RS is dead when all of its WAL file is closed.&lt;/p&gt;</comment>
                            <comment id="15046414" author="chenheng" created="Tue, 8 Dec 2015 05:54:25 +0000"  >&lt;p&gt;Why we need &apos;where to end&apos; when recover replication?   We just need to start replicate entries where replication were broken when RS failed, right? If you concern we may read &apos;unhsynced wal entries&apos;,  we could close relates WAL first before recovery if it is still open. right? (It seems already be done now)&lt;/p&gt;</comment>
                            <comment id="15046422" author="apache9" created="Tue, 8 Dec 2015 05:58:14 +0000"  >&lt;p&gt;I do not think we can benefit a lot from storing acked length to zookeeper. If we do it periodically, then we could still meet the problem that we flush things out and then crashed so the new acked length is not stored on zk yet. If we do it at every hflush synchronously, then I think zookeeper will be fucked up in a large cluster...&lt;/p&gt;</comment>
                            <comment id="15046439" author="carp84" created="Tue, 8 Dec 2015 06:15:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;HBase uses NN to do fencing which means HMaster will only consider an RS is dead when all of its WAL file is closed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Oh yes, the recover lease logic could make sure of this... Ok, now I agree that we don&apos;t need to persist the acked length on zk, during failover the existing logic could make sure of consistency. And the to-be-implemented WAL idempotent feature will make sure that &quot;WAL replay of unacked entry&quot; and &quot;client retry after RS crash&quot; won&apos;t conflict. Thanks for the explanation &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt;!&lt;/p&gt;</comment>
                            <comment id="15046475" author="carp84" created="Tue, 8 Dec 2015 07:02:58 +0000"  >&lt;p&gt;You are right Heng, and Duo also explained about this. Thanks. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15046584" author="chenheng" created="Tue, 8 Dec 2015 08:38:48 +0000"  >&lt;p&gt;I make a patch for skip duplicate entries when replay,  i found something new which we didn&apos;t considered.&lt;/p&gt;

&lt;p&gt;let&apos;s open a new issue for it, and we can discuss something there.  &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14949&quot; title=&quot;Resolve name conflict when splitting if there are duplicated WAL entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14949&quot;&gt;&lt;del&gt;HBASE-14949&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15046656" author="stack" created="Tue, 8 Dec 2015 09:53:03 +0000"  >&lt;p&gt;Doc is great. I added a few little notes...&lt;/p&gt;</comment>
                            <comment id="15047929" author="chenheng" created="Wed, 9 Dec 2015 02:57:17 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Have to be careful how we do this. We have elaborate accounting now that allows only one &apos;sync&apos; type, either a hflush or a hsync, but not a mix of both.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;After read notes in doc, i begin to agree with stack.   &lt;br/&gt;
Why we need hsync?  The concern about using &apos;hflush&apos; is we may lost data when 3 DNs and RS crash at the same time, right?  It is really small probability. But if we introduce hsync (for example hsync periodically), it will cause latency between master and slave. Is it worth to do it? &lt;/p&gt;

&lt;p&gt;And inconsistency problem about this issue in replication could be fixed if replicator only read entries &apos;acked hflushed&apos; just like we do in recovery process when hflush failed, right? And as our design,  we only use hsync to ensure  data inconsistency in replication but data lost still happen because we NOT use &apos;hsync&apos; in write path. If so, why NOT we just use hflush?  &lt;/p&gt;</comment>
                            <comment id="15047976" author="yangzhe1991" created="Wed, 9 Dec 2015 03:46:23 +0000"  >&lt;blockquote&gt;
&lt;p&gt;And as our design, we only use hsync to ensure data inconsistency in replication but data lost still happen because we NOT use &apos;hsync&apos; in write path. If so, why NOT we just use hflush?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is why I think hsync should be configurable. For a database, mostly we should have the guarantee of data persistence. But sometimes we will sacrifice it for higher performance. For example, Redis&apos;s aof can be configured to fsync every write, each second or never. Users can configure it according to their requirement. Although &quot;each second&quot; will still lose data after crashing but users will be guaranteed that they will lose data in at most one second, which is still a valuable guarantee. However, currently HBase has no guarantee about this and users may thought their data has already saved on disks, and we have no idea when our data will be saved on disks. Both WAL and SSTable have this issue. And obviously this will result in data inconsistency between two clusters, too.&lt;/p&gt;

&lt;p&gt;I will not oppose if we only fix (or fix it first) the issue that we transferring data which has been rollbacked in MemStore. And we can use the ack size of hflush which means there won&apos;t be additional latency between two clusters. But I think there should be a follow-up work on data persistence and we need a configurable hsync in HBase &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15047986" author="chenheng" created="Wed, 9 Dec 2015 03:55:31 +0000"  >&lt;blockquote&gt;
&lt;p&gt; But I think there should be a follow-up work on data persistence and we need a configurable hsync in HBase &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It is really a problem. But i think it will be done in another issue.  Maybe you can send an email to dev list.  And in this issue, i suppose we just ensure correctness with hflush, wdyt?&lt;/p&gt;</comment>
                            <comment id="15048010" author="stack" created="Wed, 9 Dec 2015 04:36:11 +0000"  >&lt;p&gt;This is the issue where client can say what durability to apply per edit. Lars did a load of work on this and issue is filled with good stuff in it. In end, what came out was how it is hard to allow client saying hflush/hsync or no sync in current setup.&lt;/p&gt;</comment>
                            <comment id="15048051" author="stack" created="Wed, 9 Dec 2015 05:14:20 +0000"  >&lt;p&gt;Link to discussion around loss of WALs though they had been closed and renamed.&lt;/p&gt;</comment>
                            <comment id="15048137" author="yangzhe1991" created="Wed, 9 Dec 2015 06:30:34 +0000"  >&lt;blockquote&gt;
&lt;p&gt;And in this issue, i suppose we just ensure correctness with hflush, wdyt?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; , we can focus on the inconsistency between WAL and Memstore which also results in inconsistency between master and slave. And if we don&apos;t use hsync, I think we need not change the logic of replicator which means we needn&apos;t only transfer data that is hflushed because all entries in WAL should finally be in memstore, right?&lt;/p&gt;

&lt;p&gt;So we may have only two subtasks now:&lt;br/&gt;
1: WAL reader can handle duplicate entries, in other words, make WAL logging idempotent. &lt;br/&gt;
2: WAL logger does not throw exception if it can not make sure whether the entry is saved on HDFS or not(for example, hflush timeout), it retry to write entry to a new file and close old file asynchronously&lt;/p&gt;

&lt;p&gt;I change the description of the second subtask because we needn&apos;t care the logic of HRegion, which is now &quot;write memstore-&amp;gt;write wal-&amp;gt;rollback if wal fail&quot; and may be changed to &quot;write wal-&amp;gt;write memstore if wal succeed&quot;&lt;/p&gt;
</comment>
                            <comment id="15048171" author="chenheng" created="Wed, 9 Dec 2015 06:58:09 +0000"  >&lt;blockquote&gt;
&lt;p&gt;And if we don&apos;t use hsync, I think we need not change the logic of replicator which means we needn&apos;t only transfer data that is hflushed because all entries in WAL should finally be in memstore, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Think about this situation.  &lt;br/&gt;
In write path, some entries hflushed but not acked,  so we close old WAL with acked length,  and try to write this entries into new WAL, then RS crashed.  Slave maybe has replicate this entries but RS after recovery in master will lost them, right?&lt;/p&gt;</comment>
                            <comment id="15048552" author="yangzhe1991" created="Wed, 9 Dec 2015 11:48:22 +0000"  >&lt;p&gt;You are right, we should change the logic of replicator.&lt;/p&gt;

&lt;p&gt;And I am not an expert so have a question about the idempotent of HBase operation: What will happen if we replay an entry more than once? Considering these scenarios, the number is the seq ids:&lt;/p&gt;

&lt;p&gt;1, 2, 3, 4, 5---this is normal order&lt;br/&gt;
1, 3, 2, 4, 5---the order is wrong but each log we only read once.&lt;br/&gt;
1, 1, 2, 3, 4, 5---we replay one entry twice but they are continuous&lt;br/&gt;
1, 2, 3, 1, 4, 5----we replay one entry twice and they are not continuous&lt;br/&gt;
1, 2, 3, 1, 2, 3, 4, 5---the order is wrong but the subsequence is repeat so we make sure the order&lt;/p&gt;

&lt;p&gt;Are they all wrong except the first? It seems that the last one is not wrong?&lt;/p&gt;</comment>
                            <comment id="15049009" author="stack" created="Wed, 9 Dec 2015 17:22:43 +0000"  >&lt;p&gt;Not sure I understand the question. Replication reads the WAL in order and sends to the remote cluster all it has read in order. When it gets to the remote side, all should be &apos;applied&apos; in order. How we get your scenarios #2, #3, etc., above?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In write path, some entries hflushed but not acked, so we close old WAL with acked length, and try to write this entries into new WAL, then RS crashed. Slave maybe has replicate this entries but RS after recovery in master will lost them, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Isn&apos;t this essentially the description that leads off this JIRA?&lt;/p&gt;</comment>
                            <comment id="15049048" author="yangzhe1991" created="Wed, 9 Dec 2015 17:44:06 +0000"  >&lt;p&gt;The reason I asked this question is if we don&apos;t have to make sure we must replay wal in order and only once for each log, it may be easier to resolve &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14949&quot; title=&quot;Resolve name conflict when splitting if there are duplicated WAL entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14949&quot;&gt;&lt;del&gt;HBASE-14949&lt;/del&gt;&lt;/a&gt; which is being fixed by Heng&lt;/p&gt;</comment>
                            <comment id="15049947" author="chenheng" created="Thu, 10 Dec 2015 03:16:29 +0000"  >&lt;blockquote&gt;
&lt;p&gt;1, 2, 3, 4, 5---this is normal order&lt;br/&gt;
1, 3, 2, 4, 5---the order is wrong but each log we only read once.&lt;br/&gt;
1, 1, 2, 3, 4, 5---we replay one entry twice but they are continuous&lt;br/&gt;
1, 2, 3, 1, 4, 5----we replay one entry twice and they are not continuous&lt;br/&gt;
1, 2, 3, 1, 2, 3, 4, 5---the order is wrong but the subsequence is repeat so we make sure the order&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yangzhe1991&quot; class=&quot;user-hover&quot; rel=&quot;yangzhe1991&quot;&gt;Phil Yang&lt;/a&gt;  only the first one is right now in current logic.&lt;/p&gt;

&lt;p&gt;WAL is RS Level,  but replay is in Region Level,  so in wal , seqId in increased one by one, but we can&apos;t ensure it in region Level when replay, that&apos;s why i mentioned i made a mistake in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14949&quot; title=&quot;Resolve name conflict when splitting if there are duplicated WAL entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14949&quot;&gt;&lt;del&gt;HBASE-14949&lt;/del&gt;&lt;/a&gt;. I will dig it deeper.&lt;/p&gt;

&lt;p&gt;Would you mind update the doc as we mentioned above, Zhe?&lt;/p&gt;</comment>
                            <comment id="15050165" author="yangzhe1991" created="Thu, 10 Dec 2015 06:22:05 +0000"  >&lt;p&gt;I have revised the doc &lt;a href=&quot;https://docs.google.com/document/d/1tTwXrip18qxxsSiPu_y4fGB-26SxqwMvZnFREswyQow/edit?usp=sharing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/1tTwXrip18qxxsSiPu_y4fGB-26SxqwMvZnFREswyQow/edit?usp=sharing&lt;/a&gt; which removes the logic about hsync.&lt;/p&gt;

&lt;p&gt;And previously the doc quotes Duo&apos;s comment&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;close old file using any length that larger than the previous succeeded hflushed length&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I changed this to close the file using the previous succeeded hflushed length which is more clear for implementation.&lt;/p&gt;

&lt;p&gt;And in section &quot;Some issues still need discussion&quot; I quote &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;Yu Li&lt;/a&gt;&apos;s comment:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We also need to consider the cross-RS case when region assign happens before wal sync acked.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yu, is it still a problem after we not using hsync?&lt;/p&gt;</comment>
                            <comment id="15050308" author="yangzhe1991" created="Thu, 10 Dec 2015 08:25:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt; it seems that &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14949&quot; title=&quot;Resolve name conflict when splitting if there are duplicated WAL entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14949&quot;&gt;&lt;del&gt;HBASE-14949&lt;/del&gt;&lt;/a&gt; only handle replaying logs before region open and it doesn&apos;t work on Distributed Log Replay? If I am right, would you please put our logic to WALSplitter so we can handle both case?&lt;/p&gt;</comment>
                            <comment id="15050322" author="carp84" created="Thu, 10 Dec 2015 08:31:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yu, is it still a problem after we not using hsync?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;My concern was mainly about whether it&apos;s possible to have duplicated entries in WAL of &lt;b&gt;different&lt;/b&gt; RS.&lt;/p&gt;

&lt;p&gt;Think about the fail over case, replication queue will be transfered to some other RS then entries of the failed RS WAL will be replicated, meanwhile the same WAL will be split, replayed and its entries written into WAL of the new RS serving the same region. In this situation we add a &lt;tt&gt;isReplay&lt;/tt&gt; flag in WALEdit to avoid duplicated replication (see below code segment in &lt;tt&gt;FSHlog#append&lt;/tt&gt;)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (entry.getEdit().isReplay()) {
  &lt;span class=&quot;code-comment&quot;&gt;// Set replication scope &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; so that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; won&apos;t be replicated
&lt;/span&gt;  entry.getKey().setScopes(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I could see a similar situation here:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;hflush timeout due to network failure but actually persisted&lt;/li&gt;
	&lt;li&gt;WAL logger tries to re-write the buffered entries to a new WAL but new WAL creation failed due to the same network failure, and returns fail to client&lt;/li&gt;
	&lt;li&gt;region got reassigned due to balance or hbase shell command&lt;/li&gt;
	&lt;li&gt;client retry writing to the same region served by a new RS and succeed&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In this case we have duplicated entries in different RS&lt;/p&gt;

&lt;p&gt;Feel free to correct me if the assumption is wrong, but if it&apos;s possible, then we need to handle it in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14949&quot; title=&quot;Resolve name conflict when splitting if there are duplicated WAL entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14949&quot;&gt;&lt;del&gt;HBASE-14949&lt;/del&gt;&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chenheng&quot; class=&quot;user-hover&quot; rel=&quot;chenheng&quot;&gt;Heng Chen&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15050332" author="chenheng" created="Thu, 10 Dec 2015 08:38:11 +0000"  >&lt;p&gt;That is my plan.&lt;/p&gt;</comment>
                            <comment id="15050376" author="chenheng" created="Thu, 10 Dec 2015 08:58:52 +0000"  >&lt;blockquote&gt;
&lt;p&gt;2.&#160;WAL logger tries to re-write the buffered entries to a new WAL but new WAL creation failed due to the same network failure, and returns fail to client&lt;br/&gt;
3. region got reassigned due to balance or hbase shell command&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;when create new WAL and write buffered entries into it. &lt;br/&gt;
If failed in this procedure,  we will tell client &apos;unacked hlushed&apos; mutation failed.  when region reassigned (NOT RS crash) at this time,  memstore will be flushed into HFile , right? As current logic, when we split WAL, we will check lastFlushedSeqId, so duplicate entries will be skipped.  see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14949&quot; title=&quot;Resolve name conflict when splitting if there are duplicated WAL entries&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14949&quot;&gt;&lt;del&gt;HBASE-14949&lt;/del&gt;&lt;/a&gt; comments.    &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carp84&quot; class=&quot;user-hover&quot; rel=&quot;carp84&quot;&gt;Yu Li&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15050409" author="carp84" created="Thu, 10 Dec 2015 09:13:55 +0000"  >&lt;p&gt;Let me confirm, do you mean we never rollback the memstore even if buffered-entries rewrite failed, so it will be included in the flushed HFile?&lt;/p&gt;

&lt;p&gt;I think current logic only makes sure already flushed entries won&apos;t be replayed, but not duplicated entries. If the memstore rolled back and the duplicated entry is the last write, I don&apos;t think lastFlushedSeqId could skip it&lt;/p&gt;</comment>
                            <comment id="15050418" author="chenheng" created="Thu, 10 Dec 2015 09:18:20 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Let me confirm, do you mean we never rollback the memstore even if buffered-entries rewrite failed, so it will be included in the flushed HFile?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Maybe i am wrong,  yeah we NOT rollback the memstore, but the mvcc will not be forward too....  Let&apos;s check it too..&lt;/p&gt;</comment>
                            <comment id="15050438" author="chenheng" created="Thu, 10 Dec 2015 09:28:02 +0000"  >&lt;p&gt;IMO we should handle this situation,  if we failed during recovery procedure,  maybe the network or hdfs has some problems.  Should we close RS directly?&lt;/p&gt;</comment>
                            <comment id="15050467" author="yangzhe1991" created="Thu, 10 Dec 2015 09:47:24 +0000"  >&lt;p&gt;We can not rollback even if buffered-entries rewrite failed, or we may still have inconsistency between memstore and wal. So we can only retry and retry... until too many retries and must crash ourselves?&lt;/p&gt;</comment>
                            <comment id="15149497" author="stack" created="Tue, 16 Feb 2016 23:20:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;We could try rejiggering the order in which memstore gets updated, putting it off till after the sync. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Since &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15158&quot; title=&quot;Change order in which we do write pipeline operations; do all under row locks!&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15158&quot;&gt;&lt;del&gt;HBASE-15158&lt;/del&gt;&lt;/a&gt; Change order in which we do write pipeline operations; do all under row locks&quot;, we don&apos;t do memstore rollbacks. FYI.&lt;/p&gt;
</comment>
                            <comment id="15149552" author="stack" created="Wed, 17 Feb 2016 00:11:24 +0000"  >&lt;p&gt;But, as noted elsewhere, this fact does not solve this issue (what made it into the stream..)&lt;/p&gt;</comment>
                            <comment id="15625326" author="apache9" created="Tue, 1 Nov 2016 12:39:50 +0000"  >&lt;p&gt;I think we need to pick this up.&lt;/p&gt;

&lt;p&gt;With AsyncFSWAL, it is not safe to use DFSInputStream to read the WAL file directly until EOF when it is still open. The data we read maybe disappear later. FSHLog also has this problem but it is much safer... See this document for more details&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.google.com/document/d/11AyWtGhItQs6vsLRIx32PwTxmBY3libXwGXI25obVEY/edit#&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://docs.google.com/document/d/11AyWtGhItQs6vsLRIx32PwTxmBY3libXwGXI25obVEY/edit#&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The problem only happens when the WAL file is still open. AFAIK, if a RS is alive, then its WAL will always be replicated by itself. So I think it is possible that we expose an API to tell the ReplicationSource the safe length to read of an opened WAL file. And for a ReplicationSource that replicates WAL of other RS, then we can make sure the RS is dead and all its WALs should also be closed(we can also make sure it by calling recoverLease). So it is safe to read it until EOF with DFSInputStream.&lt;/p&gt;

&lt;p&gt;Any concerns?  If not, Let&apos;s start working!&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12911749">HBASE-14790</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12554266">HBASE-5954</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 28 Oct 2015 04:18:45 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2gqhb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>