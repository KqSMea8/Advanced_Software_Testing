<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:58:12 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-8755/HBASE-8755.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-8755] A new write thread model for HLog to improve the overall HBase write throughput</title>
                <link>https://issues.apache.org/jira/browse/HBASE-8755</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In current write model, each write handler thread (executing put()) will individually go through a full &apos;append (hlog local buffer) =&amp;gt; HLog writer append (write to hdfs) =&amp;gt; HLog writer sync (sync hdfs)&apos; cycle for each write, which incurs heavy race condition on updateLock and flushLock.&lt;/p&gt;

&lt;p&gt;The only optimization where checking if current syncTillHere &amp;gt; txid in expectation for other thread help write/sync its own txid to hdfs and omitting the write/sync actually help much less than expectation.&lt;/p&gt;

&lt;p&gt;Three of my colleagues(Ye Hangjun / Wu Zesheng / Zhang Peng) at Xiaomi proposed a new write thread model for writing hdfs sequence file and the prototype implementation shows a 4X improvement for throughput (from 17000 to 70000+). &lt;/p&gt;

&lt;p&gt;I apply this new write thread model in HLog and the performance test in our test cluster shows about 3X throughput improvement (from 12150 to 31520 for 1 RS, from 22000 to 70000 for 5 RS), the 1 RS write throughput (1K row-size) even beats the one of BigTable (Precolator published in 2011 says Bigtable&apos;s write throughput then is 31002). I can provide the detailed performance test results if anyone is interested.&lt;/p&gt;

&lt;p&gt;The change for new write thread model is as below:&lt;br/&gt;
 1&amp;gt; All put handler threads append the edits to HLog&apos;s local pending buffer; (it notifies AsyncWriter thread that there is new edits in local buffer)&lt;br/&gt;
 2&amp;gt; All put handler threads wait in HLog.syncer() function for underlying threads to finish the sync that contains its txid;&lt;br/&gt;
 3&amp;gt; An single AsyncWriter thread is responsible for retrieve all the buffered edits in HLog&apos;s local pending buffer and write to the hdfs (hlog.writer.append); (it notifies AsyncFlusher thread that there is new writes to hdfs that needs a sync)&lt;br/&gt;
 4&amp;gt; An single AsyncFlusher thread is responsible for issuing a sync to hdfs to persist the writes by AsyncWriter; (it notifies the AsyncNotifier thread that sync watermark increases)&lt;br/&gt;
 5&amp;gt; An single AsyncNotifier thread is responsible for notifying all pending put handler threads which are waiting in the HLog.syncer() function&lt;br/&gt;
 6&amp;gt; No LogSyncer thread any more (since there is always AsyncWriter/AsyncFlusher threads do the same job it does)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12653172">HBASE-8755</key>
            <summary>A new write thread model for HLog to improve the overall HBase write throughput</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="fenghh">Honghua Feng</assignee>
                                    <reporter username="fenghh">Honghua Feng</reporter>
                        <labels>
                    </labels>
                <created>Mon, 17 Jun 2013 11:14:27 +0000</created>
                <updated>Sat, 21 Feb 2015 23:30:12 +0000</updated>
                            <resolved>Fri, 13 Dec 2013 17:34:06 +0000</resolved>
                                                    <fixVersion>0.98.0</fixVersion>
                    <fixVersion>0.99.0</fixVersion>
                                    <component>Performance</component>
                    <component>wal</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>49</watches>
                                                                                                            <comments>
                            <comment id="13685461" author="fenghh" created="Mon, 17 Jun 2013 11:16:39 +0000"  >&lt;p&gt;the patch &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt;-0.94-V0.patch is based on &lt;a href=&quot;http://svn.apache.org/repos/asf/hbase/branches/0.94&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/hbase/branches/0.94&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13685646" author="yuzhihong@gmail.com" created="Mon, 17 Jun 2013 15:22:02 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; AtomicLong failedTxid = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AtomicLong(0);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Add some comment w.r.t. the purpose of above variable.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-    &lt;span class=&quot;code-comment&quot;&gt;// When optionalFlushInterval is set as 0, don&apos;t start a thread &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; deferred log sync.
&lt;/span&gt;-    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.optionalFlushInterval &amp;gt; 0) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Is deferred log sync still supported ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
+      LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; syncer thread to die&quot;&lt;/span&gt;, e);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The error message needs to be updated.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (bufferLock) {
+        doWrite(regionInfo, logKey, logEdit, htd);
+        txid = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.unflushedEntries.incrementAndGet();
+      }
+      &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.asyncWriter.setPendingTxid(txid);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Should setPendingTxid() call be protected by the bufferLock ?&lt;br/&gt;
In setPendingTxid():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void setPendingTxid(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; txid) {
+      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (txid &amp;lt;= &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.pendingTxid)
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What if two simultaneous calls with different txid take place ? Both txid are greater than this.pendingTxid, the smaller txid may be written last.&lt;/p&gt;

&lt;p&gt;In several comments, replace &apos;local buffered writes&apos; with &apos;buffered writes&apos;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+          &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.pendingTxid &amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.lastWrittenTxid :
+            &lt;span class=&quot;code-quote&quot;&gt;&quot;pendingTxid not greater than lastWrittenTxid when wake-up!&quot;&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Include the two Txid&apos;s in the message above.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+            asyncIOE = e;
+            failedTxid.set(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.pendingTxid);
+         }
+
+          &lt;span class=&quot;code-comment&quot;&gt;// 4. update &apos;lastWrittenTxid&apos; and notify AsyncFlusher to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; &apos;sync&apos;
&lt;/span&gt;+          &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.lastWrittenTxid = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.pendingTxid;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In error condition, failedTxid is still assigned to this.lastWrittenTxid. Is that safe ?&lt;br/&gt;
Since AsyncFlusher does &apos;sync&apos;, should it be named AsyncSyncer ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
+        LOG.debug(getName() + &lt;span class=&quot;code-quote&quot;&gt;&quot; interrupted &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &quot;&lt;/span&gt; +
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Please restore interrupt status.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-comment&quot;&gt;// to make durability of those WALEdits on HDFS side&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Should be &apos;to make those WALEdits durable on HDFS side&apos;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+          &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; now = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Use EnvironmentEdge.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
                requestLogRoll();
+              }
+            } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (IOException e) {
+              LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;writer.getLength() failed&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Should the log level be at warn or error ? Is writer.getLength() relevant here ?&lt;/p&gt;

&lt;p&gt;AsyncNotifier does notification by calling syncedTillHere.notifyAll(). Can this part be folded into AsyncFlusher ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  void addPendingWrites(Entry e) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Rename the above method addPendingWrite() since there is only one Entry involved.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-comment&quot;&gt;// it&apos;s caller&apos;s responsibility to hold updateLock before call &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method
&lt;/span&gt;+  List&amp;lt;Entry&amp;gt; getPendingWrites() {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;bufferLock is held before calling the above method. Update comment accordingly.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
+          LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;interrupted &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; notification from AsyncNotifier&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Restore interrupt status.&lt;/p&gt;</comment>
                            <comment id="13685671" author="fenghh" created="Mon, 17 Jun 2013 16:10:03 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt; for the detailed code review, answers to some important questions as below:&lt;/p&gt;

&lt;p&gt;1) Deferred log sync is still supported, in which case the write handler thread in Region don&apos;t pend on HLog.syncer() waiting for its txid to be sync-ed. This behaviour keeps intact.&lt;/p&gt;

&lt;p&gt;2) Good catch, &quot;if (txid &amp;lt;= this.pendingTxid) return;&quot; in setPendingTxid() moved into below &quot;synchronized (this.writeLock) &lt;/p&gt;
{...}
&lt;p&gt;&quot; is OK. &lt;br/&gt;
   bufferLock is used only to guarantee the access to HLog&apos;s local pending buffer and unflushedEntries can&apos;t be interleaved by multiple threads.&lt;/p&gt;

&lt;p&gt;3) &quot;failedTxid is still assigned to this.lastWrittenTxid. Is that safe ?&quot;&lt;br/&gt;
  Yes. all write pending on syncer() with txid &amp;lt;= failedTxid will get failure. The lastWrittenTxid can safely proceed without incorrect behaviour, and it need to proceed to eventually wake-up pending write handler threads.&lt;/p&gt;

&lt;p&gt;I&apos;ll update the patch per your review tomorrow.&lt;/p&gt;</comment>
                            <comment id="13685820" author="stack" created="Mon, 17 Jun 2013 18:36:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt; Your approach looks good.  Thanks for working on this.  I like the numbers you are getting for throughput.  Any idea of its effect on general latencies?  Does HLogPerformanceEvaluation help evaluating this approach?  Did you deploy this code to production?&lt;/p&gt;

&lt;p&gt;Can I still (if only optionally) sync every write as it comes in? (For the paranoid).&lt;/p&gt;

&lt;p&gt;In general, remove rather than comment stuff out:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-    assertTrue(&lt;span class=&quot;code-quote&quot;&gt;&quot;Should have an outstanding WAL edit&quot;&lt;/span&gt;, log.hasDeferredEntries());
+    &lt;span class=&quot;code-comment&quot;&gt;//assertTrue(&lt;span class=&quot;code-quote&quot;&gt;&quot;Should have an outstanding WAL edit&quot;&lt;/span&gt;, log.hasDeferredEntries());&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Regards the above, the test is no longer valid given the indirection around sync/flush?&lt;/p&gt;

&lt;p&gt;To be clear, when we call doWrite, we just append the log edit to a linked list?  (We call it a bufferLock but we just doing append to the linked list?)&lt;/p&gt;

&lt;p&gt;Patch is looking good.  Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;/p&gt;



&lt;p&gt;How does deferred log flush still work when you remove stuff like optionalFlushInterval?  You say &apos;...don&apos;t pend on HLog.syncer() waiting for its txid to be sync-ed&apos; but that is another behavior than what we had here previously.&lt;/p&gt;

&lt;p&gt;If first thread throws IE, then other threads will still be running (in below):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
+      asyncNotifier.interrupt();
+      asyncNotifier.join();
+
+      asyncFlusher.interrupt();
+      asyncFlusher.join();
+
+      asyncWriter.interrupt();
+      asyncWriter.join();
+    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException e) {
+      LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; syncer thread to die&quot;&lt;/span&gt;, e);
     }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thread names usually follow a pattern where the prefix is the name of the host (RS:SHORT_SERVERNAME-AsyncWriter).. we can fix around commit.&lt;/p&gt;

&lt;p&gt;This new thread does not implement HasThread:&lt;/p&gt;

&lt;p&gt;+  private class AsyncNotifier extends Thread {&lt;/p&gt;







</comment>
                            <comment id="13686700" author="fenghh" created="Tue, 18 Jun 2013 13:29:41 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; for the detailed review. I make and attach a update patch based on trunk according to your reviews.&lt;/p&gt;

&lt;p&gt;Below are answers to some important questions Ted/stack raised in the reviews (I have already answered some from Ted in above comment):&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Ted&amp;#93;&lt;/span&gt; AsyncNotifier does notification by calling syncedTillHere.notifyAll(). Can this part be folded into AsyncFlusher ?&lt;/p&gt;

&lt;p&gt; ===&amp;gt; AsyncNotifier will compete syncedTillHere with all the write handler threads(which may finish the appendNoSync but not pend on syncer()). The performance is better by separating AsyncSyncer(which just get notified, do &apos;sync&apos; and then notify AsyncNotifier) and AsyncNotifier(get notified by AsyncSyncer and wake-up all pending write handler threads)&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;stack&amp;#93;&lt;/span&gt; Any idea of its effect on general latencies? Does HLogPerformanceEvaluation help evaluating this approach? Did you deploy this code to production?&lt;/p&gt;

&lt;p&gt;  ===&amp;gt; I don&apos;t run HLogPerformanceEvaluation for performance comparison. instead I used 5 YCSB clients to concurrently press on a single RS with a 5 data-node underlying HDFS. Everything are the same for test with Old/New write thread models except the RS bits are different. We are testing it in the test cluster for a month, but not deployed to production yet. Below is the detailed performance comparison for your reference.&lt;/p&gt;

&lt;p&gt;  a&amp;gt; 5 YCSB clients, each with 80 concurrent write theads (auto-flush = true)&lt;br/&gt;
  b&amp;gt; each YCSB writes 5000,000 rows&lt;br/&gt;
  c&amp;gt; all 20 regions of the target table are moved to a single RS&lt;/p&gt;

&lt;p&gt;Old write thread model:&lt;/p&gt;

&lt;p&gt;row size(bytes)	latency(ms)	QPS&lt;br/&gt;
------------------------------------------&lt;br/&gt;
2000	        37.3	        10715&lt;br/&gt;
1000	        32.8	        12149&lt;br/&gt;
500	        30.9	        12891&lt;br/&gt;
200	        26.9	        14803&lt;br/&gt;
10	        24.5	        16288&lt;/p&gt;

&lt;p&gt;New write thread model:&lt;/p&gt;

&lt;p&gt;row size(bytes)	latency(ms)	QPS&lt;br/&gt;
-------------------------------------------&lt;br/&gt;
2000	        17.3	        23024&lt;br/&gt;
1000	        12.6	        31523&lt;br/&gt;
500	        11.7	        33893&lt;br/&gt;
200	        11.4	        34876&lt;br/&gt;
10	        11.1	        35804&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;stack&amp;#93;&lt;/span&gt; Can I still (if only optionally) sync every write as it comes in? (For the paranoid).&lt;/p&gt;

&lt;p&gt;  ===&amp;gt; can&apos;t for now, I&apos;ll consider how to make it configurable later on.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;stack&amp;#93;&lt;/span&gt; Regards the above, the test is no longer valid given the indirection around sync/flush?&lt;/p&gt;

&lt;p&gt;  ===&amp;gt; Yes, that test is not valid by new write thread modeldeferred log flush&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;stack&amp;#93;&lt;/span&gt; To be clear, when we call doWrite, we just append the log edit to a linked list? (We call it a bufferLock but we just doing append to the linked list?)&lt;/p&gt;

&lt;p&gt;  ===&amp;gt; Yes, in both old and new write thread models what doWrite does is just appending log edit to a linked list which plays a role as a &apos;local&apos; buffer for log edits what don&apos;t hit hdfs deferred log flushyet.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;stack&amp;#93;&lt;/span&gt; How does deferred log flush still work when you remove stuff like optionalFlushInterval? You say &apos;...don&apos;t pend on HLog.syncer() waiting for its txid to be sync-ed&apos; but that is another behavior than what we had here previously.&lt;/p&gt;

&lt;p&gt;  ===&amp;gt; When say &apos;still support deferred log flush&apos; I mean for &apos;deferred log flush&apos; it can still response write success to client without wait/pend on syncer(txid), in this sense, the AsyncWriter/AsyncSyncer do what the previous LogSyncer does from the point view of the write handler threads: clients don&apos;t wait for the write persist before get reponse success.&lt;/p&gt;</comment>
                            <comment id="13686703" author="fenghh" created="Tue, 18 Jun 2013 13:31:47 +0000"  >&lt;p&gt;new write thread model patch based on trunk&lt;/p&gt;</comment>
                            <comment id="13686710" author="fenghh" created="Tue, 18 Jun 2013 13:37:20 +0000"  >&lt;p&gt;an update patch based on 0.94 according to Ted/stack&apos;s review attached&lt;/p&gt;</comment>
                            <comment id="13686711" author="fenghh" created="Tue, 18 Jun 2013 13:38:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt;-trunk-V0.patch also includes changes according to the review comment from Ted/stack. Thanks again for Ted/stack for the detailed review &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13686800" author="otis" created="Tue, 18 Jun 2013 14:58:36 +0000"  >&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;~frankfenghua&amp;#93;&lt;/span&gt; - this is about improving &lt;b&gt;writes&lt;/b&gt;, but your table shows QPS (Queries Per Second).  Is that QPS really Writes Per Second?  Thanks.&lt;/p&gt;</comment>
                            <comment id="13686818" author="fenghh" created="Tue, 18 Jun 2013 15:18:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=otis&quot; class=&quot;user-hover&quot; rel=&quot;otis&quot;&gt;Otis Gospodnetic&lt;/a&gt; Yes, QPS really means Writes Per Second here. A typo.&lt;/p&gt;

&lt;p&gt;btw: My name is Feng Honghua, not Feng Hua &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13686854" author="otis" created="Tue, 18 Jun 2013 15:58:36 +0000"  >&lt;p&gt;Thanks and I&apos;m sorry about the name messup.  Feel free to mess up mine - you&apos;ve got 15 characters to play with.  And thanks for this patch.  Crazy improvement!&lt;/p&gt;</comment>
                            <comment id="13686979" author="lhofhansl" created="Tue, 18 Jun 2013 17:21:48 +0000"  >&lt;p&gt;The numbers are great, the patch looks good. I&apos;m fine even for 0.94 (but need to study the patch a bit better).&lt;/p&gt;</comment>
                            <comment id="13687090" author="eclark" created="Tue, 18 Jun 2013 19:19:12 +0000"  >&lt;p&gt;For 0.94 this would remove a feature that people are currently using.  So for me that would make 0.94 inclusion problematic.&lt;/p&gt;

&lt;p&gt;For 0.95/0.96 this looks awesome.  I&apos;ll do a more thorough review now.&lt;/p&gt;</comment>
                            <comment id="13687171" author="sershe" created="Tue, 18 Jun 2013 20:50:59 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.writeLock) {
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (txid &amp;lt;= &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.pendingTxid)
+          &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;pendingTxid can only go up, right? If so it may make sense to speculatively check it outside the lock; same with other places.&lt;br/&gt;
How often does this condition happen?&lt;/p&gt;



&lt;blockquote&gt;&lt;p&gt;upateLock &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;typo&lt;/p&gt;



&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
LOG.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;writer.getLength() failed,&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; failure won&apos;t block here&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This message is not very clear, also exception should be logged.&lt;/p&gt;



&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+      &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.notifyLock) {
+        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.flushedTxid = txid;
+        &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.notifyLock.notify();
       }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here check is done outside lock but not inside, could this race?&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (txid &amp;lt;= &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.failedTxid.get()) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I don&apos;t quite understand the logic here. If 2 batches go thru the writer-syncer pipeline, 1st one succeeds and &lt;br/&gt;
the 2nd one fails, before notifier thread wakes up, wouldn&apos;t it report the first batch also as failed?&lt;/p&gt;

&lt;p&gt;The same interaction I wonder about in writer and syncer.&lt;br/&gt;
I am not sure how HDFS write and sync interact, is the following possible or not?&lt;br/&gt;
Writer writes the first batch and wakes up syncer. Before syncer wakes up writer starts the 2nd batch.&lt;br/&gt;
Syncer wakes up and syncs, invisibly to HBase code, to the middle of the 2nd batch that is being written (sync has no upper bound) and succeeds.&lt;br/&gt;
Then finishing to write the 2nd batch, or sync after, fails, so now we wrote to WAL but reported failure.&lt;/p&gt;

&lt;p&gt;Also can you please put comment somewhere with regard to thread safety of log rolling... I am assuming it will be thread safe&lt;br/&gt;
because if we write to one file, roll in the middle and sync a different file it will just be extra sync call, so harmless.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        addPendingWrite(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HLog.Entry(logKey, logEdit));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;addPendingWrite is called without bufferLock in some places, with in others.&lt;/p&gt;

&lt;p&gt;Can you please add comment to bufferLock to elaborate what it locks. And that updateLock cannot be taken inside bufferLock.&lt;br/&gt;
It seems that right now this holds.&lt;/p&gt;

&lt;p&gt;Also, I understand the need for writer and sync thread, but is separate notifier thread necessary? It doesn&apos;t do any blocking operations other than interacting with flusher thread, or taking syncedTillHere lock, which looks like it should be uncontested most of the time.&lt;br/&gt;
Couldn&apos;t flusher thread have the 4~ lines that set syncedTillHere?&lt;/p&gt;</comment>
                            <comment id="13687182" author="eclark" created="Tue, 18 Jun 2013 20:57:18 +0000"  >&lt;p&gt;The AsyncSyncer can throw an error on data that actually got written to the log.  If the edit actually made it to the wal but the syncer errors out, then data will be reported as not written but in fact it could be written.  I don&apos;t think there&apos;s anything that can be done about it but maybe a comment about that possibility around 1167.  It&apos;s not something new but I expect since we&apos;ll be syncing larger chunks that it&apos;s more possible.&lt;/p&gt;

&lt;p&gt;As a follow up jira, I think that we can get even better through put by reducing contention on update and buffer locks.  If we have the AsyncWriter sort wal edits then we can use a read write lock rather than two different locks (both of which are needed for append).&lt;/p&gt;

&lt;p&gt;As another follow up jira I think that we can do away with txid in FSHLog and just use seqnum.&lt;/p&gt;

&lt;p&gt;As I see it, I&apos;m +1 but with such a critical piece it would be nice if there were others.&lt;/p&gt;</comment>
                            <comment id="13687190" author="lhofhansl" created="Tue, 18 Jun 2013 21:02:02 +0000"  >&lt;p&gt;Which feature is removed?&lt;br/&gt;
You mean this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;stack&amp;#93;&lt;/span&gt; Can I still (if only optionally) sync every write as it comes in? (For the paranoid).&lt;/p&gt;

&lt;p&gt;===&amp;gt; can&apos;t for now, I&apos;ll consider how to make it configurable later on.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is the same as now. Previously all threads append to the log (without sync&apos;ing), then they all try to sync as far as they can. The only difference is that now is single thread is sync&apos;ing as much as it can while the writer thread is waiting. Is that the concern?&lt;/p&gt;

&lt;p&gt;Or you mean the deferred log flush interval? I think there the main gain was to do sync asynchronous to the writer thread not the wait interval, which is more of an implementation detail.&lt;/p&gt;

&lt;p&gt;Either way for 0.95/0.96 this is a great feature. Can decide on 0.94 after weighing the details.&lt;/p&gt;</comment>
                            <comment id="13687203" author="eclark" created="Tue, 18 Jun 2013 21:11:40 +0000"  >&lt;p&gt;For 0.94 users got one wal edit means one sync.  Meaning if you failed to sync it was for your edit only.  Not really a huge deal, but that was my convern.&lt;/p&gt;</comment>
                            <comment id="13687214" author="eclark" created="Tue, 18 Jun 2013 21:20:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t quite understand the logic here. If 2 batches go thru the writer-syncer pipeline, 1st one succeeds and &lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;the 2nd one fails, before notifier thread wakes up, wouldn&apos;t it report the first batch also as failed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I think you are correct.  The check should check that the txid is greater than last successful txid and less than the failed.&lt;/p&gt;</comment>
                            <comment id="13687219" author="stack" created="Tue, 18 Jun 2013 21:27:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; I was concerned that you could not write sync write sync anymore w/ this patch in place.  Chatting w/ Elliott, I think it fine letting this go so it is more write write sync write write write sync, etc.  Also, the way deferred flush works is different w/ this patch.  In past a background thread would sync on a period.  I think it also fine that this behavior changes a little (after talking w/ Elliott) so the write and sync are done after we have let the client go (&apos;deferred&apos;).  Just need to add it up in release note.&lt;/p&gt;</comment>
                            <comment id="13687258" author="lhofhansl" created="Tue, 18 Jun 2013 22:01:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;For 0.94 users got one wal edit means one sync. Meaning if you failed to sync it was for your edit only. Not really a huge deal, but that was my convern.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is that true, though? (say) Two services threads call doMiniBatchMutation on behalf of two clients at roughly the same time, both threads would write to the WAL without sync&apos;ing, and they would sync. Depending on the interleaving the first could sync all edits and the 2nd could be a no-op (the &lt;tt&gt;txid &amp;lt;= this.syncedTillHere&lt;/tt&gt; test) in the the write-write-sync-sync case.&lt;/p&gt;

&lt;p&gt;It is true that both threads would execute a sync in the context of their own thread, and that would no longer be the case. In fact that is the main concern with this patch: It is harder to trace the code to verify an edit is truly sync&apos;ed in all cases; I am also concerned about possible new/different deadlock scenarios with which the existing code is riddled.&lt;br/&gt;
Also I wonder if some folks do rely on changing deferred-sync-interval.&lt;/p&gt;

&lt;p&gt;If I weren&apos;t in Germany right with no VPN access I&apos;d offer to load 0.94 with the patch onto one of our test clusters and run some workload on it...&lt;/p&gt;</comment>
                            <comment id="13687320" author="sershe" created="Tue, 18 Jun 2013 22:48:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yeah I think you are correct. The check should check that the txid is greater than last successful txid and less than the failed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The successful id is currently set to the same as failed id, to solve this, tracking each batch is necessary (because otherwise one could come up with scenario with 3 batches &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;br/&gt;
Maybe, as other comment say, this is ok.&lt;/p&gt;</comment>
                            <comment id="13687372" author="eclark" created="Tue, 18 Jun 2013 23:27:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;tracking each batch is necessary (because otherwise one could come up with scenario with 3 batches ).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think that&apos;s true.  If you fail a sync.  Then you can&apos;t append to the write pipeline and there can be no more batches that can succeed either step. &lt;/p&gt;</comment>
                            <comment id="13687773" author="lhofhansl" created="Wed, 19 Jun 2013 08:33:18 +0000"  >&lt;p&gt;Should I make a backport jira for 0.94 so that we can discuss the merits of this change and 0.94 considerations separately?&lt;/p&gt;</comment>
                            <comment id="13687901" author="xieliang007" created="Wed, 19 Jun 2013 12:03:55 +0000"  >&lt;p&gt;I totally agreed with u,&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13687957" author="zjushch" created="Wed, 19 Jun 2013 13:10:38 +0000"  >&lt;p&gt;Attractd by the crazy improvement, I have tried a quick performance test, seems not same as my initial think.&lt;/p&gt;

&lt;p&gt;Test Data:&lt;br/&gt;
1.0.94 version with this patch&lt;br/&gt;
2.One client putting data to one regionserver(autoflush=true)&lt;br/&gt;
3.300 RPC handler for regionserver&lt;/p&gt;

&lt;p&gt;&lt;b&gt;a.client using 5 concurrent thread&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Without patch&#65306;&lt;/p&gt;

&lt;p&gt;Write Threads: 5 Write Rows: 200000 Consume Time: 42s&lt;br/&gt;
&lt;b&gt;Avg TPS: 4651&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;With patch&#65306;&lt;/p&gt;

&lt;p&gt;Write Threads: 5 Write Rows: 200000 Consume Time: 43s&lt;br/&gt;
&lt;b&gt;Avg TPS: 4545&lt;/b&gt;&lt;/p&gt;


&lt;p&gt;&lt;b&gt;b.client using 50 concurrent thread&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Without patch&#65306;&lt;/p&gt;

&lt;p&gt;Write Threads: 50 Write Rows: 2000000 Consume Time: 110s&lt;br/&gt;
&lt;b&gt;Avg TPS: 18018&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;With patch&#65306;&lt;/p&gt;

&lt;p&gt;Write Threads: 50 Write Rows: 2000000 Consume Time: 118s&lt;br/&gt;
&lt;b&gt;Avg TPS: 16806&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;c.client using 200concurrent thread&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Without patch&#65306;&lt;/p&gt;

&lt;p&gt;Write Threads: 200 Write Rows: 2000000 Consume Time: 80s&lt;br/&gt;
&lt;b&gt;Avg TPS: 24691&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;With patch&#65306;&lt;/p&gt;

&lt;p&gt;Write Threads: 200 Write Rows: 2000000 Consume Time: 64s&lt;br/&gt;
&lt;b&gt;Avg TPS: 30769&lt;/b&gt;&lt;/p&gt;


{format}&lt;br/&gt;
a&amp;gt; 5 YCSB clients, each with 80 concurrent write theads (auto-flush = true)&lt;br/&gt;
b&amp;gt; each YCSB writes 5000,000 rows&lt;br/&gt;
c&amp;gt; all 20 regions of the target table are moved to a single RS{format}

&lt;p&gt;As the above test description, it means 400 concurrent theads writing data to one RS.&lt;/p&gt;

&lt;p&gt;I personally think this patch will work if regionserver is under very high pressure,&lt;br/&gt;
for general pressure, it will degrade a little.&lt;/p&gt;

&lt;p&gt;I just take a quick test, maybe there&apos;s something wrong.&lt;br/&gt;
About the improvement scenario, more tests would be better.&lt;/p&gt;</comment>
                            <comment id="13687972" author="lhofhansl" created="Wed, 19 Jun 2013 13:24:30 +0000"  >&lt;p&gt;Hmm... Let&apos;s try to find out why the discrepancy between Feng&apos;s and Chunhui&apos;s tests.&lt;/p&gt;</comment>
                            <comment id="13687987" author="zjushch" created="Wed, 19 Jun 2013 13:37:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;Let&apos;s try to find out why the discrepancy between Feng&apos;s and Chunhui&apos;s tests.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The discrepancy should come from the diffrrent pressure, one client in my tests vs 5 clients * 80 concurrent threads in Feng&apos;s test.&lt;br/&gt;
Thus I personally presume that this patch just takes effect if RS is under very high pressure. &lt;br/&gt;
For the general scenarios, seems no improvement, even be worse, it needs more test to verify.&lt;/p&gt;



</comment>
                            <comment id="13688003" author="lhofhansl" created="Wed, 19 Jun 2013 13:43:49 +0000"  >&lt;p&gt;I guess that makes sense. More handoff required between threads, which would impact latency, but better throughput in high pressure cases because of fewer threads contenting for locks.&lt;/p&gt;

&lt;p&gt;Any chance to measure context switches during the test with/without the patch? I wonder if those increase due to thread handoffs, until the ctx switches from contended locks outweigh those.&lt;/p&gt;</comment>
                            <comment id="13688160" author="yehangjun" created="Wed, 19 Jun 2013 17:05:03 +0000"  >&lt;p&gt;Chunhui, just want to confirm whether your regionserver is running against a HDFS cluster or the local disk? And the row size?&lt;/p&gt;

&lt;p&gt;I asked this because the discrepancy between Honghua&apos;s test and Chunhui&apos;s test is mainly the number about the original model (w/o patch), just want to make sure the basic environment is the same.&lt;/p&gt;</comment>
                            <comment id="13688739" author="fenghh" created="Thu, 20 Jun 2013 01:21:48 +0000"  >&lt;p&gt;Thanks Chunhui for this verification test. We didn&apos;t test with small/medium write pressure, we&apos;ll do tests with small-medium write pressure soon and provide the numbers when done.&lt;/p&gt;

&lt;p&gt;A quick response on your test result: We never saw a such high throughput as 24691 for a single RS in cluster before we applied the new write thread model. We ever did a series of stress test for write throughput and the maximum we ever got is about 10000 using 1 YCSB client.&lt;/p&gt;

&lt;p&gt;&quot;Without patch&#65306;&lt;br/&gt;
Write Threads: 200 Write Rows: 2000000 Consume Time: 80s&lt;br/&gt;
Avg TPS: 24691&lt;br/&gt;
With patch&#65306;&lt;br/&gt;
Write Threads: 200 Write Rows: 2000000 Consume Time: 64s&lt;br/&gt;
Avg TPS: 30769&quot;&lt;/p&gt;</comment>
                            <comment id="13688754" author="fenghh" created="Thu, 20 Jun 2013 01:32:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  What&apos;s the row-size used in your test? You tested against a back-end HDFS, not local disk, right? &lt;/p&gt;

&lt;p&gt;  And would you test using a bit more test data(such as 5,000,000 - 10,000,000 rows)? Thanks. &lt;/p&gt;</comment>
                            <comment id="13688796" author="zjushch" created="Thu, 20 Jun 2013 02:12:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;What&apos;s the row-size used in your test?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Row:50 bytes, Value:100 bytes&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You tested against a back-end HDFS, not local disk, right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Using the real HDFS cluster&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the maximum we ever got is about 10000 using 1 YCSB client&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If the KV size is smaller than 200 and the client has used many concurrent threads, it should be abnormal.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we&apos;ll do tests with small-medium write pressure soon and provide the numbers when done&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks, it would be a good reference.&lt;/p&gt;</comment>
                            <comment id="13688936" author="fenghh" created="Thu, 20 Jun 2013 06:50:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt; We run the same tests as yours, and below are the result:&lt;/p&gt;

&lt;p&gt;  1). One YCSB client with 5/50/200 write threads respectively&lt;br/&gt;
  2). One RS with 300 RPC handlers, 20 regions  (5 data-nodes back-end HDFS running CDH 4.1.1)&lt;br/&gt;
  3). row-size = 150 bytes&lt;/p&gt;

&lt;p&gt;threads  row-count     new-throughput	 new-latency     old-throughput	 old-latency&lt;br/&gt;
---------------------------------------------------------------------------------------&lt;br/&gt;
5         200000       3191	         1.551(ms)       3172             1.561(ms)&lt;br/&gt;
50       2000000       23215             2.131(ms)       7437             6.693(ms)&lt;br/&gt;
200      2000000       35793             5.450(ms)       10816           18.312(ms)&lt;br/&gt;
---------------------------------------------------------------------------------------&lt;/p&gt;

&lt;p&gt;A). the difference is negligible when 5 threads of YCSB client&lt;br/&gt;
B). new-model still has 3X+ improvement compared to old-model when threads are 50/200&lt;/p&gt;

&lt;p&gt;Anybody else can help do the similar tests using the same test configuration as Chunhui?&lt;/p&gt;</comment>
                            <comment id="13688957" author="fenghh" created="Thu, 20 Jun 2013 07:08:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt;: We run the same tests as yours, and below are the result:&lt;br/&gt;
1). One YCSB client with 5/50/200 write threads respectively&lt;br/&gt;
2). One RS with 300 RPC handlers, 20 regions (5 data-nodes back-end HDFS running CDH 4.1.1)&lt;br/&gt;
3). row-size = 150 bytes&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;client-threads &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;row-count &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;new-model throughput &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;new-model latency &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;old-model throughput&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;old-model latency&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;200000 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3191&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.551(ms) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3172 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.561(ms)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2000000 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;23215 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.131(ms) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7437 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.693(ms)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;200 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2000000 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;35793 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.450(ms) &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10816 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18.312(ms)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;A). the difference is negligible when 5 threads of YCSB client, this is because &lt;br/&gt;
B). new-model still has 3X+ improvement compared to old-model when threads are 50/200&lt;br/&gt;
Can anybody else help do the tests using the same configurations as Chunhui?&lt;/p&gt;

&lt;p&gt;Another guess is the HDFS used by chunhui has much better performance on HLog&apos;s write/sync, which makes the new model in HBase has less impact. Just guess.&lt;/p&gt;</comment>
                            <comment id="13688962" author="zjushch" created="Thu, 20 Jun 2013 07:12:30 +0000"  >&lt;p&gt;As the above tests&#65292; try to find out why the old-throughput is so low.&lt;/p&gt;

&lt;p&gt;Do your client run on the regionserver or another separated server&#65311;&lt;/p&gt;</comment>
                            <comment id="13688973" author="fenghh" created="Thu, 20 Jun 2013 07:25:02 +0000"  >&lt;p&gt;Our comparison tests have only the RS bits different, and all others(client/HDFS/cluster/row-size...) remain the same. &lt;/p&gt;

&lt;p&gt;The client runs on a different machine other than the RS, we don&apos;t run client on RS because almost all our applications using HBase run their application in their own machines different from the HBase cluster.&lt;/p&gt;

&lt;p&gt;Actually we never saw a such high throughput as 18018/24691 for a single RS in our cluster. It&apos;s really weird &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="13689094" author="fenghh" created="Thu, 20 Jun 2013 10:09:02 +0000"  >&lt;p&gt;If possible, would anybody else help do the same comparison test as Chunhui/me? Thanks in advance. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yuzhihong%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;yuzhihong@gmail.com&quot;&gt;Ted Yu&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13689305" author="stack" created="Thu, 20 Jun 2013 14:55:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; You want to set up a rig to test this one?&lt;/p&gt;</comment>
                            <comment id="13689492" author="jmspaggi" created="Thu, 20 Jun 2013 18:28:28 +0000"  >&lt;p&gt;Sure! Let me prepare that. I will read this JIRA from the beginning and try to start the tests today.&lt;/p&gt;</comment>
                            <comment id="13690238" author="lhofhansl" created="Fri, 21 Jun 2013 12:48:55 +0000"  >&lt;p&gt;I ran some local tests, and even with a single threaded client I did not see a performance degradation with this patch (0.94).&lt;br/&gt;
I inserted 10m small rows. The cluster consists of a single RS/DN, HDFS is backed by a SSD. So the HDFS cost of writes should be low, and that in turns should bring out any new inefficiencies introduced by this patch.&lt;/p&gt;

&lt;p&gt;So from my angle this patch is good.&lt;/p&gt;</comment>
                            <comment id="13690259" author="jmspaggi" created="Fri, 21 Jun 2013 13:12:12 +0000"  >&lt;p&gt;Here are my results:&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Test&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Trunk&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;8755&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$FilteredScanTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;437101.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;431561.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomReadTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;774306.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;768559&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomScanWithRange100Test&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;21999.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;22277.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomSeekScanTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;134429.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;134370.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;112814.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;125324.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$SequentialWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;78574.42&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;80064.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;It&apos;s time. so the smaller, the better. So overall, it&apos;s pretty the same results, except for the random write which are 11% slower.&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$FilteredScanTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.27%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomReadTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.74%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomScanWithRange100Test&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-1.26%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomSeekScanTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.04%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$RandomWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-11.09%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.hadoop.hbase.PerformanceEvaluation$SequentialWriteTest&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-1.90%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I will redo the RandomWrite tests and try them with 1, 10 and 100 thread to see if only mono-threaded access is impacted...&lt;/p&gt;</comment>
                            <comment id="13690288" author="lhofhansl" created="Fri, 21 Jun 2013 13:55:16 +0000"  >&lt;p&gt;Interesting. For writing the WAL it should not make any difference whether we write sequentially or at random.&lt;br/&gt;
A possible reason is that the batching there is less efficient, maybe that brings out the extra thread hand off needed here.&lt;br/&gt;
(my test was the equivalent of a sequential write, but using a handcoded test)&lt;/p&gt;</comment>
                            <comment id="13690375" author="stack" created="Fri, 21 Jun 2013 15:20:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; Thanks boss.  What test is this?  YCSB w/ how many clients?  Thank you sir.&lt;/p&gt;</comment>
                            <comment id="13691138" author="jmspaggi" created="Sat, 22 Jun 2013 13:16:44 +0000"  >&lt;p&gt;It was PerformanceEvaluation and not YCSB. And I ran it with only one thread. It&apos;s now running with 10 threads. Will have the results tomorrow I think, or maybe by end of day if it&apos;s going fast enought. I will run YCSB after that, but not sure it&apos;s relevant in standalone mode. I will still try. And very soon add another dedicated test node...&lt;/p&gt;</comment>
                            <comment id="13691178" author="yehangjun" created="Sat, 22 Jun 2013 16:19:19 +0000"  >&lt;p&gt;It possibly doesn&apos;t have significant difference under standalone mode.&lt;br/&gt;
The point of this patch is to avoid heavy race conditions of multiple handler threads, we found many race conditions happened in HDFS client (when many threads called write/hflush concurrently)&lt;/p&gt;

&lt;p&gt;We tested on a single RS with a 5 data-node underlying HDFS cluster using YCSB, would you like to have a try on a similar environment?&lt;/p&gt;</comment>
                            <comment id="13708189" author="fenghh" created="Mon, 15 Jul 2013 01:38:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt;, what&apos;s your result of running YCSB against real cluster environment?&lt;/p&gt;</comment>
                            <comment id="13708433" author="jmspaggi" created="Mon, 15 Jul 2013 12:59:48 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;, the serves dedicated for those tests were to much different so I think it will have not been a good idea to run on them (difficult to interpret the results). So I just bought 3 absolutely identical nodes (the same as one I already have). By the end of the week I will have 4 servers with the same MB, same CPU, same memory (branch, PN, etc.) and same hard drive! Master will have a 1xSSD+1xSATA, others will have 2xSATA. To start.&lt;/p&gt;

&lt;p&gt;I will run the YCSB on that with and without this patch as soon as I get the hardware and I install the OS. Should be sometime later this week.&lt;/p&gt;

&lt;p&gt;I will also see if I can run PE.&lt;/p&gt;

&lt;p&gt;More to come.&lt;/p&gt;</comment>
                            <comment id="13708473" author="fenghh" created="Mon, 15 Jul 2013 14:00:13 +0000"  >&lt;p&gt;Thanks a lot &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt;, looking forward to your result.&lt;/p&gt;</comment>
                            <comment id="13711711" author="tychang" created="Wed, 17 Jul 2013 22:14:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt; In your test, you use 300 RPC handler. Since default is 10, and we use 50 in our production cluster, when do we really need that many RPC  handler? Will that be more of overhead than benefit? Do you see the write throughput up when increase the # of handler? Any particular scenario where high # of handler help?&lt;/p&gt;</comment>
                            <comment id="13717966" author="fenghh" created="Wed, 24 Jul 2013 04:24:40 +0000"  >&lt;p&gt;updated patch rebased on latest trunk code base&lt;/p&gt;</comment>
                            <comment id="13718252" author="jmspaggi" created="Wed, 24 Jul 2013 11:31:48 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;I was in a process to give it a try, is there a new version for 0.94 coming? Or the one attached in the JIRA is already the good one?&lt;/p&gt;</comment>
                            <comment id="13718270" author="fenghh" created="Wed, 24 Jul 2013 11:52:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt;-0.94-V1.patch is good. Let me know if any problem. Thanks Jean-Marc&lt;/p&gt;</comment>
                            <comment id="13721960" author="jmspaggi" created="Sun, 28 Jul 2013 14:49:57 +0000"  >&lt;p&gt;Ok. I did some other tries and here are the results.&lt;/p&gt;

&lt;p&gt;jmspaggi@hbasetest:~/hbase/hbase-$ cat output-1.1.2.txt&lt;br/&gt;
421428.8&lt;br/&gt;
jmspaggi@hbasetest:~/hbase/hbase-$ cat output-1.1.2-8755.txt&lt;br/&gt;
427172.1&lt;br/&gt;
jmspaggi@hbasetest:~/hbase/hbase-$ cat output-1.2.0.txt&lt;br/&gt;
419673.3&lt;br/&gt;
jmspaggi@hbasetest:~/hbase/hbase-$ cat output-1.2.0-8755.txt&lt;br/&gt;
432413.9&lt;/p&gt;

&lt;p&gt;This is elapse time. Between each iteration I totally delete (rm -rf) the hadoop directories, stop all the java processes, etc. Test is 10M randomWrite.&lt;/p&gt;

&lt;p&gt;So unfortunately I have not been able to see any real improvement. For YCSB, any specific load I should run to be able to see something better that without 8755? I guess it&apos;s a write intensive load that we want? Also, I have tested this on a pseudo-distributed instance (no more a standalone one), but I can dedicate 4 nodes to a test if required...&lt;/p&gt;</comment>
                            <comment id="13721973" author="fenghh" created="Sun, 28 Jul 2013 15:29:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt;, thanks for your test, some questions about your test: Is it against real HDFS? how many data-nodes and RS? what&apos;s the write pressure(client number, write thread number)? what&apos;s the total throughput you get?&lt;/p&gt;

&lt;p&gt;Yes this jira aims for throughput improvement under write intensive load. It should be tested and verified under write intensive load against real cluster / HDFS environment. And as you can see this jira only refactors the write thread model rather than tuning any write sub-phase along the whole write path for any individual write request, no obvious improvement is expected for low/ordinary write pressure.&lt;/p&gt;

&lt;p&gt;If you have a real cluster environment with 4 data-nodes, it would be better to re-do the test chunhui/I did with the similar test configuration/load which are listed in detail in above comments. 1 client with 200 write threads is OK for pressing a single RS and 4 clients each with 200 write threads for pressing 4 RS.&lt;/p&gt;

&lt;p&gt;Thanks again.&lt;/p&gt;</comment>
                            <comment id="13722098" author="jmspaggi" created="Mon, 29 Jul 2013 01:02:35 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;, I ran in pseudo-distributed for all the tests, which mean HBase again a real HDFS. Tests with 1.1.2 and 1.2.0. The test was a 10 million randomWrite test. Ran 10 of them.&lt;/p&gt;

&lt;p&gt;I will re-run some tests again a single node (still in the process to install the OS on the other one) and will but more load against it. Like you said, 200 threads doing 100 000 writes eaches... More to come...&lt;/p&gt;</comment>
                            <comment id="13759652" author="stack" created="Fri, 6 Sep 2013 00:03:58 +0000"  >&lt;p&gt;Let me try this.&lt;/p&gt;</comment>
                            <comment id="13761051" author="stack" created="Sat, 7 Sep 2013 14:06:01 +0000"  >&lt;p&gt;Rebase of trunk.&lt;/p&gt;</comment>
                            <comment id="13761065" author="stack" created="Sat, 7 Sep 2013 15:53:54 +0000"  >&lt;p&gt;Staring in on taking a look at this.  I tried HLogPerformanceEvaluation.  Going via ycsb would seem to add noise and indirection.  I ran with 1, 5, and 50 threads w/ sizes that are like Honghua&apos;s (key of 50 and value of 150).&lt;/p&gt;

&lt;p&gt;I ran the test like this on trunk using localfs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i in 1 5 50; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; j in 1 2 3; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation  -verify -threads &lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt; -iterations 1000000 -nocleanup -verbose -keySize 50 -valueSize 100 &amp;amp;&amp;gt; /tmp/log-patch&lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;${j}&quot;&lt;/span&gt;.txt; done; done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Needs fixes over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9460&quot; title=&quot;Fix HLogPerformanceEvaluation so runs against localfs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9460&quot;&gt;&lt;del&gt;HBASE-9460&lt;/del&gt;&lt;/a&gt; for above to work on localfs.&lt;/p&gt;

&lt;p&gt;With localfs, the patch is twice as slow.  localfs does not support sync so this is probably what makes the difference &amp;#8211; the extra threads do better w/ the dfsclient&apos;s stutter around sync.  Let me try up on hdfs next.&lt;/p&gt;

&lt;p&gt;Below are numbers.  I ran each test three times: i.e. three times without patch with one thread, then three times with patch and one thread, etc.&lt;/p&gt;

&lt;p&gt;Each thread did 1M puts.  The table is times for the test to complete in seconds so less is better.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Thread Count&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;WithoutPatch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;WithPatch&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.895&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;29.088&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.856&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;29.544&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.901&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;29.326&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;24.173&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;53.974&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;24.013&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.976&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;23.390&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.858&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;253.773&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;454.147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;247.095&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;443.215&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;254.044&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;449.043&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                            <comment id="13761563" author="fenghh" created="Mon, 9 Sep 2013 01:38:03 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;. Looking forward to your test result on hdfs&lt;/p&gt;</comment>
                            <comment id="13770455" author="stack" created="Wed, 18 Sep 2013 05:32:30 +0000"  >&lt;p&gt;Trying this on hdfs.  It takes WAY longer.  Threads stuck here:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;t1&quot;&lt;/span&gt; prio=10 tid=0x00007f49fd4fc800 nid=0xfc4 in &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait() [0x00007f49d0d99000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: TIMED_WAITING (on object monitor)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1795)
        - locked &amp;lt;0x0000000423dd1cc8&amp;gt; (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1689)
        at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1582)
        at org.apache.hadoop.hdfs.DFSOutputStream.sync(DFSOutputStream.java:1567)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:116)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:135)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1072)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1195)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:910)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:844)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:838)
        at org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation$HLogPutBenchmark.run(HLogPerformanceEvaluation.java:110)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:662)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here are numbers I have so far for WITHOUT patch:&lt;/p&gt;


&lt;p&gt;/tmp/log-patch1.1.txt:2013-09-17 21:19:31,495 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; wal.HLogPerformanceEvaluation: Summary: threads=1, iterations=1000000 took 991.258s 1008.819ops/s&lt;br/&gt;
/tmp/log-patch1.2.txt:2013-09-17 21:35:04,715 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; wal.HLogPerformanceEvaluation: Summary: threads=1, iterations=1000000 took 924.881s 1081.220ops/s&lt;br/&gt;
/tmp/log-patch1.3.txt:2013-09-17 21:51:32,416 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; wal.HLogPerformanceEvaluation: Summary: threads=1, iterations=1000000 took 979.312s 1021.125ops/s&lt;br/&gt;
/tmp/log-patch5.1.txt:2013-09-17 22:07:31,712 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; wal.HLogPerformanceEvaluation: Summary: threads=5, iterations=1000000 took 950.968s 5257.800ops/s&lt;br/&gt;
/tmp/log-patch5.2.txt:2013-09-17 22:23:39,680 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; wal.HLogPerformanceEvaluation: Summary: threads=5, iterations=1000000 took 939.312s 5323.045ops/s&lt;/p&gt;

&lt;p&gt;Will clean up later but write rate is constant whether 1 or 5 threads.  Will see when 50.  Looks like I need to mode the HLogPE.  It calls FSHLog#doWrite directory which is not as interesting since is by-passes locks in FSHLog when it does not call append.&lt;/p&gt;

&lt;p&gt;Will be back.&lt;/p&gt;</comment>
                            <comment id="13770888" author="stack" created="Wed, 18 Sep 2013 15:35:17 +0000"  >&lt;p&gt;Just parking the numbers here.   Takes same time to write 5x as much (5M entries by 5 threads) as 1M entries by 1 thread.  50x takes 6x longer (50 threads writing 50M entries) &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/tmp/log-patch1.1.txt:2013-09-17 21:19:31,495 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=1, iterations=1000000 took 991.258s 1008.819ops/s
/tmp/log-patch1.2.txt:2013-09-17 21:35:04,715 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=1, iterations=1000000 took 924.881s 1081.220ops/s
/tmp/log-patch1.3.txt:2013-09-17 21:51:32,416 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=1, iterations=1000000 took 979.312s 1021.125ops/s
/tmp/log-patch50.1.txt:2013-09-17 23:29:45,188 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=50, iterations=1000000 took 2960.095s 16891.350ops/s
/tmp/log-patch50.2.txt:2013-09-18 00:22:48,849 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=50, iterations=1000000 took 2924.844s 17094.930ops/s
/tmp/log-patch50.3.txt:2013-09-18 01:15:58,646 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=50, iterations=1000000 took 2927.617s 17078.736ops/s
/tmp/log-patch5.1.txt:2013-09-17 22:07:31,712 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=5, iterations=1000000 took 950.968s 5257.800ops/s
/tmp/log-patch5.2.txt:2013-09-17 22:23:39,680 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=5, iterations=1000000 took 939.312s 5323.045ops/s
/tmp/log-patch5.3.txt:2013-09-17 22:39:56,011 INFO  [main] wal.HLogPerformanceEvaluation: Summary: threads=5, iterations=1000000 took 947.183s 5278.811ops/s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13771587" author="stack" created="Thu, 19 Sep 2013 05:02:10 +0000"  >&lt;p&gt;I was wrong that HLogPE called doWrite.  It calls append as an HRegionServer would.&lt;/p&gt;

&lt;p&gt;Here are the numbers.  They confirm what &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt; found way back up top of this issue.&lt;/p&gt;

&lt;p&gt;I was running HLogPE against a five node HDFS cluster.  The DataNodes were persisting to a fusionio drive so little friction at the drive.&lt;/p&gt;

&lt;p&gt;Below are seconds elapsed running following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i in 1 5 50; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; j in 1 2 3; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; ./bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation  -verify -threads &lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt; -iterations 1000000 -nocleanup  -keySize 50 -valueSize 100 &amp;amp;&amp;gt; /tmp/log-patch&lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;${j}&quot;&lt;/span&gt;.txt; done; done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Thread Count&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;WithoutPatch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;WithPatch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;%diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;991.258&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1125.208&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-11.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;924.881&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1137.754&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-18.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;979.312&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1142.959&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-14.31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;950.968&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1914.448&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-50.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;939.312&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1918.188&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-51.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;947.183&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1939.806&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-51.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2960.095&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1918.808&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;54.26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2924.844&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1933.020&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;51.30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2927.617&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1955.358&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;49.72&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;So, about 20% slower when single threaded writes.  About 50% slower when five threads writing concurrently  BUT 50% faster when 50 concurrent threads (our current default).&lt;/p&gt;

&lt;p&gt;Can we have the best of both worlds somehow where we switch to this new model when high contention?&lt;/p&gt;</comment>
                            <comment id="13771714" author="yehangjun" created="Thu, 19 Sep 2013 09:03:13 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; for doing this experiment! The result looks positive in some way.&lt;br/&gt;
China is in holiday (Mid-Autumn Festival) until this Saturday, we would check your suggestion after the holiday.&lt;/p&gt;</comment>
                            <comment id="13772189" author="stack" created="Thu, 19 Sep 2013 19:23:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yehangjun&quot; class=&quot;user-hover&quot; rel=&quot;yehangjun&quot;&gt;Hangjun Ye&lt;/a&gt; Happy holidays!  I didn&apos;t profile to see where time is spent.  That would be next I&apos;d say.&lt;/p&gt;</comment>
                            <comment id="13773953" author="fenghh" created="Sun, 22 Sep 2013 02:15:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; thanks so much for the detailed comparison test!&lt;/p&gt;

&lt;p&gt;1. what are the respective OPS with/without patch? &amp;#8211; I wonder if the test stresses out the maximum throughput&lt;br/&gt;
2. is the write load against a single node, or five nodes? &amp;#8211; to confirm the throughput is per-node or per-cluster(with five nodes)&lt;/p&gt;

&lt;p&gt;We re-did a series of comparison tests in bed and use different HDFS/HBase versions, below are our conclusion (single node):&lt;br/&gt;
1. for trunk, the (maximum) throughout improvement is about 150% (2.5 times)&lt;br/&gt;
2. for 0.94.3(our internal branch), the (maximum) throughput improvement is about 200% (3 times)&lt;br/&gt;
3. for trunk against hdfs-3u3(&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt; used), the (maximum) throughput improvement is 40% (28000 vs. 20000) &amp;#8211; we wonder hdfs performance has some downgrade from 3u3 since current hdfs can&apos;t see a high hlog throughput such as 20000&lt;/p&gt;

&lt;p&gt;We&apos;ll redo the comparison test as you using HLogPE(we always use YCSB, not use HLogPE&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) to see if there is -50% downgrade when the thread number is 5.&lt;/p&gt;</comment>
                            <comment id="13774603" author="fenghh" created="Mon, 23 Sep 2013 14:58:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;If I weren&apos;t in Germany right with no VPN access I&apos;d offer to load 0.94 with the patch onto one of our test clusters and run some workload on it...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt; Wonder if you now have access to your test clusters, if possible would you please also help do the performance comparison test on your test clusters? (I&apos;m confused why no one else get comparable improvement test result as ours, while we get test results with almost same improvement for multi-tests) &lt;/p&gt;</comment>
                            <comment id="13775651" author="stack" created="Mon, 23 Sep 2013 20:56:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;What was implication of not shutting this?   Were tests failing or is this just make-work?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I did not save avg ops/s. It was a set amount of work.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;2. is the write load against a single node, or five nodes? &#8211; to confirm the throughput is per-node or per-cluster(with five nodes)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I had a client writing to a WAL hosted in hdfs on a 5-node HDFS cluster.&lt;/p&gt;

&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13779581" author="liushaohui" created="Fri, 27 Sep 2013 02:44:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;br/&gt;
We redo the comparision test using HogPE. Here are the results:&lt;/p&gt;

&lt;p&gt;Test env:&lt;/p&gt;

&lt;p&gt;hdfs: cdh 4.1.0, five datanode, each node has 12 sata disks.&lt;br/&gt;
hbase: 0.94.3&lt;br/&gt;
HLogPE is run on one of these datanodes, so one replica of hlog&apos;s block will be at local datanode.&lt;/p&gt;

&lt;p&gt;The params of HLogPE are: -iterations 1000000 -keySize 50 -valueSize 100, which are same as stack&apos;s tests.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i in 1 5 50 75 100; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; 
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; j in 1 2 3; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;
        ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -verify -threads &lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt; -iterations 1000000 -keySize 50 -valueSize 100 &amp;amp;&amp;gt; log-patch&lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;${j}&quot;&lt;/span&gt;.txt; 
        grep &lt;span class=&quot;code-quote&quot;&gt;&quot;Summary: &quot;&lt;/span&gt; log-patch&lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;${j}&quot;&lt;/span&gt;.txt
    done; 
done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Thread&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Count&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;WithoutPatch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;WithPatch&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;579.380&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;625.937&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-8.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;580.307&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;630.346&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-8.62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;577.853&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;654.20&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-13.21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;799.579&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;785.696&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;795.013&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;780.642&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;826.270&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;781.909&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3290.482&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1165.773&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3298.387&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1167.992&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3224.495&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1154.921&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4450.760&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1253.448&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;71.83&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4506.143&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1269.806&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;71.82&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4516.453&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1245.954&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;72.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5561.074&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1493.102&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5616.810&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1496.263&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5612.268&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1468.500&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.83&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;a, When thread number is 1, we see that the performance of our test is about 40% better than that of stack&apos;s test, both in old thread mode and new thread mode. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; what&apos;s the hdfs version in your test or are there special configs? &lt;br/&gt;
b, When thread number is 5, we do not see -50% downgrade.&lt;/p&gt;
</comment>
                            <comment id="13779607" author="liushaohui" created="Fri, 27 Sep 2013 03:31:09 +0000"  >&lt;p&gt;update the result table and add ops diff &lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Thread number&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Time without Patch &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Ops without Patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Time with Patch &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Ops with Patch &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Time diff % &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Ops diff % &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;579.38&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1725.983&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;625.937&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1597.605&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-8.04&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;580.307&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1723.226&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;630.346&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1586.43&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-8.62&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;577.853&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1730.544&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;654.205&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1528.573&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-13.21&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-11.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;799.579&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6253.291&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;785.696&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6363.785&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.74&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.77&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;795.013&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6289.206&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;780.642&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6404.984&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.81&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.84&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;826.27&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6051.291&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;781.909&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6394.606&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.37&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3290.482&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15195.343&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1165.773&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;42890&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64.57&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;182.26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3298.387&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15158.925&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1167.992&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;42808.516&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64.59&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;182.40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3224.495&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15506.304&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1154.921&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;43293.004&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64.18&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;179.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4450.76&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16851.055&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1253.448&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;59834.953&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;71.84&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;255.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4506.143&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16643.945&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1269.806&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;59064.141&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;71.82&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;254.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4516.453&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16605.951&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1245.954&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;60194.84&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;72.41&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;262.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5561.074&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;17982.137&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1493.102&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;66974.656&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.15&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;272.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5616.81&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;17803.699&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1496.263&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;66833.172&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.36&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;275.39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5612.268&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;17818.107&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1468.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;68096.695&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.83&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;282.18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Time diff = (Time without Patch - Time with Patch) / Time without Patch * 100&lt;br/&gt;
Ops diff = (Ops with Patch - Ops without Patch) / Ops without Patch * 100&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; What are the hdfs and hbase version of your test? We may rebo the tests in cluster with same hdfs and hbase versions as yours.&lt;/p&gt;</comment>
                            <comment id="13792279" author="liushaohui" created="Fri, 11 Oct 2013 02:42:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yehangjun&quot; class=&quot;user-hover&quot; rel=&quot;yehangjun&quot;&gt;Hangjun Ye&lt;/a&gt;&lt;br/&gt;
We redo the same HogPE comparision test using hbase trunk. Results are followed.&lt;/p&gt;

&lt;p&gt;Test env:&lt;br/&gt;
hdfs: cdh 4.1.0, five datanode, each node has 12 sata disks.&lt;br/&gt;
hbase: hbase trunk 0.97 r1516083 (for in r1516084, trunk update the protobuf to 2.5 which is incompatible with protobuf 2.4 used in cdh 4.1.0)&lt;br/&gt;
HLogPE is run on one of these datanodes, so one replica of hlog&apos;s block will be at local datanode.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Thread number &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Time without Patch(s) &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Ops without Patch &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Time with Patch(s) &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Ops with Patch &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Time diff %&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Ops diff % &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;580.309&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1723.22&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;624.709&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1600.745&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7.65&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;591.177&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1691.541&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;631.34&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1583.932&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-6.79&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-6.36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;591.948&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1689.338&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;634.518&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1575.999&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7.19&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-6.71&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;794.034&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6296.959&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1201.563&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4161.247&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-51.32&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-33.92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;781.033&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6401.778&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1191.776&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4195.419&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-52.59&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-34.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;805.597&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6206.577&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1187.179&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4211.665&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-47.37&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-32.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3222.659&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15515.139&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1815.586&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;27539.316&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;43.66&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;77.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3191.131&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15668.426&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1821.956&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;27443.033&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;42.91&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3222.407&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15516.352&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1817.754&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;27506.473&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;43.59&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;77.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4517.149&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16603.393&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2024.359&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37048.766&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.19&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;123.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4498.987&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16670.42&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2016.899&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37185.797&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.17&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;123.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4554.122&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16468.598&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2037.155&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;36816.051&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.27&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;123.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5186.292&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;19281.598&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2147.581&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;46564.016&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;58.59&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;141.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5181.344&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;19300.012&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2135.768&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;46821.563&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;58.78&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;142.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5189.396&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;19270.064&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2143.529&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;46652.039&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;58.69&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;142.10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                            <comment id="13792293" author="stack" created="Fri, 11 Oct 2013 03:11:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt; Our results are the same?&lt;/p&gt;</comment>
                            <comment id="13792301" author="fenghh" created="Fri, 11 Oct 2013 03:31:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;: Yes, seems this patch against trunk has obvious downgrade compared to against 0.94.3(our internal branch) for:&lt;br/&gt;
  1) 5 threads: withPatch has worse perf than withoutPatch (33% ops downgrade, 4.2K vs. 6.3K)&lt;br/&gt;
  2) 100 threads: withpatch&apos;s perf is about 2.5X of withoutPatch (46.6K vs. 19.2K)&lt;/p&gt;

&lt;p&gt;A short summary:&lt;br/&gt;
  1) withoutPatch, the max ops of HLog is less than 20K (19K for trunk and 17K for 0.94.3)&lt;br/&gt;
  2) withPatch, the max ops of HLog is more than 45K (46K for trunk and 68K for 0.94.3)&lt;br/&gt;
  3) for trunk, withPatch can have even worse perf than withoutPatch (about 33% downgrade)&lt;/p&gt;

&lt;p&gt;We&apos;ll try to figure out why withPatch performs worse than withoutPatch for trunk, and try to ensure the performance is about equal when stress is low and still keep obvious upgrade when stress is high. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; : would you please redo the test using 75/100 threads to re-confirm whether the ops upgrade matches our tests? (we see 37K vs 16K for 75 threads and 46K vs 19K for 100 threads)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjushch&quot; class=&quot;user-hover&quot; rel=&quot;zjushch&quot;&gt;chunhui shen&lt;/a&gt; : what version of HBase do you apply the patch to? trunk or 0.94? I wonder if the reason is the same for our difference and stack&apos;s&lt;/p&gt;</comment>
                            <comment id="13792313" author="stack" created="Fri, 11 Oct 2013 03:55:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt; will do.  Sorry.  Meant to do it earlier.  Will try to do some profiling too myself to see why worse wen low thread count.&lt;/p&gt;</comment>
                            <comment id="13792389" author="fenghh" created="Fri, 11 Oct 2013 06:18:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; thanks a lot &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;btw: the HLogPE test result withoutPatch/withPatch against 0.94.3(our internal branch) done by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=liushaohui&quot; class=&quot;user-hover&quot; rel=&quot;liushaohui&quot;&gt;Liu Shaohui&lt;/a&gt; matches the one against hdfs sequence file, described in the &apos;Description&apos; of this JIRA as below (17K vs. 68K):&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Three of my colleagues(Ye Hangjun / Wu Zesheng / Zhang Peng) at Xiaomi proposed a new write thread model for writing hdfs sequence file and the prototype implementation shows a 4X improvement for throughput (from 17000 to 70000+).&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13792422" author="stack" created="Fri, 11 Oct 2013 07:11:54 +0000"  >&lt;p&gt;Does that mean that we can just use HLogPE going forward evaluating this patch and not have to set up a cluster?  Because it gives same results as a cluster does?  Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13792430" author="fenghh" created="Fri, 11 Oct 2013 07:27:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;: Agree. Seems HLogPE is enough for profiling/evaluating, let us run YCSB on a cluster for later double verification? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13792433" author="yehangjun" created="Fri, 11 Oct 2013 07:36:10 +0000"  >&lt;p&gt;To clarify, a hdfs cluster is still needed to store the sequence file.&lt;/p&gt;</comment>
                            <comment id="13803932" author="stack" created="Thu, 24 Oct 2013 07:06:26 +0000"  >&lt;p&gt;Five datanodes with fusionio.  hbase tip of 0.96 branch and hadoop-2.1.0-beta.  HLogPE on master node.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/o patch time&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/o patch ops&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/ patch time&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/ patch ops&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1048.033s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;954.168ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1100.423s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;908.741ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1042.126s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;959.577ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1156.557s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;864.635ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1052.601s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;950.028ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1143.271s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;874.683ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;904.176s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5529.896ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1916.229s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2609.292ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;910.469s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5491.675ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1911.841s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2615.280ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;925.778s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5400.863ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1970.565s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2537.344ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2699.752s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18520.221ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1889.877s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;26456.748ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2689.678s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18589.586ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1922.716s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;26004.881ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2711.144s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18442.398ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1893.439s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;26406.977ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4945.563s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15165.108ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1997.553s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37545.938ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4852.779s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15455.063ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1992.425s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37642.570ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4921.685s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15238.684ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6224.527s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16065.479ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2086.691s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;47922.766ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6195.727s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16140.156ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2091.869s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;47804.145ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;Diffs are small when 1 thread only.  Its bad at 5 threads but thereafter the patch starts to shine.  If we could make the 5 threads better, we could commit this patch.&lt;/p&gt;</comment>
                            <comment id="13804957" author="fenghh" created="Fri, 25 Oct 2013 02:53:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; thanks a lot for the detailed perf test. agree with you. we&apos;ll try to make the 5 threads better&lt;/p&gt;</comment>
                            <comment id="13807214" author="sershe" created="Mon, 28 Oct 2013 21:02:28 +0000"  >&lt;p&gt;Question from above: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;...is separate notifier thread necessary? It doesn&apos;t do any blocking operations other than interacting with flusher thread, or taking syncedTillHere lock, which looks like it should be uncontested most of the time.&lt;br/&gt;
Couldn&apos;t flusher thread have the 4~ lines that set syncedTillHere?&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13807618" author="yehangjun" created="Tue, 29 Oct 2013 02:58:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt;, that&apos;s a very good question.&lt;br/&gt;
We found syncedTillHere.notifyAll() was a blocking operation and it might take a long time if many threads were waiting on it (which also surprised us). So we put this logic in a separate thread.&lt;/p&gt;</comment>
                            <comment id="13808163" author="sershe" created="Tue, 29 Oct 2013 16:34:43 +0000"  >&lt;p&gt;nice. Thanks!&lt;/p&gt;</comment>
                            <comment id="13819827" author="stack" created="Tue, 12 Nov 2013 04:52:32 +0000"  >&lt;p&gt;Moving out of 0.96.1.  We&apos;ll get it when we get it.&lt;/p&gt;</comment>
                            <comment id="13820068" author="jmspaggi" created="Tue, 12 Nov 2013 13:05:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;, any chance to rebase on the last 0.96 and optimize for less than 5 clients? If so I can give it a spin on a small cluster and run the PerfEval suite for 24 h to see the results? &lt;/p&gt;</comment>
                            <comment id="13820794" author="fenghh" created="Wed, 13 Nov 2013 01:34:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; sure, I&apos;m tuning for less than 5 threads, and will provide patch based on the last 0.96 when the bottleneck is found and the tuning is done.&lt;/p&gt;</comment>
                            <comment id="13829631" author="fenghh" created="Fri, 22 Nov 2013 03:53:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; : I fixed the downgrade for 5 threads and below are the test result, the write throughput capacity after tuning is &lt;b&gt;about 3.5 times&lt;/b&gt; of the one before tuning while keeping the perf is almost equal when thread number is &amp;lt;= 5. (for protobuf/test compatibility reason I based on hbase trunk 0.97 r1516083)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i in 1 3 5 10 25 50 100 200; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; j in 1; &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;
        ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLogPerformanceEvaluation -path /user/h_fenghonghua/&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;-thread-v2/ -verify -threads &lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt; -iterations 1000000 -keySize 50 -valueSize 100 &amp;amp;&amp;gt; log-patch&lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;${j}&quot;&lt;/span&gt;.txt;
        grep &lt;span class=&quot;code-quote&quot;&gt;&quot;Summary: &quot;&lt;/span&gt; log-patch&lt;span class=&quot;code-quote&quot;&gt;&quot;${i}&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;${j}&quot;&lt;/span&gt;.txt
    done;
done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;time-without-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-without-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;time-with-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-with-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;582&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1716&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;630&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1586&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7.5%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;943&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3179&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;951&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3153&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-0.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;820&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6091&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;847&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5899&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-3.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1141&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8760&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;983&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10166&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+16%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1920&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13019&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1286&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;19426&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+49.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3334&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;14995&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1627&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;30715&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+104.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5312&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18824&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1925&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;51943&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+185%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;200&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;11022&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18144&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3229&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;61922&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+241.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt; : I attached patches for latest trunk (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt;-trunk-v4.patch) and for the last 0.96 (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt;-0.96-v0.patch). Thanks very much if you can run similar comparison perf tests, and any issue please feel free to raise &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13829656" author="stack" created="Fri, 22 Nov 2013 05:11:33 +0000"  >&lt;p&gt;I&apos;m starting tests running now.&lt;/p&gt;
</comment>
                            <comment id="13830356" author="stack" created="Fri, 22 Nov 2013 22:03:02 +0000"  >&lt;p&gt;I ran Feng&apos;s script except I left out trying 200 threads.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;time-wo-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-wo-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;time-w-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-w-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;973.673&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1027.039&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1119.825&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;892.997&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-15%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1303.891&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2300.806&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1400.848&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2141.560&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;855.775&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5842.657&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;873.990&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5720.889&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1093.330&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9146.370&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1090.158&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9172.982&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1632.263&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15316.160&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1215.196&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;20572.813&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+25%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2432.653&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;20553.691&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1341.847&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37262.070&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+45%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4058.650&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;24638.734&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1725.729&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;57946.527&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;57%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;This was stock hadoop 2.2 and tip of the hbase trunk.&lt;/p&gt;

&lt;p&gt;Those are pretty big improvements once we pass out ten concurrent threads.  Some small down when one writer only is acceptable I&apos;d say.  Let me look again at the patch.&lt;/p&gt;</comment>
                            <comment id="13830360" author="stack" created="Fri, 22 Nov 2013 22:09:39 +0000"  >&lt;p&gt;We looked at using disruptor here?  MPSC seems to be what we have going on here to which disruptor seems pretty well suited?  Looking at the patch now...&lt;/p&gt;</comment>
                            <comment id="13830516" author="fenghh" created="Sat, 23 Nov 2013 02:04:47 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A small clarification of why we got so different ops-diff: I used &lt;b&gt;ops-diff = (new - old) / old&lt;/b&gt;  and you used &lt;b&gt;ops-diff = (new - old) / new&lt;/b&gt;. for example, the 100 threads result, old one is 24638 and new one is 57946, the ops-diff is &lt;b&gt;135.2%&lt;/b&gt; (new one is 2.35 times of old one), while you got &lt;b&gt;57%&lt;/b&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13830563" author="stack" created="Sat, 23 Nov 2013 04:40:46 +0000"  >&lt;p&gt;Pardon my bad math &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;  Below is correction (numbers are better now).&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;time-wo-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-wo-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;time-w-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-w-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;973.673&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1027.039&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1119.825&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;892.997&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-15%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1303.891&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2300.806&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1400.848&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2141.560&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;855.775&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5842.657&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;873.990&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5720.889&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1093.330&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9146.370&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1090.158&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9172.982&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1632.263&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15316.160&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1215.196&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;20572.813&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+34%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2432.653&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;20553.691&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1341.847&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37262.070&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+81%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4058.650&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;24638.734&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1725.729&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;57946.527&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+135%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                            <comment id="13830604" author="stack" created="Sat, 23 Nov 2013 07:15:32 +0000"  >&lt;p&gt;Looking at patch...&lt;/p&gt;

&lt;p&gt;Do these no longer pass?&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(&quot;Should have an outstanding WAL edit&quot;, ((FSHLog) log).hasDeferredEntries());&lt;br/&gt;
+    //assertTrue(&quot;Should have an outstanding WAL edit&quot;, ((FSHLog) log).hasDeferredEntries());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We have hard-coded 5 asyncSyncers?  Why 5?&lt;/p&gt;

&lt;p&gt;We are starting 7 threads to run this new write model : the 5 syncers, a notifier, and a writer (I suppose we are also removing one, the old LogSyncer &amp;#8211; so 6 new threads).&lt;/p&gt;

&lt;p&gt;We set the below outside of a sync block:&lt;/p&gt;

&lt;p&gt;+          this.asyncWriter.setPendingTxid(txid);&lt;/p&gt;

&lt;p&gt;Could we set the txid out of order here?   If so, will that be a problem?  Hmm... it seems not.  If we get a lower txid, we just return and prevail w/ the highest we&apos;ve seen.&lt;/p&gt;

&lt;p&gt;If we fail to find a free syncer, i don&apos;t follow what is going on w/ choosing a random syncer and setting txid as in below (Generally comments in patch are good &amp;#8211; this looks like a section that could do w/ some):&lt;/p&gt;

&lt;p&gt;+          this.lastWrittenTxid = this.txidToWrite;&lt;br/&gt;
+          boolean hasIdleSyncer = false;&lt;br/&gt;
+          for (int i = 0; i &amp;lt; asyncSyncers.length; ++i) {&lt;br/&gt;
+            if (!asyncSyncers&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.isSyncing()) &lt;/p&gt;
{
+              hasIdleSyncer = true;
+              asyncSyncers[i].setWrittenTxid(this.lastWrittenTxid);
+              break;
+            }
&lt;p&gt;+          }&lt;br/&gt;
+          if (!hasIdleSyncer) &lt;/p&gt;
{
+            int idx = rd.nextInt(asyncSyncers.length);
+            asyncSyncers[idx].setWrittenTxid(this.lastWrittenTxid);
+          }

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="13830678" author="fenghh" created="Sat, 23 Nov 2013 14:34:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do these no longer pass?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;=&amp;gt; yes, under new thread model, no explicit method to do the sync and can&apos;t tell if there is outstanding deferred entries (the AsyncWriter/AsyncSyncer threads do write/sync in a best-effort way)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We have hard-coded 5 asyncSyncers? Why 5?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;=&amp;gt; yes, I tried 2/3/5/10 and found 5 is the best number (2/3 have worse perf, 10 has equal perf but introduces too many extra threads)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we fail to find a free syncer, i don&apos;t follow what is going on w/ choosing a random syncer and setting txid as in below&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;=&amp;gt; when fail to find a idle syncer(which is doing sync), choosing a random syncer and setting txid that way fall into the same way before introducing extra asyncSyncer threads: when asyncWriter pushes new entries to hdfs before asyncSyncer sync the previously pushed ones, asyncSyncer gets notified the newly pushed txid, but these txid will be synced by next time after asyncSyncer is done with the current ones, notice we use txidToFlush to record txid each sync is for, and it can&apos;t change during each sync, while writtenTxid can change during each sync)&lt;/p&gt;

&lt;p&gt;To summary: the sync operation is the most time-consuming phase, under old write model every write handler issues a separate sync directly for itself(if not return early by syncedTillHere). and under new write model, though separate threads significantly reduce the lock race, but if concurrent write threads is few, the benefit by reducing lock race(fewer write threads, fewer benefit) can&apos;t offset the inefficiency by using a single asyncSyncer threads(each time asyncSyncer thread can only sync for a portion of the writes, but the write handlers which already have their entries in buffer or pushed to hdfs also need to wait for its completeness, and can&apos;t proceed until its next sync phase is done)&lt;br/&gt;
By introducing extra asyncSyncer threads, the correctness of this model is the same as before: still a single asyncWriter thread which push buffered entries to hdfs sequentially(txid increases sequentially), and when each asyncSyncer is done, it&apos;s guaranteed all txids smaller are pushed to hdfs and successfully sync-ed.&lt;/p&gt;</comment>
                            <comment id="13830681" author="fenghh" created="Sat, 23 Nov 2013 14:40:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;=&amp;gt; yes, under new thread model, no explicit method to do the sync and can&apos;t tell if there is outstanding deferred entries (the AsyncWriter/AsyncSyncer threads do write/sync in a best-effort way)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I meant calling &apos;sync&apos; can guarantee no outstanding deferred entries, but no calling &apos;sync&apos; after writew can&apos;t guarantee there must be some outstanding deferred entries since they can be sync-ed by asyncWriter/asyncSyncer threads. This is not the same behavior as under old write model.&lt;/p&gt;</comment>
                            <comment id="13830753" author="yuzhihong@gmail.com" created="Sat, 23 Nov 2013 19:35:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;2/3 have worse perf, 10 has equal perf but introduces too many extra threads&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do you remember how many extra threads were introduced ?&lt;/p&gt;</comment>
                            <comment id="13830755" author="stack" created="Sat, 23 Nov 2013 19:51:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you remember how many extra threads were introduced ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This question is answered above in my last review.  Why do you ask?&lt;/p&gt;</comment>
                            <comment id="13832233" author="fenghh" created="Tue, 26 Nov 2013 02:50:12 +0000"  >&lt;p&gt;tried 4 asyncSyncer threads, below are the results. a bit worse than 5 threads but looks like acceptable?&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-wo-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-w-patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ops-diff&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1716&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1572&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-8.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3179&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3189&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+0.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6091&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5593&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;-8.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8760&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9450&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+7.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13019&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18055&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+38.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;14995&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;26597&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+77.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18824&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;51441&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+173.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;200&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;18144&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;61531&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;+239.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;additional explanation on correctness when introducing extra asyncSyncer threads:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;when a txid(t0) is notified, all txid smaller than t0 must already be written to hdfs and by sync-ed: before t0 is notified, t0 must be sync-ed by an asyncSyncer thread; before t0 is sync-ed, t0 must be written to hdfs by asyncWriter thread; before t0 is written to hdfs, all txid smaller than t0 must be written to hdfs, so the sync of t0 can guarantee all txid smaller than t0 must be sync-ed (either before the sync of t0, or by the sync of t0)&lt;/li&gt;
	&lt;li&gt;when a txid(t0) can&apos;t find free(idle) asyncSyncer thread and added to a random one, it won&apos;t be sync-ed until its asyncSyncer thread is done with the txid at hand. but its entries already have been written to hdfs, and if any bigger txid than t0 (say t1) is successfully sync-ed by another parallel asyncSyncer thread, that sync can guarantee t0 also successfully sync-ed, hence when t1 is notified, t0 can also be correctly notified.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;any further comments?&lt;/p&gt;</comment>
                            <comment id="13838679" author="stack" created="Wed, 4 Dec 2013 07:18:36 +0000"  >&lt;p&gt;Sorry for the delay getting back to this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt; and thanks for the explanation.&lt;/p&gt;

&lt;p&gt;I am having trouble reviewing the patch because I am trying to understand what is going on here in FSHLog.  It is hard to follow (not your patch necessarily but what is there currently) in spite of multiple reviews.  I keep trying to grok what is going on because this is critical code.&lt;/p&gt;

&lt;p&gt;The numbers are hard to argue with and it does some nice cleanup of FSHLog which makes it easier to understand.  We could commit this patch and then work on undoing the complexity that is rife here; your patch adds yet more because it adds interacting threads w/ new synchronizations, notifications, AtomicBoolean states, etc., which cost performance-wise but at least it is clearer what is going on and we have tools for comparing approaches now.  We could work on simplication and removal of sync points in a follow-on (See below for a note on one approach).&lt;/p&gt;

&lt;p&gt;I now get why the need for multiple syncers.  It is a little counter-intuitiive given we want to batch up edits more to get more performance on the one hand, but then on the other, we have to sync more often because sync&apos;ing is outstanding for too much time, so much time it holds up handlers too long.&lt;/p&gt;

&lt;p&gt;+ I am trying to understand why we keep aside the edits in a linked-list.  This was there before your time.  You just continue the practice.  The original comment says &quot;We keep them cached here instead of writing them to HDFS piecemeal, because the HDFS write-method is pretty heavyweight as far as locking is concerned.&quot;    Yet, when we eventually flush the edits, we don&apos;t do anything special; we just call write on the dfsoutputstream.  We are not avoiding locking in hdfs.  It must be the hbase flush/update locking that is being referred to here.&lt;br/&gt;
+ AsyncSyncer is a confounding name for a class &amp;#8211; but it makes sense in this context.  The flush object in this thread is a syncer synchronization object not for memstore flushes... as I thought it was (there is use of flush in here when it probably should be sync to be consistent).&lt;/p&gt;

&lt;p&gt;Off-list, a few other lads are interested in reviewing this patch (it is a popular patch!)... our &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jon%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;jon@cloudera.com&quot;&gt;Jonathan Hsieh&lt;/a&gt; and possible &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=himanshu%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;himanshu@cloudera.com&quot;&gt;Himanshu Vashishtha&lt;/a&gt; because they are getting stuck in this area.  If they don&apos;t get to it soon, I&apos;ll commit unless objection.&lt;/p&gt;

</comment>
                            <comment id="13838687" author="fenghh" created="Wed, 4 Dec 2013 07:37:11 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, and also the review from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jon%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;jon@cloudera.com&quot;&gt;Jonathan Hsieh&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=himanshu%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;himanshu@cloudera.com&quot;&gt;Himanshu Vashishtha&lt;/a&gt;, and other experienced guys is welcome, it&apos;s better to have this patch reviewed by as many guys as possible.&lt;/p&gt;

&lt;p&gt;Any question on this patch is welcome &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13839662" author="stack" created="Thu, 5 Dec 2013 01:37:02 +0000"  >&lt;p&gt;Here is more review on the patch.  Make the changes suggested below and I&apos;ll +1 it.&lt;/p&gt;

&lt;p&gt;(Discussion off-line w/ Feng on this issue helped me better understand this patch and put to rest any notion that there is an easier &apos;fix&apos; than the one proposed here.  That said.  There is much room for improvement but this can be done in a follow-on)&lt;/p&gt;

&lt;p&gt;Remove these asserts rather than comment them out given they depended on a facility this patch removes.  Leaving them in will only make the next reader of the code &amp;#8211; very likely lacking the context you have &amp;#8211; feel uneasy thinking someone removed asserts just to get tests to pass.&lt;/p&gt;

&lt;p&gt; 8 -    assertTrue(&quot;Should have an outstanding WAL edit&quot;, ((FSHLog) log).hasDeferredEntries());&lt;br/&gt;
 9 +    //assertTrue(&quot;Should have an outstanding WAL edit&quot;, ((FSHLog) log).hasDeferredEntries());&lt;/p&gt;

&lt;p&gt;On the below...&lt;/p&gt;

&lt;p&gt;+import java.util.Random;&lt;/p&gt;

&lt;p&gt;... using a Random for choosing an arbitrary thread for a list of 4 is heavyweight.  Can you not take last digit of timestamp or nano timestamp or some attribute of the edit instead?  Something more lightweight?&lt;/p&gt;

&lt;p&gt;Please remove all mentions of AsyncFlush since it no longer exists:&lt;/p&gt;

&lt;p&gt;// all writes pending on AsyncWrite/AsyncFlush thread with&lt;/p&gt;

&lt;p&gt;Leaving it in will confuse readers when they can&apos;t find any such thread class.&lt;/p&gt;

&lt;p&gt;Is this comment right?&lt;/p&gt;

&lt;p&gt;// txid &amp;lt;= failedTxid will fail by throwing asyncIOE&lt;/p&gt;

&lt;p&gt;Should it be &amp;gt;= failedTxid?&lt;/p&gt;

&lt;p&gt;This should be volatile since it is set by AsyncSync and then used by the main FSHLog thread (you have an assert to check it not null &amp;#8211; maybe you ran into an issue here already?):&lt;/p&gt;

&lt;p&gt; +  private IOException asyncIOE = null;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;+  private final Object bufferLock = new Object();&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&apos;bufferLock&apos; if a very generic name. Could it be more descriptive?  It is a lock held for a short while while AsyncWriter moves queued edits off the globally seen queue to a local queue just before we send the edits to the WAL.  You add a method named getPendingWrites that requires this lock be held.  Could we tie the method and the lock together better? Name it pendingWritesLock?  (The name of the list to hold the pending writes is pendingWrites).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...because the HDFS write-method is pretty heavyweight as far as locking is concerned.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the heavyweight referred to in the above is hbase locking, not hdfs locking as the comment would imply.  If you agree (you know this code better than I), please adjust the comment.&lt;/p&gt;

&lt;p&gt;Comments on what these threads do will help the next code reader.  AsyncWriter does adding of edits to HDFS.  AsyncSyncer needs a comment because it is oxymoronic (though it makes sense in this context).  In particular, a comment would draw out why we need so many instances of a syncer thread because everyone&apos;s first thought here is going to be why do we need this?  Ditto on the AsyncNotifier.  In the reviews above, folks have asked why we need this thread at all and a code reader will likely think similar on a first pass.  Bottom-line, your patch raised questions from reviewers; it would be cool if the questions were answered in code comments where possible so the questions do not come up again.&lt;/p&gt;

&lt;p&gt;  4 +  private final AsyncWriter   asyncWriter;&lt;br/&gt;
  5 +  private final AsyncSyncer[] asyncSyncers = new AsyncSyncer&lt;span class=&quot;error&quot;&gt;&amp;#91;5&amp;#93;&lt;/span&gt;;&lt;br/&gt;
  6 +  private final AsyncNotifier asyncNotifier;&lt;/p&gt;

&lt;p&gt;You remove the LogSyncer facility in this patch.  That is good (need to note this in release notes).  Your patch should remove the optional flush config from hbase-default.xml too since it no longer is relevant.&lt;/p&gt;

&lt;p&gt;  3 -    this.optionalFlushInterval =&lt;br/&gt;
  4 -      conf.getLong(&quot;hbase.regionserver.optionallogflushinterval&quot;, 1 * 1000);&lt;/p&gt;

&lt;p&gt;I see it here...&lt;/p&gt;

&lt;p&gt;hbase-common/src/main/resources/hbase-default.xml:    &amp;lt;name&amp;gt;hbase.regionserver.optionallogflushinterval&amp;lt;/name&amp;gt;&lt;/p&gt;

&lt;p&gt;A small nit is you might look at other threads in hbase and see how they are named... &lt;/p&gt;

&lt;p&gt;3 +    asyncWriter = new AsyncWriter(&quot;AsyncHLogWriter&quot;);&lt;/p&gt;

&lt;p&gt;Ditto here:&lt;/p&gt;

&lt;p&gt; +      asyncSyncers&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; = new AsyncSyncer(&quot;AsyncHLogSyncer&quot; + i);&lt;/p&gt;

&lt;p&gt;Probably make the number of asyncsyncers a configuration (you don&apos;t have to put the option out in hbase-default.xml.. just make it so that if someone is reading the code and trips over this issue, they can change it by adding to hbase-site.xml w/o having to change code &amp;#8211; lets not reproduce the hard-coded &apos;80&apos; that is in the head of dfsclient we discussed yesterday &amp;#8211; smile).&lt;/p&gt;

&lt;p&gt;... and here: asyncNotifier = new AsyncNotifier(&quot;AsyncHLogNotifier&quot;);&lt;/p&gt;

&lt;p&gt;Not important but check out how other threads are named in hbase.  It might be good if these better align.&lt;/p&gt;

&lt;p&gt;Maybe make a method for shutting down all these thread or use the Threads#shutdown method in Threads.java?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;LOG.error(&quot;Exception while waiting for AsyncNotifier threads to die&quot;, e);&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do LOG.error(&quot;Exception while waiting for&quot; + t.getName() + &quot; threads to die&quot;, e); instead?&lt;/p&gt;

&lt;p&gt;Call this method per AsyncSyncer rather than do as you have here:&lt;/p&gt;

&lt;p&gt;  6 +    try {&lt;br/&gt;
  7 +      for (int i = 0; i &amp;lt; asyncSyncers.length; ++i) &lt;/p&gt;
{
  8 +        asyncSyncers[i].interrupt();
  9 +        asyncSyncers[i].join();
 10        }
&lt;p&gt; 11 +    } catch (InterruptedException e) &lt;/p&gt;
{
 12 +      LOG.error(&quot;Exception while waiting for AsyncSyncer threads to die&quot;, e);
 13      }

&lt;p&gt;where if we fail to join on the first asyncsync, we will not shut down the rest.  This is a nit.  Not important since we are closing down anyways and if an exception here, it is probably fatal.&lt;/p&gt;

&lt;p&gt;These threads are NOT daemon threads though so a fail to shut them down will hold up the JVM&apos;s going down.  Maybe they should all be daemon threads just in case?&lt;/p&gt;

&lt;p&gt;I see this in close:&lt;/p&gt;

&lt;p&gt;  4 +          synchronized (bufferLock) &lt;/p&gt;
{
  5 +            doWrite(info, logKey, edits, htd);
  6 +            txid = this.unflushedEntries.incrementAndGet();
  7 +          }

&lt;p&gt;... but we do not seem to be doing it on the other call to doWrite at around line #969 inside in append.  Should we be holding the lock at this location (maybe we do and I am just not seeing it because I am just looking at FSHLog and the patch side-by-side)?&lt;/p&gt;

&lt;p&gt;What is going on here?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  1 +    &lt;span class=&quot;code-comment&quot;&gt;// wake up (called by (write) handler thread) AsyncWriter thread
&lt;/span&gt;  0 +    &lt;span class=&quot;code-comment&quot;&gt;// to write buffered writes to HDFS
&lt;/span&gt;  1 +    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void setPendingTxid(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; txid) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This method is only called at close time if I read the patch right.  It is not called by &apos;...(write) handler thread&apos;?  I am having trouble understanding how this signaling works.&lt;/p&gt;

&lt;p&gt;Is this &apos;fatal&apos;?  Or is it an &apos;error&apos; since we are going into recovery... trying to role the log?&lt;/p&gt;

&lt;p&gt;  2 +            LOG.fatal(&quot;Error while AsyncWriter write, request close of hlog &quot;, e);&lt;/p&gt;

&lt;p&gt;Here we failed a write:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 11 +          } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt;(IOException e) {
 10 +            LOG.fatal(&lt;span class=&quot;code-quote&quot;&gt;&quot;Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; AsyncWriter write, request close of hlog &quot;&lt;/span&gt;, e);
  9              requestLogRoll();
  8 -            Threads.sleep(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.optionalFlushInterval);
  7 +
  6 +            asyncIOE = e;
  5 +            failedTxid.set(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.txidToWrite);
  4            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.. and request a log roll yet we carry on to try and sync, an op that will likely fail?   We are ok here?  We updated the write txid but not the sync txid so that should be fine.&lt;/p&gt;

&lt;p&gt;Do we need this: if (!asyncSyncers&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.isSyncing())  DFSClient will allow us call sync concurrently?&lt;/p&gt;

&lt;p&gt;Yeah... could use the last digit of the txid rather than random?  That should be enough?&lt;/p&gt;

&lt;p&gt; 3 +            int idx = rd.nextInt(asyncSyncers.length);&lt;br/&gt;
 2 +            asyncSyncers&lt;span class=&quot;error&quot;&gt;&amp;#91;idx&amp;#93;&lt;/span&gt;.setWrittenTxid(this.lastWrittenTxid);&lt;/p&gt;

&lt;p&gt;In the above, what if the writtentxid is &amp;gt; than what we are to sync?  Will that cause a problem?  What will the sync do in this case?  (nvm... I asked this question already.  It is answered later in the code.  The sync threads are running all the time.  I thought they fired once and completed.  My misunderstanding.   Ignore the above).&lt;/p&gt;

&lt;p&gt;Can these be static classes or do they need context form the hosting FSHLog?&lt;/p&gt;

&lt;p&gt;private class AsyncSyncer extends HasThread {&lt;/p&gt;

&lt;p&gt;This class definetly needs more comment either here or above when it is set up as a data member:&lt;/p&gt;

&lt;p&gt;  3 +  // thread to request HDFS to sync the WALEdits written by AsyncWriter&lt;br/&gt;
  2 +  // to make those WALEdits durable on HDFS side&lt;/p&gt;

&lt;p&gt;These method names should not talk about &apos;flush&apos;.  They should be named &apos;sync&apos; instead:&lt;/p&gt;

&lt;p&gt;  3 +    private long txidToFlush = 0;&lt;br/&gt;
  2 +    private long lastFlushedTxid = 0;&lt;/p&gt;

&lt;p&gt;Same for the flushlock.&lt;/p&gt;

&lt;p&gt;Why atomic boolean and not just a volatile here?&lt;/p&gt;

&lt;p&gt;private AtomicBoolean isSyncing = new AtomicBoolean(false);&lt;/p&gt;

&lt;p&gt;You are not using any feature of Atomic other than get and set so volatile should work.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;+        LOG.info(getName() + &quot; exiting&quot;);&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The above is very important.  All your threads do this?  Nothing worse than a silently exiting thread (smile).&lt;/p&gt;

&lt;p&gt;Here is that comment again:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  1 +  &lt;span class=&quot;code-comment&quot;&gt;// appends &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; writes to the pendingWrites. It is better to keep it in
&lt;/span&gt;  0 +  &lt;span class=&quot;code-comment&quot;&gt;// our own queue rather than writing it to the HDFS output stream because
&lt;/span&gt;  1 +  &lt;span class=&quot;code-comment&quot;&gt;// HDFSOutputStream.writeChunk is not lightweight at all.
&lt;/span&gt;  2 +  &lt;span class=&quot;code-comment&quot;&gt;// it&apos;s caller&apos;s responsibility to hold updateLock before call &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It talks about writeChunk being expensive but we are not doing anything to ameliorate dfsclient writes if I dig down into our log writer... we just write as we go.  So, the comment is either correct and we are ignoring it in implementation or the comment is wrong and we should be removed.  This is out of scope of your patch.  You are just moving this comment I&apos;d say.  If so, update the comment to add a TODO saying needs investigation.&lt;/p&gt;

&lt;p&gt;Patch is great. Lets get it in.  The above remarks are mostly minor just looking for clarification.  Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;/p&gt;











</comment>
                            <comment id="13840001" author="v.himanshu" created="Thu, 5 Dec 2013 10:32:58 +0000"  >&lt;p&gt;This is some awesome stuff happening here. &lt;/p&gt;

&lt;p&gt;I have few comments apart from what Stack already mentioned.&lt;/p&gt;

&lt;p&gt;1) log rolling thread safety: Log rolling happens in parallel with flush/sync. Currently, in FSHLog, sync call grabs the updateLock to ensure it has a non-null writer (because of parallel log rolling). How does this patch address the non-null writer? Or is it not needed anymore? Also, if you go for the updatelock in sync, that might result in deadlock.&lt;/p&gt;

&lt;p&gt;2) Error handling: It is not very clear how is flush/sync failures are being handled? For example, if a write fails for txid 10, it notifies AsyncSyncer that writer is done with txid 10. And, AsyncSyncer notifies the notifier thread, which finally notifies the blocked handler using notifyAll. The handler checks for the failedTxid here:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (txid &amp;lt;= &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.failedTxid.get()) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Let&apos;s say there are two handlers waiting for sync, t1 on txid 8 and t2 on txid 10. And, t1 wakes up on notification. Would t1 also get this exception? Wouldn&apos;t it be wrong, because txid 8 may have succeeded? Please correct me if I missed anything.&lt;/p&gt;

&lt;p&gt;3) I think the current HLogPE doesn&apos;t do justice to the real use case. Almost &lt;b&gt;all&lt;/b&gt; HLog calls are appendNoSync, followed by a sync call.&lt;br/&gt;
In the current HLogPE, we are calling append calls, which also does the sync. When I changed it to represent the above common case,&lt;br/&gt;
the performance numbers of current FSHLog using HLogPE improves quite a bit. I still need to figure out the reason, but the HLogPE change affects the perf numbers considerably IMO.&lt;br/&gt;
For example, on a 5 node cluster, I see this difference on trunk:&lt;br/&gt;
Earlier: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Summary: threads=3, iterations=100000 took 218.590s 1372.432ops/s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Post HlogPE change:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Summary: threads=3, iterations=100000 took 172.648s 1737.640ops/s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I think it would be great if we can test this with the correct HLogPE. What do you think Fenghua?&lt;/p&gt;

&lt;p&gt;4) Perf numbers are super impressive. It would have been wonderful to have such numbers for lesser number of handler threads also (e.g., 5-10 threads). IMHO, that represents most common case scenario, but I could be wrong. I know this has been beaten to death in the discussions above, but just echoing my thoughts here. &lt;/p&gt;

&lt;p&gt;5) I should also mention that while working on a different use case, I was trying to bring a layer of indirection b/w regionserver handlers and sync operation (Sync is the most costly affair in all HLog story). What resulted is a separate set of syncer threads, which does the work of flushing the edits and syncing to HDFS. This is what it looks like:&lt;br/&gt;
a) The handlers append their entries to the FSHLog buffer as they do currently. &lt;br/&gt;
b) They invoke sync API. There, they wait on the Syncer threads to do the work for them and notify. &lt;br/&gt;
c) It results in batched sync effort but without much extra locking/threads. &lt;br/&gt;
Basically, it is similar to what you did here but minus Writer &amp;amp; Notifier threads and minus the bufferlock.&lt;br/&gt;
I mentioned it here because with that approach, I see some perf improvement even with lesser number of handler threads. And, it also keeps the current deferredLogFlush behavior. This work is still in progress and is still a prototype. It would be great to know your take on it.&lt;/p&gt;

&lt;p&gt;Here are some numbers on a 5 node cluster; hdfs 2.1.0; hbase is trunk; client on a different node. I haven&apos;t tested it with larger number of threads but would be good to compare I think (its 2am here... ). It uses 3 syncers at the moment (and varying it would be a good thing to experiment too). &lt;br/&gt;
Also, without patch in the below table means trunk + HLogPE patch.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/o patch time&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/o patch ops&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/ patch time&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;w/ patch ops&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;172.648s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1737.640ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;170.332s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1761.266ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;170.977s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1754.622ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;174.568s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1718.528ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;213.738s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2339.313ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;191.119s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2616.171ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;211.072s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2368.860ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;189.671s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2636.144ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;254.641s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3926.419ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;216.494s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4619.319ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;251.503s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3978.564ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;215.333s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4643.266ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;251.692s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3970.579ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;217.151s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4605.854ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;20&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;648.943s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6163.870ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;646.279s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6189.277ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;20&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;658.654s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6072.991ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;656.277s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6094.987ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;282.654s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8861.991ops/s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;249.277s&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10033.987ops/s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                            <comment id="13840004" author="v.himanshu" created="Thu, 5 Dec 2013 10:42:03 +0000"  >&lt;p&gt;Attached is the prototype patch (HLogPE change inclusive). Thanks.&lt;/p&gt;</comment>
                            <comment id="13840074" author="stack" created="Thu, 5 Dec 2013 13:35:10 +0000"  >&lt;p&gt;Nice review &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=himanshu%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;himanshu@cloudera.com&quot;&gt;Himanshu Vashishtha&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On 3., above, yes, that is true.  HLogPE does not seem to be representative as you suggest.  But does your  change below....&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-            hlog.append(hri, hri.getTable(), walEdit, now, htd, region.getSequenceId());
+            &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is how almost all users of HLog use it (all but compaction calls).
&lt;/span&gt;+            &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; txid = hlog.appendNoSync(hri, hri.getTable(), walEdit, clusters, now, htd,
+              region.getSequenceId(), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, nonce, nonce);
+            hlog.sync(txid);
+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... bring it closer to a &apos;real&apos; use case?  I see over in HRegion that we do a bunch of appendNoSync in minibatch or even in put before we call sync.   Should we append more than just one set of edits before we call the sync?&lt;/p&gt;

&lt;p&gt;I suppose on a regionserver with a load of regions loaded up on it, all these syncs can come crashing in on top of each other on to the underlying WAL in an arbitary manner &amp;#8211; something Feng Honghua&apos;s patch mitigates some by making it so syncs are done when FSHLog thinks it appropriate rather than when some arbitrary HRegion call thinks it right ... and this is probably part of the reason for the perf improvement.&lt;/p&gt;

&lt;p&gt;Could we better regulate the sync calls so they are even less arbitrary?  Even them out?  It could make for better performance if there was a mechanism against syncs clumping together.&lt;/p&gt;

&lt;p&gt;Looking at your patch, the syncer is very much like Feng Honghua&apos;s &amp;#8211; it is interesting that you two independently came up w/ similar multithreaded syncing mechanism.  That would seem to &apos;prove&apos; this is a good approach.  Feng&apos;s patch is much further along with a bunch of cleanup of FSHLog.  Will wait on his comments on what he thinks of doing without AsyncWriter and AsyncNotifier.&lt;/p&gt;

&lt;p&gt;Looks like your patch is far enough long for us to do tests comparing the approaches? &lt;/p&gt;
</comment>
                            <comment id="13840663" author="jmhsieh" created="Thu, 5 Dec 2013 22:58:29 +0000"  >&lt;p&gt;I posted v4 for trunk on reviewboard and will reviewing there.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://reviews.apache.org/r/16052/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/16052/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13840710" author="stack" created="Thu, 5 Dec 2013 23:54:53 +0000"  >&lt;p&gt;Let me look into changing HLogPE after Himanshu&apos;s suggestion above.  I&apos;ll add an option to batch up edits some.  My sense is that it will make the difference between current code and Honghua&apos;s patch even larger when the count of threads is low but that it will not change the numbers when thread count is high.&lt;/p&gt;</comment>
                            <comment id="13840733" author="v.himanshu" created="Fri, 6 Dec 2013 00:24:03 +0000"  >&lt;p&gt;I agree on HLogPE comment. Yes, grouping appendNoSync before calling sync is the right way to go. The existing one is way-off the real use.&lt;/p&gt;

&lt;p&gt;I figured that the w/o patch col in the above table contains Feng&apos;s patch too. I am re-running the experiments at the moment with three versions: Trunk, Trunk + Feng&apos;s patch, Trunk + Syncer&apos;s approach. And, all versions have that HLogPe fix on it; will report back the numbers once done.&lt;/p&gt;

&lt;p&gt;Going by what we are seeing here, batching sync calls is definitely the right way IMO.&lt;br/&gt;
I agree that Feng Honghua&apos;s patch has been tested well enough, and I really like the radical cleanup it does. The code reads pretty clean now, though it involves far more number of threads and synchronization stuff (which makes it more interesting to debug too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;), I just wanted to ensure that it is safe. &lt;/p&gt;

&lt;p&gt;The reason I mentioned this different approach is it adds lesser number of threads (while keeping the current behaviour), and also shows improvement with smaller number of handlers, which to me looks like a nice win over current FSHLog. This is still in a prototype stage, and I absolutely don&apos;t want to block Feng&apos;s superb piece of work here. It would be good to know his thoughts on this. Thanks.&lt;/p&gt;</comment>
                            <comment id="13840812" author="fenghh" created="Fri, 6 Dec 2013 01:49:44 +0000"  >&lt;p&gt;Thanks very much for &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;&apos;s review and further improvement suggestion. Currently I have some other stuff on hand to handle... I&apos;ll try best to come back soon and read through above comments. Thanks again.&lt;/p&gt;</comment>
                            <comment id="13841093" author="apurtell" created="Fri, 6 Dec 2013 08:23:04 +0000"  >&lt;p&gt;This work looks pretty far along. If we can get it in soon I would be willing to try putting this in to 0.98 so this major improvement can manifest in a release. Exciting results.&lt;/p&gt;</comment>
                            <comment id="13842858" author="v.himanshu" created="Mon, 9 Dec 2013 02:35:22 +0000"  >&lt;p&gt;Patch is remarkably stale after 10010 and 10048 went in; attaching a rebased patch of the original version.&lt;/p&gt;</comment>
                            <comment id="13843224" author="fenghh" created="Mon, 9 Dec 2013 15:05:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; : thanks for the comment, below are the comments after corresponding change. a new patch based on &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;&apos;s latest v5 patch is attached(thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Remove these asserts rather than comment them out given they depended on a facility this patch removes.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;using a Random for choosing an arbitrary thread for a list of 4 is heavyweight&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Please remove all mentions of AsyncFlush since it no longer exists&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Is this comment right? // txid &amp;lt;= failedTxid will fail by throwing asyncIOE;  Should it be &amp;gt;= failedTxid?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; this comment is right: txid larger than failedTxid isn&apos;t sync-ed by the one that notifies failedTxid. but txid smaller than or equal to failedTxid is (not must be, but since we don&apos;t maintain a txid range to syncer mapping, so we fail all txid smaller than or equal to failedTxid, this aligns with HBase&apos;s write semantic of &apos;failed write may succeed in fact&apos;. this is a point we can refine later on by adding txid range to sync operation mapping to precisely indicate failure)&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This should be volatile since it is set by AsyncSync and then used by the main FSHLog thread (you have an assert to check it not null &#8211; maybe you ran into an issue here already?):   + private IOException asyncIOE = null;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&apos;bufferLock&apos; if a very generic name. Could it be more descriptive? It is a lock held for a short while while AsyncWriter moves queued edits off the globally seen queue to a local queue just before we send the edits to the WAL. You add a method named getPendingWrites that requires this lock be held. Could we tie the method and the lock together better? Name it pendingWritesLock? (The name of the list to hold the pending writes is pendingWrites).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;(because the HDFS write-method is pretty heavyweight as far as locking is concerned.) I think the heavyweight referred to in the above is hbase locking...please adjust the comment&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Comments on what these threads do will help the next code reader&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Your patch should remove the optional flush config from hbase-default.xml too since it no longer is relevant&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A small nit is you might look at other threads in hbase and see how they are named...It might be good if these better align&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Probably make the number of asyncsyncers a configuration&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;but we do not seem to be doing it on the other call to doWrite at around line #969 inside in append&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; doWrite is called inside append, and the bufferLock(now renamed to pendingWritesLock) is held there&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This method(setPendingTxid) is only called at close time if I read the patch right&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; it&apos;s called inside append() once doWrite() is done to notify AsyncWriter there are new pendingWrites to write to HDFS, it&apos;s not called at close time. you can double check it&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Is this &apos;fatal&apos;? Or is it an &apos;error&apos; &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;and request a log roll yet we carry on to try and sync, an op that will likely fail? We are ok here? We updated the write txid but not the sync txid so that should be fine.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; we can&apos;t retry to sync after a log roll since we can&apos;t sync to a new hlog while the writes were written to the old hlog. we failed all the transactions with txid &amp;lt;= write txid, it&apos;s ok here.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Do we need this: if (!asyncSyncers[ i ].isSyncing()) DFSClient will allow us call sync concurrently. I think DFSClient allow us call sync concurrently...HDFS will handle(synchronize) concurrent sync?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Can these be static classes or do they need context form the hosting FSHLog?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; they all need context from hosting FSHLog (such as writer/asyncWriter/asyncSyncer/asyncNotifier)&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;These method names should not talk about &apos;flush&apos;. They should be named &apos;sync&apos; instead. Same for the flushlock.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Why atomic boolean and not just a volatile here? private AtomicBoolean isSyncing = new AtomicBoolean(false);&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The above is very important. All your threads do this?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; yes&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It talks about writeChunk being expensive but we are not doing anything to ameliorate dfsclient writes if I dig down into our log writer&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; done (remove it)&lt;/p&gt;</comment>
                            <comment id="13843227" author="fenghh" created="Mon, 9 Dec 2013 15:07:50 +0000"  >&lt;p&gt;I&apos;ll read and update according to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;&apos;s comment tomorrow. thanks.&lt;/p&gt;</comment>
                            <comment id="13843231" author="jmspaggi" created="Mon, 9 Dec 2013 15:18:49 +0000"  >&lt;p&gt;For the random, you can also use something like System.currentTimeMillis() % asyncSyncers.length; Not saying that yours is not correct &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13843925" author="fenghh" created="Tue, 10 Dec 2013 04:28:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;: &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;1) log rolling thread safety: Log rolling happens in parallel with flush/sync. Currently, in FSHLog, sync call grabs the updateLock to ensure it has a non-null writer (because of parallel log rolling). How does this patch address the non-null writer? Or is it not needed anymore? Also, if you go for the updatelock in sync, that might result in deadlock.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; Good question&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. in rollWriter(), before switching this.writer to the newly created writer, &lt;b&gt;updateLock is held&lt;/b&gt; and &lt;b&gt;cleanupCurrentWriter() is called&lt;/b&gt;.  &lt;b&gt;updateLock is held&lt;/b&gt; guarantees no new edits enters pendingWrites, and &lt;b&gt;cleanupCurrentWriter() is called&lt;/b&gt; guarantees all edits in current pendingWrites must be written to hdfs and sync-ed(inside this method &apos;sync()&apos; is called to provide this guarantee). This means when switching this.writer to newly HLog writer, no new edits enter and all current edits have already been sync-ed to hdfs, all AsyncWriter/AsyncSyncer threads have nothing to do and are idle, so no log rolling thread safety issue here&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;2) Error handling: It is not very clear how is flush/sync failures are being handled?... Let&apos;s say there are two handlers waiting for sync, t1 on txid 8 and t2 on txid 10. And, t1 wakes up on notification. Would t1 also get this exception? Wouldn&apos;t it be wrong, because txid 8 may have succeeded? Please correct me if I missed anything.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; your understanding here is correct, but the write semantic of HBase is &apos;successful write response means a successful write, but failed write response can mean either a successful write or a failed write&apos;, right? and I have already mentioned this in above comment. this behavior can be improved by adding a txid-range to txid mapping, this mapping can help exactly indicate a failed txid fail which pending txid-range&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;3) I think the current HLogPE doesn&apos;t do justice to the real use case...Almost all HLog calls are appendNoSync, followed by a sync call. In the current HLogPE, we are calling append calls, which also does the sync&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; you&apos;re right, but from an whole view of write process, these two have no big difference concerning performance: append() calls sync() inside, and in real case of HRegion.java, appendNoSync and sync is called, since sync now is a pending on notification, the difference of these two behavior is sooner/later to pend on notification, no impact on overall write performance(after put edits to pendingWrites, write handler thread can just wait for write/sync to hdfs to finish and can&apos;t help/influence write performance)...right?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;4) Perf numbers are super impressive. It would have been wonderful to have such numbers for lesser number of handler threads also (e.g., 5-10 threads). IMHO, that represents most common case scenario, but I could be wrong. I know this has been beaten to death in the discussions above, but just echoing my thoughts here&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; I think maybe many guys hold a same point of view as yours here, but I also have my personal thought on this&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Throughput is different from latency: latency represents how quickly a system perform user requests, the quicker the better; while throughput represents how many requests a system perform user requests(concurrently, or to be more accurate, within a given time frame), the more the better. these two are both indicate a system&apos;s capability for performing user requests, but in different angles. certainly for application with low-medium write stress less write threads within client to issue requests is OK, but for application with high write stress, users/clients may feel bad if system just can&apos;t serve/reach their real-world throughput, &lt;b&gt;no matter&lt;/b&gt; how many client threads are configured/added. with the improvement of this patch, at least we &lt;b&gt;can&lt;/b&gt; satisfy users&apos; such high throughput requirement by adding client threads. And without improving individual write request&apos;s latency(as this patch does, it does nothing for individual write&apos;s latency), it&apos;s hard to improve throughput for &lt;b&gt;synchronous&lt;/b&gt; client write thread(it can issue next request only after it&apos;s done with the current one), that&apos;s why this patch has less effect for single/less write threads. If HBase supports &lt;b&gt;asynchronous&lt;/b&gt; client write thread, I think this patch can also provide big improvement even with less client write threads.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;5) I should also mention that while working on a different use case, I was trying to bring a layer of indirection b/w regionserver handlers and sync operation (Sync is the most costly affair in all HLog story). ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;===&amp;gt; glad to see similar approach&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. wonder if have same improvement under heavy write stress(such as 50/100/200 client write threads). Looking forward to seeing your final test results&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;, thank you very much, you raised some &lt;b&gt;really good&lt;/b&gt; questions. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13843926" author="fenghh" created="Tue, 10 Dec 2013 04:30:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmspaggi&quot; class=&quot;user-hover&quot; rel=&quot;jmspaggi&quot;&gt;Jean-Marc Spaggiari&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;For the random, you can also use something like System.currentTimeMillis() % asyncSyncers.length&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; yeah. System.currentTimeMillis() is a good candidate. txid can do the same thing as well. thanks&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13844039" author="stack" created="Tue, 10 Dec 2013 07:31:51 +0000"  >&lt;p&gt;See what hadoopqa thinks of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt; &apos;s patch&lt;/p&gt;</comment>
                            <comment id="13844108" author="hadoopqa" created="Tue, 10 Dec 2013 09:10:40 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12617842/HBASE-8755-trunk-v6.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12617842/HBASE-8755-trunk-v6.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 6 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.0&lt;/font&gt;.  The patch compiles against the hadoop 1.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.1&lt;/font&gt;.  The patch compiles against the hadoop 1.1 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 site&lt;/font&gt;.  The patch appears to cause mvn site goal to fail.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8118//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13844827" author="stack" created="Tue, 10 Dec 2013 23:44:04 +0000"  >&lt;p&gt;All sounds good above &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;  I&apos;m running a few tests here on cluster to see it basically works.  Any other comments by anyone else?  Otherwise, was planning on committing.  We can work on further speedup in new issues; e.g. see if we can do less threads as per &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=himanshu%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;himanshu@cloudera.com&quot;&gt;Himanshu Vashishtha&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13844838" author="yuzhihong@gmail.com" created="Tue, 10 Dec 2013 23:56:43 +0000"  >&lt;p&gt;+1 from me.&lt;/p&gt;</comment>
                            <comment id="13844839" author="jmhsieh" created="Tue, 10 Dec 2013 23:57:51 +0000"  >&lt;p&gt;I did most of a review of v4 last week &amp;#8211; here are a few nits:&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;nit:  (fix on commit)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+          &lt;span class=&quot;code-comment&quot;&gt;//    up and holds the lock
&lt;/span&gt;+          &lt;span class=&quot;code-comment&quot;&gt;// NOTE! can&apos;t hold &apos;upateLock&apos; here since rollWriter will pend
&lt;/span&gt;+          &lt;span class=&quot;code-comment&quot;&gt;// on &apos;sync()&apos; with &apos;updateLock&apos;, but &apos;sync()&apos; will wait &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;
&lt;/span&gt;+          &lt;span class=&quot;code-comment&quot;&gt;// AsyncWriter/AsyncSyncer/AsyncNotifier series. without upateLock
&lt;/span&gt;+          &lt;span class=&quot;code-comment&quot;&gt;// can leads to pendWrites more than pendingTxid, but not problem&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;spelling 2x: upate -&amp;gt; update&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;This can go in a follow up issue &amp;#8211; and please add a description of the threads / queues / invariants and  how a wal writes happens in the class javadoc. An updated version of the 1-6 list in the description would be great.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Good stuff &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;!&lt;/p&gt;</comment>
                            <comment id="13844859" author="v.himanshu" created="Wed, 11 Dec 2013 00:16:20 +0000"  >&lt;p&gt;Thanks for the explanation &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;; and it pretty much answers all my questions. Also, looking more, getting rid of LogSyncer thread eases out the locking semantics of rolling.&lt;/p&gt;

&lt;p&gt;Yes, I reran the above experiments on a more standard environment (4 DNs with HLogPE running on a&#160;DN, and log level set to INFO instead of Debug), and got mixed results this time. Varied threads from 2 to 100 and didn&apos;t get a clear winner. Given the current state of this patch and the cleanup it does, I am +1 for committing this.&lt;/p&gt;

&lt;p&gt;Looking forward to it. Thanks.&lt;/p&gt;</comment>
                            <comment id="13845035" author="fenghh" created="Wed, 11 Dec 2013 03:19:01 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt;, I&apos;ve made and attached a new patch based on your comment.&lt;br/&gt;
Thanks everyone again for your valuable comment and feedback.&lt;/p&gt;</comment>
                            <comment id="13845038" author="fenghh" created="Wed, 11 Dec 2013 03:34:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=v.himanshu&quot; class=&quot;user-hover&quot; rel=&quot;v.himanshu&quot;&gt;Himanshu Vashishtha&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;Looking forward to your performance comparison test result (trunk, with-this-patch, with-syncer-only) using latest HLogPE with new &apos;appendWithoutSync + sync&apos; logic. And I&apos;ll also try to do the same test for double comparison/confirm.&lt;/p&gt;

&lt;p&gt;Performance improvement is the first and foremost goal of this patch, code cleanup is a just by-the-way side-effect, so we want to see this patch accepted/checked-in because of the performance improvement it brings, but because of the code cleanup it does &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13845103" author="hadoopqa" created="Wed, 11 Dec 2013 05:46:53 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12618171/HBASE-8755-trunk-v7.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12618171/HBASE-8755-trunk-v7.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 6 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.0&lt;/font&gt;.  The patch compiles against the hadoop 1.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.1&lt;/font&gt;.  The patch compiles against the hadoop 1.1 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 3 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 site&lt;/font&gt;.  The patch appears to cause mvn site goal to fail.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8132//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13845106" author="stack" created="Wed, 11 Dec 2013 05:49:43 +0000"  >&lt;p&gt;I&apos;m running some tests local just to make sure.  Will report back...&lt;/p&gt;</comment>
                            <comment id="13846090" author="stack" created="Thu, 12 Dec 2013 06:17:27 +0000"  >&lt;p&gt;Pardon me. This is taking a while; hardware issues and now I trunk seems to have issue where it hangs syncing, pre-patch I believe... investigating.&lt;/p&gt;

&lt;p&gt;Here is what I see. Lots of threads BLOCKED here:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;RpcServer.handler=0,port=60020&quot;&lt;/span&gt; daemon prio=10 tid=0x00000000012f1800 nid=0x3cb5 waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; monitor entry [0x00007fdb0eb55000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:1006)
        - waiting to lock &amp;lt;0x0000000456c00390&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.appendNoSync(FSHLog.java:1054)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2369)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2087)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2037)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2041)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4175)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3424)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3328)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28460)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then the fella w/ the lock is doing this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;regionserver60020.logRoller&quot;&lt;/span&gt; daemon prio=10 tid=0x0000000001159800 nid=0x3ca7 in &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait() [0x00007fdb0f964000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: WAITING (on object monitor)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:503)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1307)
        - locked &amp;lt;0x0000000456bf37a8&amp;gt; (a java.util.concurrent.atomic.AtomicLong)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1299)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1412)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanupCurrentWriter(FSHLog.java:760)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:566)
        - locked &amp;lt;0x0000000456c00390&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        - locked &amp;lt;0x0000000456c00330&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:96)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Server is bound up.&lt;/p&gt;</comment>
                            <comment id="13846107" author="stack" created="Thu, 12 Dec 2013 06:59:39 +0000"  >&lt;p&gt;Hmm... Happens when this patch is in place.  Stuck here:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;regionserver60020.logRoller&quot;&lt;/span&gt; daemon prio=10 tid=0x00007f6f08822800 nid=0x5b0a in &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait() [0x00007f6eeccef000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: WAITING (on object monitor)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(Native Method)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.wait(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;.java:503)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1304)
        - locked &amp;lt;0x000000045756db98&amp;gt; (a java.util.concurrent.atomic.AtomicLong)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1296)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1409)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanupCurrentWriter(FSHLog.java:759)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:565)
        - locked &amp;lt;0x000000045756dc70&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        - locked &amp;lt;0x000000045756dc10&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:96)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which is here:&lt;/p&gt;

&lt;p&gt;1294   // sync all known transactions&lt;br/&gt;
1295   private void syncer() throws IOException &lt;/p&gt;
{
1296     syncer(this.unflushedEntries.get()); // sync all pending items
1297   }
&lt;p&gt;1298&lt;br/&gt;
1299   // sync all transactions upto the specified txid&lt;br/&gt;
1300   private void syncer(long txid) throws IOException {&lt;br/&gt;
1301     synchronized (this.syncedTillHere) {&lt;br/&gt;
1302       while (this.syncedTillHere.get() &amp;lt; txid) {&lt;br/&gt;
1303         try {&lt;br/&gt;
1304           this.syncedTillHere.wait();&lt;br/&gt;
1305&lt;br/&gt;
1306           if (txid &amp;lt;= this.failedTxid.get()) &lt;/p&gt;
{
1307             assert asyncIOE != null :
1308               &quot;current txid is among(under) failed txids, but asyncIOE is null!&quot;;
1309             throw asyncIOE;
1310           }
&lt;p&gt;1311         } catch (InterruptedException e) &lt;/p&gt;
{
1312           LOG.debug(&quot;interrupted while waiting for notification from AsyncNotifier&quot;);
1313         }
&lt;p&gt;1314       }&lt;br/&gt;
1315     }&lt;br/&gt;
1316   }&lt;/p&gt;

&lt;p&gt;All other threads are trying to do an appendnosync:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;RpcServer.handler=0,port=60020&quot;&lt;/span&gt; daemon prio=10 tid=0x00007f6f08a26800 nid=0x5b1b waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; monitor entry [0x00007f6eebee1000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:1005)
        - waiting to lock &amp;lt;0x000000045756dc70&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.appendNoSync(FSHLog.java:1053)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2369)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2087)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2037)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2041)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4175)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3424)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3328)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28460)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... but can&apos;t make progress blocked on updateLock.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 995   &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; append(HRegionInfo info, TableName tableName, WALEdit edits, List&amp;lt;UUID&amp;gt; clusterIds,
 996       &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; now, HTableDescriptor htd, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; doSync, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isInMemstore,
 997       AtomicLong sequenceId, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; nonceGroup, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; nonce) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
 998       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (edits.isEmpty()) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.unflushedEntries.get();
 999       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.closed) {
1000         &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Cannot append; log is closed&quot;&lt;/span&gt;);
1001       }
1002       TraceScope traceScope = Trace.startSpan(&lt;span class=&quot;code-quote&quot;&gt;&quot;FSHlog.append&quot;&lt;/span&gt;);
1003       &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
1004         &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; txid = 0;
1005         &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.updateLock) {
1006           &lt;span class=&quot;code-comment&quot;&gt;// get the sequence number from the passed &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;. In normal flow, it is coming from the
&lt;/span&gt;1007           &lt;span class=&quot;code-comment&quot;&gt;// region.
&lt;/span&gt;1008           &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; seqNum = sequenceId.incrementAndGet();
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The update lock is held when rolling log here:&lt;/p&gt;

&lt;p&gt; 562         synchronized (updateLock) {&lt;br/&gt;
 563           // Clean up current writer.&lt;br/&gt;
 564           oldNumEntries = this.numEntries.get();&lt;br/&gt;
 565           oldFile = cleanupCurrentWriter(currentFilenum);&lt;br/&gt;
 566           this.writer = nextWriter;&lt;br/&gt;
 567           this.hdfs_out = nextHdfsOut;&lt;br/&gt;
 568           this.numEntries.set(0);&lt;br/&gt;
 569           if (oldFile != null) &lt;/p&gt;
{
 570             this.hlogSequenceNums.put(oldFile, this.latestSequenceNums);
 571             this.latestSequenceNums = new HashMap&amp;lt;byte[], Long&amp;gt;();
 572           }
&lt;p&gt; 573         }&lt;/p&gt;
</comment>
                            <comment id="13846110" author="stack" created="Thu, 12 Dec 2013 07:08:28 +0000"  >&lt;p&gt;Will provide more on this tomorrow.&lt;/p&gt;</comment>
                            <comment id="13846130" author="fenghh" created="Thu, 12 Dec 2013 07:51:14 +0000"  >&lt;p&gt;The stack-traces are ok: when log-roll occurs, it holds the updateLock to prevent any subsequent writer handler put edits to pendingWrites (that&apos;s why all writer handler threads pend on updateLock), and then it calls &lt;b&gt;sync&lt;/b&gt; to wait for Async* threads to sync all edits currently in pendingWrites (that&apos;s why logroller pend on sync())...&lt;br/&gt;
Why no progress, we need to see why Async* threads don&apos;t finish the sync of current pendingWrites, would you please provide the Async* threads&apos; stack traces? &lt;br/&gt;
Only AsyncWriter from Async* threads needs the pendingWritesLock to grab the edits from pendingWrites, and AsyncNotifier needs syncTillHere to update it and notifyAll, these two both are hold-able under current situation: write handler threads can&apos;t hold pendingWritesLock before holding updateLock(within append()), logroller doesn&apos;t hold pendingWritesLock at all, and logroller is waiting for syncTillHere&apos;s change...&lt;/p&gt;</comment>
                            <comment id="13846408" author="stack" created="Thu, 12 Dec 2013 16:20:23 +0000"  >&lt;p&gt;A couple of thread dumps w/ WAL hang in them.&lt;/p&gt;

&lt;p&gt;Sorry, should have attached this last night &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13846412" author="stack" created="Thu, 12 Dec 2013 16:22:41 +0000"  >&lt;p&gt;All the Async* are just hanging out waiting.  Missed notification or no notification?&lt;/p&gt;</comment>
                            <comment id="13846583" author="stack" created="Thu, 12 Dec 2013 18:52:43 +0000"  >&lt;p&gt;I notice a syncer exited (it is missing from the thread dump).&lt;/p&gt;

&lt;p&gt;2013-12-11 22:40:28,887 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver60020-AsyncHLogSyncer3-1386830380159&amp;#93;&lt;/span&gt; wal.FSHLog: regionserver60020-AsyncHLogSyncer3-1386830380159 exiting&lt;/p&gt;

&lt;p&gt;On a new run, again a thread exits:&lt;/p&gt;

&lt;p&gt;2013-12-12 10:34:34,620 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver60020.logRoller&amp;#93;&lt;/span&gt; regionserver.LogRoller: HLog roll requested&lt;br/&gt;
2013-12-12 10:34:35,526 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver60020.logRoller&amp;#93;&lt;/span&gt; wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37240 synced till here 37210&lt;br/&gt;
2013-12-12 10:34:36,560 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver60020-AsyncHLogSyncer1-1386873205908&amp;#93;&lt;/span&gt; wal.FSHLog: regionserver60020-AsyncHLogSyncer1-1386873205908 exiting&lt;/p&gt;


&lt;p&gt;Let me try and figure the why.&lt;/p&gt;</comment>
                            <comment id="13846657" author="stack" created="Thu, 12 Dec 2013 19:45:28 +0000"  >&lt;p&gt;Catching all exceptions, I got a NPE.&lt;/p&gt;

&lt;p&gt;2013-12-12 11:03:43,870 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;Thread-14&amp;#93;&lt;/span&gt; regionserver.DefaultStoreFlusher: Flushed, sequenceid=2680455, memsize=129.3 M, hasBloomFilter=true, into tmp file hdfs://c2020.halxg.cloudera.com:8020/hbase/data/default/usertable/8dcf9c17c090f476346c8a31e4c9eddb/.tmp/64aaf14b38224f4fbce0a999f92dd8f4&lt;br/&gt;
2013-12-12 11:03:43,879 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;regionserver60020-AsyncHLogSyncer2-1386874930310&amp;#93;&lt;/span&gt; wal.FSHLog: UNEXPECTED&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1205)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744)&lt;/p&gt;

&lt;p&gt;Writer is null.&lt;/p&gt;</comment>
                            <comment id="13846928" author="stack" created="Thu, 12 Dec 2013 23:49:06 +0000"  >&lt;p&gt;Same as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt;&apos;s patch only it checks for null writer before using it &amp;#8211; this is currently in the code and seems to make this patch work again (i&apos;m testing)  &amp;#8211; and adds this on tail of each Async* thread:&lt;/p&gt;

&lt;p&gt;+      } catch (Exception e) &lt;/p&gt;
{
+        LOG.error(&quot;UNEXPECTED&quot;, e);
       }
&lt;p&gt; finally {&lt;/p&gt;

&lt;p&gt;Also renames the threads from AsyncHLog* to WAL.Async.  Minor.&lt;/p&gt;</comment>
                            <comment id="13846984" author="hadoopqa" created="Fri, 13 Dec 2013 00:44:12 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12618499/8755v8.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12618499/8755v8.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 6 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 hadoop1.0&lt;/font&gt;.  The patch failed to compile against the hadoop 1.0 profile.&lt;br/&gt;
    Here is snippet of errors:&lt;/p&gt;
    &lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-compile) on project hbase-server: Compilation failure: Compilation failure:
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java:[436,42] unclosed string literal
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java:[436,63] &apos;;&apos; expected
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java:[437,17] illegal start of expression
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java:[437,23] &apos;;&apos; expected
[ERROR] -&amp;gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-compile) on project hbase-server: Compilation failure
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:213)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
--
Caused by: org.apache.maven.plugin.CompilationFailureException: Compilation failure
	at org.apache.maven.plugin.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:729)
	at org.apache.maven.plugin.CompilerMojo.execute(CompilerMojo.java:128)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
	... 19 more&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8148//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8148//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13846991" author="stack" created="Fri, 13 Dec 2013 00:49:47 +0000"  >&lt;p&gt;Fix compile issue.&lt;/p&gt;</comment>
                            <comment id="13846994" author="stack" created="Fri, 13 Dec 2013 00:53:14 +0000"  >&lt;p&gt;This is what I&apos;ll commit.  I&apos;ve been running it on small cluster this afternoon and after fixing hardware, it seems to run fine at about the same speed as what we have currently (ycsb read/write loading).&lt;/p&gt;</comment>
                            <comment id="13846997" author="stack" created="Fri, 13 Dec 2013 00:58:15 +0000"  >&lt;p&gt;32 threads on one node writing a cluster of 4 nodes (~8 threads per server which according to our tests to date shows this model running slower than what we have).  It does 10% less throughput after ~25minutes.  We need to get the other speedups in after this goes in.&lt;/p&gt;</comment>
                            <comment id="13847069" author="hadoopqa" created="Fri, 13 Dec 2013 02:23:39 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12618509/8755v9.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12618509/8755v9.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 6 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.0&lt;/font&gt;.  The patch compiles against the hadoop 1.0 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop1.1&lt;/font&gt;.  The patch compiles against the hadoop 1.1 profile.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 site&lt;/font&gt;.  The patch appears to cause mvn site goal to fail.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/8149//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13847194" author="fenghh" created="Fri, 13 Dec 2013 05:54:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; thanks. seems no further blocking issue?&lt;/p&gt;</comment>
                            <comment id="13847195" author="fenghh" created="Fri, 13 Dec 2013 05:58:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;It does 10% less throughput after ~25minutes. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;==&amp;gt; it&apos;s normal when flush/compact occurs after ~25 minutes write, we saw such level downgrade when doing long time tests for both with/without patch.&lt;/p&gt;</comment>
                            <comment id="13847690" author="stack" created="Fri, 13 Dec 2013 17:29:06 +0000"  >&lt;p&gt;I did a compare over three hours and throughput flips in favor of this patch the longer we run (15% more writes after three hours)&lt;/p&gt;

&lt;p&gt;Let me commit.  This patch makes for better throughput when under high contention, cleans up the code, and lays groundwork for multiwal with its detaching syncing from handlers but I don&apos;t like the slowdown at low numbers.  Let us fix that promptly.  I created a subtask to address this and assigned myself.&lt;/p&gt;</comment>
                            <comment id="13847694" author="stack" created="Fri, 13 Dec 2013 17:34:06 +0000"  >&lt;p&gt;Committed to trunk.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrew.purtell%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andrew.purtell@gmail.com&quot;&gt;Andrew Purtell&lt;/a&gt; Do you want this in 0.98?&lt;/p&gt;

&lt;p&gt;Thanks for the hard work &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fenghh&quot; class=&quot;user-hover&quot; rel=&quot;fenghh&quot;&gt;Honghua Feng&lt;/a&gt; and persistence getting this in.&lt;/p&gt;</comment>
                            <comment id="13847698" author="apurtell" created="Fri, 13 Dec 2013 17:38:34 +0000"  >&lt;p&gt;Yes &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, let&apos;s get this out into a near release so we can see how it holds up. If we see perf problems during RC evaluation, we can revert if necessary.&lt;/p&gt;</comment>
                            <comment id="13847711" author="stack" created="Fri, 13 Dec 2013 17:51:37 +0000"  >&lt;p&gt;Committed to 0.98 also.&lt;/p&gt;</comment>
                            <comment id="13847958" author="hudson" created="Fri, 13 Dec 2013 22:01:55 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-TRUNK #4722 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/4722/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/4722/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt; A new write thread model for HLog to improve the overall HBase write throughput (stack: rev 1550778)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-common/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13848055" author="hudson" created="Fri, 13 Dec 2013 23:26:01 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-0.98 #11 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.98/11/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.98/11/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt; A new write thread model for HLog to improve the overall HBase write throughput (stack: rev 1550782)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-common/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13848092" author="hudson" created="Sat, 14 Dec 2013 00:19:05 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-0.98-on-Hadoop-1.1 #8 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/8/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/8/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt; A new write thread model for HLog to improve the overall HBase write throughput (stack: rev 1550782)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-common/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.98/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13848207" author="hudson" created="Sat, 14 Dec 2013 03:47:52 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-TRUNK-on-Hadoop-1.1 #5 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-1.1/5/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-1.1/5/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt; A new write thread model for HLog to improve the overall HBase write throughput (stack: rev 1550778)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-common/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13867544" author="ram_krish" created="Fri, 10 Jan 2014 06:11:37 +0000"  >&lt;p&gt;Nice one.  &lt;/p&gt;</comment>
                            <comment id="13882215" author="hudson" created="Sun, 26 Jan 2014 07:51:58 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-TRUNK #4856 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/4856/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/4856/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10156&quot; title=&quot;FSHLog Refactor (WAS -&amp;gt; Fix up the HBASE-8755 slowdown when low contention)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10156&quot;&gt;&lt;del&gt;HBASE-10156&lt;/del&gt;&lt;/a&gt; FSHLog Refactor (WAS -&amp;gt; Fix up the &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt; slowdown when low contention) (stack: rev 1561450)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FailedLogCloseException.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FailedSyncBeforeLogCloseException.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/pom.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/RingBufferTruck.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/HLogPerformanceEvaluation.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/pom.xml&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13882220" author="hudson" created="Sun, 26 Jan 2014 07:58:38 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-TRUNK-on-Hadoop-1.1 #65 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-1.1/65/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-1.1/65/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-10156&quot; title=&quot;FSHLog Refactor (WAS -&amp;gt; Fix up the HBASE-8755 slowdown when low contention)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-10156&quot;&gt;&lt;del&gt;HBASE-10156&lt;/del&gt;&lt;/a&gt; FSHLog Refactor (WAS -&amp;gt; Fix up the &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8755&quot; title=&quot;A new write thread model for HLog to improve the overall HBase write throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8755&quot;&gt;&lt;del&gt;HBASE-8755&lt;/del&gt;&lt;/a&gt; slowdown when low contention) (stack: rev 1561450)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FailedLogCloseException.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-client/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FailedSyncBeforeLogCloseException.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/pom.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/RingBufferTruck.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SyncFuture.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/HLogPerformanceEvaluation.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestDurability.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollAbort.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRollingNoCluster.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/pom.xml&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14330662" author="enis" created="Sat, 21 Feb 2015 23:30:12 +0000"  >&lt;p&gt;Closing this issue after 0.99.0 release. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12653534">ACCUMULO-1522</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12617147" name="8755-syncer.patch" size="16482" author="v.himanshu" created="Thu, 5 Dec 2013 10:42:03 +0000"/>
                            <attachment id="12601978" name="8755trunkV2.txt" size="25776" author="stack" created="Sat, 7 Sep 2013 14:06:01 +0000"/>
                            <attachment id="12618499" name="8755v8.txt" size="29496" author="stack" created="Thu, 12 Dec 2013 23:49:06 +0000"/>
                            <attachment id="12618509" name="8755v9.txt" size="28764" author="stack" created="Fri, 13 Dec 2013 00:49:47 +0000"/>
                            <attachment id="12588130" name="HBASE-8755-0.94-V0.patch" size="26559" author="fenghh" created="Mon, 17 Jun 2013 11:16:39 +0000"/>
                            <attachment id="12588356" name="HBASE-8755-0.94-V1.patch" size="27601" author="fenghh" created="Tue, 18 Jun 2013 13:37:20 +0000"/>
                            <attachment id="12615265" name="HBASE-8755-0.96-v0.patch" size="26161" author="fenghh" created="Fri, 22 Nov 2013 03:54:12 +0000"/>
                            <attachment id="12588353" name="HBASE-8755-trunk-V0.patch" size="26877" author="fenghh" created="Tue, 18 Jun 2013 13:31:47 +0000"/>
                            <attachment id="12593848" name="HBASE-8755-trunk-V1.patch" size="25096" author="fenghh" created="Wed, 24 Jul 2013 04:24:40 +0000"/>
                            <attachment id="12615266" name="HBASE-8755-trunk-v4.patch" size="26139" author="fenghh" created="Fri, 22 Nov 2013 03:54:12 +0000"/>
                            <attachment id="12617842" name="HBASE-8755-trunk-v6.patch" size="27133" author="fenghh" created="Mon, 9 Dec 2013 15:06:00 +0000"/>
                            <attachment id="12618171" name="HBASE-8755-trunk-v7.patch" size="28585" author="fenghh" created="Wed, 11 Dec 2013 03:19:31 +0000"/>
                            <attachment id="12617774" name="HBASE-8755-v5.patch" size="27345" author="v.himanshu" created="Mon, 9 Dec 2013 02:35:22 +0000"/>
                            <attachment id="12618429" name="thread.out" size="444440" author="stack" created="Thu, 12 Dec 2013 16:20:23 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12669120">HBASE-9562</subtask>
                            <subtask id="12683054">HBASE-10094</subtask>
                            <subtask id="12684499">HBASE-10156</subtask>
                            <subtask id="12684506">HBASE-10158</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>14.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 17 Jun 2013 15:22:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>333495</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 42 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1lj3b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>333823</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Redo of thread model writing edits to the WAL; slower when few clients but as concurrency rises, it makes for better throughput.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>