<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:00:11 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2238/HBASE-2238.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2238] Review all transitions -- compactions, splits, region opens, log roll/splitting -- for crash-proofyness and atomicity</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2238</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;This issue is about reviewing state transitions in hbase to ensure we&apos;re sufficently hardened against crashes.  This issue I see as an umbrella issue under which we&apos;d look at compactions, splits, log splits, region opens &amp;#8211; what else is there?  We&apos;d look at each in turn to see how we survive crash at any time during the transition.  For example, we think compactions idempotent but we need to prove it so.  Splits are for sure not, not at the moment (Witness disabled parents but daughters missing or only one of them available).&lt;/p&gt;

&lt;p&gt;Part of this issue would be writing tests that aim to break transitions.&lt;/p&gt;

&lt;p&gt;In light of above, here is recent off-list note from Todd Lipcon (and &quot;another&quot;):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
I thought a bit more last night about the discussion we were having
regarding various HBase components doing operations on the HDFS data,
and ensuring that in various racy scenarios that we don&apos;t have two
region servers or masters overlapping.

I came to the conclusion that ZK data can&apos;t be used to actually have
effective locks on HDFS directories, since we can never know that we
still have a ZK lock when we &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; an operation. Thus the operations
themselves have to be idempotent, or recoverable in the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; of
multiple nodes trying to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; the same thing. Or, we have to use HDFS
itself as a locking mechanism - &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is what we discussed using write
leases essentially as locks.

Since I didn&apos;t really trust myself, I ran my thoughts by &lt;span class=&quot;code-quote&quot;&gt;&quot;Another&quot;&lt;/span&gt;
and he concurs (see
below). Figured &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is food &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; thought &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; designing HBase data
management to be completely safe/correct.

...

---------- Forwarded message ----------
From: Another &amp;lt;another@XXXXXX.com&amp;gt;
Date: Wed, Feb 17, 2010 at 10:50 AM
Subject: locks
To: Todd Lipcon &amp;lt;todd@XXXXXXX.com&amp;gt;


&lt;span class=&quot;code-object&quot;&gt;Short&lt;/span&gt; answer is no, you&apos;re right.
Because HDFS and ZK are partitioned (in the sense that there&apos;s no
communication between them) and there may be an unknown delay between
acquiring the lock and performing the operation on HDFS you have no
way of knowing that you still own the lock, like you say.
If the lock cannot be revoked &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; you have it (no timeouts) then you
can atomically check that you still have the lock and &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; the operation
on HDFS, because checking is a no-op. Designing a system with no lock
revocation in the face of failures is an exercise &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the reader :)
The right way is &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; HDFS and ZK to communicate to construct an atomic
operation. ZK could give a token to the client which it also gives to
HDFS, and HDFS uses that token to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; admission control. There&apos;s
probably some neat theorem about causality and the impossibility of
doing distributed locking without a sufficiently strong atomic
primitive here.

Another
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12456730">HBASE-2238</key>
            <summary>Review all transitions -- compactions, splits, region opens, log roll/splitting -- for crash-proofyness and atomicity</summary>
                <type id="14" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Umbrella</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 Feb 2010 23:58:32 +0000</created>
                <updated>Wed, 16 Jul 2014 23:08:33 +0000</updated>
                            <resolved>Wed, 16 Jul 2014 23:08:33 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                <comments>
                            <comment id="12835607" author="tlipcon" created="Fri, 19 Feb 2010 06:27:19 +0000"  >&lt;p&gt;I think the general pattern that all transitions need to follow is:&lt;/p&gt;

&lt;p&gt;1) HLog that the RS intends to do some operation&lt;br/&gt;
2) Perform the operation in a way that is still undoable (eg create compacted HFile but don&apos;t yet remove old ones)&lt;br/&gt;
3) HLog that the RS has finished the action&lt;br/&gt;
4) Clean up from part 2 (eg remove the pre-compaction HFiles)&lt;/p&gt;

&lt;p&gt;We assume:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Whenever a RS has failed, the master will open its HLog for append.&lt;/li&gt;
	&lt;li&gt;This steals the write lease and increases the generation stamp on its last block.&lt;/li&gt;
	&lt;li&gt;Thus the next time the RS attempts to hflush(), it will receive an IOException (I think a LeaseExpiredException to be specific?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Failure cases at each step:&lt;/p&gt;

&lt;p&gt;Fail before 1) no problem, data isn&apos;t touched&lt;br/&gt;
Fail after 1 but before 3) the transition is an indeterminate state. When the master recovers, it can roll back to the pre-transition state&lt;br/&gt;
Fail after 3) when the master recovers, it can complete the &quot;cleanup&quot; transition for the regionserver (even if the regionserver got halfway through cleanup)&lt;/p&gt;

&lt;p&gt;This pattern relies on cleanup being idempotent, and state transitions being undoable.&lt;/p&gt;

&lt;p&gt;The above examples are for the compaction case, but I think the same general ideas apply elsewhere.&lt;/p&gt;</comment>
                            <comment id="12835609" author="tlipcon" created="Fri, 19 Feb 2010 06:31:27 +0000"  >&lt;p&gt;Lastly, as an optimization, we can add a step 5 on the regionserver which is &quot;log that the state transition is entirely complete&quot;. Thus the master knows it doesn&apos;t have to do anything with regards to this transition.&lt;/p&gt;

&lt;p&gt;For discussion, it may be worth giving some terminology to the phases. It seems to me we have &lt;em&gt;prepare&lt;/em&gt; (enters the &quot;will rollback&quot; state), then &lt;em&gt;commit&lt;/em&gt; (enters the &quot;will roll forward state&quot;), then &lt;em&gt;complete&lt;/em&gt; (ends the state machine, no action necessary).&lt;/p&gt;</comment>
                            <comment id="12876485" author="jdcryans" created="Mon, 7 Jun 2010 23:30:15 +0000"  >&lt;p&gt;I ran into a weird situation today running TestReplication (from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2223&quot; title=&quot;Handle 10min+ network partitions between clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2223&quot;&gt;&lt;del&gt;HBASE-2223&lt;/del&gt;&lt;/a&gt;&apos;s latest patch up on rb), the test kills a region server by expiring its session and then the following happened (almost all at the same time):&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Master lists all hlogs to split (total of 12)&lt;/li&gt;
	&lt;li&gt;RS does a log roll&lt;/li&gt;
	&lt;li&gt;RS tries to register the new log in ZK for replication and fails because the session was expired, but the log is already rolled&lt;/li&gt;
	&lt;li&gt;RS takes 3 more edits into the new log&lt;/li&gt;
	&lt;li&gt;RS cleans 6 logs over 13&lt;/li&gt;
	&lt;li&gt;Master fails at splitting the 3rd log it listed, delays the log splitting process&lt;/li&gt;
	&lt;li&gt;Master tries again to split logs, lists 7 of them and is successful&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;In the end, the master wasn&apos;t missing any edits (because log splitting failed and got the new log the second time) but the slave cluster was missing 3. This makes me think that the region server should also do a better job at handling KeeperException.SessionExpiredException because we currently don&apos;t do it at all. &lt;/p&gt;</comment>
                            <comment id="12877494" author="jdcryans" created="Thu, 10 Jun 2010 17:15:47 +0000"  >&lt;p&gt;I&apos;m upgrading this to blocker for 0.21, any GC that kills a RS that rolls after sleeping and still gets some edits can result in data loss.&lt;/p&gt;</comment>
                            <comment id="12885723" author="streamy" created="Tue, 6 Jul 2010 22:51:10 +0000"  >&lt;p&gt;Over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2696&quot; title=&quot;ZooKeeper cleanup and refactor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2696&quot;&gt;&lt;del&gt;HBASE-2696&lt;/del&gt;&lt;/a&gt; we now have one place that we handle ZK Expired and Disconnected events.  We need to figure out how to handle a Disconnect, especially on RS side, which relates to much of the discussion here.&lt;/p&gt;</comment>
                            <comment id="12918201" author="stack" created="Tue, 5 Oct 2010 21:18:06 +0000"  >&lt;p&gt;Made this an umbrella issue, critical rather than blocker and moved it out of 0.90.&lt;/p&gt;

&lt;p&gt;The reason to take down priority is that perhaps half of the issues raised in this umbrella have been addressed elsewhere (review of splits and log splitting).  The other items have also had some work done (region open) but more to do so will leave issue open though moving it out of 0.90 since thought is we can release w/o completion (I&apos;m in a room w/ j-d and ryan going over 0.90 issues and this is what we think).&lt;/p&gt;</comment>
                            <comment id="14064304" author="apurtell" created="Wed, 16 Jul 2014 23:08:33 +0000"  >&lt;p&gt;Resolving fixed as all mentioned and linked issues are resolved.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12456510">HBASE-2231</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12466484">HBASE-2696</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 19 Feb 2010 06:27:19 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32473</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 22 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hgvr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99997</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>