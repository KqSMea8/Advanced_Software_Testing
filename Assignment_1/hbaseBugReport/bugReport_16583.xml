<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:15:37 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-16583/HBASE-16583.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-16583] Make Region&apos;s implementation asynchronous</title>
                <link>https://issues.apache.org/jira/browse/HBASE-16583</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Staged Event-Driven Architecture (SEDA) splits request-handling logic into several stages, each stage is executed in a thread pool and they are connected by queues.&lt;/p&gt;

&lt;p&gt;Currently, in region server we use a thread pool to handle requests from client. The number of handlers is configurable, reading and writing use different pools. The current architecture has two limitations:&lt;/p&gt;

&lt;p&gt;Performance:&lt;br/&gt;
Different part of the handling path has different bottleneck. For example, accessing MemStore and cache mainly consumes CPU but accessing HDFS mainly consumes network/disk IO. If we use SEDA and split them into two different stages, we can use different numbers for two pools according to the CPU/disk/network performance case by case.&lt;/p&gt;

&lt;p&gt;Availability:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16388&quot; title=&quot;Prevent client threads being blocked by only one slow region server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16388&quot;&gt;&lt;del&gt;HBASE-16388&lt;/del&gt;&lt;/a&gt; described a scene that if the client use a thread pool and use blocking methods to access region servers, only one slow server may exhaust most of threads of the client. For HBase, we are the client and HDFS datanodes are the servers. A slow datanode may exhaust most of handlers. The best way to resolve this issue is make HDFS requests non-blocking, which is exactly what SEDA does.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13003459">HBASE-16583</key>
            <summary>Make Region&apos;s implementation asynchronous</summary>
                <type id="14" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/genericissue.png">Umbrella</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="yangzhe1991">Phil Yang</reporter>
                        <labels>
                    </labels>
                <created>Thu, 8 Sep 2016 06:06:49 +0000</created>
                <updated>Mon, 10 Oct 2016 10:29:43 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>20</watches>
                                                                <comments>
                            <comment id="15472923" author="yangzhe1991" created="Thu, 8 Sep 2016 06:11:29 +0000"  >&lt;p&gt;If we think it is worth to make our read/write path move to SEDA, it will be a long-term work. We can add a async Region interface first in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16505&quot; title=&quot;Save deadline in RpcCallContext according to request&amp;#39;s timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16505&quot;&gt;&lt;del&gt;HBASE-16505&lt;/del&gt;&lt;/a&gt;, and the RPC protocol will not be changed so we can keep the compatibility&lt;/p&gt;</comment>
                            <comment id="15473163" author="apache9" created="Thu, 8 Sep 2016 08:11:44 +0000"  >&lt;p&gt;I think &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16505&quot; title=&quot;Save deadline in RpcCallContext according to request&amp;#39;s timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16505&quot;&gt;&lt;del&gt;HBASE-16505&lt;/del&gt;&lt;/a&gt; blocks us? Not we block &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16505&quot; title=&quot;Save deadline in RpcCallContext according to request&amp;#39;s timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16505&quot;&gt;&lt;del&gt;HBASE-16505&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15473175" author="yangzhe1991" created="Thu, 8 Sep 2016 08:15:19 +0000"  >&lt;p&gt;I set &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16505&quot; title=&quot;Save deadline in RpcCallContext according to request&amp;#39;s timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16505&quot;&gt;&lt;del&gt;HBASE-16505&lt;/del&gt;&lt;/a&gt; &quot;is part of&quot; this issue(because it is already a sub-task of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16492&quot; title=&quot;Setting timeout on blocking operations in read/write path&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16492&quot;&gt;HBASE-16492&lt;/a&gt;), incorporates doesn&apos;t not mean block?&lt;/p&gt;</comment>
                            <comment id="15473180" author="apache9" created="Thu, 8 Sep 2016 08:17:48 +0000"  >&lt;p&gt;My page is old. It is OK now after a refresh.&lt;/p&gt;</comment>
                            <comment id="15475134" author="apurtell" created="Thu, 8 Sep 2016 21:53:21 +0000"  >&lt;p&gt;One of the most common and classic critique of the SEDA architecture, by the original proponent of the idea as well as others, is the overhead of connecting stages through event queues lowers the ceiling for performance. Consider &lt;a href=&quot;http://matt-welsh.blogspot.com/2010/07/retrospective-on-seda.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://matt-welsh.blogspot.com/2010/07/retrospective-on-seda.html&lt;/a&gt; .&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If I were to design SEDA today, I would decouple stages (i.e., code modules) from queues and thread pools (i.e., concurrency boundaries). Stages are still useful as a structuring primitive, but it is probably best to group multiple stages within a single &quot;thread pool domain&quot; where latency is critical. Most stages should be connected via direct function call. I would only put a separate thread pool and queue in front of a group of stages that have long latency or nondeterministic runtime, such as performing disk I/O. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;and note he no longer considers the benchmarks that validated the original SEDA paper as actually supporting application of SEDA for real-world applications. &lt;/p&gt;

&lt;p&gt;I&apos;m not saying don&apos;t go in this direction, but let&apos;s not assume the payoff will be automatic. &lt;/p&gt;</comment>
                            <comment id="15475209" author="aoxiang" created="Thu, 8 Sep 2016 22:29:18 +0000"  >&lt;p&gt;Also reference to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10989&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-10989&lt;/a&gt; (Move away from SEDA to TPC)&lt;/p&gt;</comment>
                            <comment id="15475247" author="yuzhihong@gmail.com" created="Thu, 8 Sep 2016 22:46:49 +0000"  >&lt;p&gt;The above JIRA has been inactive for more than half year.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8520&quot; title=&quot;Prototype thread per core&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8520&quot;&gt;&lt;del&gt;CASSANDRA-8520&lt;/del&gt;&lt;/a&gt;, referenced by the JIRA, was marked Won&apos;t Fix.&lt;/p&gt;</comment>
                            <comment id="15475302" author="apurtell" created="Thu, 8 Sep 2016 23:09:09 +0000"  >&lt;p&gt;How was it resolved? Just because it&apos;s too late to move away from SEDA now for them doesn&apos;t mean there were not good reasons to propose it&lt;/p&gt;</comment>
                            <comment id="15475377" author="aoxiang" created="Thu, 8 Sep 2016 23:49:07 +0000"  >&lt;p&gt;Yes&#65292;they have the same concerns on SEDA, and do not have too much progress on the new model &quot;TPC( thread per core)&quot;. Just for reference.&lt;/p&gt;</comment>
                            <comment id="15475456" author="enis" created="Fri, 9 Sep 2016 00:30:25 +0000"  >&lt;p&gt;This is a pretty good point. I think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; experimented with RPC reader threads directly executing the call instead of using the RPC queues, and saw dramatic improvements. Adding more thread pools and queues in the mix will only increase latency and decrease throughput. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15967?focusedCommentId=15316448&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15316448&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-15967?focusedCommentId=15316448&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15316448&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="15475486" author="davelatham" created="Fri, 9 Sep 2016 00:47:05 +0000"  >&lt;p&gt;From the last comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8520&quot; title=&quot;Prototype thread per core&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8520&quot;&gt;&lt;del&gt;CASSANDRA-8520&lt;/del&gt;&lt;/a&gt;, it looks like it was resolved simply due to being superseded by &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10989&quot; title=&quot;Move away from SEDA to TPC&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10989&quot;&gt;CASSANDRA-10989&lt;/a&gt; as the effort to move away from SEDA.&lt;/p&gt;</comment>
                            <comment id="15475492" author="apache9" created="Fri, 9 Sep 2016 00:52:18 +0000"  >&lt;p&gt;I&apos;d say TPC is the dream but SEDA is the reality &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;For C*, the first problem is how to deal with disk io. There is no AIO Filesystem implementation in Java, and after a learning of ScyllaDB, one of the TPC implementation in real world, I found that only zfs has a good support of AIO... So they still need a thread pool for disk io, that is still SEDA&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;And we have the same problem...Although most of our IO is network based, we have short circuit read...&lt;/p&gt;

&lt;p&gt;And TPC will make the code very very flaky, a simple sleep or some other time consuming operation can kill all the server. For ScyllaDB, they have a really powerful framework to write TPC code, for example, a sleep in that framework does not hang the current thread, it equals to schedule a delayed task. And more, for some time consuming work, such as filtering or compaction, we need to cut timeslice and do scheduling by ourselves. The guys of ScyllaDB used to write KVM if I do not remember wrong, so maybe this is not a difficult mission for them. But for us, with Java, I will not say it is impossible, but...&lt;/p&gt;

&lt;p&gt;Will add more comments later. I need to go to work now...&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="15475617" author="apache9" created="Fri, 9 Sep 2016 02:01:17 +0000"  >&lt;p&gt;In fact, the current architecture of RS is already SEDA. We have several threads reading or writing socket, and a thread pool for read request(read handler), a thead pool for write request(write handler), and a thread for writing WAL(or multiple threads if you use MultiWAL).&lt;/p&gt;

&lt;p&gt;I think the thread pool introduced in the WAL layer is reasonble as we will doing network IO, this is consider to be time consuming. But the rpc handler thread pool is just a simple pattern which is copied from other rpc framework. For a general rpc framework, the thread pool is needed as we do not know what the users will do in a rpc call so the safe way is to give them a seperated thread. But for us, we do know what will happen in the rpc handler, the code is written by us. For example, with AsyncFSWAL, ideally, we could execute a write request directly in the RpcServer&apos;s thread. When we reach the WAL layer, we schedule a sync request and just return. The AsyncFSWAL will trigger a callback when the sync is done and finish the remaining work and write the response back(or let the RpcServer&apos;s thread to do the work). And if we make the RpcServer also run in the netty EventLoopGroup, we get a TPC architecture, right?&lt;/p&gt;

&lt;p&gt;But sadly, it is only an ideal... We have increment, which will acquire a write row lock and hold it for a really long time, and also, we will wait mvcc completion before return. So the first intention here, is to find the place where the thread could be blocked for a long time. We could split our workflow to several stages using these points. And in fact, thread swtich is not always needed when we cross these points. For example, if you can acquire the row lock directly, just run it directly. At a high level, this means we could share thread pool between different stages. And if we could share a thread pool across all stages finally, this is TPC. Of course, there are still lots of work to be done, such as making the request from one connection always run in the same thread(good for locking and cpu caching), using lock-free data structure as much as possible...(And how to implement priority?).&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="15475866" author="yangzhe1991" created="Fri, 9 Sep 2016 04:19:59 +0000"  >&lt;blockquote&gt;
&lt;p&gt;One of the most common and classic critique of the SEDA architecture, by the original proponent of the idea as well as others, is the overhead of connecting stages through event queues lowers the ceiling for performance.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;SDEA is proposed &quot;many&quot; years ago, whose original idea is not perfectly matched with current hardware/kernel. What we need is referencing this idea to help us. I agree that queues decrease the performance. We don&apos;t need many stages. And maybe we indeed can remove some queues like mentioned above that if RPC reader threads directly executing the call, the performance can be improved. I think we don&apos;t need to split into two stages if before and after this point the bottleneck is almost same, for example, both of them mainly consume CPU. More specifically, I think the only place we need to discuss if we should use a queue is IO, especially network IO.&lt;/p&gt;

&lt;p&gt;One of the features that HBase is different from other databases is that we use a distributed file system, HDFS, rather than save data locally. In other databases which saves data locally, in the database-engine logic(LSM-tree or BTree) they don&apos;t need any RPC call. All the resources it need is CPU/memory/disks. And the local resources(especially disks) will not be accessed by other nodes. So in that architecture, if one of resources(CPU/memory/disks) reach the bottleneck, the whole node reach the bottleneck, one &quot;stage&quot; from start to end is enough.&lt;/p&gt;

&lt;p&gt;However, HBase has RPC calls in read/write path. Even if we make the locality to 1.0 and use short circuit read, we still call RPC. And now we call them in a blocking way(even use AsyncWAL we will blocked on WAL.sync in handler). We don&apos;t know if the RPC server works well. And if a DN is reaching its bottleneck, it doesn&apos;t mean our RS is also reaching our bottleneck. So if we can use two thread pools that one do the local logic and the other do the RPC logic, I am not sure if the performance must can be improved, but I think the availability which means the performance(both throughput and latency) when one of DNs is slow than others can be protected.&lt;/p&gt;

</comment>
                            <comment id="15477931" author="tlipcon" created="Fri, 9 Sep 2016 18:44:04 +0000"  >&lt;p&gt;&lt;a href=&quot;http://matt-welsh.blogspot.com/2010/07/retrospective-on-seda.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://matt-welsh.blogspot.com/2010/07/retrospective-on-seda.html&lt;/a&gt; is worth a read on this (from the original author of SEDA)&lt;/p&gt;</comment>
                            <comment id="15477995" author="stack" created="Fri, 9 Sep 2016 19:13:25 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;. Yeah, handoff between threads via a Queue is a demonstrable performance killer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yangzhe1991&quot; class=&quot;user-hover&quot; rel=&quot;yangzhe1991&quot;&gt;Phil Yang&lt;/a&gt; -1 on SEDA as the title of this JIRA but +1 on the discussion you have provoked by opening this issue (smile)&lt;/p&gt;</comment>
                            <comment id="15478105" author="stack" created="Fri, 9 Sep 2016 20:05:38 +0000"  >&lt;p&gt;The &apos;Availability&apos; issue &amp;#8211; how our current server &apos;architecture&apos; can lock up if a single slow DN &amp;#8211; is reason enough to revisit core. But I am also stuck at the moment unable to drive all CPU on a recent generation server. The bottleneck is more than just our architecture &lt;span class=&quot;error&quot;&gt;&amp;#91;see interesting quote below&amp;#93;&lt;/span&gt; but I&apos;d be interested in any revamp that ups our resource utilization.&lt;/p&gt;

&lt;p&gt;Agree with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; that rewriting HBase core in seastar-style would be &apos;difficult&apos; and require a discipline that might be, &apos;ahem&apos;, in short supply around these parts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Apache9&quot; class=&quot;user-hover&quot; rel=&quot;Apache9&quot;&gt;Duo Zhang&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yangzhe1991&quot; class=&quot;user-hover&quot; rel=&quot;yangzhe1991&quot;&gt;Phil Yang&lt;/a&gt; I like the suggestions and the call-out of &apos;natural&apos; boundaries. Lets write them out and write out how we can do stuff like: &quot;....thread swtich is not always needed when we cross these points.&quot; Is it too ideal having a high-level goal and then going there piecemeal? (Though I hate our thread-per-handler model. To change that, it&apos;d be a big change. Funny though how having a Handler per CPU &amp;#8211; i.e. &quot;TPC&quot; &amp;#8211; is what gives best perf doing YCSB random read test when all cached).&lt;/p&gt;

&lt;p&gt;&#8220;The typical design of NoSQL data stores...consists of a JVM which runs on top of Linux, utilizes the page cache, and uses complex memory allocation strategies to &#8220;trick&#8221; the JVM garbage collector to avoid stop-the-world pauses. Such a design suffers from sudden latency hiccups, expensive locking, and low throughput due to low processor utilization.&#8221; from Scylladb Markitecture Page&lt;/p&gt;</comment>
                            <comment id="15479622" author="yangzhe1991" created="Sat, 10 Sep 2016 11:04:15 +0000"  >&lt;p&gt;As I said, what we need is not the &quot;traditional&quot; SEDA. Maybe the title should be changed &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;I think our real goal is to reduce the time of being blocked (even no blocked) in our worker threads, especially when we have slow data nodes. And then in all threads. When there is no any thread will be blocked, we have TPC. Now we have to set a large number of handlers to counteract the blocking handlers, which results in more thread scheduling. &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16492&quot; title=&quot;Setting timeout on blocking operations in read/write path&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16492&quot;&gt;HBASE-16492&lt;/a&gt; will do a simple work that set timeout on blocking points according to rpc timeout from clients so the handlers will not being blocked too long and wasting time. It is easy and not a long-term work so I think we can do this before 1.4 releases.&lt;/p&gt;

&lt;p&gt;We have several blocking points in read/write path. Some blocking operations can be easily change to asynchronous but the others are still synchronous. Now we have AsyncWAL so we have an asynchronous way to write to HDFS. But our HDFS reader and API to NN are still synchronous. We can use EventLoop to do all asynchronous logic, but we have to use a thread pool to do synchronous work, although the API exposed to read/write path can be non-blocking, like FSHLog. The thread pools are why I titled SEDA. So before we go to TPC completely, we may use a mixed architecture that some places are like SEDA and some places are like TPC.&lt;/p&gt;

&lt;p&gt;No matter how to change the current blocking operations(use thread pool or make them async). We should split each blocking point into before/after two stages. We will have several stages and when we end up a stage because there is a blocking point, we can schedule this work and end up this stage task which lets other work can be executed in current thread. And sometimes maybe we can avoid switch the task in current thread if we can get the result immediately. For example, when we should acquire a lock in our read path, we can tryLock first and if we get the lock we can continue the next logic, otherwise the logic should wait and this thread can do other things until the task has acquired the lock or timeout.&lt;/p&gt;

&lt;p&gt;And if we make our worker threads (handlers) async and there is no blocking worker threads(may be still have other thread pools), we can schedule all requests in a same region into same thread. If we can do this our MemStore can be non thread-safe. And the logic of mvcc/rowlock/idlock may be much easier. Of course, this is not easy because different region has different qps. C* can do this easily after TPC because they have no any guarantee across rows and no mvcc so they can shard by partition key directly if they want.&lt;/p&gt;

&lt;p&gt;Fully asynchronous Region is not a easy work, but at some important points we may can do something first. Maybe executing some synchronous work in another thread(pool) is easier?&lt;/p&gt;</comment>
                            <comment id="15479671" author="yangzhe1991" created="Sat, 10 Sep 2016 11:41:04 +0000"  >&lt;p&gt;Thread pools and queues indeed enlarge the latency of the queries, especially when the cluster is in normal status. We have to consider the advantage and disadvantage of introducing them. We indeed meet some issues on slow DNs. For a online service, .99 or .999 latency depends on GC, and throughput may not be the most important thing because we can add more nodes. But availability is absolutely the most important thing for HBase users (and of course, the most most important is no data-loss..). It is different from offline service. So we have to avoid any scene that one dead/slow server leads to the availability reduce more than 1/n for this cluster.&lt;/p&gt;</comment>
                            <comment id="15481231" author="stack" created="Sun, 11 Sep 2016 07:02:03 +0000"  >&lt;p&gt;Started a doc to list Stages (qua SEDA) and then,  how we might improve transition across stages.&lt;/p&gt;</comment>
                            <comment id="15489838" author="yangzhe1991" created="Wed, 14 Sep 2016 08:42:59 +0000"  >&lt;p&gt;Change to a more general name. After &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-16505&quot; title=&quot;Save deadline in RpcCallContext according to request&amp;#39;s timeout&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-16505&quot;&gt;&lt;del&gt;HBASE-16505&lt;/del&gt;&lt;/a&gt; the interface of Region will be changed to asynchronous and returns CompletableFuture, but implementation in HRegion is still synchronous at first. We should find an appropriate method to make them async. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12999591">HBASE-16492</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 8 Sep 2016 08:11:44 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i33dif:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>