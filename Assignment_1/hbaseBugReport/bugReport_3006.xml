<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:06:34 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3006/HBASE-3006.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3006] Reading compressed HFile blocks causes way too many DFS RPC calls severly impacting performance</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3006</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;On some read perf tests, we noticed several perf outliers (10 second plus range). The rows were large (spanning multiple blocks, but still the numbers didn&apos;t add up). We had compression turned on.&lt;/p&gt;

&lt;p&gt;We enabled DN clienttrace logging,&lt;br/&gt;
log4j.logger.org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace=DEBUG&lt;/p&gt;

&lt;p&gt;and noticed lots of 516 byte reads at the DN level, several of them at the same offset in the block.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-09-16 09:28:32,335 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:38713, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 203000
2010-09-16 09:28:32,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40547, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 119000
2010-09-16 09:28:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40650, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 149000
2010-09-16 09:28:32,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:40861, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 135000
2010-09-16 09:28:32,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:41129, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 117000
2010-09-16 09:28:32,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:41691, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 148000
2010-09-16 09:28:32,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:42881, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 114000
2010-09-16 09:28:32,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:49511, bytes: 516, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.3\
0.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 153000
2010-09-16 09:28:32,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.30.251.189:50010, dest: /10.30.251.189:51158, bytes: 3096, op: HDFS_READ, cliID: DFSClient_-436329957, offset: 39884800, srvID: DS-1757894045-10.\
30.251.189-50010-1283993662994, blockid: blk_-4686540439725119008_1985, duration: 139000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This was strange coz our block size was 64k, and on disk block size after compression should generally have been around 6k.&lt;/p&gt;

&lt;p&gt;Some print debugging at the HFile and BoundedRangeFileInputStream (which is wrapped by createDecompressionStream) revealed the following:&lt;/p&gt;

&lt;p&gt;We are trying to read 20k from DFS @ HFile layer. The BounderRangeFileInputStream instead reads several header bytes 1 byte at a time, and then reads a 11k chunk and later a 9k chunk.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 34386760 compressedSize = 20711 decompressedSize = 92324
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386760; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386761; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386762; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386763; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386764; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386765; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386766; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386767; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34386768; bytes: 11005
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397773; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397774; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397775; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397776; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397777; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397778; bytes: 1
2010-09-16 09:21:27,912 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397779; bytes: 1
2010-09-16 09:21:27,913 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397780; bytes: 1
2010-09-16 09:21:27,913 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### seeking to reading @ 34397781; bytes: 9690
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Seems like it should be an easy fix to prefetch the compressed size... rather than incremental fetches.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12474298">HBASE-3006</key>
            <summary>Reading compressed HFile blocks causes way too many DFS RPC calls severly impacting performance</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kannanm">Kannan Muthukkaruppan</assignee>
                                    <reporter username="kannanm">Kannan Muthukkaruppan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 16 Sep 2010 16:40:48 +0000</created>
                <updated>Fri, 20 Nov 2015 12:42:11 +0000</updated>
                            <resolved>Fri, 17 Sep 2010 03:57:38 +0000</resolved>
                                    <version>0.89.20100621</version>
                                    <fixVersion>0.89.20100924</fixVersion>
                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12910226" author="stack" created="Thu, 16 Sep 2010 17:01:15 +0000"  >&lt;p&gt;@Kannan You are my new hero.  I love the debug trace above.  You think though this will add up to the 10 seconds?&lt;/p&gt;</comment>
                            <comment id="12910233" author="kannanm" created="Thu, 16 Sep 2010 17:23:22 +0000"  >&lt;p&gt;Stack: This was just a snippet of the trace. Some rows span a lot more blocks.&lt;/p&gt;

&lt;p&gt;I am now looking at this in hfile/Compression.java:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; InputStream createDecompressionStream(
        InputStream downStream, Decompressor decompressor,
        &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; downStreamBufferSize) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
      CompressionCodec codec = getCodec();
      &lt;span class=&quot;code-comment&quot;&gt;// Set the internal buffer size to read from down stream.
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (downStreamBufferSize &amp;gt; 0) {
        Configurable c = (Configurable) codec;
        c.getConf().setInt(&lt;span class=&quot;code-quote&quot;&gt;&quot;io.file.buffer.size&quot;&lt;/span&gt;, downStreamBufferSize);
      }
      CompressionInputStream cis =
          codec.createInputStream(downStream, decompressor);
      BufferedInputStream bis2 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BufferedInputStream(cis, DATA_IBUF_SIZE);
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; bis2;

    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Off-hand don&apos;t understand all the params to the various Stream abstractions &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I tried passing the&lt;br/&gt;
compressed size of the block to above function instead of the current value 0 (for downStreamBufferSize) but &lt;br/&gt;
that didn&apos;t do it. Still digging, but if you know off-hand what setting to change, let me know.&lt;/p&gt;</comment>
                            <comment id="12910236" author="kannanm" created="Thu, 16 Sep 2010 17:33:29 +0000"  >&lt;p&gt;stack: To clarify regarding your: &lt;/p&gt;

&lt;p&gt;&amp;lt;&amp;lt;I love the debug trace above. You think though this will add up to the 10 seconds? &amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;The snippet was for a much smaller row. The outliers we saw were on larger blocks. But this should just help even the small row case. Basically the cost of missing any block in the block cache when compression is turned on was quite steep right now (will do about 5x-10x more DFS RPCs than needed).&lt;/p&gt;</comment>
                            <comment id="12910257" author="kannanm" created="Thu, 16 Sep 2010 18:33:47 +0000"  >&lt;p&gt;Wrapping a BufferedInputStream around did it.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-09-16 11:30:40,842 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32060739 compressedSize = 6197 decompressedSize = 65800
2010-09-16 11:30:40,842 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32060739; bytes: 6197
2010-09-16 11:30:40,843 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32066936 compressedSize = 6083 decompressedSize = 65658
2010-09-16 11:30:40,843 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32066936; bytes: 6083
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32073019 compressedSize = 5003 decompressedSize = 65708
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32073019; bytes: 5003
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32078022 compressedSize = 4834 decompressedSize = 65700
2010-09-16 11:30:40,844 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32078022; bytes: 4834
2010-09-16 11:30:40,845 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32082856 compressedSize = 6137 decompressedSize = 65566
2010-09-16 11:30:40,845 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32082856; bytes: 6137
2010-09-16 11:30:40,846 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32088993 compressedSize = 4727 decompressedSize = 65766
2010-09-16 11:30:40,846 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32088993; bytes: 4727
2010-09-16 11:30:40,876 INFO org.apache.hadoop.hbase.io.hfile.HFile: ### fs read @ offset = 32093720 compressedSize = 5025 decompressedSize = 65760
2010-09-16 11:30:40,876 INFO org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream: ### reading @ 32093720; bytes: 5025
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and perf is screaming back again.&lt;/p&gt;

&lt;p&gt;We can debate about whether the buffer size for the BufferInputStream should be the compressedSize (read everything) or some uniform sized chunks like 16k to play well with JVM. But basically the fix seems to work really well.&lt;/p&gt;

&lt;p&gt;Will post a patch shortly.&lt;/p&gt;


</comment>
                            <comment id="12910269" author="kannanm" created="Thu, 16 Sep 2010 19:01:52 +0000"  >&lt;p&gt;Patch submitted. Fixes perf problems we were seeing.&lt;/p&gt;

&lt;p&gt;Averages dropped from 1-2 seconds to 100ms for my test runs. No outliers &amp;gt; 10 seconds. Previously we had many, and some which took 40-50 seconds.&lt;/p&gt;

&lt;p&gt;Yet to run unit tests (will report back on that once it completes).&lt;/p&gt;

&lt;p&gt;Thoughts on what BufferedInputStream&apos;s buffer setting would be? The disadvantage of using compressedSize (as i n provided patch) might lots of varying length allocations. Does JVM prefer allocations to all be of a uniform size? If we set a different value, say 16k, it&apos;ll incur more DFS RPCs (but at least not silly 1 byte RPCs).&lt;/p&gt;</comment>
                            <comment id="12910286" author="stack" created="Thu, 16 Sep 2010 19:59:27 +0000"  >&lt;p&gt;Patch looks great to me (This code was robbed from tfile a long time back IIRC).&lt;/p&gt;

&lt;p&gt;My sense is that though this a short-lived allocation, allocating compressedSize is overdoing it and as you suggest, it can vary, possibly widely.  If a cell was large, then we could have an hfile much larger than 64k.  A buffer of 8 or 16k would just as well save against the 1 byte rpc reads.  If you agree, I can add this in on commit (allocate a 16k buffer by default)?&lt;/p&gt;</comment>
                            <comment id="12910313" author="tlipcon" created="Thu, 16 Sep 2010 20:46:30 +0000"  >&lt;p&gt;Rather than adding buffering, can we just avoid using the random-access API to DFSInputStream? If you use the other APIs, you will take advantage of the internal buffering of DFSInputStream instead of double buffering, and it should be faster.&lt;/p&gt;</comment>
                            <comment id="12910316" author="pranavkhaitan" created="Thu, 16 Sep 2010 20:54:17 +0000"  >&lt;p&gt;Looks good to me.. nice catch Kannan.. This will significantly improve read durations&lt;/p&gt;</comment>
                            <comment id="12910320" author="kannanm" created="Thu, 16 Sep 2010 21:11:03 +0000"  >&lt;p&gt;Caught up with Todd in IRC. Avoiding double buffering sounds good. But not clear what the alternate DFS api is.&lt;/p&gt;

&lt;p&gt;I am planning to try with min(16k, compressedSize);&lt;/p&gt;</comment>
                            <comment id="12910343" author="ryanobjc" created="Thu, 16 Sep 2010 22:01:48 +0000"  >&lt;p&gt;if the objects are short lived, it doesnt matter the object allocation size.  If they are used only for the lifecycle of reading a single block, i&apos;d say size it to the compressed Size.&lt;/p&gt;
</comment>
                            <comment id="12910351" author="kannanm" created="Thu, 16 Sep 2010 22:10:55 +0000"  >&lt;p&gt;For our test case, min(16k, compressedSize) will be a no-op since our &lt;br/&gt;
compressed sizes were almost always smaller than 16k. In our internal&lt;br/&gt;
branch, we&apos;re planning to go with 64k. Thought, for large objects, 16k &lt;br/&gt;
batching might be too small. Will upload the new patch.&lt;/p&gt;</comment>
                            <comment id="12910437" author="stack" created="Fri, 17 Sep 2010 03:57:38 +0000"  >&lt;p&gt;Committed.  Thanks for nice fix Kannan (I left it at 64k &amp;#8211; use the HFile.DEFAULT_BLOCKSIZE define instead &amp;#8211; thinking that less rpc&apos;ing is a better saving than a bit of memory in local heap).  &lt;/p&gt;</comment>
                            <comment id="12914574" author="jdcryans" created="Fri, 24 Sep 2010 18:23:11 +0000"  >&lt;p&gt;Adding to the latest 0.89&lt;/p&gt;</comment>
                            <comment id="15017180" author="lars_francke" created="Fri, 20 Nov 2015 12:42:11 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12454810" name="HBASE-3006-2.txt" size="1225" author="kannanm" created="Thu, 16 Sep 2010 22:12:21 +0000"/>
                            <attachment id="12454790" name="HBASE-3006.txt" size="1208" author="kannanm" created="Thu, 16 Sep 2010 19:01:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 16 Sep 2010 17:01:15 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26584</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hkaf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100549</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>