<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:47:54 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-7667/HBASE-7667.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-7667] Support stripe compaction</title>
                <link>https://issues.apache.org/jira/browse/HBASE-7667</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;So I was thinking about having many regions as the way to make compactions more manageable, and writing the level db doc about how level db range overlap and data mixing breaks seqNum sorting, and discussing it with Jimmy, Matteo and Ted, and thinking about how to avoid Level DB I/O multiplication factor.&lt;/p&gt;

&lt;p&gt;And I suggest the following idea, let&apos;s call it stripe compactions. It&apos;s a mix between level db ideas and having many small regions.&lt;br/&gt;
It allows us to have a subset of benefits of many regions (wrt reads and compactions) without many of the drawbacks (managing and current memstore/etc. limitation).&lt;br/&gt;
It also doesn&apos;t break seqNum-based file sorting for any one key.&lt;br/&gt;
It works like this.&lt;br/&gt;
The region key space is separated into configurable number of fixed-boundary stripes (determined the first time we stripe the data, see below).&lt;br/&gt;
All the data from memstores is written to normal files with all keys present (not striped), similar to L0 in LevelDb, or current files.&lt;br/&gt;
Compaction policy does 3 types of compactions.&lt;br/&gt;
First is L0 compaction, which takes all L0 files and breaks them down by stripe. It may be optimized by adding more small files from different stripes, but the main logical outcome is that there are no more L0 files and all data is striped.&lt;br/&gt;
Second is exactly similar to current compaction, but compacting one single stripe. In future, nothing prevents us from applying compaction rules and compacting part of the stripe (e.g. similar to current policy with rations and stuff, tiers, whatever), but for the first cut I&apos;d argue let it &quot;major compact&quot; the entire stripe. Or just have the ratio and no more complexity.&lt;br/&gt;
Finally, the third addresses the concern of the fixed boundaries causing stripes to be very unbalanced.&lt;br/&gt;
It&apos;s exactly like the 2nd, except it takes 2+ adjacent stripes and writes the results out with different boundaries.&lt;br/&gt;
There&apos;s a tradeoff here - if we always take 2 adjacent stripes, compactions will be smaller but rebalancing will take ridiculous amount of I/O.&lt;br/&gt;
If we take many stripes we are essentially getting into the epic-major-compaction problem again. Some heuristics will have to be in place.&lt;br/&gt;
In general, if, before stripes are determined, we initially let L0 grow before determining the stripes, we will get better boundaries.&lt;br/&gt;
Also, unless unbalancing is really large we don&apos;t need to rebalance really.&lt;br/&gt;
Obviously this scheme (as well as level) is not applicable for all scenarios, e.g. if timestamp is your key it completely falls apart.&lt;/p&gt;

&lt;p&gt;The end result:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;many small compactions that can be spread out in time.&lt;/li&gt;
	&lt;li&gt;reads still read from a small number of files (one stripe + L0).&lt;/li&gt;
	&lt;li&gt;region splits become marvelously simple (if we could move files between regions, no references would be needed).&lt;br/&gt;
Main advantage over Level (for HBase) is that default store can still open the files and get correct results - there are no range overlap shenanigans.&lt;br/&gt;
It also needs no metadata, although we may record some for convenience.&lt;br/&gt;
It also would appear to not cause as much I/O.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12629201">HBASE-7667</key>
            <summary>Support stripe compaction</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sershe">Sergey Shelukhin</assignee>
                                    <reporter username="sershe">Sergey Shelukhin</reporter>
                        <labels>
                    </labels>
                <created>Fri, 25 Jan 2013 01:30:31 +0000</created>
                <updated>Sat, 21 Feb 2015 23:32:50 +0000</updated>
                            <resolved>Fri, 6 Dec 2013 22:30:14 +0000</resolved>
                                                    <fixVersion>0.98.0</fixVersion>
                    <fixVersion>0.99.0</fixVersion>
                                    <component>Compaction</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>45</watches>
                                                                                                            <comments>
                            <comment id="13562257" author="sershe" created="Fri, 25 Jan 2013 01:35:03 +0000"  >&lt;p&gt;What do you guys think about the general idea?&lt;/p&gt;</comment>
                            <comment id="13562268" author="mcorgan" created="Fri, 25 Jan 2013 01:53:36 +0000"  >&lt;p&gt;So a stripe is like a sub-region?  In terms of compactions, it sounds like it serves the same purpose as splitting a table into regions, so you can compact a region that is hot while letting cold regions stay cold.&lt;/p&gt;

&lt;p&gt;If this is the case and stripes are allowed to auto-split, it may be very beneficial for time series data.  If you had a region approaching 10gb with 100mb stripes, the last stripe would keep splitting and the first 99 would never get touched.  The problem without stripes is that the first 9900mb keeps getting re-written even though it never changes.&lt;/p&gt;

&lt;p&gt;Am i understanding it correctly that stripes in a region would act similarly to regions in a table?&lt;/p&gt;</comment>
                            <comment id="13562273" author="zjushch" created="Fri, 25 Jan 2013 02:02:36 +0000"  >&lt;p&gt;Interesting ideas.&lt;br/&gt;
With stripe compaction, we could support many files in one region, each file belongs to one stripe, and no overlap keys cross stripes except LO, is it right?&lt;/p&gt;

&lt;p&gt;I think it is useful for the sequential write scenario.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;if we could move files between regions, no references would be needed)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Moving files would break snapshots, references are needed all the same&lt;/p&gt;</comment>
                            <comment id="13562278" author="sershe" created="Fri, 25 Jan 2013 02:12:13 +0000"  >&lt;p&gt;Hmm, my thinking would be that number of stripes will be fixed and we would rebalance, but never split as such. Yes, the idea for sequential data plays very well into this, the code will probably be almost the same. With non-seq data, it would just try to achieve effect similar to level.&lt;/p&gt;</comment>
                            <comment id="13562286" author="sershe" created="Fri, 25 Jan 2013 02:20:00 +0000"  >&lt;p&gt;If there are no general objections I will try to start on this tomorrow, in preference to level... I am looking at some random HCM issues now so patch may be next week provided that &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7516&quot; title=&quot;Make compaction policy pluggable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7516&quot;&gt;&lt;del&gt;HBASE-7516&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7603&quot; title=&quot;refactor storefile management in HStore in order to support things like LevelDB-style compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7603&quot;&gt;&lt;del&gt;HBASE-7603&lt;/del&gt;&lt;/a&gt; can go in.&lt;/p&gt;</comment>
                            <comment id="13562290" author="mcorgan" created="Fri, 25 Jan 2013 02:27:08 +0000"  >&lt;p&gt;I&apos;ve been brainstorming something similar for splitting the memstore into stripes, mentioned in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3484?focusedCommentId=13410934&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13410934&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3484?focusedCommentId=13410934&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13410934&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think it&apos;s a good idea now that region sizes have become so large.  It&apos;s easy to have a few hot stripes in a region if it&apos;s 10GB, and not necessarily wrong from a primary-key design perspective.  It&apos;s often very wasteful to be compacting the whole region.&lt;/p&gt;</comment>
                            <comment id="13575012" author="jxiang" created="Sat, 9 Feb 2013 00:56:08 +0000"  >&lt;p&gt;A stripe is like a sub-region. That&apos;s a good idea.&lt;/p&gt;</comment>
                            <comment id="13575024" author="mcorgan" created="Sat, 9 Feb 2013 01:39:53 +0000"  >&lt;p&gt;Right now there is a tug-of-war between region size and number of regions.  &lt;/p&gt;

&lt;p&gt;You want fewer regions for:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;server startup/shutdown&lt;/li&gt;
	&lt;li&gt;minimizing RPC calls&lt;/li&gt;
	&lt;li&gt;having fewer/bigger memstore flushes&lt;/li&gt;
	&lt;li&gt;fewer open files per server&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;You want more regions for:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;spreading load among servers&lt;/li&gt;
	&lt;li&gt;having more efficient compactions&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;spreading load among servers&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Since BigTable was designed machines have grown tremendously, with many running 24 cpus and 48+GB memory.  These machines can serve many regions even if each region is 30GB.  So we can achieve good load distribution even with enormous regions.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;having more efficient compactions&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;But, huge regions are bad for compactions.  30GB compactino is still considered expensive.  Data in HBase is generally not perfectly evenly distributed across a table, or if it is you are not taking full advantage of HBase&apos;s sorted architecture.  Huge regions therefore have hot and cold stripes/sub-regions.  If you compact a 30GB region, you are wasting a ton of time on the cold stripes.&lt;/p&gt;

&lt;p&gt;Overall, I think the sub-region approach cuts the rope in the tug-of-war.  It lets you have a smaller number of regions at the same time as having the most efficient compactions.&lt;/p&gt;</comment>
                            <comment id="13575206" author="jxiang" created="Sat, 9 Feb 2013 17:26:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;Overall, I think the sub-region approach cuts the rope in the tug-of-war. It lets you have a smaller number of regions at the same time as having the most efficient compactions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree. I was wondering if it will take longer to open a region if there are many hfiles.&lt;/p&gt;</comment>
                            <comment id="13575297" author="stack" created="Sat, 9 Feb 2013 22:53:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mcorgan&quot; class=&quot;user-hover&quot; rel=&quot;mcorgan&quot;&gt;Matt Corgan&lt;/a&gt; Nice write up.  So you are in favor of Sergey&apos;s project?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;Jimmy Xiang&lt;/a&gt; If I were to guess, it will take longer than the case where we have monolithic files that cover the total region namespace as we currently have (Because there will be more files).  If rather than strip&apos;ing inside a region, we instead had a region per stripe, my guess is that stripe&apos;ing will take less time to open since less region machinations going on (less .regioninfos to open, less looking in fs for stuff to clean up since last file open, less listing of storefiles under families).&lt;/p&gt;</comment>
                            <comment id="13575309" author="mcorgan" created="Sun, 10 Feb 2013 01:35:13 +0000"  >&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;So you are in favor of Sergey&apos;s project?&lt;/p&gt;&lt;/blockquote&gt;oh yes, if you could not tell.  Thinking of an &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7667&quot; title=&quot;Support stripe compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7667&quot;&gt;&lt;del&gt;HBASE-7667&lt;/del&gt;&lt;/a&gt; tatoo =)&lt;/p&gt;

&lt;p&gt;One of the few major things hbase is missing in my opinion is the ability to load time-series through the normal api, rather than having to go off and write some separate bulk load code.  HBase currently takes a dump when you do that.  Main culprits are &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5479&quot; title=&quot;Postpone CompactionSelection to compaction execution time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5479&quot;&gt;&lt;del&gt;HBASE-5479&lt;/del&gt;&lt;/a&gt; and my comment in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3484&quot; title=&quot;Replace memstore&amp;#39;s ConcurrentSkipListMap with our own implementation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3484&quot;&gt;HBASE-3484&lt;/a&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3484?focusedCommentId=13410934&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13410934&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3484?focusedCommentId=13410934&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13410934&lt;/a&gt;).  Even during normal operation as opposed to a one-off import of data, the inefficiencies are still happening, just at a less obvious pace.&lt;/p&gt;

&lt;p&gt;It may be a follow on to this jira, but having &quot;striper&quot; dynamically add stripes at the end of the region would let allow all the stripes before the last one &quot;go cold&quot; which is critical for avoiding hugely wasteful compactions of non-changing data.  Ideally, it would be able to allocate small stripes as new data comes in (each flush?) and then later go on to merge older stripes to reduce hfile count (at major compaction time?).  With this in place on an N node cluster, you could partition your data with N or 2N regions using a hash prefix and basically let the regions grow infinitely large.  Currently I have to limit region size to ~2GB which results in hundreds of regions per node which is a bit of a management hassle because it&apos;s beyond human readable, and a bit wasteful with all the empty memstores among other things.&lt;/p&gt;

&lt;p&gt;I do wonder if there&apos;s a more accurate name than stripe.  Stripes make me think of RAID stripes which is a different concept than sub-regions.  Sub-region is not a good name either though.&lt;/p&gt;

&lt;p&gt;It would be cool if you could set a column family attribute like layout=TIME_SERIES which HBase could use to automatically pick the compaction strategy, split-point strategy, balancer strategy, and allow future niceties like using stronger compression on old data.&lt;/p&gt;</comment>
                            <comment id="13575312" author="lhofhansl" created="Sun, 10 Feb 2013 01:53:47 +0000"  >&lt;p&gt;Stripes can have overlapping keyrange with other stripes, correct? I.e. if two L0 files are compacted with a L0-compation their each L0 is striped but since the L0-files could overlap, the stripes could too with other stripes from different L0 files.&lt;/p&gt;

&lt;p&gt;A nice property in LevelDB is that only L0 files have overlapping keyspaces with other files, all level &amp;gt; L0 have no overlapping keys within a level and no file at level L overlaps more than 10 files at L+1.&lt;/p&gt;

&lt;p&gt;Right now only major compactions can remove delete markers because only by looking at all data you can guarantee that you will see each KV that might be affected by the delete marker.&lt;br/&gt;
It&apos;s not clear to me how we get around this, unless we introduce a formal notion of levels and know which L+1 files overlap with a file at level L.&lt;/p&gt;

&lt;p&gt;Would love to discuss more at the PowWow on the Feb 19th.&lt;/p&gt;</comment>
                            <comment id="13575449" author="yuzhihong@gmail.com" created="Sun, 10 Feb 2013 15:30:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;Sub-region is not a good name either though.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Other names we can consider: arena, range, realm, section, sector, zone.&lt;/p&gt;</comment>
                            <comment id="13575578" author="jxiang" created="Sun, 10 Feb 2013 23:56:31 +0000"  >&lt;p&gt;Stripes don&apos;t have overlapping keyrange with other stripes.  So each stripe is just like a sub-region.  L0 files are special, could overlap with multiple stripes.&lt;/p&gt;</comment>
                            <comment id="13575583" author="lhofhansl" created="Mon, 11 Feb 2013 00:21:50 +0000"  >&lt;p&gt;I see, then a major compaction needs to at least include all current L0 files (in any), right?&lt;/p&gt;</comment>
                            <comment id="13575589" author="jxiang" created="Mon, 11 Feb 2013 01:01:07 +0000"  >&lt;p&gt;That&apos;s right.  To major compact a stripe, all L0 files, if any, can be compacted and split into stripes, then merge/compact all files belonging to the stripe.&lt;/p&gt;</comment>
                            <comment id="13575883" author="nspiegelberg" created="Mon, 11 Feb 2013 16:39:13 +0000"  >&lt;p&gt;Some thoughts I had about this:&lt;/p&gt;

&lt;p&gt;Overall, I think it&apos;s a good idea.  Seems like it&apos;s not crazy to add and would have multiple benefits.  Logical striping across the L1 boundary is a simple solution to both proactively handle splits and reduce compaction times.&lt;/p&gt;

&lt;p&gt;Thoughts on this feature&lt;br/&gt;
1. Fixed configs : in the same way that we got a lot of stability by limiting the regions/server to a fixed number, we might want to similarly limit the number of stripes per region to 10 (or X) instead of &quot;every Y bytes&quot;.  This will help us understand the benefit we get from striping and it&apos;s easy to double the striping and chart the difference.&lt;br/&gt;
2. NameNode pressure :  Obviously, a 10x striping factor will cause 10x scaling of the FS.  Can we offset this by increasing the HDFS block size, since addBlock dominates at scale?  Really, unlike Hadoop, you have all of the HFile or none of it.  Missing a portion of the HFile currently invalidates the whole file.  You really need 1 HDFS block == 1 HFile.  However, we could probably just toy with increasing it by the striping factor right now and seeing if that balances things.&lt;br/&gt;
3. Open Times : I think this will be an issue, specifically on server start.  Need to be careful here.&lt;br/&gt;
4. Major compaction : you can perform a major compaction (remove deletes) as long as you have [i,end) contiguous.  I don&apos;t think you&apos;d need to involve L0 files in an MC at all.  Save the complexity.  Furthermore, part of the reason why we created the tiered compaction is to prevent small/new files from participating in MC because of cache thrashing, poor minor compactions, and a handful of other reasons.&lt;/p&gt;

&lt;p&gt;So, some thoughts on related pain points we seem to have that tie into this feature:&lt;br/&gt;
1. Reduce Cache thrashing : region moves kill us a lot of time because we have a cold cache.  There is a worry that more aggressive compactions mean more thrashing.  I think it will actual even this out better since right now a MC causes a lot of churn.  Just should be thinking about this if perf after the feature isn&apos;t what we desire.&lt;br/&gt;
2. Unnecessary IOPS : outside of this algorithm, we should just completely get rid of the requirement to compact after a split.  We have the block cache, so given a [start,end) in the file, we can easily tell our mid point for future splits.  There&apos;s little reason to aggressively churn in this way after splitting.&lt;br/&gt;
3. Poor locality : for grid topology setups, we should eventually make the striping algorithm a little more intelligent about picking our replicas.  If all stripes go to the same secondary &amp;amp; tertiary node, then splits have a very restricted set of servers to chose for datanode locality.&lt;/p&gt;</comment>
                            <comment id="13575934" author="jxiang" created="Mon, 11 Feb 2013 17:51:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think you&apos;d need to involve L0 files in an MC at all.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agree. The major compaction is not the traditional one any more.&lt;/p&gt;

&lt;p&gt;We can limit the number of stripes per region.  But we don&apos;t have to start with that many stripes at the beginning.&lt;/p&gt;

&lt;p&gt;One thing I&apos;d like to mention is that auto-split a stripe will be much simpler than auto-split a region; auto-merge couple stripes will be much simpler than auto-merge couple regions too. I&apos;d see we should prefer more stripes than more regions, for a given table. &lt;/p&gt;</comment>
                            <comment id="13575936" author="sershe" created="Mon, 11 Feb 2013 17:56:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;It may be a follow on to this jira, but having &quot;striper&quot; dynamically add stripes at the end of the region would let allow all the stripes before the last one &quot;go cold&quot; which is critical for avoiding hugely wasteful compactions of non-changing data&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Actually, it can be added as part of the main work, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7679&quot; title=&quot;implement store file management for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7679&quot;&gt;&lt;del&gt;HBASE-7679&lt;/del&gt;&lt;/a&gt; (file management) code includes such capabilities. &lt;br/&gt;
I wonder how, no matter the compactions, does region management work for such scenario. Wouldn&apos;t all the load always be on last region if you have TS keys?&lt;br/&gt;
Or, if you have artificial partitioning but query by TS, wouldn&apos;t all queries go to all servers?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;To major compact a stripe, all L0 files, if any, can be split into stripes, then merge all files belonging to the stripe.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can you explain more about the delete marker limitation?&lt;br/&gt;
Suppose in current compaction selection, I choose a set of files starting at the oldest file but not including all files.&lt;br/&gt;
Wouldn&apos;t that be enough to process delete markers that delete the updates within those files? Granted, I might not process all delete markers, but I don&apos;t have to see all files. E.g. if I only have 3 files with one entry for K each, &quot;K=V&quot;, &quot;delete K&quot;, &quot;K=V2&quot;, and I compact the first two, I can remove entries for K from them, right?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1. Fixed configs : in the same way that we got a lot of stability by limiting the regions/server to a fixed number, we might want to similarly limit the number of stripes per region to 10 (or X) instead of &quot;every Y bytes&quot;. This will help us understand the benefit we get from striping and it&apos;s easy to double the striping and chart the difference.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That is the original idea.&lt;/p&gt;

&lt;p&gt;Thanks for other comments &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13575951" author="stack" created="Mon, 11 Feb 2013 18:29:55 +0000"  >&lt;p&gt;In a striped region, if stripes &amp;gt;= 2 and the key distribution is basically even, a split could be done w/o references, halfhfiles, and rewriting from parent to daughter; a split could just rename the parent files into the daughter regions.   It could make for split simplification and possibly make for some i/o savings.&lt;/p&gt;

&lt;p&gt;This is a load of great stuff in this issue.  Best read I&apos;ve had in a long time.&lt;/p&gt;</comment>
                            <comment id="13575995" author="mcorgan" created="Mon, 11 Feb 2013 19:12:13 +0000"  >&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;Open Times : I think this will be an issue, specifically on server start. Need to be careful here.&lt;/p&gt;&lt;/blockquote&gt;Hopefully could be mitigated by making regions larger, like doubling region size and setting max 2 stripes/region.  Theoretically should be able to have the same overall number of files as normal regions, or are there other factors at play?&lt;/p&gt;

&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;Wouldn&apos;t all the load always be on last region if you have TS keys?  Or, if you have artificial partitioning but query by TS, wouldn&apos;t all queries go to all servers?&lt;/p&gt;&lt;/blockquote&gt;An easy strategy for P partitions is to &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;prepend a single byte to each key where prefix=hash(row)%P&lt;/li&gt;
	&lt;li&gt;pre-split the table into P regions&lt;/li&gt;
	&lt;li&gt;tweak the balancer to evenly spread the tail partitions for each region&lt;/li&gt;
	&lt;li&gt;writes get sprayed evenly to all tail partitions&lt;/li&gt;
	&lt;li&gt;a single Get query will only hit one region since you know hash(row)%P beforehand&lt;/li&gt;
	&lt;li&gt;you scan all P partitions using a P-way collating iterator
	&lt;ul&gt;
		&lt;li&gt;so yes, scans go to all servers but presumably they are huge and would hit lots of data anyway&lt;/li&gt;
		&lt;li&gt;because they are huge, a client that scans the partitions concurrently will be faster&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;a big multi-Get will spray to the exact servers necessary, possibly all of them, but like scans may be faster because done in parallel&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m not sure what most people are doing with time series data but this seems like a good approach to me.  You basically just choose arbitrarily large P.  An MD5 prefix is essentially P=2^128 (I wouldn&apos;t recommend pre-splitting at that granularity).&lt;/p&gt;</comment>
                            <comment id="13576023" author="sershe" created="Mon, 11 Feb 2013 19:31:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;a split could just rename the parent files into the daughter regions&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was told this is impossible due to snapshots relying on files not moving between regions (or on references during splits?)&lt;br/&gt;
We just discussed this here, some improvements for splits should definitely be possible.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;so yes, scans go to all servers but presumably they are huge and would hit lots of data anyway&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, I meant the scans, was assuming scans for TS data mostly. I see.&lt;/p&gt;</comment>
                            <comment id="13576044" author="jxiang" created="Mon, 11 Feb 2013 19:45:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;Wouldn&apos;t that be enough to process delete markers that delete the updates within those files?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think so, as long as the files not included are newer (like in L0).&lt;/p&gt;</comment>
                            <comment id="13576296" author="yuzhihong@gmail.com" created="Tue, 12 Feb 2013 02:11:12 +0000"  >&lt;p&gt;I think we need to find the best way of storing stripe boundaries.&lt;/p&gt;

&lt;p&gt;Currently Sergey puts the boundaries in metadata of store files. This is not flexible. Once this feature is released, users would request ways to configure stripes in different manners.&lt;/p&gt;</comment>
                            <comment id="13576798" author="sershe" created="Tue, 12 Feb 2013 17:26:55 +0000"  >&lt;p&gt;The stripe boundaries are supposed to be an internal detail not visible to the user, the user only configured the scheme (#of stripes, or size-based stripes as described for time series data). &lt;br/&gt;
What kind of stripe configuration do you have in mind?&lt;br/&gt;
After talking about splits yesterday I am planning to make it a little more flexible for splits, but still internal.&lt;/p&gt;</comment>
                            <comment id="13576826" author="stack" created="Tue, 12 Feb 2013 17:53:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;Currently Sergey puts the boundaries in metadata of store files. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is wrong w/ this?  It is the bounds of the keys the file contains?  (Is this not there already, the first and last key?)&lt;/p&gt;

&lt;p&gt;What are the thoughts on figuring region stripe boundaries.  Will it be done by looking at the memstore content just before flush and dividing it into n files?  Or will it be done after the first flush on subsequent compactions by looking at content of L0?&lt;/p&gt;</comment>
                            <comment id="13576829" author="sershe" created="Tue, 12 Feb 2013 17:59:44 +0000"  >&lt;p&gt;In the &quot;N stripes&quot; scheme, boundaries will be determined at first L0 compaction. I am intending to add parameter to config to make first L0 compaction wait for more files to get better ones. Then, the stripes can be rebalanced but the hope is that it happens infrequently.&lt;br/&gt;
In the &quot;growing key range&quot; scheme (for time series above) stripe boundaries don&apos;t matter much, and are basically determined based on size. If nothing intervenes, by EOW or next week I hope to have preliminary compaction policy/compactor patch.&lt;/p&gt;</comment>
                            <comment id="13576836" author="yuzhihong@gmail.com" created="Tue, 12 Feb 2013 18:07:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;It is the bounds of the keys the file contains? (Is this not there already, the first and last key?)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Currently HFile provides the following methods:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] getFirstRowKey();

    &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] getLastRowKey();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Stripe boundaries should be different from the above.&lt;/p&gt;</comment>
                            <comment id="13576845" author="stack" created="Tue, 12 Feb 2013 18:15:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted_yu&quot; class=&quot;user-hover&quot; rel=&quot;ted_yu&quot;&gt;Ted Yu&lt;/a&gt; You raise a &apos;concern&apos; that is nebulous to shoot down a suggested implementation and then when asked why you do not answer the question asked.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; Would it be easier looking at memstore than at content of L0?  Would have to do appropriate weighting... in that there will be hot spots in the keyspace and these should get more stripes.  Rejiggering the stripes joining up the infrequently used and allotting more stripes to the hot keyspace area sounds right.   Good stuff.&lt;/p&gt;

&lt;p&gt;Maybe its worth writing up a doc at this stage.  This issue is full of goodness.  A doc could distill it out and make it easier on the digestion and easier to think about it.&lt;/p&gt;</comment>
                            <comment id="13576881" author="sershe" created="Tue, 12 Feb 2013 18:52:04 +0000"  >&lt;p&gt;Discussed with Ted, what was meant is configuring it like region splitting, which is not planned at this point.&lt;br/&gt;
Doc - probably needed... I will get to it eventually.&lt;/p&gt;</comment>
                            <comment id="13577311" author="yuzhihong@gmail.com" created="Wed, 13 Feb 2013 03:44:17 +0000"  >&lt;p&gt;Here is my understanding of the difference between existing store file metadata and the new stripe boundary metadata.&lt;br/&gt;
Existing store file metadata (first row key, last row key, etc) is intrinsic to the store file. i.e., their meaning doesn&apos;t change when the store file gets copied to another cluster.&lt;br/&gt;
However stripe boundary doesn&apos;t seem to carry the same characteristics.&lt;br/&gt;
Let&apos;s consider table A1 in cluster C1 and table A2 in cluster C2. They have the same schema and region boundaries. When store file F1 is copied from C1 to C2, user may switch to a different striping strategy. The reason could be that clusters C1 and C2 serve different patterns of load.&lt;br/&gt;
Embedding stripe boundary in store file may potentially cause confusion in cluster C2.&lt;/p&gt;

&lt;p&gt;Another factor is that StoreFileManager needs to scan all store files to establish / validate stripe boundaries when region opens.&lt;/p&gt;

&lt;p&gt;Please correct me if I am wrong.&lt;/p&gt;</comment>
                            <comment id="13578037" author="sershe" created="Wed, 13 Feb 2013 23:39:53 +0000"  >&lt;p&gt;If the user copies the file, few things may happen. If he uses the non-stripe compaction, files will just get compacted into fewer files eventually. If all files are copied, stripe compactions are used and stripe config is different, SFM will load old stripes and eventually re-stripe the data to conform to new config, if necessary. That will depend on compactionpolicy implementation. If part of the files are copied (whatever sense that makes), either the above will still happen, or SFM will give up on metadata, and put all files to L0, re-striping them according to new config after some time.&lt;/p&gt;

&lt;p&gt;SFM needs metadata from all files, but as far as I see from HStore that is already loaded, because other code makes use of metadata without special considerations. One thing is that with more files there will be more to open...&lt;/p&gt;</comment>
                            <comment id="13578087" author="mcorgan" created="Thu, 14 Feb 2013 01:46:24 +0000"  >&lt;p&gt;Sergey - I&apos;m curious what&apos;s the reasoning behind flushing memstore to a single L0 file rather than splitting the memstore into the stripes during each flush?  Keep flushes faster, less files, etc?&lt;/p&gt;</comment>
                            <comment id="13578090" author="sershe" created="Thu, 14 Feb 2013 01:49:23 +0000"  >&lt;p&gt;My reasoning was - too many really tiny files, plus scope creep into memstore.&lt;/p&gt;</comment>
                            <comment id="13578099" author="stack" created="Thu, 14 Feb 2013 01:59:59 +0000"  >&lt;p&gt;Would it simplify the filesystem implementation if you did the split in memory (caveat the scope creep up into memstore) so no special L0 tier?  Regards files being too small, that is a different issue (e.g. a Toddea &amp;#8211; Todd+idea &amp;#8211; that I tripped over recently in an issue here is that rather than flush immediately, that we&apos;d instead do a purge and compaction in memory before flushing to ensure the content large enough to make a file... but yeah, that&apos;d be something else).&lt;/p&gt;</comment>
                            <comment id="13578105" author="mcorgan" created="Thu, 14 Feb 2013 02:21:39 +0000"  >&lt;p&gt;Gotcha.  Agree about limiting scope.  If the special L0 tier turns out to be more difficult to implement than originally intended for whatever reason, might be worth evaluating splitting during flush.  Seems like the same number of files might get created anyway when you split the L0 file?  Or do you plan on doing some &quot;logical striping across the L1 boundary&quot; as Nicolas says above where the L0 files are never truly split?&lt;/p&gt;

&lt;p&gt;Like Stack mentions, longer term I think we&apos;ll need to split memstore while in use, and those splits should probably have some alignment with these stripe boundaries.  For another day...&lt;/p&gt;</comment>
                            <comment id="13578108" author="sershe" created="Thu, 14 Feb 2013 02:25:17 +0000"  >&lt;p&gt;L0 actually should be relatively easy to implement, it&apos;s the special cases about all the possible stripe boundaries that cause all the complexity.&lt;br/&gt;
L0 files will hopefully not be split individually, but as soon as we reach some number of files, similarly to level db algorithm.&lt;br/&gt;
But yeah with insta-stripe solution we could get rid of L0 and also get less files for reads. Could be future improvement.&lt;/p&gt;</comment>
                            <comment id="13578192" author="stack" created="Thu, 14 Feb 2013 07:10:43 +0000"  >&lt;p&gt;I&apos;d like to get rid of L0 so can do splitting w/o resorting to file References and half hfiles (smile).... but yes, could be future as you say Sergey.&lt;/p&gt;</comment>
                            <comment id="13578445" author="jxiang" created="Thu, 14 Feb 2013 16:01:18 +0000"  >&lt;p&gt;Getting rid of L0 means memstore flushing will take longer and hold update/read longer?&lt;/p&gt;</comment>
                            <comment id="13578513" author="stack" created="Thu, 14 Feb 2013 17:34:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Getting rid of L0 means memstore flushing will take longer and hold update/read longer?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes but on back side, less compactions (no need to compact on region open since no half files/references) and for many workloads, less compactions overall because only a subset of files will be picked up from L0 tier.&lt;/p&gt;

&lt;p&gt;Can do work too to minimize how much we hold flushes.  Could be dumb and on flush, inline, examine key spread so can make ruling on where to insert boundaries (figuring boundaries would be for first flush only?  Or I suppose boundary making would be ongoing over the life of a region... four boundaries per region seems like a nice number to work with....)... so this would be a scan of the 64MB memstore keys.... or, we could keep a running tally as we insert into the memstore so at flush time we knew where the boundaries were... could do stuff like //  request to NN to open the files to flush too.&lt;/p&gt;

&lt;p&gt;In general, we want to add smarts around flushing (stripes, in-memory compacting, etc.) so eventually we are going to have this friction on flush (IMO).&lt;/p&gt;</comment>
                            <comment id="13609840" author="sershe" created="Fri, 22 Mar 2013 03:01:30 +0000"  >&lt;p&gt;Ok, here&apos;s the proper functional spec-like document. Future improvements is incomplete...&lt;br/&gt;
Should make CRs easier.&lt;/p&gt;</comment>
                            <comment id="13609873" author="yuzhihong@gmail.com" created="Fri, 22 Mar 2013 03:52:38 +0000"  >&lt;p&gt;Nice document, Sergey.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This can obviously be improved since most (or all) stripes, see future improvements.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The first half of the sentence seems to be incomplete.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Before starting a new writer, compactor ensures that all the KVs for the last row in the previous writer go to the previous writer.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think I understand what you mean - such KVs would be written by previous writer.&lt;/p&gt;</comment>
                            <comment id="13611324" author="sershe" created="Fri, 22 Mar 2013 22:01:38 +0000"  >&lt;p&gt;Hmm... I am starting to think that we might want to consider getting rid of L0 after all. Somehow it escaped me that L0 gives you x2 write amplification right there as all data has to be re-striped. Small files actually don&apos;t increase the number of files for gets/non-overlapping scans because each L0 file still counts for each stripe.&lt;/p&gt;</comment>
                            <comment id="13611728" author="srivas" created="Sat, 23 Mar 2013 14:59:46 +0000"  >&lt;p&gt;There is of course one major caveat with this approach. If data insertion is uniformly spread (ie, key is uniform random), this proposal performs much worse than the existing scheme.&lt;/p&gt;</comment>
                            <comment id="13611820" author="sershe" created="Sat, 23 Mar 2013 18:59:37 +0000"  >&lt;p&gt;There are two approaches discussed, one similar to having many small regions and one for sequential data. Which one do you mean? I am testing the first one with uniformly distributed keys now and it&apos;s somewhat slower than default case on average (on writes mostly) but has no big compaction associated latency spikes... I suspect if there was not so much compaction due to L0 the write slowness could also be alleviated.&lt;br/&gt;
I haven&apos;t tested the 2nd one yet (requires a more custom test, next week) but it&apos;s very specialized, for sequential data, so yes it is not good for common case.&lt;/p&gt;</comment>
                            <comment id="13611843" author="stack" created="Sat, 23 Mar 2013 20:16:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;There is of course one major caveat with this approach. If data insertion is uniformly spread (ie, key is uniform random), this proposal performs much worse than the existing scheme.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srivas&quot; class=&quot;user-hover&quot; rel=&quot;srivas&quot;&gt;M. C. Srivas&lt;/a&gt; Why?  Won&apos;t it do same total i/o?&lt;/p&gt;</comment>
                            <comment id="13611844" author="stack" created="Sat, 23 Mar 2013 20:17:49 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
On attached doc, it is lovely.

Missing author, date, and JIRA pointer?

An interesting comment by LarsH recently was that maybe we should ship w /major compactions off; most folks don&apos;t delete

Missing is one a pointer at least to how it currently works (could just point at src file I&apos;d say with its description of &apos;sigma&apos; compactions) and a sentence on whats wrong w/ it
or the problems it leads too when left run amok (you say it &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; major compactions but even w/o major compactions enabled, an i/o tsunami can hit and wipe us out

What does &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; mean &lt;span class=&quot;code-quote&quot;&gt;&quot;and	old	boundaries	rarely,	&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;	ever,	moving.&quot;&lt;/span&gt;?  Give doc an edit?

I think you need to say stripe == sub-range of the region key range.  You almost &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;.  Just &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; it explicitly.

I see your extra justification &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; l0, the need to be able to bulk load.  It is kinda important that we &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt; to support that.  Good one.

Later I suppose we could have a combination of count-based and size-based.... &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; an edge stripe is N time bigger than any other, add a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; stripe?

I was wondering &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you could make use of liang xie&apos;s bit of code &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; making keys &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the block cache where he chooses a &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; sequence that falls between
the last key in the former block and the first in the next block but the key is shorter than either..... but it doesn&apos;t make sense here I believe;
your boundaries have to be hard actual keys given inserts are always coming in.... so nevermind &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; suggestion.

You write the stripe info to the storefile.  I suppose it is up to the hosting region whether or not it chooses to respect those boundaries.  It
could ignore them and just respect the seqnum and we&apos;d have the old-style storefile handling, right?  (Oh, I see you allow &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; -- good)

Say in doc that you mean storefile metadata &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; it is ambiguous.

Thinking on L0 again, as has been discussed, we could have flushes skip L0 and flush instead to stripes (one flush turns into N files, one per stripe)
but even &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; we had &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; optimization, it looks like we&apos;d still want the L0 option &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; only &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; bulk loaded files or &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; files whose metadata makes
no sense to the current region context.

&lt;span class=&quot;code-quote&quot;&gt;&quot;&#8226; The	aggregate	range	of	files	going	in	must	be	contiguous...&quot;&lt;/span&gt; Not sure I follow.  Hmm... could &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; with &lt;span class=&quot;code-quote&quot;&gt;&quot;.... going into a compaction&quot;&lt;/span&gt;

&lt;span class=&quot;code-quote&quot;&gt;&quot;If	the	stripe	boundaries	are	changed	by	compaction,	the	entire	stripes	with	old	boundaries	must	be	replaced&quot;&lt;/span&gt; ...What would bring &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; on?
And then how would old boundaries get redone?  This one is a bit confusing.

Get key before is a PITA

Not sure I follow here: &quot;This	compaction	is	performed	when	the	number	of	L0	files	
exceeds	some	threshold	and	produces	the	number	of	files	equivalent	to	the	number	
of	stripes,	with	enforced	existing	boundaries.&quot;

I was going to suggest an optimization &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; later &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; that an L0 fits fully inside a stripe, I was thinking you could just &apos;move&apos; it into
its respective stripe... but I suppose you can&apos;t &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; that because you need to write the metadata to put a file into a stripe...

Would it help naming files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the stripe they belong too?  Would that help?  In other words &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; NOT write stripe data to the storefiles and just
let the region in memory figure which stripe a file belongs too.  When we write, we write with say a L0 suffix.  When compacting we add S1, S2, 
etc suffix &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; stripe1, etc.  To figure what the boundaries of an S0 are, it&apos;d be something the region knew.  On open of the store files, it could
use the start and end keys that are currently in the file metadata to figure which stripe they fit in.

Would be a bit looser.  Would allow moving a file between stripes with a rename only.

The delete dropping section looks right.  I like the major compaction along a stripe only option.

&lt;span class=&quot;code-quote&quot;&gt;&quot;For	empty	 ranges,	empty	files	are	created.&quot;&lt;/span&gt;  Is &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; necessary?  Would be good to avoid doing &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.


&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13611909" author="mcorgan" created="Sun, 24 Mar 2013 00:13:08 +0000"  >&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;If data insertion is uniformly spread (ie, key is uniform random), this proposal performs much worse than the existing scheme.&lt;/p&gt;&lt;/blockquote&gt;I think the goal for uniformly random keys is to have the same amount of total work done but to stagger that work.  Instead of doing 1 big 24 GB compaction per day, it could do a 1 GB compaction each hour.&lt;/p&gt;

&lt;p&gt;The savings/efficiency become more pronounced with less random keys, with the biggest savings for sequential keys.&lt;/p&gt;</comment>
                            <comment id="13612653" author="srivas" created="Mon, 25 Mar 2013 14:16:12 +0000"  >&lt;p&gt;@mcorgan and @stack:&lt;/p&gt;

&lt;p&gt;The total i/o in terms of i/o bandwidth consumed is the same. But the disk iops are much, much worse. And disk iops are at a premium, and &quot;bg activity&quot; like compactions should consume as few as possible.&lt;/p&gt;

&lt;p&gt;Let&apos;s say we split a region into a 100 sub-regions, such that each sub-region is in the few 10&apos;s of MB. If the data is written uniformly randomly, each sub-region will write out a store at approx the same time. That is, a RS will write 100x more files into HDFS (100x more random i/o on the local file-system). Next, all sub-regions will do a compaction at almost the same time, which is again 100x more read iops to read the old stores for merging.&lt;/p&gt;

&lt;p&gt;One can try to stagger the compactions to avoid the sudden burst by incorporating, say, a queue of to-be-compacted-subregions. But while the sub-regions at the head of the queue will compact &quot;in time&quot;, the ones at the end of the queue will have many more store files to merge, and will use much more than their &quot;fair-share&quot; of iops (not to mention that the read-amplification in these sub-regions will be higher too). The iops profile will be worse than just 100x.&lt;/p&gt;</comment>
                            <comment id="13612808" author="stack" created="Mon, 25 Mar 2013 16:33:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srivas&quot; class=&quot;user-hover&quot; rel=&quot;srivas&quot;&gt;M. C. Srivas&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But the disk iops are much, much worse.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;See Sergey&apos;s writeup.  We flush same as we always did writing a single file to the L0 tier.  It is later at compaction time &amp;#8211; i.e. NOT random i/o &amp;#8211; that we&apos;d write a file per &quot;sub-region/stripe&quot;.  If the write is evenly distributed, we&apos;d do the same overall i/o except with stripe compacting it would be done in smaller bite sizes (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt; Would the compaction of stripes run in //?  Hopefully, for the case Srivas describes, we&apos;d progress serially through the stripes/sub-regions or at least it would be an option and then later, ergonomically, we&apos;d recognize the even-loading case and add compaction accordingly)&lt;/p&gt;

&lt;p&gt;You have a point that we will be making more files in the fs.&lt;/p&gt;</comment>
                            <comment id="13612847" author="mcorgan" created="Mon, 25 Mar 2013 17:02:33 +0000"  >&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;a RS will write 100x more files into HDFS (100x more random i/o on the local file-system)&lt;/p&gt;&lt;/blockquote&gt;I think this is a point of confusion.  A typical HBase file could be 4MB to 40GB, where those files are a series of 4KB (very small) underlying disk blocks.  Ignoring complexities of multiple tasks running simultaneously on the regionserver, only the first 4KB block of each file is a random write, while the following blocks are sequential writes.  The few extra random writes should be lost in the noise of all the other random IO requests happening.&lt;/p&gt;</comment>
                            <comment id="13612956" author="sershe" created="Mon, 25 Mar 2013 18:34:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;The first half of the sentence seems to be incomplete.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think I understand what you mean - such KVs would be written by previous writer.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Missing author, date, and JIRA pointer?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think you need to say stripe == sub-range of the region key range.  You almost do.  Just do it explicitly.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;What does this mean &quot;and	old	boundaries	rarely,	if	ever,	moving.&quot;?  Give doc an edit?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Say in doc that you mean storefile metadata else it is ambiguous.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Not sure I follow here: &quot;This	compaction	is	performed	when	the	number	of	L0	files	exceeds	some	threshold	and	produces	the	number	of	files	equivalent	to	the	number	of	stripes,	with	enforced	existing	boundaries.&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Fixed these.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;An interesting comment by LarsH recently was that maybe we should ship w /major compactions off; most folks don&apos;t delete&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hmm... in general I agree but we&apos;ll have to insert really good warnings everywhere. Can we detect if they delete? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Missing is one a pointer at least to how it currently works (could just point at src file I&apos;d say with its description of &apos;sigma&apos; compactions) and a sentence on whats wrong w/ it&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;Later I suppose we could have a combination of count-based and size-based.... if an edge stripe is N time bigger than any other, add a new stripe?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, it&apos;s mentioned in code comment somewhere. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I was wondering if you could make use of liang xie&apos;s bit of code for making keys for the block cache where he chooses a byte sequence that falls between the last key in the former block and the first in the next block but the key is shorter than either..... but it doesn&apos;t make sense here I believe; your boundaries have to be hard actual keys given inserts are always coming in.... so nevermind this suggestion.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For boundary determination it does make sense; can you point at the code? After cursory look I cannot find it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You write the stripe info to the storefile.  I suppose it is up to the hosting region whether or not it chooses to respect those boundaries.  It could ignore them and just respect the seqnum and we&apos;d have the old-style storefile handling, right?  (Oh, I see you allow for this &amp;#8211; good)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Thinking on L0 again, as has been discussed, we could have flushes skip L0 and flush instead to stripes (one flush turns into N files, one per stripe) but even if we had this optimization, it looks like we&apos;d still want the L0 option if only for bulk loaded files or for files whose metadata makes no sense to the current region context. &quot;&#8226; The	aggregate	range	of	files	going	in	must	be	contiguous...&quot; Not sure I follow.  Hmm... could do with &quot;.... going into a compaction&quot;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, that was my thinking too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;If	the	stripe	boundaries	are	changed	by	compaction,	the	entire	stripes	with	old	boundaries	must	be	replaced&quot; ...What would bring this on? And then how would old boundaries get redone?  This one is a bit confusing.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Clarified. Basically one cannot have 3 files in (-inf, 3) and 3 in [3, inf), then take 3 and 2 respectively, and rewrite them with boundary 4, because then there will be a file with [3, inf) remaining that overlaps.&lt;/p&gt;



&lt;blockquote&gt;&lt;p&gt;I was going to suggest an optimization for later for the case that an L0 fits fully inside a stripe, I was thinking you could just &apos;move&apos; it into its respective stripe... but I suppose you can&apos;t do that because you need to write the metadata to put a file into a stripe...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah. Also wouldn&apos;t expect it to be a common case.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Would it help naming files for the stripe they belong too?  Would that help?  In other words do NOT write stripe data to the storefiles and just let the region in memory figure which stripe a file belongs too.  When we write, we write with say a L0 suffix.  When compacting we add S1, S2, etc suffix for stripe1, etc.  To figure what the boundaries of an S0 are, it&apos;d be something the region knew.  On open of the store files, it could use the start and end keys that are currently in the file metadata to figure which stripe they fit in.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Would be a bit looser.  Would allow moving a file between stripes with a rename only. The delete dropping section looks right.  I like the major compaction along a stripe only option. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This could be done as future improvement. The implications of change of naming scheme for other parts of the systems need to be determined.&lt;br/&gt;
Also for all I know it might break snapshots (moving files does). And, code to figure ut stripes on the fly would be more complex.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&quot;For	empty	 ranges,	empty	files	are	created.&quot;  Is this necessary?  Would be good to avoid doing this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Let me think about this... &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The total i/o in terms of i/o bandwidth consumed is the same. But the disk iops are much, much worse. And disk iops are at a premium, and &quot;bg activity&quot; like compactions should consume as few as possible.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Let&apos;s say we split a region into a 100 sub-regions, such that each sub-region is in the few 10&apos;s of MB. If the data is written uniformly randomly, each sub-region will write out a store at approx the same time. That is, a RS will write 100x more files into HDFS (100x more random i/o on the local file-system). Next, all sub-regions will do a compaction at almost the same time, which is again 100x more read iops to read the old stores for merging.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Memstore for region is preserved as unified... it may be written out to multiple files indeed in future.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;One can try to stagger the compactions to avoid the sudden burst by incorporating, say, a queue of to-be-compacted-subregions. But while the sub-regions at the head of the queue will compact &quot;in time&quot;, the ones at the end of the queue will have many more store files to merge, and will use much more than their &quot;fair-share&quot; of iops (not to mention that the read-amplification in these sub-regions will be higher too). The iops profile will be worse than just 100x.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In current implementation the region is limited to one compaction at a time, mostly for simplicity sake. Yes, if all stripes compact at the same time for the uniform scheme all improvement will disappear; this will have to be controlled if ability to do so is added.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You have a point that we will be making more files in the fs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, that is inevitable.&lt;br/&gt;
I hear from someone from Accumulo that they have tons of files opened without any problems... it may make sense to investigate if we have problems.&lt;/p&gt;
</comment>
                            <comment id="13613004" author="stack" created="Mon, 25 Mar 2013 19:18:11 +0000"  >&lt;p&gt;&quot; &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7845&quot; title=&quot;optimize hfile index key&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7845&quot;&gt;&lt;del&gt;HBASE-7845&lt;/del&gt;&lt;/a&gt; optimize hfile index key&quot; is the key/&quot;boundary determination&quot; work I was referring to (I don&apos;t think it applies here but adding the reference since you asked for it)&lt;/p&gt;</comment>
                            <comment id="13613394" author="sershe" created="Tue, 26 Mar 2013 01:47:56 +0000"  >&lt;p&gt;updated doc&lt;/p&gt;</comment>
                            <comment id="13613454" author="lhofhansl" created="Tue, 26 Mar 2013 03:36:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;An interesting comment by LarsH recently was that maybe we should ship w /major compactions off; most folks don&apos;t delete&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm... I don&apos;t doubt that I said this, but I&apos;m not sure that I agree &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Many people do delete and just not removing the delete markers would be unexpected.&lt;/p&gt;</comment>
                            <comment id="13614775" author="sershe" created="Wed, 27 Mar 2013 01:08:13 +0000"  >&lt;p&gt;perf doc... size test is not finished yet.&lt;/p&gt;</comment>
                            <comment id="13614805" author="apurtell" created="Wed, 27 Mar 2013 02:03:56 +0000"  >&lt;p&gt;I&apos;d recommend retesting with c1.xlarge instance types, this will get you a lot closer to real hardware IMHO. The IO capability of the c1.xlarge is &quot;high&quot; vs. only &quot;moderate&quot; for m1.large and the c1.xlarge has 8 vcores as opposed to &lt;em&gt;2 only&lt;/em&gt; for the m1.large. The c1.xlarge will have 4 locally attached instance-store volumes while the m1.large has only 2 IIRC. Also, I didn&apos;t see it mentioned in the perf doc but you should use only the locally attached instance store volumes as datanode storage volumes to avoid variance introduced by EBS.&lt;/p&gt;</comment>
                            <comment id="13616654" author="sershe" created="Thu, 28 Mar 2013 21:06:46 +0000"  >&lt;p&gt;I will update the doc, although in this case m1.large moderate IO capacity works even better for the test, making it easier to simulate IO-constrained cluster on such small number of nodes/time frame.&lt;/p&gt;</comment>
                            <comment id="13616975" author="sershe" created="Fri, 29 Mar 2013 01:51:22 +0000"  >&lt;p&gt;Updating both docs. Size-based logic test result, as well as design improvement based on that.&lt;/p&gt;</comment>
                            <comment id="13616976" author="sershe" created="Fri, 29 Mar 2013 01:52:09 +0000"  >&lt;p&gt;Btw, the 3 next child JIRAs are rady for review. Please feel free to +1 them, I will only commit all 3 together and with integration test included.&lt;/p&gt;</comment>
                            <comment id="13617951" author="sershe" created="Sat, 30 Mar 2013 01:41:08 +0000"  >&lt;p&gt;I did a c1.xlarge test (with default, 3, 10 and 25 stripes, 2 times each). The results for different stripe configurations are very consistent across both runs.&lt;br/&gt;
Compared to m1.large test the positive effect of increasing number of stripes on write speed is less.&lt;/p&gt;

&lt;p&gt;For this load, sweet spot appears to be around 10-12 stripes based on two tests. 3 stripes have large compactions similar to default (well, not as large); 25 stripes does too many small compactions, so select-compact loop cannot keep up with the number of files produced - on &quot;Iteration 2&quot; test described in the doc at least some stripes in 25-stripe case always have 6-8 small files (as they get compacted other stripes get more files). This appears to be the limiting factor on increasing the number of stripes. &lt;br/&gt;
I think the main point is that, for count scheme, there&apos;s perf parity (writes are generally slightly slower, reads slightly faster), despite existing and fixable write amplification; and there&apos;s reduction of variability, which was the goal. I will try to devise a more realistic read workload, but I don&apos;t think it should change much given above.&lt;br/&gt;
For sequential data, with size-based stripe scheme there&apos;s reduction in compactions, as expected, despite even L0.&lt;/p&gt;

&lt;p&gt;Next steps:&lt;br/&gt;
1) On existing data I want to correlate read/write perf with compactions. It is interesting that stripe scheme has slower writes in general, as Jimmy has noted - it touches read path but not anything at all on write path, so it is probably I/O related, or stresses some interaction between existing write and compaction paths.&lt;br/&gt;
2) Run tests for more realistic read workloads (and parallel read/writes), by not using LoadTestTool? Optional-ish.&lt;br/&gt;
3) Clean up integration test patch in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8000&quot; title=&quot;create integration/perf tests for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8000&quot;&gt;&lt;del&gt;HBASE-8000&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
4) Review and commit? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;5) Get rid of L0?&lt;/p&gt;</comment>
                            <comment id="13617956" author="yuzhihong@gmail.com" created="Sat, 30 Mar 2013 02:23:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;5) Get rid of L0?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Can we do this frist ?&lt;/p&gt;</comment>
                            <comment id="13618189" author="mcorgan" created="Sun, 31 Mar 2013 00:06:18 +0000"  >&lt;p&gt;Sergey - i&apos;m curious how are compactions of the stripes being scheduled/queued?  Does a region still make a single region-wide compaction request, and the compactor picks a single stripe?  Or can multiple stripes be in the compaction queue at once?  &lt;/p&gt;

&lt;p&gt;Given that regions could be allowed to grow much larger with stripe compaction enabled it would probably be good to allow multiple stripes to compact in parallel.  Just a thought for another next step... you&apos;ve probably considered it already.&lt;/p&gt;</comment>
                            <comment id="13618967" author="sershe" created="Mon, 1 Apr 2013 17:52:40 +0000"  >&lt;p&gt;Currently only one compaction per store is allowed. The need to compact several stripes in parallel can probably be alleviated by just having less stripes? As future improvement it is possible to add. &lt;/p&gt;</comment>
                            <comment id="13619380" author="sershe" created="Tue, 2 Apr 2013 00:48:49 +0000"  >&lt;p&gt;Actually, judging by logs what can be done is triggering compaction thread if store can compact. In 25-stripe case I see gaps between compactions which are unnecessary, when the compaction only triggers on flush despite plenty of tiny stripe compactions being possible&lt;/p&gt;</comment>
                            <comment id="13620460" author="sershe" created="Wed, 3 Apr 2013 00:49:34 +0000"  >&lt;p&gt;Updating the perf evaluation, I think I&apos;m done with that for now. Looking for CRs &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
I will not have time next few days but I will get to noted optimizations (L0) after that&lt;/p&gt;</comment>
                            <comment id="13643405" author="sershe" created="Sat, 27 Apr 2013 00:07:22 +0000"  >&lt;p&gt;First draft of user-level doc. After trying to describe the size-based scheme, I think it should be improved. I will do that. Meanwhile there&apos;s design doc and user doc, so I&apos;d like to get some reviews &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
I will rebase and update all patches between now and monday. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; what do you guys think?&lt;/p&gt;</comment>
                            <comment id="13646164" author="sershe" created="Tue, 30 Apr 2013 23:45:11 +0000"  >&lt;p&gt;Updating design and user doc for latest changes. Now all the changes for the first cut are definitely in&lt;/p&gt;</comment>
                            <comment id="13649312" author="raymondshiquan" created="Sun, 5 May 2013 10:47:45 +0000"  >&lt;p&gt;great, more region lead to better load balance and good compaction effect, less region lead to easy management and fast failover, I think stripe (or sub-region) is a good trade-off. &lt;br/&gt;
And I think stripe compaction is similar with Level compaction with L0+L1 only. &lt;br/&gt;
Another difficult is about configuration, in big hbase cluster, there are so many applications, how to build suitable configuation for each one will be a huge challenge.&lt;/p&gt;</comment>
                            <comment id="13651519" author="sershe" created="Wed, 8 May 2013 01:12:45 +0000"  >&lt;p&gt;On recent HBase meeting &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jmhsieh&quot; class=&quot;user-hover&quot; rel=&quot;jmhsieh&quot;&gt;Jonathan Hsieh&lt;/a&gt; asked me to provide an easier to understand chart of perf.&lt;br/&gt;
I haven&apos;t ran new experiments since then, and to set up new ones it will take some time (because I want to get good ones to use for con slides &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;). For now attaching a primitive one I made out of old data, for reads using loadtesttool against default-compacted and stripe-compacted table. 500 data points for each.&lt;/p&gt;

&lt;p&gt;The experiment setup is described in perf doc and is the one on c1.xlarge instances. Fixed 10-stripe scheme vs. default scheme was used, with 3 relatively large (growing to several gigs) regions, with interleaving batches of writes and reads.&lt;/p&gt;</comment>
                            <comment id="13651556" author="eclark" created="Wed, 8 May 2013 02:07:40 +0000"  >&lt;p&gt;Thanks for the doc.  Reading this tonight.&lt;/p&gt;</comment>
                            <comment id="13654572" author="jmhsieh" created="Fri, 10 May 2013 16:13:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt;, thanks for the graph.  Seeing that, I think a avg/std deviation could be an even simpler way of showing where this compaction approach demonstrates a win.  It looks like the variance of times will be significantly higher with default and it seems that avg time would be about the same.&lt;/p&gt;</comment>
                            <comment id="13688945" author="stack" created="Thu, 20 Jun 2013 06:55:11 +0000"  >&lt;p&gt;Rereading the design doc and how-to-use.  They are very nice.  Can go into the book.&lt;/p&gt;

&lt;p&gt;High-level, and I think you have suggested this yourself elsewhere, it&apos;d be coolio if user didn&apos;t have to choose between size and count &amp;#8211; if it&apos;d just figure itself based off incoming load.&lt;/p&gt;

&lt;p&gt;I&apos;ve seen case where a compaction produces a zero-length file (all deletes) so would that mess w/ this invariant: &quot;Compaction	must	produce	at	least	one	file	(see	&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6059&quot; title=&quot;Replaying recovered edits would make deleted data exist again&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6059&quot;&gt;&lt;del&gt;HBASE-6059&lt;/del&gt;&lt;/a&gt;).&quot; or &quot;...No	stripe	can	ever	be	left	with	0	files...&quot;&lt;/p&gt;

&lt;p&gt;I almost asked a few questions you&apos;d already answered above in my previous read of the doc (smile).&lt;/p&gt;

&lt;p&gt;How would region merge work?  We&apos;d just drop all files into L0?  Sounds like we&apos;d have to drop references if we are not to break snapshotting.&lt;/p&gt;

&lt;p&gt;You think this true? &quot;....stripe	scheme	uses	larger	number	of	files	than	&lt;br/&gt;
default	to	ensure	all	compactions	are	small,	which	can	affect	very	wide	scans.&quot;  Any measure of how much?&lt;/p&gt;

&lt;p&gt;Should stripe be on by default?  Or have it as experimental for now until we get more data?&lt;/p&gt;

&lt;p&gt;How to use doc is excellent (though too many configs).  Will review patch again next.&lt;/p&gt;
</comment>
                            <comment id="13689954" author="sershe" created="Fri, 21 Jun 2013 01:45:07 +0000"  >&lt;p&gt;I&apos;m looking at how to merge policies. Unfortunately splitting uses more I/O than not splitting (who would have though... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;), resulting in worse perf. Also, the system cannot really predict future data patterns, no more than region splitting can do it (at least not with a lot of complexity added), so hint flag for how to split would need to be provided.&lt;/p&gt;

&lt;p&gt;W.r.t. producing files to contain metadata, that is unfortunately necessary. These files shouldn&apos;t have effect. Stripes with only expired files can be merged. I&apos;ve taken a stab at auto-detecting stripes from file metadata, in general case it&apos;s very complex, in simplified realistic case it&apos;s just complex.&lt;/p&gt;

&lt;p&gt;Merge will drop everything into L0, yes. This could be improved, but has to be done now anyway due to references, same with split, so no need to do it now.&lt;/p&gt;

&lt;p&gt;On-by-default would require smart default settings.&lt;/p&gt;

&lt;p&gt;Let me comment tomorrow on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7680&quot; title=&quot;implement compaction policy for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7680&quot;&gt;&lt;del&gt;HBASE-7680&lt;/del&gt;&lt;/a&gt;, if I can make a size-count hybrid quickly I will post final patch without a lot of logic changes there, and hopefully we can commit 3 initial patches and build on top of that.&lt;/p&gt;</comment>
                            <comment id="13690015" author="stack" created="Fri, 21 Jun 2013 04:31:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;splitting uses more I/O than not splitting&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry.  You mean stripes uses more i/o because we L0 first then rewrite into stripes?&lt;/p&gt;</comment>
                            <comment id="13690757" author="sershe" created="Fri, 21 Jun 2013 21:44:33 +0000"  >&lt;p&gt;Size-based scheme works by splitting stripes when they grow big. This splitting is good for sequential sharded keys, because lower part of the split is written as one file, and doesn&apos;t receive new data (or doesn&apos;t receive a lot of it anyway), so it doesn&apos;t have to participate in compactions. If you have uniform data, splitting result in more rewriting and both stripes keep growing after the split.&lt;/p&gt;</comment>
                            <comment id="13690761" author="sershe" created="Fri, 21 Jun 2013 21:46:04 +0000"  >&lt;p&gt;So it&apos;s easier to configure (just say I want 500Mb-1Gb-... stripes) but in the net, results in more I/O during initial data population before region reaches stable size.&lt;/p&gt;</comment>
                            <comment id="13806903" author="sershe" created="Mon, 28 Oct 2013 16:37:57 +0000"  >&lt;p&gt;I have run the maven tests and rebased all patches (no changes except a tiny one in compator)... if no objections I will commit &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7679&quot; title=&quot;implement store file management for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7679&quot;&gt;&lt;del&gt;HBASE-7679&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7680&quot; title=&quot;implement compaction policy for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7680&quot;&gt;&lt;del&gt;HBASE-7680&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-7967&quot; title=&quot;implement compactor for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-7967&quot;&gt;&lt;del&gt;HBASE-7967&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8000&quot; title=&quot;create integration/perf tests for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8000&quot;&gt;&lt;del&gt;HBASE-8000&lt;/del&gt;&lt;/a&gt; to trunk today afternoon if there are no objections by then.&lt;/p&gt;</comment>
                            <comment id="13807290" author="stack" created="Mon, 28 Oct 2013 22:03:10 +0000"  >&lt;p&gt;So, stripe compactions does more i/o unless it is the time series use case?   I cannot turn this on by default?  Where do I go to read on benefits of this new addition?  Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sershe&quot; class=&quot;user-hover&quot; rel=&quot;sershe&quot;&gt;Sergey Shelukhin&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13807397" author="sershe" created="Mon, 28 Oct 2013 22:15:06 +0000"  >&lt;p&gt;Yes, unless there&apos;s non-uniform key access there will be more, but smaller, compactions. &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8541&quot; title=&quot;implement flush-into-stripes in stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8541&quot;&gt;&lt;del&gt;HBASE-8541&lt;/del&gt;&lt;/a&gt; makes for less IO amplification, Enis is reviewing it, it will probably follow quickly.&lt;/p&gt;

&lt;p&gt;You probably don&apos;t want to turn it on by default, as it makes sense either for non-uniform data or for large regions.&lt;/p&gt;

&lt;p&gt;The documents, one of them targeted at users (ans all of them out of date), are attached to this very JIRA &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;br/&gt;
The configuration was simplified quite a bit compared to the state of the doc.&lt;br/&gt;
Let me file a JIRA to document things in e.g. the book.&lt;br/&gt;
For the first release it will be positioned as an &quot;experimental&quot; feature...&lt;/p&gt;</comment>
                            <comment id="13808236" author="stack" created="Tue, 29 Oct 2013 17:56:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yes, unless there&apos;s non-uniform key access there will be more, but smaller, compactions. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As per &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srivas&quot; class=&quot;user-hover&quot; rel=&quot;srivas&quot;&gt;M. C. Srivas&lt;/a&gt; supposition above I suppose.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You probably don&apos;t want to turn it on by default, as it makes sense either for non-uniform data or for large regions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So for what cases should we turn it on?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The documents, one of them targeted at users (ans all of them out of date), are attached to this very JIRA&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Pardon me.  I&apos;ve reviewed a bunch of this feature &amp;#8211; docs and code &amp;#8211; and am just having trouble quantifying the benefit this slew of new code brings in.  Sorry if I am being thick.&lt;/p&gt;

&lt;p&gt;If I read the attached user doc, will it be clear (though it is out of date?)   Let me try it.&lt;/p&gt;</comment>
                            <comment id="13808248" author="stack" created="Tue, 29 Oct 2013 18:06:34 +0000"  >&lt;p&gt;If I read the user doc., it says:&lt;/p&gt;

&lt;p&gt;&quot;This improves	read	performance in common scenarios and greatly reduces variability, by avoiding large and/or	 inefficient compactions&quot;&lt;/p&gt;

&lt;p&gt;If I read further, there is no clear message on when enabling stripe compactions makes sense.  Doc is missing a section on when NOT to use stripe compactions.&lt;/p&gt;

&lt;p&gt;The doc. has detail but seems like a bunch no longer applies after recent reworkings (as you say above).&lt;/p&gt;

&lt;p&gt;Do you have some stats on how it can improve life under certain workloads and what those workloads are?&lt;/p&gt;

&lt;p&gt;My concern is that a bunch of code will go into hbase and it will sit there unused.  We have enough of that already.  I&apos;d like to have some clear messaging around this feature, both how it can benefit, and also how a user could enable it and see the effects of its workings.&lt;/p&gt;</comment>
                            <comment id="13808404" author="sershe" created="Tue, 29 Oct 2013 20:24:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; I understand your concern. I will get to documenting it this or next week, so it should be able to get at least into 98.&lt;br/&gt;
As far as I know, there are some people who wanted to try it out,and also there&apos;s timestamp compaction jira which might be obviated... let&apos;s see if as experimental feature it can get adoption. It&apos;s pretty well isolated now, so it should be easy to remove later, or move into separate module out of the way.&lt;/p&gt;</comment>
                            <comment id="13841804" author="sershe" created="Fri, 6 Dec 2013 22:30:14 +0000"  >&lt;p&gt;All the pertinent patches have been committed for some time (before 98 was branched).&lt;/p&gt;</comment>
                            <comment id="13842713" author="otis" created="Sun, 8 Dec 2013 21:27:47 +0000"  >&lt;p&gt;Btw. is this going to get into any 0.96.x releases by any chance?  Thanks.&lt;/p&gt;</comment>
                            <comment id="13843340" author="sershe" created="Mon, 9 Dec 2013 17:27:08 +0000"  >&lt;p&gt;With 98 coming so soon, probably not.&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; wdyt?&lt;/p&gt;</comment>
                            <comment id="13843355" author="stack" created="Mon, 9 Dec 2013 17:53:08 +0000"  >&lt;p&gt;Yeah.  Too late for 0.96.  We are trying to get back on to a &quot;bugs-only&quot; in point releases praxis &amp;#8211; unless there a citizen revolt.  Also need reason for folks to upgrade to 0.98!&lt;/p&gt;</comment>
                            <comment id="13883479" author="apurtell" created="Mon, 27 Jan 2014 22:54:55 +0000"  >&lt;p&gt;Any doc updates available? 0.98.0RC1 is open.&lt;/p&gt;</comment>
                            <comment id="13885673" author="sershe" created="Wed, 29 Jan 2014 19:15:05 +0000"  >&lt;p&gt;the docs were committed to the book as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-9854&quot; title=&quot;initial documentation for stripe compactions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-9854&quot;&gt;&lt;del&gt;HBASE-9854&lt;/del&gt;&lt;/a&gt;; sorry for late reply&lt;/p&gt;</comment>
                            <comment id="14331015" author="enis" created="Sat, 21 Feb 2015 23:32:50 +0000"  >&lt;p&gt;Closing this issue after 0.99.0 release. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12627982">HBASE-7603</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12626643">HBASE-7519</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12632622">HBASE-7857</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12576697" name="Stripe compaction perf evaluation.pdf" size="346812" author="sershe" created="Wed, 3 Apr 2013 00:49:34 +0000"/>
                            <attachment id="12576006" name="Stripe compaction perf evaluation.pdf" size="219573" author="sershe" created="Fri, 29 Mar 2013 01:51:22 +0000"/>
                            <attachment id="12575634" name="Stripe compaction perf evaluation.pdf" size="210654" author="sershe" created="Wed, 27 Mar 2013 01:08:13 +0000"/>
                            <attachment id="12581272" name="Stripe compactions.pdf" size="132111" author="sershe" created="Tue, 30 Apr 2013 23:45:11 +0000"/>
                            <attachment id="12576005" name="Stripe compactions.pdf" size="132288" author="sershe" created="Fri, 29 Mar 2013 01:51:22 +0000"/>
                            <attachment id="12575449" name="Stripe compactions.pdf" size="130286" author="sershe" created="Tue, 26 Mar 2013 01:47:56 +0000"/>
                            <attachment id="12574946" name="Stripe compactions.pdf" size="116740" author="sershe" created="Fri, 22 Mar 2013 03:01:30 +0000"/>
                            <attachment id="12581271" name="Using stripe compactions.pdf" size="144724" author="sershe" created="Tue, 30 Apr 2013 23:45:11 +0000"/>
                            <attachment id="12580791" name="Using stripe compactions.pdf" size="141343" author="sershe" created="Sat, 27 Apr 2013 00:24:42 +0000"/>
                            <attachment id="12580789" name="Using stripe compactions.pdf" size="138068" author="sershe" created="Sat, 27 Apr 2013 00:09:43 +0000"/>
                            <attachment id="12582226" name="stripe-cdf.pdf" size="23844" author="sershe" created="Wed, 8 May 2013 01:12:45 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12629372">HBASE-7678</subtask>
                            <subtask id="12631191">HBASE-7784</subtask>
                            <subtask id="12629373">HBASE-7679</subtask>
                            <subtask id="12629374">HBASE-7680</subtask>
                            <subtask id="12634732">HBASE-7967</subtask>
                            <subtask id="12635276">HBASE-8000</subtask>
                            <subtask id="12647427">HBASE-8541</subtask>
                            <subtask id="12676229">HBASE-9854</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 25 Jan 2013 01:53:36 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>309045</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 42 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1dym7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>289705</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>