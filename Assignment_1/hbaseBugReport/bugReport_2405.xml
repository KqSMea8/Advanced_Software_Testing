<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:01:36 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2405/HBASE-2405.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2405] Close, split, open of regions in RegionServer are run by a single thread only.</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2405</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;JGray and Karthik observed yesterday that a regoin open message arrived at the regionserver but that the regionserver worker thread did not get around to the actually opening until 45 seconds later (region offline for 45 seconds).  We only run a single Worker thread in a regoinserver processing open, close, and splits.  In this case, a long running close (or two) held up the worker thread.  We need to run more than a single worker.  A pool of workers?  Should opens be prioritized?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12461053">HBASE-2405</key>
            <summary>Close, split, open of regions in RegionServer are run by a single thread only.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                            <label>moved_from_0_20_5</label>
                    </labels>
                <created>Fri, 2 Apr 2010 18:47:29 +0000</created>
                <updated>Fri, 20 Nov 2015 12:42:20 +0000</updated>
                            <resolved>Wed, 1 Sep 2010 05:26:19 +0000</resolved>
                                                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                <comments>
                            <comment id="12852928" author="apurtell" created="Fri, 2 Apr 2010 19:25:41 +0000"  >&lt;p&gt;Multithreaded open and close seems a good idea. Maybe not for splits and compactions, or keep two worker pools. Former is good for liveness with minimal impact on HDFS. Latter can have high impact on HDFS, e.g. 20 servers (+DNs) with e.g. 4 split/compact threads each is 160 concurrent streaming reads or writes when a table is major compacted. &lt;/p&gt;</comment>
                            <comment id="12852954" author="streamy" created="Fri, 2 Apr 2010 21:19:06 +0000"  >&lt;p&gt;Thanks for opening this stack.&lt;/p&gt;

&lt;p&gt;I&apos;m unsure if we really need multi-threaded Workers to process the messages, or if we should just have the worker we have now be able to do things more asynchronously.  Right now it&apos;s bad that we allow any compacting outside of the CompactSplitThread, we should be shuffling everything through there so that we respect compaction limits.  I&apos;m thinking we need some way to allow things like compactions to know that they need to trigger some other action once completed.  Maybe the compaction requests could carry some reference to an optional callback method... seems like there&apos;s a bunch of cool stuff we could do with that.&lt;/p&gt;

&lt;p&gt;I&apos;m worried about introducing parallelism where we don&apos;t currently have it so think we should try to avoid it where possible.  As long as we don&apos;t block in the thread processing messages, should not be a problem.&lt;/p&gt;

&lt;p&gt;Stack, how does this relate to possible changes from the master rewrite.  If the regionservers are no longer just processing a message queue coming from the master, and instead are processing events triggered by zk watchers and such, maybe a move towards parallelism in message processing is necessary anyways.  Thoughts?&lt;/p&gt;</comment>
                            <comment id="12852960" author="stack" created="Fri, 2 Apr 2010 21:37:32 +0000"  >&lt;p&gt;@Andrew I like two queues idea; liveness queue and background-task queue. We&apos;d have multiple threads servicing the liveness queue but perhaps the single worker doing the compactions and splits queue.&lt;/p&gt;

&lt;p&gt;@Jon Yes, all should go via compactsplitthread.  Lets fix that and make even manual requests for compactions go this route.  Understood about the worry introducing more parallellism but ain&apos;t sure how else to do it if you don&apos;t want to &quot;block the thread processing the message&quot; (even if we redo close so we dbl-flush, it&apos;ll still take the same amount of elapsed time to close, it&apos;ll just be taking writes longer before it puts up the close flag).  And good point that close, open, etc., will come in via callbacks though I envisioned the callback would just add to the aforementioned queues and then return &amp;#8211; not wait on close/open to complete.&lt;/p&gt;</comment>
                            <comment id="12852990" author="streamy" created="Fri, 2 Apr 2010 23:12:57 +0000"  >&lt;p&gt;The way we prevent blocking is by not doing anything long-running in the thread that processes messages.  If it&apos;s a close, he can asynchronously trigger the close to happen (into compaction queue w/ a callback perhaps).&lt;/p&gt;

&lt;p&gt;Or we have a process thread that handles all incoming messages, decides what to do and hands that off to a set of workers or something.  Anyways, I&apos;m not against adding worker pools just want to think about all possible designs.&lt;/p&gt;</comment>
                            <comment id="12853611" author="stack" created="Tue, 6 Apr 2010 00:20:59 +0000"  >&lt;p&gt;I&apos;m going to put up two executor pools &amp;#8211; one with a single thread to do the old QUIESCE, SPLIT, and COMPACTs and then another pool with perhaps 4 or 5 threads to do concurrent OPEN and CLOSEs.&lt;/p&gt;

&lt;p&gt;Attached patch cleans up the ugly NSRE thread dump in the RS log.  Will roll that into this patch.&lt;/p&gt;</comment>
                            <comment id="12853618" author="streamy" created="Tue, 6 Apr 2010 00:34:29 +0000"  >&lt;p&gt;Do we need to shuffle open/close compactions and flushes via a shared thread pool w/ current compactions and flushes?  This kind of thing needs to be configurable and strictly enforced imo&lt;/p&gt;</comment>
                            <comment id="12853738" author="stack" created="Tue, 6 Apr 2010 04:37:27 +0000"  >&lt;p&gt;So worker handles stuff like master ordained OPENs, CLOSEs, QUESCE&apos;s as well as SPLITs, COMPACTs, and FLUSHEs.  The latter are user ordained but are passed out as events by the master.   I looked at them and SPLITs and COMPACTIONs are just queued. A compaction/split requested is called on the compactSplitThread.  FLUSH is run inline.&lt;/p&gt;

&lt;p&gt;What we want to do here is make it so a long-running close (or open) doesn&apos;t hinder other OPENs, the phenomenon observed by Karthik.  The way around this is to run each open/close in its own thread.  You cool w/ that JGray?&lt;/p&gt;</comment>
                            <comment id="12854048" author="streamy" created="Tue, 6 Apr 2010 16:31:35 +0000"  >&lt;p&gt;The issue is that an OPEN and CLOSE can/will trigger compactions and flushes.  These should not be done inline, they should be queued like user triggered events are.  That&apos;s where the new complexity comes in.&lt;/p&gt;

&lt;p&gt;On OPEN, if the region contains references, will be compacted.  Does this currently happen inline in the worker?  I think that might be queued.  If not, it should be.&lt;/p&gt;

&lt;p&gt;On CLOSE, we flush the region.  This flush should be turned into a double-flush and done through a separate thread pool that can be configured w/ a max concurrent flushes.&lt;/p&gt;

&lt;p&gt;I don&apos;t want to run opens/closes in their own threads because then you don&apos;t have easy ways to configure how much you want to use HDFS at once.  maxConcurrentCompactions and maxConcurrentFlushes should (exist and) be strictly followed.&lt;/p&gt;</comment>
                            <comment id="12854052" author="apurtell" created="Tue, 6 Apr 2010 16:38:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;maxConcurrentCompactions and maxConcurrentFlushes should (exist and) be strictly followed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 &lt;/p&gt;

&lt;p&gt;but from my point of view it would be fine if that can be accomplished along with multithreaded OPENs and CLOSEs.&lt;/p&gt;




</comment>
                            <comment id="12854737" author="stack" created="Wed, 7 Apr 2010 23:53:29 +0000"  >&lt;p&gt;Moving to 0.20.5 but also marking it critical.  Being single-threaded only makes onlining/offlining of regions take longer than need be.&lt;/p&gt;</comment>
                            <comment id="12866796" author="stack" created="Wed, 12 May 2010 23:48:07 +0000"  >&lt;p&gt;Bulk move of 0.20.5 issues into 0.21.0 after vote that we merge branch into TRUNK up on list.&lt;/p&gt;</comment>
                            <comment id="12867055" author="streamy" created="Thu, 13 May 2010 05:27:03 +0000"  >&lt;p&gt;Going to look at this as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2485&quot; title=&quot;Persist Master in-memory state so on restart or failover, new instance can pick up where the old left off&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2485&quot;&gt;&lt;del&gt;HBASE-2485&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12874325" author="stack" created="Wed, 2 Jun 2010 00:07:12 +0000"  >&lt;p&gt;Just to say I ran into a recent issue where a region that was taking a very long time to open &amp;#8211; bug &amp;#8211; blocked the opening of all those behind it.&lt;/p&gt;</comment>
                            <comment id="12876462" author="posix4e" created="Mon, 7 Jun 2010 22:28:48 +0000"  >&lt;p&gt;Does distributed log splitting change what we should do here?&lt;/p&gt;</comment>
                            <comment id="12902184" author="jdcryans" created="Wed, 25 Aug 2010 00:08:57 +0000"  >&lt;p&gt;Jon, do you still want to do this as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2485&quot; title=&quot;Persist Master in-memory state so on restart or failover, new instance can pick up where the old left off&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2485&quot;&gt;&lt;del&gt;HBASE-2485&lt;/del&gt;&lt;/a&gt; in the scope of 0.90? IMO we already have enough.&lt;/p&gt;</comment>
                            <comment id="12902258" author="streamy" created="Wed, 25 Aug 2010 01:13:18 +0000"  >&lt;p&gt;this is for free with the new master, already implemented in the branch.  opens/closes are multi-threaded.&lt;/p&gt;</comment>
                            <comment id="12904918" author="stack" created="Wed, 1 Sep 2010 05:26:19 +0000"  >&lt;p&gt;Opens and closes are done by executors now.    By default regionservers have 3 opener handlers (with one extra each for meta and root).   Configuration can up or down the number.  Splits are not yet done by executors nor compactors and flushers but thats coming in a different issue.&lt;/p&gt;

&lt;p&gt;Closing as fixed by &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2692&quot; title=&quot;Master rewrite and cleanup for 0.90&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2692&quot;&gt;&lt;del&gt;HBASE-2692&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15017224" author="lars_francke" created="Fri, 20 Nov 2015 12:42:20 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12462917">HBASE-2485</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12440817" name="nsre.txt" size="2135" author="stack" created="Tue, 6 Apr 2010 00:20:59 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 2 Apr 2010 19:25:41 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26291</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hhm7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100116</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>