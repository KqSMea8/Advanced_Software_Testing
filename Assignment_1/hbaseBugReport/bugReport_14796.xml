<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:56:33 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-14796/HBASE-14796.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-14796] Enhance the Gets in the connector</title>
                <link>https://issues.apache.org/jira/browse/HBASE-14796</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Current the Spark-Module Spark SQL implementation gets records from HBase from the driver if there is something like the following found in the SQL.&lt;/p&gt;

&lt;p&gt;rowkey = 123&lt;/p&gt;

&lt;p&gt;The reason for this original was normal sql will not have many equal operations in a single where clause.&lt;/p&gt;

&lt;p&gt;Zhan, had brought up too points that have value.&lt;br/&gt;
1. The SQL may be generated and may have many many equal statements in it so moving the work to an executor protects the driver from load&lt;br/&gt;
2. In the correct implementation the drive is connecting to HBase and exceptions may cause trouble with the Spark application and not just with the a single task execution&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12912024">HBASE-14796</key>
            <summary>Enhance the Gets in the connector</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zhanzhang">Zhan Zhang</assignee>
                                    <reporter username="ted.m">Theodore michael Malaska</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Nov 2015 00:00:09 +0000</created>
                <updated>Tue, 29 Dec 2015 03:27:56 +0000</updated>
                            <resolved>Mon, 28 Dec 2015 23:48:45 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                <comments>
                            <comment id="14999633" author="ted.m" created="Wed, 11 Nov 2015 00:05:43 +0000"  >&lt;p&gt;So there is value in this idea for the generated queries, but for normal SQL operations it may be over kill that we need to use a task on an executor to get a single record from HBase.&lt;/p&gt;

&lt;p&gt;As for the argument about protecting the driver there is some merit to this.&lt;/p&gt;

&lt;p&gt;I think there is more merit to the first argument for distributed the get load to the executers to support multi user environments. &lt;/p&gt;

&lt;p&gt;But honestly if the developer is using Spark SQL to gets on HBase I question the approach.  The user would be better off using the Spark-Module Bulk Get functionality that is already checked in.  That implementation will distribute the gets across N number of tasks and executors.&lt;/p&gt;</comment>
                            <comment id="14999635" author="ted.m" created="Wed, 11 Nov 2015 00:07:20 +0000"  >&lt;p&gt;If implemented this code would fit great right around&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/hbase/blob/master/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/DefaultSource.scala#L347&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hbase/blob/master/hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/DefaultSource.scala#L347&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15001143" author="zzhan" created="Wed, 11 Nov 2015 21:31:52 +0000"  >&lt;p&gt;I agree that we don&apos;t expect the BulkGet to be huge. We should avoid doing get in driver. Similar to the approaches proposed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14795&quot; title=&quot;Enhance the spark-hbase scan operations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14795&quot;&gt;&lt;del&gt;HBASE-14795&lt;/del&gt;&lt;/a&gt;. When constructing RDD in buildScan, we can have the partition also include the information for bulkGets on that partition. In this way, we treat the Get and Scan in a unified way, avoid performing the get in driver with data locality.&lt;/p&gt;</comment>
                            <comment id="15001148" author="zzhan" created="Wed, 11 Nov 2015 21:33:27 +0000"  >&lt;p&gt;In addition, we want the full support in DataFrame level, and don&apos;t expect the user mixes the use of Dataframe and RDD operation.&lt;/p&gt;</comment>
                            <comment id="15001156" author="ted.m" created="Wed, 11 Nov 2015 21:36:42 +0000"  >&lt;p&gt;My only concern here is if we are adding latency for the normal single row get query.&lt;/p&gt;

&lt;p&gt;Can you run some tests to see what if an impact there is on this?  Not just a unit test but a test of a real cluster.  &lt;/p&gt;

&lt;p&gt;If the latency difference is nothing big they I don&apos;t see any problem with the full change to the executor get design.  If the latency change is huge, maybe we can make this configurable. &lt;/p&gt;</comment>
                            <comment id="15001233" author="zzhan" created="Wed, 11 Nov 2015 22:19:57 +0000"  >&lt;p&gt;In theory I think performing the get in executors should have lower latency if piggyback with other scans, because&lt;br/&gt;
1. The task serialization is minimum since it is piggybacked with other scan tasks. (Getting from driver does not have this overhead)&lt;br/&gt;
2. executors have better data locality since get will be in the partition co-located with region sever. (Driver does not have data locality and have to fetch data from remote)&lt;br/&gt;
3. we don&apos;t need to redistribute the get result to executors. (Driver has to parallelize the record to executors for form RDD)&lt;/p&gt;

&lt;p&gt;Definitely we should get some performance number.&lt;/p&gt;</comment>
                            <comment id="15001301" author="ted.m" created="Wed, 11 Nov 2015 23:06:48 +0000"  >&lt;p&gt;I agreed with point one.  But the use case I&apos;m thinking about is one like this.&lt;/p&gt;

&lt;p&gt;HBase table 100 million or a billion records (number does matter much, just make it a lot)&lt;/p&gt;

&lt;p&gt;Then the select looks like this&lt;/p&gt;

&lt;p&gt;Select * from hbase_table where rowkey = &quot;foobar&quot;&lt;/p&gt;

&lt;p&gt;I can see this being very common not optimal but common.&lt;/p&gt;</comment>
                            <comment id="15001356" author="zzhan" created="Wed, 11 Nov 2015 23:55:20 +0000"  >&lt;p&gt;The number does not matter here. &lt;/p&gt;

&lt;p&gt;Given the scenario,&lt;br/&gt;
If we perform the get on driver, we will do:&lt;br/&gt;
1. issue BulkGet on driver, and collect the result remotely from region server&lt;br/&gt;
2. distribute the result to one executor to form an RDD.&lt;/p&gt;

&lt;p&gt;If we send tasks to executors, we will do:&lt;br/&gt;
1. Task sent to the executor co-located with the region server that host the data&lt;br/&gt;
2. Get the data from the local region server&lt;/p&gt;

&lt;p&gt;The difference is that for the first approach, the row data traverse the network twice (from region server to driver then to the executor. Actually the parallelize also send the task to executor), and the second approach the task traverse the network once (from the driver to one executor). &lt;/p&gt;

&lt;p&gt;Then the latency depends on how much data in the row comparing to the task size. That is hard to say, but it is not obvious that the driver doing the get has the advantage latency-wise.&lt;/p&gt;</comment>
                            <comment id="15001362" author="ted.m" created="Wed, 11 Nov 2015 23:58:19 +0000"  >&lt;p&gt;Yeah agreed. It also depends on the time it takes to start a task.  But yeah I&apos;m very interested to see if there is a difference.  It is a great science experiment &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15070092" author="zzhan" created="Wed, 23 Dec 2015 19:42:30 +0000"  >&lt;p&gt;We have use case where bulkget may consists of thousands of gets. Move BulkGet to executor side from driver, which will improve the  failure recovery, and potentially improve the performance as well when the gets number is big.&lt;/p&gt;</comment>
                            <comment id="15070177" author="ted.m" created="Wed, 23 Dec 2015 20:49:30 +0000"  >&lt;p&gt;I reviewed the code and I&apos;m giving it a +1&lt;/p&gt;

&lt;p&gt;Did you do the performance tests?  As long as we are not going slower I&apos;m good here.&lt;/p&gt;

&lt;p&gt;The test should be done on a cluster not in local mode.  It can be done on warmed Yarn containers, we don&apos;t need to count the time to start Yarn.&lt;/p&gt;

&lt;p&gt;I would like to se what the different in time is when running the following tests:&lt;br/&gt;
1. a select statement with a single get&lt;br/&gt;
2. a select statement with a 10 get&lt;br/&gt;
3. a select statement with a 1000 get&lt;/p&gt;

&lt;p&gt;Maybe also we should test with different row sizes.  &lt;br/&gt;
1. 300bits&lt;br/&gt;
2. 3Kb&lt;br/&gt;
3. 30KB&lt;/p&gt;

&lt;p&gt;Let me know what you think.&lt;/p&gt;

&lt;p&gt;Thanks again Zhan&lt;/p&gt;</comment>
                            <comment id="15070286" author="zzhan" created="Wed, 23 Dec 2015 22:53:59 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted.m&quot; class=&quot;user-hover&quot; rel=&quot;ted.m&quot;&gt;Theodore michael Malaska&lt;/a&gt; for the quick review. It is reasonable to have a performance test, and I will try to grab some physical cluster for it. It may take some time, as I don&apos;t have physical cluster for this. &lt;/p&gt;

&lt;p&gt;On the other hand, I do think we should change it to perform BulkGet in executors regardless the performance (although I think it should improve the performance instead of the other way), because:&lt;/p&gt;

&lt;p&gt;1. Current implementation do gather-scatter in driver, which would increase network overhead and latency if the number of gets is big.&lt;/p&gt;

&lt;p&gt;2. Failure recovery. It is hard to do failure recovery as it is performed in driver, which is single point of failure.&lt;/p&gt;

&lt;p&gt;The above two have been discussed in details. But I just realized there is another potential issue, which the current implementation may be against Spark SQL engine design as below.&lt;/p&gt;

&lt;p&gt;3. Currently, the bulkGet is happening in the query plan (buildScan), and the results will stay in driver (1st). The result is distributed to executors in query execution(2nd). &lt;br/&gt;
  3.1 1st and 2nd are not always happening in pair. Even worse, sometimes only 1st is happening, for example, users do plan.explain, but may never trigger the plan execution. &lt;br/&gt;
  3.2 Memory taken by table.get may never get released in driver, increase the driver memory overhead.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ted.m&quot; class=&quot;user-hover&quot; rel=&quot;ted.m&quot;&gt;Theodore michael Malaska&lt;/a&gt; Please let me know how do you think, and correct me if my understanding is wrong.&lt;/p&gt;</comment>
                            <comment id="15070326" author="hadoopqa" created="Wed, 23 Dec 2015 23:47:20 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12779304/HBASE-14976.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12779304/HBASE-14976.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 8e0854c64be553595b8ed44b9856a3d74ad3005f.&lt;br/&gt;
  ATTACHMENT ID: 12779304&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;. The applied patch does not generate new checkstyle errors.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.master.TestMaster&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestSnapshotFromClientWithRegionReplicas&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestMultiParallel&lt;br/&gt;
                  org.apache.hadoop.hbase.util.TestHBaseFsckTwoRS&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestMobSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestCloneSnapshotFromClientWithRegionReplicas&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestScannersFromClientSide&lt;br/&gt;
                  org.apache.hadoop.hbase.regionserver.TestCorruptedRegionStoreFile&lt;br/&gt;
                  org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestCloneSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestMobCloneSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.master.procedure.TestModifyNamespaceProcedure&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestHCM&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestHMasterRPCException&lt;br/&gt;
                  org.apache.hadoop.hbase.snapshot.TestSnapshotClientRetries&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorEndpoint&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestMetaWithReplicas&lt;br/&gt;
                  org.apache.hadoop.hbase.master.procedure.TestDeleteNamespaceProcedure&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestLeaseRenewal&lt;br/&gt;
                  org.apache.hadoop.hbase.namespace.TestNamespaceAuditor&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestMobRestoreSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.snapshot.TestMobRestoreFlushSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestAdmin1&lt;br/&gt;
                  org.apache.hadoop.hbase.coprocessor.TestMasterCoprocessorExceptionWithRemove&lt;br/&gt;
                  org.apache.hadoop.hbase.master.procedure.TestCreateNamespaceProcedure&lt;br/&gt;
                  org.apache.hadoop.hbase.util.TestHBaseFsckReplicas&lt;br/&gt;
                  org.apache.hadoop.hbase.snapshot.TestRestoreFlushSnapshotFromClient&lt;br/&gt;
                  org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClientWithRegionReplicas&lt;br/&gt;
                  org.apache.hadoop.hbase.master.handler.TestTableDeleteFamilyHandler&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterFailover&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 zombies&lt;/font&gt;. No zombie tests found running at the end of the build.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/16998//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="15072918" author="yuzhihong@gmail.com" created="Mon, 28 Dec 2015 17:09:58 +0000"  >&lt;p&gt;The test failures above were not related to the patch.&lt;/p&gt;

&lt;p&gt;Suggest attaching patch again to get good QA run.&lt;/p&gt;</comment>
                            <comment id="15072938" author="ted.m" created="Mon, 28 Dec 2015 17:34:27 +0000"  >&lt;p&gt;Zhan good points.  I agree, even if it is slower it is better.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="15073077" author="zzhan" created="Mon, 28 Dec 2015 19:49:34 +0000"  >&lt;p&gt;solve review comments&lt;/p&gt;</comment>
                            <comment id="15073247" author="hadoopqa" created="Mon, 28 Dec 2015 23:11:59 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12779689/HBASE-14796-1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12779689/HBASE-14796-1.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 449fb81286f405cd7b373499c5db65ea61f9ffab.&lt;br/&gt;
  ATTACHMENT ID: 12779689&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;. The applied patch does not generate new checkstyle errors.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 zombies&lt;/font&gt;. No zombie tests found running at the end of the build.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/17045//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="15073268" author="yuzhihong@gmail.com" created="Mon, 28 Dec 2015 23:48:45 +0000"  >&lt;p&gt;Thanks for the patch, Zhan.&lt;/p&gt;

&lt;p&gt;Thanks for the review, Ted.&lt;/p&gt;</comment>
                            <comment id="15073420" author="hudson" created="Tue, 29 Dec 2015 03:27:56 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-Trunk_matrix #595 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-Trunk_matrix/595/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-Trunk_matrix/595/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14796&quot; title=&quot;Enhance the Gets in the connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14796&quot;&gt;&lt;del&gt;HBASE-14796&lt;/del&gt;&lt;/a&gt; Enhance the Gets in the connector (Zhan Zhang) (tedyu: rev 6868c6366002d5b4e25980f37ede8839e7a7e92d)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseSparkConf.scala&lt;/li&gt;
	&lt;li&gt;hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/Bound.scala&lt;/li&gt;
	&lt;li&gt;hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseTableScanRDD.scala&lt;/li&gt;
	&lt;li&gt;hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/datasources/HBaseResources.scala&lt;/li&gt;
	&lt;li&gt;hbase-spark/src/main/scala/org/apache/hadoop/hbase/spark/DefaultSource.scala&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12911693">HBASE-14789</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12779689" name="HBASE-14796-1.patch" size="10931" author="zzhan" created="Mon, 28 Dec 2015 19:49:34 +0000"/>
                            <attachment id="12779304" name="HBASE-14976.patch" size="10931" author="zzhan" created="Wed, 23 Dec 2015 19:42:30 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 11 Nov 2015 21:31:52 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            50 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2o81b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>spark.hbase.bulkGetSize  in HBaseSparkConf is for grouping bulkGet, and default value is 1000.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>