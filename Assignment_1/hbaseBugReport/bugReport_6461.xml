<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:36:58 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6461/HBASE-6461.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6461] Killing the HRegionServer and DataNode hosting ROOT can result in a malformed root table.</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6461</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Spun up a new dfs on hadoop-0.20.2-cdh3u3&lt;br/&gt;
Started hbase&lt;br/&gt;
started running loadtest tool.&lt;br/&gt;
killed rs and dn holding root with killall -9 java on server sv4r27s44 at about 2012-07-25 22:40:00&lt;/p&gt;

&lt;p&gt;After things stabilize Root is in a bad state. Ran hbck and got:&lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.hadoop.hbase.client.NoServerForRegionException: No server address listed in &lt;del&gt;ROOT&lt;/del&gt; for region .META.,,1.1028785192 containing row &lt;br/&gt;
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:1016)&lt;br/&gt;
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:841)&lt;br/&gt;
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:810)&lt;br/&gt;
at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:232)&lt;br/&gt;
at org.apache.hadoop.hbase.client.HTable.&amp;lt;init&amp;gt;(HTable.java:172)&lt;br/&gt;
at org.apache.hadoop.hbase.util.HBaseFsck.connect(HBaseFsck.java:241)&lt;br/&gt;
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3236)&lt;/p&gt;



&lt;p&gt;hbase(main):001:0&amp;gt; scan &apos;&lt;del&gt;ROOT&lt;/del&gt;&apos;&lt;br/&gt;
ROW                                           COLUMN+CELL                                                                                                                       &lt;br/&gt;
12/07/25 22:43:18 INFO security.UserGroupInformation: JAAS Configuration already set up for Hadoop, not re-installing.&lt;br/&gt;
 .META.,,1                                    column=info:regioninfo, timestamp=1343255838525, value=&lt;/p&gt;
{NAME =&amp;gt; &apos;.META.,,1&apos;, STARTKEY =&amp;gt; &apos;&apos;, ENDKEY =&amp;gt; &apos;&apos;, ENCODED =&amp;gt; 1028785192,}
&lt;p&gt; .META.,,1                                    column=info:v, timestamp=1343255838525, value=\x00\x00                                                                            &lt;br/&gt;
1 row(s) in 0.5930 seconds&lt;/p&gt;


&lt;p&gt;Here&apos;s the master log: &lt;a href=&quot;https://gist.github.com/3179194&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/3179194&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I tried the same thing with 0.92.1 and I was able to get into a similar situation, so I don&apos;t think this is anything new. &lt;/p&gt;</description>
                <environment>&lt;p&gt;hadoop-0.20.2-cdh3u3&lt;br/&gt;
HBase 0.94.1 RC1&lt;/p&gt;</environment>
        <key id="12600378">HBASE-6461</key>
            <summary>Killing the HRegionServer and DataNode hosting ROOT can result in a malformed root table.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="5">Cannot Reproduce</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="eclark">Elliott Clark</reporter>
                        <labels>
                    </labels>
                <created>Thu, 26 Jul 2012 20:13:31 +0000</created>
                <updated>Wed, 16 Nov 2016 22:04:36 +0000</updated>
                            <resolved>Wed, 16 Nov 2016 22:04:35 +0000</resolved>
                                    <version>0.94.14</version>
                                                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="13423474" author="lhofhansl" created="Thu, 26 Jul 2012 21:15:30 +0000"  >&lt;p&gt;Marked Critical against 0.94.2. Could also see this as a blocker for 0.94.1&lt;br/&gt;
(Although 0.94.1 needs to go out at some point, because of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6311&quot; title=&quot;Data error after majorCompaction caused by keeping MVCC for opened scanners&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6311&quot;&gt;&lt;del&gt;HBASE-6311&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13423522" author="lhofhansl" created="Thu, 26 Jul 2012 22:50:58 +0000"  >&lt;p&gt;Any chance to repeat this with distributed log splitting disabled?&lt;/p&gt;</comment>
                            <comment id="13423535" author="eclark" created="Thu, 26 Jul 2012 23:06:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any chance to repeat this with distributed log splitting disabled?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sure gimme a little bit.  Reading the logs on my latest example.  Once JD and I are done (assuming that we don&apos;t solve it) I&apos;ll try with distributed log splitting off.&lt;/p&gt;


&lt;p&gt;On the latest run JD noticed that this in the logs of the server that was doing the splitting (There was only one hlog nothing had rolled yet). 10.4.3.44 is the server that had all java processes killed.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;453-2012-07-26 22:13:18,079 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: File hdfs://sv4r11s38:9901/hbase/.logs/sv4r3s44,9913,1343340207863-splitting/sv4r3s44%2C9913%2C1343340207863.1343340210132 might be still open, length is 0
454-2012-07-26 22:13:18,081 INFO org.apache.hadoop.hbase.util.FSHDFSUtils: Recovering file hdfs://sv4r11s38:9901/hbase/.logs/sv4r3s44,9913,1343340207863-splitting/sv4r3s44%2C9913%2C1343340207863.1343340210132
455-2012-07-26 22:13:19,083 INFO org.apache.hadoop.hbase.util.FSHDFSUtils: Finished lease recover attempt for hdfs://sv4r11s38:9901/hbase/.logs/sv4r3s44,9913,1343340207863-splitting/sv4r3s44%2C9913%2C1343340207863.1343340210132
456-2012-07-26 22:13:20,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 0 time(s).
457-2012-07-26 22:13:21,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 1 time(s).
458-2012-07-26 22:13:22,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 2 time(s).
459-2012-07-26 22:13:23,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 3 time(s).
460-2012-07-26 22:13:24,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 4 time(s).
461-2012-07-26 22:13:25,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 5 time(s).
462-2012-07-26 22:13:26,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 6 time(s).
463-2012-07-26 22:13:27,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 7 time(s).
464-2012-07-26 22:13:27,936 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: LRU Stats: total=50.28 MB, free=5.94 GB, max=5.99 GB, blocks=0, accesses=0, hits=0, hitRatio=0cachingAccesses=0, cachingHits=0, cachingHitsRatio=0evictions=0, evicted=0, evictedPerRun=NaN
465-2012-07-26 22:13:28,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 8 time(s).
466-2012-07-26 22:13:29,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: /10.4.3.44:9910. Already tried 9 time(s).
467:2012-07-26 22:13:29,100 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Could not open hdfs://sv4r11s38:9901/hbase/.logs/sv4r3s44,9913,1343340207863-splitting/sv4r3s44%2C9913%2C1343340207863.1343340210132 for reading. File is empty
468-java.io.EOFException
469-	at java.io.DataInputStream.readFully(DataInputStream.java:180)
470-	at java.io.DataInputStream.readFully(DataInputStream.java:152)
471-	at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1521)
472-	at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1493)
473-	at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1480)
474-	at org.apache.hadoop.io.SequenceFile$Reader.&amp;lt;init&amp;gt;(SequenceFile.java:1475)
475-	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.&amp;lt;init&amp;gt;(SequenceFileLogReader.java:55)
476-	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:175)
477-	at org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:716)
478-	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:821)
479-	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:734)
480-	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:381)
481-	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:348)
482-	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:111)
483-	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:264)
484-	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:195)
485-	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:163)
486-	at java.lang.Thread.run(Thread.java:662)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the log splitter is trying the dead server over and over.  Then failing to read anything so root is empty.&lt;/p&gt;</comment>
                            <comment id="13423601" author="eclark" created="Fri, 27 Jul 2012 01:02:16 +0000"  >&lt;p&gt;Small update:&lt;br/&gt;
I tried it again(well 30 times actually) with more logs enabled.  and I noticed this in the NameNode&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;eclark@sv4r11s38:/export1/eclark$ grep &quot;recovery started&quot; /export1/eclark/logs/hadoop-eclark-namenode-sv4r11s38.log 
2012-07-27 00:39:45,094 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* blk_3380368109770176913_1021 recovery started, primary=10.4.6.38:9908
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the primary listed is the server that was just killed. and the block id is the id for the RegionServer&apos;s hlog.  According to the comments around the log message the primary is supposed to be an alive data node.  I&apos;m wondering if this is an hdfs bug.  Thoughts ?&lt;/p&gt;</comment>
                            <comment id="13423605" author="zhihyu@ebaysf.com" created="Fri, 27 Jul 2012 01:09:39 +0000"  >&lt;p&gt;So by 2012-07-27 00:39:45,094 , java processes on 10.4.6.38 had been dead ? For how long ?&lt;/p&gt;</comment>
                            <comment id="13423608" author="lhofhansl" created="Fri, 27 Jul 2012 01:12:37 +0000"  >&lt;p&gt;Related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6401&quot; title=&quot;HBase may lose edits after a crash if used with HDFS 1.0.3 or older&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6401&quot;&gt;&lt;del&gt;HBASE-6401&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="13423654" author="eclark" created="Fri, 27 Jul 2012 03:39:56 +0000"  >&lt;p&gt;@Ted&lt;br/&gt;
~30 seconds&lt;/p&gt;

&lt;p&gt;@Lars&lt;br/&gt;
Yep it seems like exactly the same.  Root is getting created from a 0 Length hlog.&lt;/p&gt;</comment>
                            <comment id="13423689" author="nkeywal" created="Fri, 27 Jul 2012 05:38:48 +0000"  >&lt;p&gt;For &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6401&quot; title=&quot;HBase may lose edits after a crash if used with HDFS 1.0.3 or older&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6401&quot;&gt;&lt;del&gt;HBASE-6401&lt;/del&gt;&lt;/a&gt;, the hdfs jira is &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3701&quot; title=&quot;HDFS may miss the final block when reading a file opened for writing if one of the datanode is dead&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3701&quot;&gt;&lt;del&gt;HDFS-3701&lt;/del&gt;&lt;/a&gt;, Uma said he will have a patch next week.&lt;/p&gt;

&lt;p&gt;You can validate that it&apos;s the root cause by changing the log level to debug for org.apache.hadoop.hdfs.DFSClient. You should get this log line:&lt;br/&gt;
          LOG.debug(&quot;DFSClient file &quot; + src&lt;br/&gt;
              + &quot; is being concurrently append to&quot; + &quot; but datanode &quot;&lt;br/&gt;
              + primaryNode.getHostName() + &quot; probably does not have block &quot;&lt;br/&gt;
              + last.getBlock());&lt;/p&gt;


&lt;p&gt;There is a patch in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt;. It lowers a lot the probability to hit this bug, so you may want to try it (it&apos;s not totally finished however, but some reviews will help, so... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ). As well it&apos;s for trunk but should go easily on 0.94&lt;/p&gt;</comment>
                            <comment id="13423695" author="lhofhansl" created="Fri, 27 Jul 2012 06:05:42 +0000"  >&lt;p&gt;I thought HBase neither uses nor needs &quot;append&quot; (the reason for initial using the append branch was that it also had the sync patches).&lt;br/&gt;
Where do we ever rely on a reader and a writer concurrently reader-from/writing-to the same file (Hlog in this case).&lt;br/&gt;
Is this only a special case for the WAL when we start to replay the edits, when the file is not closed, yet, because the writer died?&lt;/p&gt;</comment>
                            <comment id="13423700" author="eclark" created="Fri, 27 Jul 2012 06:25:35 +0000"  >&lt;p&gt;@nkeywal&lt;br/&gt;
I&apos;m not seeing that log exception.  However everything else is exactly the same.  This only shows when I kill the DN and RS at the same time.  And does seem to manifest itself more when the log splitting starts very soon after the java processes are killed.&lt;/p&gt;</comment>
                            <comment id="13423712" author="ram_krish" created="Fri, 27 Jul 2012 07:17:43 +0000"  >&lt;p&gt;Sorry for pitching in late here. We faced some issues related to 0 length file in Hlogs.  As Uma has pointed out in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3701&quot; title=&quot;HDFS may miss the final block when reading a file opened for writing if one of the datanode is dead&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3701&quot;&gt;&lt;del&gt;HDFS-3701&lt;/del&gt;&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3222&quot; title=&quot;DFSInputStream#openInfo should not silently get the length as 0 when locations length is zero for last partial block.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3222&quot;&gt;&lt;del&gt;HDFS-3222&lt;/del&gt;&lt;/a&gt; we had some patches from HDFS and the same some work around was done on HBASE side also to fix this issue internally.&lt;br/&gt;
I shall discuss with Uma further on this.  Thanks all.&lt;/p&gt;</comment>
                            <comment id="13423883" author="nkeywal" created="Fri, 27 Jul 2012 13:57:41 +0000"  >&lt;p&gt;@Ram&lt;br/&gt;
Thanks a lot!&lt;/p&gt;

&lt;p&gt;@Lars&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I thought HBase neither uses nor needs &quot;append&quot; (the reason for initial using the append branch was that it also had the sync patches).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, the log line is actually confusing, you just need to have someone writing a file and someone else reading it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is this only a special case for the WAL when we start to replay the edits, when the file is not closed, yet, because the writer died?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think so. Even if I wonder if we could not have similar stuff for large store hfiles + crashes, even if on paper it should be ok.&lt;/p&gt;</comment>
                            <comment id="13424012" author="lhofhansl" created="Fri, 27 Jul 2012 17:55:50 +0000"  >&lt;p&gt;I think HFiles are fine, since new HFiles are (or should not) be visible until the write succeeded. A compaction write a new file, only when that succeeds are the old files deleted.&lt;br/&gt;
Flushes are more interesting. If a flush fails half-way because the RS/DN dies, the in memory state will be lost and the HLog is the source of truth. But the HFile have been 1/2 written. Hmmm.&lt;/p&gt;</comment>
                            <comment id="13441336" author="lhofhansl" created="Fri, 24 Aug 2012 17:49:59 +0000"  >&lt;p&gt;Do we have a reasonable chance of fixing this soon, since this is an HDFS bug?&lt;br/&gt;
Does it make sense to port &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; to 0.94 and defer this one to 0.96?&lt;/p&gt;</comment>
                            <comment id="13817363" author="jmspaggi" created="Fri, 8 Nov 2013 15:44:57 +0000"  >&lt;p&gt;No-one look at this issue?&lt;/p&gt;</comment>
                            <comment id="13817393" author="stack" created="Fri, 8 Nov 2013 16:13:22 +0000"  >&lt;p&gt;Putting against next 0.94 so it gets some attention if only to have it punted again.&lt;/p&gt;</comment>
                            <comment id="15671794" author="stack" created="Wed, 16 Nov 2016 22:04:36 +0000"  >&lt;p&gt;No ROOT in modern hbases. Resolving as cannot repro.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 26 Jul 2012 21:15:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>241783</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02cwn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11692</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>