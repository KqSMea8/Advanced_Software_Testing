<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:51:22 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1177/HBASE-1177.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1177] Delay when client is located on the same node as the regionserver</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1177</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;During testing of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-80&quot; title=&quot;[hbase] Add a cache of &amp;#39;hot&amp;#39; cells&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-80&quot;&gt;&lt;del&gt;HBASE-80&lt;/del&gt;&lt;/a&gt;, we uncovered a strange 40ms delay for random reads.  We ran a series of tests and found that it only happens when the client is on the same node as the RS and for a certain range of payloads (not specifically related to number of columns or size of them, only total payload).  It appears to be precisely 40ms every time.&lt;/p&gt;

&lt;p&gt;Unsure if this is particular to our architecture, but it does happen on all nodes we&apos;ve tried.  Issue completely goes away with very large payloads or moving the client.&lt;/p&gt;

&lt;p&gt;Will post a test program tomorrow if anyone can test on a different architecture.&lt;/p&gt;

&lt;p&gt;Making a blocker for 0.20.  Since this happens when you have an MR task running local to the RS, and this is what we try to do, might also consider making this a blocker for 0.19.1.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Linux 2.6.25 x86_64&lt;/p&gt;</environment>
        <key id="12413846">HBASE-1177</key>
            <summary>Delay when client is located on the same node as the regionserver</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="6">Invalid</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="streamy">Jonathan Gray</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 Feb 2009 04:51:20 +0000</created>
                <updated>Tue, 5 Aug 2014 20:11:40 +0000</updated>
                            <resolved>Thu, 20 Jun 2013 21:57:04 +0000</resolved>
                                    <version>0.19.0</version>
                                                    <component>Performance</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12669873" author="streamy" created="Tue, 3 Feb 2009 04:59:55 +0000"  >&lt;p&gt;This is not meant for contribution, I have something much more generic if we want to add something about this to tests.&lt;/p&gt;

&lt;p&gt;Test creates table, imports rows, and performs random reads for two differently sized rows.  The 7 column rows will be fetched quickly, 8 column rows show the 40ms delay behavior.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;gt; java RandomReadTest
09/02/02 20:54:53 INFO client.HBaseAdmin: Enabled table test_table
Read 1 row with 7 columns 100 times in 66ms
Read 1 row with 8 columns 100 times in 4001ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12670001" author="streamy" created="Tue, 3 Feb 2009 16:18:20 +0000"  >&lt;p&gt;Removed dependencies.&lt;/p&gt;</comment>
                            <comment id="12670003" author="streamy" created="Tue, 3 Feb 2009 16:27:30 +0000"  >&lt;p&gt;New version shows larger payload going faster.&lt;/p&gt;

&lt;p&gt;09/02/03 08:26:07 INFO client.HBaseAdmin: Enabled table test_table2&lt;br/&gt;
Read 1 row with 7 columns 100 times in 59ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 4001ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 1161ms&lt;/p&gt;</comment>
                            <comment id="12670004" author="jdcryans" created="Tue, 3 Feb 2009 16:28:21 +0000"  >&lt;p&gt;On my rig:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
jdcryans@jdcryans-lasi:~/svn/hbase/trunk$ ./bin/hbase ReadDelayTest
09/02/03 11:22:36 INFO zookeeper.ZooKeeperWrapper: Quorum servers: localhost:2181
09/02/03 11:22:42 INFO client.HBaseAdmin: Enabled table test_table
Read 1 row with 7 columns 100 times in 78ms
Read 1 row with 8 columns 100 times in 4000ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12709581" author="jimk" created="Thu, 14 May 2009 20:50:27 +0000"  >&lt;p&gt;On our cluster:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Same machine as server hosting table

Read 1 row with 7 columns 100 times in 206ms
Read 1 row with 8 columns 100 times in 4018ms
Read 1 row with 1000 columns 100 times in 13392ms

Different machine as server hosting table

Read 1 row with 7 columns 100 times in 152ms
Read 1 row with 8 columns 100 times in 168ms
Read 1 row with 1000 columns 100 times in 9386ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12709648" author="jimk" created="Fri, 15 May 2009 00:44:00 +0000"  >&lt;p&gt;When I did not deploy a region server on the same machine as the namenode and master, I got much better times:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
When run on node hosting region that is not the same as namenode or master:

Read 1 row with 7 columns 100 times in 113ms
Read 1 row with 8 columns 100 times in 4057ms
Read 1 row with 1000 columns 100 times in 10570ms

When run on node not hosting region and is not namenode or master:

Read 1 row with 7 columns 100 times in 109ms
Read 1 row with 8 columns 100 times in 121ms
Read 1 row with 1000 columns 100 times in 8838ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So running an application on the same server as the namenode. master and region server definately slows things down.&lt;/p&gt;

&lt;p&gt;There is basically no difference in the test where we read 1 row with 7 columns.&lt;/p&gt;

&lt;p&gt;Why the read 1 row with 8 columns or the read 1 row with 1000 columns is still slower if the application is run on a different machine than the one hosting the region, is still a mystery.&lt;/p&gt;

&lt;p&gt;Will continue investigation.......&lt;/p&gt;</comment>
                            <comment id="12709980" author="jimk" created="Fri, 15 May 2009 21:28:35 +0000"  >&lt;p&gt;When the application runs on the same host as the one serving the table, there is a lot more context switching going on which accounts for the differences in times. See below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Application running on a host different from the one serving the table:

Read 1 row with 7 columns 100 times in 108ms
Read 1 row with 8 columns 100 times in 111ms
Read 1 row with 1000 columns 100 times in 8995ms

procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  1    112 182440  76356 6429056    0    0   282  1955    7    6  4  3 86  6  0
 0  0    112 182532  76368 6429112    0    0    16    40   25  335  0  0 100  0  0
 0  0    112 182532  76368 6429112    0    0     0     0   15  312  0  0 100  0  0
 0  0    112 182656  76368 6429112    0    0     0     0   16  330  0  0 100  0  0
 0  0    112 182896  76368 6429112    0    0     0     0   16  305  0  0 100  0  0
 0  0    112 182896  76376 6429112    0    0     0    40   28  348  0  0 100  0  0
 0  0    112 182896  76380 6429112    0    0     0    36   28  341  0  0 100  0  0
 1  0    112 177516  76380 6429144    0    0     0     0   17  511  1  1 98  0  0
 0  0    112 153648  76380 6429144    0    0     0     0  128 1164 33  2 65  0  0
 0  0    112 153396  76380 6429152    0    0     0     5   45  694  1  0 99  0  0
 0  0    112 153396  76380 6429152    0    0     0    44   18  342  0  0 100  0  0
 2  0    112 153388  76388 6429152    0    0     0    16   29  480  0  0 100  0  0
 0  0    112 153388  76392 6429152    0    0     0    64   27  380  0  0 100  0  0
 0  0    112 153132  76392 6429152    0    0     0     1   29  487  1  0 99  0  0
 0  0    112 153008  76392 6429152    0    0     0     0   10  492  0  0 100  0  0
 0  0    112 141904  76392 6429992    0    0     0    25 1901 4647 21  2 77  0  0
 0  0    112 135344  76392 6429992    0    0     0     0 1693  528  4  2 94  0  0
 0  0    112 131084  76404 6429992    0    0     0    48 1886  411  1  3 96  0  0
 0  0    112 119920  76404 6429992    0    0     0     0 1769 2398  4  1 95  0  0
 0  0    112 117428  76408 6429992    0    0     0    24 1972  461  2  1 97  0  0
 0  0    112 117300  76408 6429992    0    0     0    12 1810  386  1  1 97  0  0
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0    112 108104  76408 6429992    0    0     0     0 1944  445  6  1 92  0  0
 0  0    112  83452  76408 6429992    0    0     0     0 1746  456 16  2 82  0  0
 0  0    112  83796  76408 6429992    0    0     0     0 1492  448  0  2 98  0  0
 0  0    112  83608  76420 6429992    0    0     0    57  897  411  0  1 99  0  0
 0  0    112 180416  76420 6429960    0    0     0     0   30  414  0  0 100  0  0
 0  0    112 180340  76420 6429960    0    0     0     0   20  326  0  0 100  0  0
 0  0    112 180340  76420 6429960    0    0     0     0   15  311  0  0 100  0  0
 0  0    112 180676  76420 6429960    0    0     0     0   15  328  0  0 100  0  0
 0  0    112 180680  76420 6429960    0    0     0     0   14  321  0  0 100  0  0
 0  0    112 180680  76420 6429960    0    0     0    77   35  350  0  0 100  0  0
 0  0    112 180804  76420 6429960    0    0     0     0   23  323  0  0 100  0  0
 0  0    112 180804  76420 6429960    0    0     0     0   10  308  0  0 100  0  0
 0  0    112 180928  76428 6429960    0    0    24    16   33  380  4  0 95  1  0
 0  0    112 180556  76428 6430036    0    0    52     0   46  458  4  0 95  1  0
 0  0    112 178656  76436 6430036    0    0     0   112   40 1288  1  0 99  0  0
 0  0    112 178820  76436 6430036    0    0     0     0   20  325  0  0 100  0  0
 2  0    112 178820  76436 6430036    0    0     0     0   24  352  0  0 100  0  0
 1  0    112 178824  76436 6430036    0    0     0     0   13  337  0  0 100  0  0
 0  0    112 179136  76436 6430036    0    0     0     0   22  414  0  0 100  0  0
 0  0    112 179136  76436 6430036    0    0     0    40   18  354  0  0 100  0  0
 0  0    112 178840  76440 6430036    0    0     0    28   44  420  0  0 100  0  0
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0    112 178880  76448 6430036    0    0     0    16   31  342  0  0 100  0  0
 0  0    112 178288  76448 6430036    0    0     0     0   15  525  1  1 98  0  0
 0  0    112 178384  76448 6430040    0    0     0     0   13  322  0  0 100  0  0
 0  0    112 178384  76448 6430040    0    0     0   919   36  335  0  0 100  0  0
 1  0    112 178384  76452 6430040    0    0     0    48   26  357  0  0 100  0  0
 0  0    112 178536  76452 6430040    0    0     0     0   41  327  0  0 100  0  0
 0  0    112 178544  76452 6430040    0    0     0     0   42  330  0  0 100  0  0
 1  0    112 178560  76452 6430040    0    0     0     0   47  328  0  0 100  0  0
 0  0    112 178684  76452 6430040    0    0     0    30   37  347  0  0 100  0  0
 0  0    112 178676  76460 6430040    0    0     0    60   41  399  0  0 100  0  0
 0  0    112 178684  76460 6430040    0    0     0     0   19  336  0  0 100  0  0
 0  0    112 178684  76460 6430040    0    0     0     0   23  337  0  0 100  0  0
 1  0    112 178808  76460 6430040    0    0     0     0   16  331  0  0 100  0  0
 1  0    112 178932  76460 6430040    0    0     0     8   14  338  0  0 100  0  0
 0  0    112 178932  76460 6430040    0    0     0     0   30  326  0  0 100  0  0
 0  0    112 178784  76464 6430040    0    0     0    16   12  403  0  0 100  0  0
 0  0    112 178808  76464 6430040    0    0     0     0   13  330  0  0 100  0  0
 0  0    112 178808  76464 6430040    0    0     0     0    6  338  0  0 100  0  0
 0  0    112 178932  76472 6430040    0    0     0    25   20  357  0  0 100  0  0
 1  0    112 179056  76472 6430040    0    0     0     0   17  323  0  0 100  0  0
 1  0    112 179056  76472 6430040    0    0     0     0    8  316  0  0 100  0  0
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0    112 179172  76480 6430040    0    0     0    20   20  436  0  0 100  0  0
 0  0    112 179296  76480 6430040    0    0     0     0   24  319  0  0 100  0  0
 0  0    112 179296  76480 6430040    0    0     0    28   51  329  0  0 100  0  0
 0  0    112 179296  76480 6430040    0    0     0     0   33  335  0  0 100  0  0
 1  0    112 179384  76480 6430040    0    0     0     0   13  338  0  0 100  0  0
 0  0    112 179508  76480 6430040    0    0     0     0   10  310  0  0 100  0  0
 0  0    112 179508  76492 6430044    0    0     0    65   31  373  0  0 100  0  0
 0  0    112 179508  76492 6430044    0    0     0    16   39  334  0  0 100  0  0


Application running on the same host as the one serving the table:

Read 1 row with 7 columns 100 times in 97ms
Read 1 row with 8 columns 100 times in 3997ms
Read 1 row with 1000 columns 100 times in 10460ms

procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0    116  35120  89092 6380936    0    0   307  1993    5   13  5  4 85  6  0
 0  0    116  34988  89092 6380936    0    0     0     4   29  385  0  0 100  0  0
 1  0    116  34988  89092 6380936    0    0     0     0    3  320  0  0 100  0  0
 1  0    116  34492  89096 6380936    0    0     0    36   22  425  0  0 100  0  0
 1  0    116  34616  89096 6380936    0    0     0     0   27  354  0  0 100  0  0
 1  0    116  34616  89096 6380936    0    0     0     0   13  338  0  0 100  0  0
 1  0    116  34864  89104 6380936    0    0     0    20   21  361  0  0 100  0  0
 0  0    116  34912  89104 6380936    0    0     0     0   12  351  0  0 100  0  0
 1  0    116  34912  89104 6380936    0    0     0    28   16  337  0  0 100  0  0
 1  0    116  28964  89104 6371916    0    0     0     0   20  964 12  2 86  0  0
 0  0    116  28388  89104 6358320    0    0     0     0  183 1078 22  1 77  0  0
 2  0    116  28384  89104 6358320    0    0     0    69   58  358  0  0 100  0  0
 0  0    116  27880  89112 6358312    0    0     0     4  111  937  0  1 99  0  0
 0  0    116  27832  89112 6358316    0    0     0     0   26  396  0  0 100  0  0
 1  0    116  27840  89116 6358316    0    0     0    96   41  386  0  0 100  0  0
 0  0    116  27964  89116 6358316    0    0     0     0   23  404  0  0 100  0  0
 4  0    116  25128  89116 6353476    0    0     0    58  134  774 14  0 85  0  0
 1  0    116  27368  89124 6351088    0    0     0    32  258 1977  4  1 96  0  0
 0  0    116  27368  89124 6351096    0    0     0     8   71  674  0  0 100  0  0
 0  0    116  26684  89124 6351096    0    0     0     6   13  710  0  0 100  0  0
 3  0    116  26960  89132 6351096    0    0     0    72   26  645  0  0 100  0  0
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 3  0    116  28552  89132 6342908    0    0     0    26   37  688 24  1 74  0  0
 1  0    116  26540  89132 6326496    0    0     0     0   21 1201 22  2 76  0  0
 1  0    116  27480  89140 6318244    0    0     0    16   19 1450 22  1 76  0  0
 1  0    116  28620  89140 6314104    0    0     0     0   26 2092 21  1 78  0  0
 0  0    116  40952  89148 6314096    0    0     0    28   40 3456 21  1 77  0  0
 3  0    116  40080  89148 6314104    0    0     0    12   25 1406 25  0 75  0  0
 1  0    116  33888  89148 6314104    0    0     0     0   20 1480 27  0 73  0  0
 1  0    116  24888  89148 6309924    0    0     0     0   17 1564 17  1 82  0  0
 2  0    116  27952  89156 6305776    0    0     0    16   20 1594 19  0 81  0  0
 0  0    116  28156  89156 6305788    0    0     0     0   24 1499 18  0 82  0  0
 0  0    116  42532  89164 6305788    0    0     0    32   40  990  9  0 91  0  0
 0  0    116 122276  89164 6305788    0    0     0     0   17  438  0  0 100  0  0
 0  0    116 122256  89164 6305756    0    0     0     0   19  425  0  0 100  0  0
 0  0    116 122552  89164 6305756    0    0     0     0   21  362  0  0 100  0  0
 0  0    116 122676  89172 6305748    0    0     0    32   31  361  0  0 100  0  0
 0  0    116 122676  89172 6305756    0    0     0    40   35  365  0  0 100  0  0
 0  0    116 123048  89172 6305756    0    0     0     0   67  367  0  0 100  0  0
 0  0    116 123048  89172 6305756    0    0     0     0   43  336  0  0 100  0  0
 0  0    116 123048  89172 6305756    0    0     0     0   57  380  0  0 100  0  0
 0  0    116 123544  89172 6305756    0    0     0     0   27  354  0  0 100  0  0
 0  0    116 123544  89180 6305748    0    0     0    28   16  372  0  0 100  0  0
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0    116 123544  89184 6305756    0    0     0    24   18  361  0  0 100  0  0
 0  0    116 123820  89184 6305756    0    0     0     0   20  356  0  0 100  0  0
 0  0    116 123820  89184 6305760    0    0     0     0   17  355  0  0 100  0  0
 0  0    116 123944  89184 6305760    0    0     0     0   23  388  0  0 100  0  0
 0  0    116 124184  89184 6305760    0    0     0    56   23  353  0  0 100  0  0
 0  0    116 124184  89200 6305744    0    0     0    89   39  392  0  0 100  0  0
 0  0    116 124184  89200 6305760    0    0     0     0   16  376  0  0 100  0  0
 0  0    116 124308  89200 6305760    0    0     0     0   12  366  0  0 100  0  0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think this issue is a red herring, and there is no basic difference between running the test program on the same host as the one serving the table vs running the test program on a different server.&lt;/p&gt;

&lt;p&gt;The difference when running on the same server is that the OS needs to do context switches between the client application and the region server.&lt;/p&gt;</comment>
                            <comment id="12709981" author="jimk" created="Fri, 15 May 2009 21:31:20 +0000"  >&lt;p&gt;Closing as invalid because it appears this is not an HBase problem. It appears that everything is working as it should, the difference being how much CPU load/context switching is required when the client runs on the same machine as the region server.&lt;/p&gt;</comment>
                            <comment id="12710003" author="streamy" created="Fri, 15 May 2009 22:34:41 +0000"  >&lt;p&gt;Jim, I don&apos;t believe this issue should be closed.  It may not have anything to do with our code, but it affects us in a very significant way so we need to get to the bottom of it.  This is an exploratory issue, hopefully with a solution, but we&apos;re not done with it yet.&lt;/p&gt;

&lt;p&gt;Your conclusion is not correct.  You are writing off the delay as context switching that occurs when client is on the same machine.  First of all, those context switches are orders of magnitude below the timings of these queries.  The queries in question run 40 times slower (seems to be something about 4000ms, dunno what) when running local to the hosting node.  This amount of time is clearly not explainable by the additional context switching of having these two things running concurrently.&lt;/p&gt;

&lt;p&gt;But more importantly, this explanation does not address what we&apos;re seeing.  Given what you say above, we should be seeing ALL queries running slower by some fixed factor when running on the same node.  But we don&apos;t.  There is a very specific and definable range of payload sizes for which this extra delay of ~4 seconds exists.  The 7 column case and the 1000 column case both perform nearly identical in both situations, so the affect of the context switching is negligible.&lt;/p&gt;

&lt;p&gt;Have you done network-level debugging?  We need to figure out where in the chain the delay is introduced and go from there.&lt;/p&gt;

&lt;p&gt;There could be an issue in Linux, RPC, who knows... but we should keep digging whether or not we figure this out for 0.20&lt;/p&gt;</comment>
                            <comment id="12710007" author="jimk" created="Fri, 15 May 2009 22:48:13 +0000"  >&lt;p&gt;@Jgray&lt;/p&gt;

&lt;p&gt;If you&apos;ll note, there is almost an order of magnitude of context switches when run on the same host.&lt;/p&gt;

&lt;p&gt;On a different host we are hovering at about 300-400 cs/sec. On the same host, when the app runs we are running 1200-1400 cs/sec.&lt;/p&gt;

&lt;p&gt;However I will try to gather more information with strace, etc.&lt;/p&gt;</comment>
                            <comment id="12710012" author="streamy" created="Fri, 15 May 2009 23:11:26 +0000"  >&lt;p&gt;The most important point is, if it&apos;s context switches, there should be a constant overhead for ALL queries.  The 4 second delay shows up for a very specific slice of payload sizes and then goes away.  So whatever is wrong seems related to payload size or the nature of the payload in some way.&lt;/p&gt;</comment>
                            <comment id="12710021" author="streamy" created="Fri, 15 May 2009 23:21:27 +0000"  >&lt;p&gt;I did misunderstand part of your explanation.  The jump in CS is just in this delayed case.&lt;/p&gt;

&lt;p&gt;But we should still not close the issue.  It&apos;s not the context switching caused by additional concurrency that hurts us then, it&apos;s context switching because there&apos;s something else happening.  The jump in context switches is a symptom of something happening differently along the way for this special range of payloads.&lt;/p&gt;</comment>
                            <comment id="12710022" author="streamy" created="Fri, 15 May 2009 23:22:57 +0000"  >&lt;p&gt;Again, I think our best approach will be to try to isolate further and further until we know what the culprit is.  First off, does this happen in the client or the server?&lt;/p&gt;</comment>
                            <comment id="12713717" author="jimk" created="Wed, 27 May 2009 20:22:11 +0000"  >&lt;p&gt;Ran ReadDelayTest again after adding flush, compact and major compaction and waiting until all were complete before running the read test. No change.&lt;/p&gt;

&lt;p&gt;I have inserted timers in HRegionServer.getRow and can see increased times throughout the test, so I believe at least part of the delay is on the server side.&lt;/p&gt;

&lt;p&gt;Next step is to insert lower level timers to isolate Memcache or reading from store files as a culprit.&lt;/p&gt;</comment>
                            <comment id="12714600" author="jimk" created="Fri, 29 May 2009 21:01:19 +0000"  >&lt;p&gt;Contribution of methods to response time. Clearly getFullFromStoreFile dominates.&lt;/p&gt;</comment>
                            <comment id="12714640" author="jimk" created="Sat, 30 May 2009 01:02:34 +0000"  >&lt;p&gt;Store.getClosest&lt;/p&gt;</comment>
                            <comment id="12714726" author="stack" created="Sat, 30 May 2009 14:50:33 +0000"  >&lt;p&gt;Whats the diagram saying?  And how does getClosest relate to getFullFromStoreFile (Is the latter whats taking time)?&lt;/p&gt;</comment>
                            <comment id="12714731" author="jimk" created="Sat, 30 May 2009 15:29:12 +0000"  >&lt;p&gt;The first graph shows the time consumed by methods called from Store.getFull. &lt;/p&gt;

&lt;p&gt;getFullFromStoreFile is where the most time is being spent . Those peaks contribute to what causes the 100 gets to have a high average time.&lt;/p&gt;

&lt;p&gt;getClosest is the biggest contributor to the time spent in getFullFromStoreFile.&lt;/p&gt;

&lt;p&gt;I have data for methods called by getClosest, but have not graphed them yet.&lt;/p&gt;</comment>
                            <comment id="12714757" author="streamy" created="Sat, 30 May 2009 19:04:14 +0000"  >&lt;p&gt;What is the X-axis on these graphs?  Iteration or number of columns retrieved?&lt;/p&gt;</comment>
                            <comment id="12714759" author="streamy" created="Sat, 30 May 2009 19:35:31 +0000"  >&lt;p&gt;Here is a graph I made at the same time I originally filed this issue.&lt;/p&gt;

&lt;p&gt;What it shows are very explicit ranges of payloads for which an enormous and fixed delay occurs, and then goes away as the row returned gets larger.&lt;/p&gt;

&lt;p&gt;I&apos;m trying to match up this behavior with what you&apos;re plotting, Jim.  Can you explain further what the axis are on yours?&lt;/p&gt;</comment>
                            <comment id="12714770" author="jimk" created="Sat, 30 May 2009 20:58:09 +0000"  >&lt;p&gt;In the first graph the x axis is number of calls to getRow and the y axis is time in nano seconds per call.&lt;/p&gt;

&lt;p&gt;The second graph&apos;s x axis is the time for each call to getClosest.&lt;/p&gt;

&lt;p&gt;I&apos;ll post some better ones shortly.&lt;/p&gt;</comment>
                            <comment id="12714802" author="jimk" created="Sun, 31 May 2009 01:04:02 +0000"  >&lt;p&gt;All I am seeing now (on trunk) is increasing times:&lt;/p&gt;

&lt;p&gt;Read 1 row with 7 columns 100 times in 921ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 4,089ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 16,330ms&lt;/p&gt;

&lt;p&gt;Read 1 row with 7 columns 100 times in 875ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 4,025ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 16,860ms&lt;/p&gt;

&lt;p&gt;Read 1 row with 7 columns 100 times in 993ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 4,087ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 16,530ms&lt;/p&gt;

&lt;p&gt;Which is what I would expect.&lt;/p&gt;

&lt;p&gt;Sometimes I get fetching 8 columns is faster than 7 as in:&lt;/p&gt;

&lt;p&gt;Read 1 row with 7 columns 100 times in 907ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 622ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 15,062ms&lt;/p&gt;</comment>
                            <comment id="12714803" author="jimk" created="Sun, 31 May 2009 01:19:57 +0000"  >&lt;p&gt;I just ran it again twice and got increasing times:&lt;/p&gt;

&lt;p&gt;Read 1 row with 7 columns 100 times in 996ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 4044ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 16175ms&lt;/p&gt;

&lt;p&gt;Read 1 row with 7 columns 100 times in 976ms&lt;br/&gt;
Read 1 row with 8 columns 100 times in 4034ms&lt;br/&gt;
Read 1 row with 1000 columns 100 times in 16240ms&lt;/p&gt;

&lt;p&gt;I have not observed the decrease between 8 and 1000 columns today at all.&lt;/p&gt;</comment>
                            <comment id="12714812" author="jimk" created="Sun, 31 May 2009 03:39:47 +0000"  >&lt;p&gt;Obviously, from this graph, not all the time is consumed in getRow&lt;/p&gt;

&lt;p&gt;Need to look elsewhere&lt;/p&gt;</comment>
                            <comment id="12715175" author="jimk" created="Mon, 1 Jun 2009 17:54:19 +0000"  >&lt;p&gt;In this run (trunk latest as of 2009/05/31), times increased with the number of columns.&lt;/p&gt;

&lt;p&gt;7 columns took: 840ms&lt;br/&gt;
8 columns took: 4080ms&lt;br/&gt;
1000 columns took 16170ms&lt;/p&gt;</comment>
                            <comment id="12715239" author="jimk" created="Mon, 1 Jun 2009 20:22:11 +0000"  >&lt;p&gt;As one might expect, almost no seeking is necessary for 7 columns. Why it jumps so much for 8 columns (because 7 is at the beginning of the file?) is odd.&lt;/p&gt;

&lt;p&gt;What is expected is that more seeking is done for 1000 columns than for 8.&lt;/p&gt;</comment>
                            <comment id="12715240" author="jimk" created="Mon, 1 Jun 2009 20:24:27 +0000"  >&lt;p&gt;This graph, at least, makes sense. Not much &apos;nexting&apos; is needed for 7 or 8 columns, but it jumps up for 1000 columns&lt;/p&gt;</comment>
                            <comment id="12715242" author="jimk" created="Mon, 1 Jun 2009 20:27:49 +0000"  >&lt;p&gt;This also makes sense to me. getClosest is going to occupy a higher percentage of time for 7 or 8 columns than for 1000.&lt;/p&gt;

&lt;p&gt;I expect that marshalling data is the largest contributor to getRow(1000 columns), but I do not have hard data to support that.&lt;/p&gt;</comment>
                            <comment id="12715340" author="jimk" created="Tue, 2 Jun 2009 02:32:05 +0000"  >&lt;p&gt;Here you can see the blowup in elapsed time vs number of columns.&lt;/p&gt;</comment>
                            <comment id="12715346" author="jimk" created="Tue, 2 Jun 2009 02:44:51 +0000"  >&lt;p&gt;Based on the graph &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12409623/zoom+of+columns+vs+round-trip+blowup.jpg&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12409623/zoom+of+columns+vs+round-trip+blowup.jpg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;the blow-up in round-trip time happens between 8 and 23 columns (approximately - different runs will have different end points), but the amount of time spent in getRow remains pretty constant through this range.&lt;/p&gt;

&lt;p&gt;For larger numbers of columns (as depicted in &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12409622/getRow+%2B+round-trip+vs+%23+columns.jpg&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12409622/getRow+%2B+round-trip+vs+%23+columns.jpg&lt;/a&gt; ), getRow and round-trip time seem to scale pretty linearly.&lt;/p&gt;

&lt;p&gt;This is pretty strange, but at least it doesn&apos;t seem related to server-side I/O.&lt;/p&gt;</comment>
                            <comment id="12715354" author="stack" created="Tue, 2 Jun 2009 03:40:12 +0000"  >&lt;p&gt;So the time is not being spent in getRow?  This scales linearly with number of columns?  Time is elsewhere?&lt;/p&gt;</comment>
                            <comment id="12715970" author="jimk" created="Wed, 3 Jun 2009 16:16:02 +0000"  >&lt;p&gt;It would appear that the time explosion happens during deserialization.&lt;/p&gt;</comment>
                            <comment id="12717170" author="streamy" created="Mon, 8 Jun 2009 06:49:46 +0000"  >&lt;p&gt;Jim, can you run on current trunk, post-1304 commit?&lt;/p&gt;

&lt;p&gt;I will try to run a test tomorrow, but it&apos;s a busy day.  Let&apos;s not hold up 0.20.0 for this if we can&apos;t figure it and drop to 0.20.1/0.21.0&lt;/p&gt;</comment>
                            <comment id="12717183" author="jimk" created="Mon, 8 Jun 2009 07:48:45 +0000"  >&lt;p&gt;Will do tomorrow (Mon 6/8)&lt;/p&gt;</comment>
                            <comment id="12717369" author="jimk" created="Mon, 8 Jun 2009 19:15:30 +0000"  >&lt;p&gt;This problem still exists in trunk after &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1304&quot; title=&quot;New client server implementation of how gets and puts are handled. &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1304&quot;&gt;&lt;del&gt;HBASE-1304&lt;/del&gt;&lt;/a&gt; was committed. I&apos;m going to instrument the RPC layer tomorrow.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Read 1 row with 6 columns 100 times in 53 ms
Read 1 row with 7 columns 100 times in 81 ms
Read 1 row with 8 columns 100 times in 4133 ms
Read 1 row with 9 columns 100 times in 4000 ms
Read 1 row with 10 columns 100 times in 4000 ms
Read 1 row with 11 columns 100 times in 4000 ms
Read 1 row with 12 columns 100 times in 4000 ms
Read 1 row with 13 columns 100 times in 4000 ms
Read 1 row with 14 columns 100 times in 4000 ms
Read 1 row with 15 columns 100 times in 4000 ms
Read 1 row with 16 columns 100 times in 4010 ms
Read 1 row with 17 columns 100 times in 4000 ms
Read 1 row with 18 columns 100 times in 4000 ms
Read 1 row with 19 columns 100 times in 4000 ms
Read 1 row with 20 columns 100 times in 4000 ms
Read 1 row with 21 columns 100 times in 4000 ms
Read 1 row with 22 columns 100 times in 4000 ms
Read 1 row with 23 columns 100 times in 4000 ms
Read 1 row with 24 columns 100 times in 83 ms
Read 1 row with 25 columns 100 times in 72 ms
Read 1 row with 26 columns 100 times in 63 ms
Read 1 row with 27 columns 100 times in 66 ms
Read 1 row with 28 columns 100 times in 71 ms
Read 1 row with 29 columns 100 times in 67 ms
Read 1 row with 30 columns 100 times in 88 ms
Read 1 row with 31 columns 100 times in 78 ms
Read 1 row with 32 columns 100 times in 174 ms
Read 1 row with 33 columns 100 times in 208 ms
Read 1 row with 34 columns 100 times in 62 ms
Read 1 row with 35 columns 100 times in 130 ms
Read 1 row with 36 columns 100 times in 137 ms
Read 1 row with 37 columns 100 times in 142 ms
Read 1 row with 38 columns 100 times in 279 ms
Read 1 row with 39 columns 100 times in 184 ms
Read 1 row with 40 columns 100 times in 367 ms
Read 1 row with 41 columns 100 times in 202 ms
Read 1 row with 42 columns 100 times in 452 ms
Read 1 row with 43 columns 100 times in 206 ms
Read 1 row with 44 columns 100 times in 669 ms
Read 1 row with 45 columns 100 times in 242 ms
Read 1 row with 46 columns 100 times in 199 ms
Read 1 row with 47 columns 100 times in 250 ms
Read 1 row with 48 columns 100 times in 62 ms
Read 1 row with 49 columns 100 times in 62 ms
Read 1 row with 50 columns 100 times in 81 ms
Read 1 row with 51 columns 100 times in 66 ms
Read 1 row with 52 columns 100 times in 65 ms
Read 1 row with 53 columns 100 times in 64 ms
Read 1 row with 54 columns 100 times in 65 ms
Read 1 row with 55 columns 100 times in 113 ms
Read 1 row with 56 columns 100 times in 89 ms
Read 1 row with 57 columns 100 times in 72 ms
Read 1 row with 58 columns 100 times in 67 ms
Read 1 row with 59 columns 100 times in 67 ms
Read 1 row with 60 columns 100 times in 66 ms
Read 1 row with 61 columns 100 times in 78 ms
Read 1 row with 62 columns 100 times in 88 ms
Read 1 row with 63 columns 100 times in 72 ms
Read 1 row with 64 columns 100 times in 85 ms
Read 1 row with 65 columns 100 times in 82 ms
Read 1 row with 66 columns 100 times in 122 ms
Read 1 row with 67 columns 100 times in 82 ms
Read 1 row with 68 columns 100 times in 110 ms
Read 1 row with 69 columns 100 times in 168 ms
Read 1 row with 70 columns 100 times in 85 ms
Read 1 row with 71 columns 100 times in 85 ms
Read 1 row with 72 columns 100 times in 86 ms
Read 1 row with 73 columns 100 times in 109 ms
Read 1 row with 74 columns 100 times in 85 ms
Read 1 row with 75 columns 100 times in 97 ms
Read 1 row with 76 columns 100 times in 108 ms
Read 1 row with 77 columns 100 times in 128 ms
Read 1 row with 78 columns 100 times in 91 ms
Read 1 row with 79 columns 100 times in 102 ms
Read 1 row with 80 columns 100 times in 221 ms
Read 1 row with 81 columns 100 times in 138 ms
Read 1 row with 82 columns 100 times in 260 ms
Read 1 row with 83 columns 100 times in 474 ms
Read 1 row with 84 columns 100 times in 99 ms
Read 1 row with 85 columns 100 times in 110 ms
Read 1 row with 86 columns 100 times in 93 ms
Read 1 row with 87 columns 100 times in 313 ms
Read 1 row with 88 columns 100 times in 231 ms
Read 1 row with 89 columns 100 times in 246 ms
Read 1 row with 90 columns 100 times in 189 ms
Read 1 row with 91 columns 100 times in 162 ms
Read 1 row with 92 columns 100 times in 153 ms
Read 1 row with 93 columns 100 times in 157 ms
Read 1 row with 94 columns 100 times in 144 ms
Read 1 row with 95 columns 100 times in 156 ms
Read 1 row with 96 columns 100 times in 172 ms
Read 1 row with 97 columns 100 times in 362 ms
Read 1 row with 98 columns 100 times in 139 ms
Read 1 row with 99 columns 100 times in 171 ms
Read 1 row with 100 columns 100 times in 103 ms
Read 1 row with 101 columns 100 times in 149 ms
Read 1 row with 102 columns 100 times in 104 ms
Read 1 row with 103 columns 100 times in 410 ms
Read 1 row with 104 columns 100 times in 594 ms
Read 1 row with 1000 columns 100 times in 2041 ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12717846" author="streamy" created="Tue, 9 Jun 2009 22:35:29 +0000"  >&lt;p&gt;Downgrading from critical to major.  We need to fix this but with no real leads yet, we should not hold up any releases.&lt;/p&gt;</comment>
                            <comment id="12718723" author="jimk" created="Fri, 12 Jun 2009 05:51:00 +0000"  >&lt;p&gt;Tried running the read test backwards (reading 104 columns...6 columns). The first few rows were slower, but the big blip still existed between 23...10 columns. Something is really goofy.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Read 1 row with 104 columns 100 times in 1170 ms
Read 1 row with 103 columns 100 times in 313 ms
Read 1 row with 102 columns 100 times in 820 ms
Read 1 row with 101 columns 100 times in 809 ms
Read 1 row with 100 columns 100 times in 403 ms
Read 1 row with 99 columns 100 times in 704 ms
Read 1 row with 98 columns 100 times in 317 ms
Read 1 row with 97 columns 100 times in 246 ms
Read 1 row with 96 columns 100 times in 169 ms
Read 1 row with 95 columns 100 times in 179 ms
...
Read 1 row with 24 columns 100 times in 65 ms
Read 1 row with 23 columns 100 times in 3999 ms
Read 1 row with 22 columns 100 times in 4000 ms
Read 1 row with 21 columns 100 times in 4000 ms
Read 1 row with 20 columns 100 times in 4000 ms
Read 1 row with 19 columns 100 times in 4000 ms
Read 1 row with 18 columns 100 times in 4000 ms
Read 1 row with 17 columns 100 times in 4000 ms
Read 1 row with 16 columns 100 times in 4000 ms
Read 1 row with 15 columns 100 times in 4000 ms
Read 1 row with 14 columns 100 times in 4000 ms
Read 1 row with 13 columns 100 times in 4000 ms
Read 1 row with 12 columns 100 times in 4000 ms
Read 1 row with 11 columns 100 times in 4000 ms
Read 1 row with 10 columns 100 times in 4000 ms
Read 1 row with 9 columns 100 times in 4000 ms
Read 1 row with 8 columns 100 times in 4000 ms
Read 1 row with 7 columns 100 times in 42 ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Go figure...&lt;/p&gt;</comment>
                            <comment id="12721584" author="streamy" created="Fri, 19 Jun 2009 00:35:09 +0000"  >&lt;p&gt;Anything doing on this?&lt;/p&gt;

&lt;p&gt;If not, let&apos;s punt to 0.20.1 so we can clear up 0.20.0 remaining issues.&lt;/p&gt;</comment>
                            <comment id="12721597" author="jimk" created="Fri, 19 Jun 2009 01:04:13 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;It&apos;s not the RPC&lt;/li&gt;
	&lt;li&gt;it&apos;s not the server&lt;/li&gt;
	&lt;li&gt;it (sort of) appears to be in deserializing the result but I don&apos;t understand why it just for those keys and also why is local different from remote?&lt;/li&gt;
	&lt;li&gt;It balloons up during the same set of rows no matter if you run the reads forwards or backwards.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I suppose it could have something to do with row/column keys for those rows, but I don&apos;t know what (and again, why local and not remote?)&lt;/p&gt;

&lt;p&gt;Punt it to 0.21.&lt;/p&gt;</comment>
                            <comment id="12761066" author="streamy" created="Thu, 1 Oct 2009 01:06:30 +0000"  >&lt;p&gt;Anyone want to dig in to this again? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12867038" author="stack" created="Thu, 13 May 2010 04:42:56 +0000"  >&lt;p&gt;Moved from 0.21 to 0.22 just after merge of old 0.20 branch into TRUNK.&lt;/p&gt;</comment>
                            <comment id="13047557" author="stack" created="Fri, 10 Jun 2011 22:45:49 +0000"  >&lt;p&gt;Moving out of 0.92.0. Pull it back in if you think different.&lt;/p&gt;</comment>
                            <comment id="13212822" author="fitchj" created="Tue, 21 Feb 2012 19:26:23 +0000"  >&lt;p&gt;After seeing the same 40ms. delay during some testing of random reads against hbase, we were able to determine that it was being caused by the Nagle &quot;ACK delay&quot;. Setting &quot;ipc.server.tcpnodelay&quot; to &quot;true&quot; resolved the issue for us. Further discussion of this issue can be found in the linked issues.&lt;/p&gt;</comment>
                            <comment id="13279876" author="stack" created="Sun, 20 May 2012 21:52:35 +0000"  >&lt;p&gt;Any new dev want to take on proving this phenomeon is because of Nagles?  The 40ms does seem to correlate strongly.&lt;/p&gt;</comment>
                            <comment id="13279943" author="lhofhansl" created="Mon, 21 May 2012 03:39:33 +0000"  >&lt;p&gt;If this caused by Nagles it should happen regardless of whether the DN is local to the RS or not, right? (Or maybe we&apos;re seeing an issue different from the OP&apos;s).&lt;/p&gt;</comment>
                            <comment id="13689742" author="stack" created="Thu, 20 Jun 2013 21:57:04 +0000"  >&lt;p&gt;Resolving as no longer valid.  Looks like Nagles&apos; anyways.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12445567">HBASE-2125</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12542478">HADOOP-8069</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12409596" name="Contribution of getClosest to getRow time.jpg" size="170159" author="jimk" created="Mon, 1 Jun 2009 20:27:49 +0000"/>
                            <attachment id="12409595" name="Contribution of next to getRow time.jpg" size="131203" author="jimk" created="Mon, 1 Jun 2009 20:24:27 +0000"/>
                            <attachment id="12409594" name="Contribution of seekTo to getClosest time.jpg" size="142119" author="jimk" created="Mon, 1 Jun 2009 20:22:11 +0000"/>
                            <attachment id="12409787" name="Elapsed time of RowResults.readFields.jpg" size="108031" author="jimk" created="Wed, 3 Jun 2009 16:16:02 +0000"/>
                            <attachment id="12399364" name="ReadDelayTest.java" size="4128" author="streamy" created="Tue, 3 Feb 2009 16:27:30 +0000"/>
                            <attachment id="12409789" name="RowResults.readFields zoomed.jpg" size="103570" author="jimk" created="Wed, 3 Jun 2009 16:17:24 +0000"/>
                            <attachment id="12409622" name="getRow + round-trip vs # columns.jpg" size="113287" author="jimk" created="Tue, 2 Jun 2009 02:32:05 +0000"/>
                            <attachment id="12409582" name="getRow times.jpg" size="132722" author="jimk" created="Mon, 1 Jun 2009 17:54:19 +0000"/>
                            <attachment id="12409420" name="screenshot-1.jpg" size="144136" author="jimk" created="Fri, 29 May 2009 21:01:19 +0000"/>
                            <attachment id="12409440" name="screenshot-2.jpg" size="122177" author="jimk" created="Sat, 30 May 2009 01:02:34 +0000"/>
                            <attachment id="12409477" name="screenshot-3.jpg" size="83952" author="streamy" created="Sat, 30 May 2009 19:35:31 +0000"/>
                            <attachment id="12409488" name="screenshot-4.jpg" size="135085" author="jimk" created="Sun, 31 May 2009 03:39:47 +0000"/>
                            <attachment id="12409623" name="zoom of columns vs round-trip blowup.jpg" size="96287" author="jimk" created="Tue, 2 Jun 2009 02:35:46 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>13.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 3 Feb 2009 16:28:21 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25614</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 26 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02c7b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11578</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>