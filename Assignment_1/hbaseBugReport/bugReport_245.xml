<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:42:25 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-245/HBASE-245.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-245] getRow() is orders of magnitudes slower than get(), even on rows with one column</title>
                <link>https://issues.apache.org/jira/browse/HBASE-245</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;HTable.getRow(Text) is several orders of magnitude slower than&lt;br/&gt;
HTable.get(Text, Text), even on rows with a single column.&lt;/p&gt;

&lt;p&gt;This problem can be observed by the attached patch of&lt;br/&gt;
PerformanceEvaluation.java which changes SequentialRead to use getRow,&lt;br/&gt;
and prints out the time for each read. &lt;/p&gt;

&lt;p&gt;The test can the be run with:&lt;/p&gt;

&lt;p&gt;bin/hbase org.apache.hadoop.hbase.PerformaeEvaluation sequentialRead 1&lt;/p&gt;

&lt;p&gt;On my laptop, the original test (using get()) produces reads on the order of 5-20&lt;br/&gt;
milliseconds. Using getRow(), the reads take 50-2000 ms. &lt;/p&gt;
</description>
                <environment>&lt;p&gt;latest from trunk&lt;/p&gt;</environment>
        <key id="12381936">HBASE-245</key>
            <summary>getRow() is orders of magnitudes slower than get(), even on rows with one column</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="clint.morgan">Clint Morgan</reporter>
                        <labels>
                    </labels>
                <created>Tue, 6 Nov 2007 17:33:52 +0000</created>
                <updated>Fri, 22 Aug 2008 21:34:56 +0000</updated>
                            <resolved>Fri, 9 Nov 2007 21:57:51 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12540499" author="clint.morgan" created="Tue, 6 Nov 2007 17:35:52 +0000"  >&lt;p&gt;Modifies SequentialReadTest to use getRow, and print the read time to standard out.&lt;/p&gt;</comment>
                            <comment id="12540631" author="stack" created="Wed, 7 Nov 2007 00:05:34 +0000"  >&lt;p&gt;Thanks Clint.  Good find.  HRegion.get(...) quits after its accumulated sufficient versions &amp;#8211; usually one.  HRegion.getFull, which is eventually called by HTable.getFull is pig-headed insisting on running through memcache and all store files for every possible version regardless of version setting.&lt;/p&gt;</comment>
                            <comment id="12540661" author="stack" created="Wed, 7 Nov 2007 03:44:34 +0000"  >&lt;p&gt;Actually, I misspoke.  getFull scanning memory and all on-disk files is not &apos;wrong&apos; &amp;#8211; though it is slow.  Here&apos;s why. &lt;/p&gt;

&lt;p&gt;Columns can be added willy-nilly.  There is no need of an ALTER TABLE-like statement adding a column as there is in a traditional RDBMS &amp;#8211; as long as the column belongs to an existing column family (has an extant column family for a prefix). &lt;/p&gt;

&lt;p&gt;And there is no accounting anywhere in hbase of all the columns made in any particular family.   Since there is no list of all-columns to consult, the only way hbase can be sure its found all column mentions is if it scans all data.  This is main difference between get and getFull.  Because you provide a list of columns to fetch to get, it can know when its done.  Not so with getFull.&lt;/p&gt;

&lt;p&gt;Is it important to you that this run faster Clint?  If so, there may be some things we can do like keep an integer of counts of unique column names.  getFull would know that when it had hit the count of all column names, it could return (Keeping a list of all column names would probably not be viable since in some schemas it might grow without bound).&lt;/p&gt;</comment>
                            <comment id="12540898" author="clint.morgan" created="Wed, 7 Nov 2007 21:22:45 +0000"  >&lt;p&gt;Well, 2 sec read times are not acceptable for us. But looking into&lt;br/&gt;
this further, there appears to be a bug causing the excessive time &lt;br/&gt;
(rather than the need to look everywhere for the row).&lt;/p&gt;

&lt;p&gt;Looking at HStore.getFull (line 1103), the break from the while loop&lt;br/&gt;
should occur when key.compareTo(readKey) is LESS than zero. This&lt;br/&gt;
cuts the read times back down to 5-20 ms.&lt;/p&gt;

&lt;p&gt;However, there still seems to be a problem:&lt;/p&gt;

&lt;p&gt;When there are two MapFiles in the HStore, (again in getFull()) After&lt;br/&gt;
calling map.getClosest(), matching the key and storing the results,&lt;br/&gt;
the call to map.next() produces a key that is much less than the key&lt;br/&gt;
returned by closest. So time is wasted again iterating through all&lt;br/&gt;
theses keys to get back to closest, and out of the loop.&lt;/p&gt;

&lt;p&gt;This problem can be observed by applying my patch to HStore, running&lt;br/&gt;
my pached performance test, and when the times start to climb tracing&lt;br/&gt;
through getFull(). &lt;/p&gt;

&lt;p&gt;I had quick look at what was going on with getClosest and next, but&lt;br/&gt;
did not understand all that was going on.&lt;/p&gt;</comment>
                            <comment id="12540922" author="clint.morgan" created="Wed, 7 Nov 2007 23:10:11 +0000"  >&lt;p&gt;I am having trouble getting the problem I raised in the last comment&lt;br/&gt;
to occur in the stock PerformanceEvaluation. However, if I drop the&lt;br/&gt;
number of rows in the test down an order of magnitude, by changing line 82 to&lt;/p&gt;

&lt;p&gt;private static final int ONE_GB = 1024 * 1024 * 100;&lt;/p&gt;

&lt;p&gt;I can see the problem. It happens around the 50,000th read, the read times begin to gradually rise from 5ms to 200ms by the time we reach the 60,000th read. If I do a trace during this period, I see the abnormal behavior with getClosest() and next() that I described in the above comment.&lt;/p&gt;

&lt;p&gt;So something fishy is still going on. Possibly in HStoreFile.HalfMapFileReader related to having two mapfiles.&lt;/p&gt;</comment>
                            <comment id="12540927" author="stack" created="Wed, 7 Nov 2007 23:25:43 +0000"  >&lt;p&gt;I&apos;m currently looking at this too.  One issue I&apos;ve just found is that rows are lexicographically sorted but rows in PE are numbers (This makes it so 4500 &amp;lt; 46).  The MapFiles are messed up.  Am going to zero-pad to see what that does (By the way, thanks for digging in here and finding non-breaking after we&apos;d left the row).&lt;/p&gt;</comment>
                            <comment id="12540944" author="stack" created="Thu, 8 Nov 2007 01:41:50 +0000"  >&lt;p&gt;This patch  zero pads numeric rows in PE.   Running a little test, the getFulls all return in a fairly constant couple of milliseconds.&lt;/p&gt;</comment>
                            <comment id="12541119" author="stack" created="Thu, 8 Nov 2007 20:01:38 +0000"  >&lt;p&gt;Looks like memcache has the same issue.  Let me write a test...&lt;/p&gt;</comment>
                            <comment id="12541155" author="stack" created="Thu, 8 Nov 2007 22:16:15 +0000"  >&lt;p&gt;memcache had same problem as hstore always scanning to the end instead of stopping once it&apos;d fallen out of the asked-for row.&lt;/p&gt;</comment>
                            <comment id="12541156" author="stack" created="Thu, 8 Nov 2007 22:20:36 +0000"  >&lt;p&gt;Remove the Clint patch from PerformanceEvaluation that logged times and used getFull instead of get.&lt;/p&gt;</comment>
                            <comment id="12541158" author="stack" created="Thu, 8 Nov 2007 22:21:31 +0000"  >&lt;p&gt;Are you up trying out this patch Clint? (Again, thanks for digging in to figure what was actually going on in here).&lt;/p&gt;</comment>
                            <comment id="12541235" author="stack" created="Fri, 9 Nov 2007 05:39:19 +0000"  >&lt;p&gt;This patch passes tests locally.&lt;/p&gt;</comment>
                            <comment id="12541237" author="stack" created="Fri, 9 Nov 2007 05:55:46 +0000"  >&lt;p&gt;Passes tests locally.&lt;/p&gt;</comment>
                            <comment id="12541375" author="clint.morgan" created="Fri, 9 Nov 2007 16:54:40 +0000"  >&lt;p&gt;Tried out the patch and it works fine. The memcache fix did take care of the behavior I commented on previously.&lt;/p&gt;</comment>
                            <comment id="12541404" author="hadoopqa" created="Fri, 9 Nov 2007 19:11:43 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
&lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12369212/2161-4.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12369212/2161-4.patch&lt;/a&gt;&lt;br/&gt;
against trunk revision r592860.&lt;/p&gt;

&lt;p&gt;    @author +1.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    javadoc +1.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    javac +1.  The applied patch does not generate any new compiler warnings.&lt;/p&gt;

&lt;p&gt;    findbugs +1.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    core tests +1.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    contrib tests +1.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Patch/1081/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12541449" author="stack" created="Fri, 9 Nov 2007 21:57:51 +0000"  >&lt;p&gt;Committed.  Thanks for the initial patch Clint.&lt;/p&gt;</comment>
                            <comment id="12541538" author="hudson" created="Sat, 10 Nov 2007 12:34:01 +0000"  >&lt;p&gt;Integrated in Hadoop-Nightly #299 (See &lt;a href=&quot;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/299/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/299/&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="12545566" author="stack" created="Mon, 26 Nov 2007 19:42:32 +0000"  >&lt;p&gt;TestGet2 that works against TRUNK for BryanD&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12369194" name="2161-2.patch" size="13218" author="stack" created="Thu, 8 Nov 2007 22:16:14 +0000"/>
                            <attachment id="12369195" name="2161-3.patch" size="16084" author="stack" created="Thu, 8 Nov 2007 22:20:36 +0000"/>
                            <attachment id="12369212" name="2161-4.patch" size="13022" author="stack" created="Fri, 9 Nov 2007 05:39:19 +0000"/>
                            <attachment id="12369151" name="2161.txt" size="4834" author="stack" created="Thu, 8 Nov 2007 01:41:50 +0000"/>
                            <attachment id="12369141" name="HADOOP-2161-2.patch" size="1307" author="clint.morgan" created="Wed, 7 Nov 2007 21:22:44 +0000"/>
                            <attachment id="12369039" name="PerformanceEvaluation-patch.txt" size="806" author="clint.morgan" created="Tue, 6 Nov 2007 17:35:52 +0000"/>
                            <attachment id="12370234" name="tg2.patch" size="4119" author="stack" created="Mon, 26 Nov 2007 19:42:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 7 Nov 2007 00:05:34 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>25029</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 4 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h5tb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98204</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>