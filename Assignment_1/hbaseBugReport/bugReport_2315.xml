<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:00:51 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2315/HBASE-2315.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2315] BookKeeper for write-ahead logging</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2315</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;BookKeeper, a contrib of the ZooKeeper project, is a fault tolerant and high throughput write-ahead logging service. This issue provides an implementation of write-ahead logging for hbase using BookKeeper. Apart from expected throughput improvements, BookKeeper also has stronger durability guarantees compared to the implementation currently used by hbase.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12458957">HBASE-2315</key>
            <summary>BookKeeper for write-ahead logging</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="7">Later</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="fpj">Flavio Junqueira</reporter>
                        <labels>
                    </labels>
                <created>Fri, 12 Mar 2010 16:21:23 +0000</created>
                <updated>Wed, 16 Jul 2014 20:44:06 +0000</updated>
                            <resolved>Wed, 16 Jul 2014 20:44:06 +0000</resolved>
                                                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>36</watches>
                                                                <comments>
                            <comment id="12844561" author="fpj" created="Fri, 12 Mar 2010 16:27:29 +0000"  >&lt;p&gt;I&apos;m attaching a preliminary patch. I haven&apos;t tested extensively, but it works in my small setup with one computer running hbase+dfs, three bookies, and one zookeeper server. &lt;/p&gt;</comment>
                            <comment id="12844562" author="fpj" created="Fri, 12 Mar 2010 16:30:55 +0000"  >&lt;p&gt;Attaching BookKeeper jar for testing purposes. The same jar can be also obtained from the ZooKeeper trunk.&lt;/p&gt;</comment>
                            <comment id="12844564" author="fpj" created="Fri, 12 Mar 2010 16:53:03 +0000"  >&lt;p&gt;To compile hbase with this patch, it is necessary to add the bookkeeper jar to the lib folder. I&apos;ve attached the jar to this jira for convenience, but you can also get it and compile from zookeeper/trunk.&lt;/p&gt;

&lt;p&gt;This patch requires a few new configuration properties on hbase-site.xml:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;hbase.wal.bk.zkserver&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;xxx.yyy.zzz:10281&amp;lt;/value&amp;gt; 
&amp;lt;/property&amp;gt;

&amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;hbase.regionserver.hlog.reader.impl&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;org.apache.hadoop.hbase.regionserver.wal.BookKeeperLogReader&amp;lt;/value&amp;gt;
 &amp;lt;/property&amp;gt;

 &amp;lt;property&amp;gt;
       &amp;lt;name&amp;gt;hbase.regionserver.hlog.writer.impl&amp;lt;/name&amp;gt;
       &amp;lt;value&amp;gt;org.apache.hadoop.hbase.regionserver.wal.BookKeeperLogWriter&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The zookeeper server (or servers) used with bookkeeper can be the same as the one used with hbase, but the value of the property has to be set regardless of which server you&apos;re using. If this is not ok, we may consider changing on the next iteration, assuming there will be one.&lt;/p&gt;

&lt;p&gt;It is also important to create the following znodes in the zookeeper instance pointed by hbase.wal.bk.zkserver:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;/ledgers
/ledgers/available/
/ledgers/available/&amp;lt;bookie-addr&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;where &quot;/ledgers/available/&amp;lt;bookie-addr&amp;gt;&quot; is a node representing a bookie and the format of &amp;lt;bookie-addr&amp;gt; should be &quot;xxx.yyy.zzz:port&quot;. Consequently, there must be one such znode for each available bookie. We have a jira open to make bookie registration automatic, and it should be available soon. &lt;/p&gt;

&lt;p&gt;I have been creating these znodes manually for my tests, but I acknowledge that a script to bootstrap the process would be quite handy.&lt;/p&gt;

&lt;p&gt;Finally, I would be happy to get some feedback and suggestions for improvement.&lt;/p&gt;</comment>
                            <comment id="12844593" author="stack" created="Fri, 12 Mar 2010 18:06:30 +0000"  >&lt;p&gt;Thanks you for this work Flavio.  Before going to the patch, a few of us, when BookKeeper showed up first, had this notion of writing HBase WAL to a BK Ensemble but we never did any work on it because we figured it just wouldn&apos;t be able to keep up.&lt;/p&gt;

&lt;p&gt;For example, posit a smallish hbase cluster of 3-5 nodes each taking on writes of 10bytes-1MB of edits at rates of 100-5k writes a second.   Would BK be able to keep up?  What if is was a 10-20 node cluster?  What you think?&lt;/p&gt;</comment>
                            <comment id="12844627" author="fpj" created="Fri, 12 Mar 2010 19:18:11 +0000"  >&lt;p&gt;Those are good questions, Stack. BookKeeper scales throughput with the number of servers, so adding more bookies should increase your current throughput if your traffic is not saturating the network already. Consequently, if you don&apos;t have a constraint on the number of bookies you&apos;d like to use, your limitation would be the amount of bandwidth you have available. &lt;/p&gt;

&lt;p&gt;Just to give you some numbers, we have so far been able to saturate the network when writing around 1k bytes per entry, and the number of writes/s for 1k writes is of the order of tens of thousands for 3-5 bookies. Now, if I pick the largest numbers in your small example to consider the worst case (5 nodes, 1MB writes, 5k writes/s), then we would need a 40Gbit/s network, so I&apos;m not sure you can do it with any distributed system unless you write locally in all nodes, in which case you can&apos;t guarantee the data will be available upon a node crash. Let me know if I&apos;m misinterpreting your comment and you have something else in mind. &lt;/p&gt;

&lt;p&gt;I also have to mention that we added a feature to enable thousands of concurrent ledgers with minimal performance penalty on the writes, so I don&apos;t see any trouble in increasing the number of concurrent nodes as long as the BookKeeper cluster is provisioned accordingly. Of course, it would be great to measure it with hbase, though.&lt;/p&gt;</comment>
                            <comment id="12844628" author="breed" created="Fri, 12 Mar 2010 19:18:26 +0000"  >&lt;p&gt;one thing that would be great for BookKeeper is to try to characterize your load and then run some saturation tests to see how the performance looks. previously we varied messages sizes from 128 bytes to 8K. should we try something like 100 bytes, 1000 bytes, 10K, 100K, 1M?&lt;/p&gt;

&lt;p&gt;we should also vary the number of clients writing. what is a good number? 10-1000 or 10-10000? is the WAL per server or per region. &lt;/p&gt;

&lt;p&gt;do you have WAL throughput numbers for the throughput you are getting from HDFS?&lt;/p&gt;

&lt;p&gt;just to give you an idea, we are in the 10s of thousands of messages/sec for 1K messages. (the performance increases as you add more servers, aka bookies.) for larger messages it really is the network bandwidth that becomes the bottleneck.&lt;/p&gt;</comment>
                            <comment id="12844634" author="stack" created="Fri, 12 Mar 2010 19:36:16 +0000"  >&lt;p&gt;Thanks for the interesting comments.&lt;/p&gt;

&lt;p&gt;WAL is per regionserver, not per region.&lt;/p&gt;

&lt;p&gt;On WAL throughput, since the &apos;append&apos; to the WAL is what takes the bulk of the time making an hbase update, depending on hardware and how often we flush, we&apos;ll see variously 1-10k writes a second.  We wil see hdfs holding up writes on occasion when struggling.. with latency going way up.&lt;/p&gt;

&lt;p&gt;I&apos;m up for more exploration if you fellas are.   Let me take a look at this patch of yours Flavio.&lt;/p&gt;

&lt;p&gt;Thanks for happening by lads.&lt;/p&gt;</comment>
                            <comment id="12844635" author="ryanobjc" created="Fri, 12 Mar 2010 19:38:29 +0000"  >&lt;p&gt;A few more questions...&lt;/p&gt;

&lt;p&gt;What&apos;s the persistence story? How many nodes is the log data stored on?&lt;/p&gt;

&lt;p&gt;On performance, how about 100-200k ops/sec with data sizes about 150 bytes&lt;br/&gt;
or so? This would be aggregately generated on 20 nodes.&lt;/p&gt;

&lt;p&gt;On Mar 12, 2010 2:18 PM, &quot;Flavio Paiva Junqueira (JIRA)&quot; &amp;lt;jira@apache.org&amp;gt;&lt;br/&gt;
wrote:&lt;/p&gt;


&lt;p&gt;   [&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2315?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12844627#action_12844627&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-2315?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12844627#action_12844627&lt;/a&gt;]&lt;/p&gt;


&lt;p&gt;Flavio Paiva Junqueira commented on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2315&quot; title=&quot;BookKeeper for write-ahead logging&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2315&quot;&gt;&lt;del&gt;HBASE-2315&lt;/del&gt;&lt;/a&gt;:&lt;br/&gt;
-----------------------------------------------&lt;br/&gt;
Those are good questions, Stack. BookKeeper scales throughput with the&lt;br/&gt;
number of servers, so adding more bookies should increase your current&lt;br/&gt;
throughput if your traffic is not saturating the network already.&lt;br/&gt;
Consequently, if you don&apos;t have a constraint on the number of bookies you&apos;d&lt;br/&gt;
like to use, your limitation would be the amount of bandwidth you have&lt;br/&gt;
available.&lt;/p&gt;

&lt;p&gt;Just to give you some numbers, we have so far been able to saturate the&lt;br/&gt;
network when writing around 1k bytes per entry, and the number of writes/s&lt;br/&gt;
for 1k writes is of the order of tens of thousands for 3-5 bookies. Now, if&lt;br/&gt;
I pick the largest numbers in your small example to consider the worst case&lt;br/&gt;
(5 nodes, 1MB writes, 5k writes/s), then we would need a 40Gbit/s network,&lt;br/&gt;
so I&apos;m not sure you can do it with any distributed system unless you write&lt;br/&gt;
locally in all nodes, in which case you can&apos;t guarantee the data will be&lt;br/&gt;
available upon a node crash. Let me know if I&apos;m misinterpreting your comment&lt;br/&gt;
and you have something else in mind.&lt;/p&gt;

&lt;p&gt;I also have to mention that we added a feature to enable thousands of&lt;br/&gt;
concurrent ledgers with minimal performance penalty on the writes, so I&lt;br/&gt;
don&apos;t see any trouble in increasing the number of concurrent nodes as long&lt;br/&gt;
as the BookKeeper cluster is provisioned accordingly. Of course, it would be&lt;br/&gt;
great to measure it with hbase, though.&lt;/p&gt;


</comment>
                            <comment id="12844645" author="stack" created="Fri, 12 Mar 2010 19:54:48 +0000"  >&lt;p&gt;One other thing, can you speak to the following?&lt;/p&gt;

&lt;p&gt;HBase gets dissed on occasion because we are &apos;complicated&apos;, made up of different systems &amp;#8211; hdfs, zk, etc. &amp;#8211; so adding a new system (bk to do wals) would make this situation even worse?&lt;/p&gt;

&lt;p&gt;In our integration of zk, we did our best to hide it from user.  We manage start up and shutdown of ensemble for those who don&apos;t want to be bothered with managing their own zk ensemble.  Would such a thing be possible with a bk cluster?&lt;/p&gt;</comment>
                            <comment id="12844655" author="apurtell" created="Fri, 12 Mar 2010 20:10:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;On performance, how about 100-200k ops/sec with data sizes about 150 bytes or so? This would be aggregately generated on 20 nodes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Extend that to 100. What does this look like? Can BK cope? &lt;/p&gt;</comment>
                            <comment id="12844656" author="tlipcon" created="Fri, 12 Mar 2010 20:14:11 +0000"  >&lt;p&gt;Hey Flavio,&lt;/p&gt;

&lt;p&gt;Would you mind pointing us to (or uploading) a design doc / architectural overview for BK? I think that would be very helpful for us to understand the scaling characteristics better.&lt;/p&gt;

&lt;p&gt;-Todd&lt;/p&gt;</comment>
                            <comment id="12844727" author="mahadev" created="Fri, 12 Mar 2010 22:33:34 +0000"  >&lt;p&gt;todd,&lt;br/&gt;
 here is some documentation that might help you understand the architecture of BooKkeeper:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://hadoop.apache.org/zookeeper/docs/r3.2.2/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hadoop.apache.org/zookeeper/docs/r3.2.2/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;this documentation has been improved in 3.3 (not branched yet), so you can checkout &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://svn.apache.org/repos/asf/hadoop/zookeeper/trunk/docs/bookkeeperOverview.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/hadoop/zookeeper/trunk/docs/bookkeeperOverview.html&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;and read that as well. &lt;/p&gt;

&lt;p&gt;hope this helps.&lt;/p&gt;</comment>
                            <comment id="12844731" author="fpj" created="Fri, 12 Mar 2010 22:36:40 +0000"  >&lt;p&gt;Thanks for all comments and questions so far. Let me try to answer all in a batch:&lt;/p&gt;

&lt;p&gt;Stack: We are certainly interested in exploring, and learning your requirements would certainly help to guide our exploration. On making BK transparent to the user installing BK, I don&apos;t have any reason to think that it would be difficult to achieve. The script I use to start a bookie is very simple and we&apos;ll soon have the feature I was mentioning above of a bookie registering automatically with ZooKeeper. In any case, I made sure that it is possible to use BK optionally, so hdfs is still used by default with the current patch.&lt;/p&gt;

&lt;p&gt;Ryan, Andrew: The number of nodes going from 20 to a 100 is not a problem; We have tested BK with a few thousand concurrent ledgers. One important aspect to pay attention to is the length of writes. In our experience, BK is more efficient when we batch small requests. If it is possible to batch small requests, then I don&apos;t see an issue with obtaining the throughput figures you&apos;re asking for. In fact, with a previous version of BK, we were able to get 70kwrites/s for writes of 128 bytes. &lt;/p&gt;

&lt;p&gt;Todd: There is some documentation available on the ZooKeeper site, and we have added more for the next release (3.3.0). You can get the new documentation from trunk, but I&apos;ll also add a pdf here for your convenience. &lt;/p&gt;
</comment>
                            <comment id="12844736" author="apurtell" created="Fri, 12 Mar 2010 22:51:38 +0000"  >&lt;p&gt;@Flavio: Thanks for your kind reply.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In fact, with a previous version of BK, we were able to get 70kwrites/s for writes of 128 bytes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Dedicated hardware, I presume. How many BK nodes would be needed alongside HBase+Hadoop nodes to achieve 1M ops/sec over 100 concurrent ledgers? HBase WAL has group commit but that can only batch so much... depends on the order of edits coming from userland. So what amount of batching? (Average transaction size to BK for example.)&lt;/p&gt;</comment>
                            <comment id="12844759" author="breed" created="Fri, 12 Mar 2010 23:43:46 +0000"  >&lt;p&gt;to get performance you need a dedicated drive, not necessarily dedicated hardware. bk takes care of the batching for you. there is opportunistic batching that happens in the network channel and to the disk. (the later being the most important.)&lt;/p&gt;

&lt;p&gt;i don&apos;t think ryan&apos;s persistence question has been answered. when bk responds successfully to a write request, it means that the data has been synced to the requisite number of devices. when testing  we usually use 3 or 5 bookies with a replication factor of 2 or 3. (bookkeeper does striping.)&lt;/p&gt;

&lt;p&gt;we have an asynchronous write interface to allow high throughput by pipelining requests, we invoke completions as each write completes successfully.&lt;/p&gt;</comment>
                            <comment id="12844873" author="fpj" created="Sat, 13 Mar 2010 13:49:11 +0000"  >&lt;p&gt;Ryan: I don&apos;t have much to add to what Ben said in his comment. I just wanted to mention that in the current patch, I have added a configuration property to set the number of replicas for each write:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;hbase.wal.bk.quorumsize
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the default is 2.&lt;/p&gt;

&lt;p&gt;Andrew: As we reduce the number of bytes in each write, the overhead per byte increases, so batching writes and appending writes of the order of 1kbytes would give us a more efficient use of the BK client. Achieving 1M ops/s over 100 nodes (or larger values if you will) depends on the length of writes, the replication factor, and the amount of bandwidth (both I/O and network) you have available. In our observations, it is not a problem for a client to produce more than 10k appends/s of 1k-4kbytes, so in your example, it is just a matter of provisioning your system appropriately with respect to disk drives and network.  &lt;/p&gt;</comment>
                            <comment id="12844881" author="fpj" created="Sat, 13 Mar 2010 14:05:31 +0000"  >&lt;p&gt;Todd, I&apos;m attaching the BK overview document as promised.&lt;/p&gt;</comment>
                            <comment id="12846531" author="tlipcon" created="Wed, 17 Mar 2010 18:58:41 +0000"  >&lt;p&gt;Thanks for attaching the design doc, Flavio. Here are a few questions generally about BK (would be great to address these in the docs too, not just this JIRA):&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Logs are stored replicated on several bookies. If a bookie goes down or loses a disk, the replication count obviously decreases. Will BK re-replicate the &quot;under-replicated&quot; parts of the log files? Or do we assume that logs are short-lived enough that we&apos;ll never lose all the replicas during the span of time for which the log is necessary?&lt;/li&gt;
	&lt;li&gt;Similar to the above, is there any notion of rack awareness? eg does it ensure that each part of the log is replicated at least on two racks so that a full rack failure doesn&apos;t lose the logs?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basically on a broad level what I&apos;m asking is this: HDFS takes a lot of care to ensure availability of data &quot;forever&quot; across a variety of failure scenarios. From the design doc it wasn&apos;t clear that BK provides the same levels of safety. Can you please clarify what features (if any) are missing from BK that are present in HDFS with regard to reliability, etc?&lt;/p&gt;</comment>
                            <comment id="12846959" author="fpj" created="Thu, 18 Mar 2010 15:46:06 +0000"  >&lt;p&gt;Hi Todd, Great points:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;We plan to have a mechanism to re-replicate the ledger fragments of a bookie; some of our applications require it. There is a wiki page I had created with some initial thoughts, but I need to update it: &quot;http://wiki.apache.org/hadoop/BookieRecoveryPage&quot;. I also thought there was a jira open about it already, but I couldn&apos;t find it, so I created a new one: &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-712&quot; title=&quot;Bookie recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-712&quot;&gt;&lt;del&gt;ZOOKEEPER-712&lt;/del&gt;&lt;/a&gt;.&lt;/li&gt;
	&lt;li&gt;We currently don&apos;t have a notion of rack awareness, but it is a good idea and not difficult to add. We just need to have a bookie adding that information to its corresponding znode and use it during the selection of bookies upon creating a leader. I have also opened a jira to add it: &lt;a href=&quot;https://issues.apache.org/jira/browse/BOOKKEEPER-13&quot; title=&quot;Bookie rack awareness&quot; class=&quot;issue-link&quot; data-issue-key=&quot;BOOKKEEPER-13&quot;&gt;&lt;del&gt;ZOOKEEPER-711&lt;/del&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;On the documentation, please feel free to open a jira and mention all point that you believe could be improved.&lt;/p&gt;</comment>
                            <comment id="12868133" author="fpj" created="Mon, 17 May 2010 08:45:57 +0000"  >&lt;p&gt;Just a couple of quick points about the developments of this jira:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;We have introduced deletion and garbage collection of ledgers to BookKeeper (&lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-464&quot; title=&quot;Need procedure to garbage collect ledgers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-464&quot;&gt;&lt;del&gt;ZOOKEEPER-464&lt;/del&gt;&lt;/a&gt;);&lt;/li&gt;
	&lt;li&gt;Our next goal as far as features for bookkeeper goes is &lt;a href=&quot;https://issues.apache.org/jira/browse/ZOOKEEPER-712&quot; title=&quot;Bookie recovery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;ZOOKEEPER-712&quot;&gt;&lt;del&gt;ZOOKEEPER-712&lt;/del&gt;&lt;/a&gt; (see my previous post);&lt;/li&gt;
	&lt;li&gt;I have started benchmarking hbase+bookkeeper with the Yahoo! benchmark, but I stumbled upon a problem. When I wrote this preliminary patch, I didn&apos;t have a nice way of creating a unique znode name and keeping it for fetching the last log of a region server, given the current interface of WAL. My understanding of the code is that the hdfs file where the is stored is passed to the reader/writer. I&apos;m considering the current hdfs path as the znode path (or at least a part of it), but I would accept suggestions if anyone is willing to give me one.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="12868989" author="stack" created="Wed, 19 May 2010 04:50:34 +0000"  >&lt;p&gt;.bq I have started benchmarking hbase+bookkeeper with the Yahoo! benchmark, but I stumbled upon a problem. When I wrote this preliminary patch, I didn&apos;t have a nice way of creating a unique znode name and keeping it for fetching the last log of a region server, given the current interface of WAL. My understanding of the code is that the hdfs file where the is stored is passed to the reader/writer. I&apos;m considering the current hdfs path as the znode path (or at least a part of it), but I would accept suggestions if anyone is willing to give me one.&lt;/p&gt;

&lt;p&gt;The HDFS path should map to a znode path?  That&apos;d work.  The WAL HDFS path has the regionserver name in it IIRC.  You probably don&apos;t need the full thing.  Just from the regionserver name on down (should be shallow).&lt;/p&gt;

&lt;p&gt;Let us know if you want us to change stuff in here around WAL.  You might want to wait a day or so till hbase-2437 goes in (refactoring of hlog).  Stuff should be easier to grok after that patch has had its way.&lt;/p&gt;

&lt;p&gt;Is &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2527&quot; title=&quot;Add the ability to easily extend some HLog actions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2527&quot;&gt;&lt;del&gt;HBASE-2527&lt;/del&gt;&lt;/a&gt; of any use to you?  It just went into trunk.&lt;/p&gt;</comment>
                            <comment id="12898424" author="breed" created="Fri, 13 Aug 2010 21:31:07 +0000"  >&lt;p&gt;we looked into the problem of figuring out the path to use for the WAL and found the following: it appears that the assumption that the WAL is stored in HDFS is embedded in HBase. when looking up a WAL, for example, the FileSystem object is used to check existence. Deletion of logs also happens outside of the WAL interfaces. to be truly pluggable a WAL interface should be used to enumerate and delete logs. have you guys thought about doing this?&lt;/p&gt;</comment>
                            <comment id="12898435" author="stack" created="Fri, 13 Aug 2010 21:59:21 +0000"  >&lt;p&gt;No.&lt;/p&gt;

&lt;p&gt;If you want us to switch to an interface, just say (will happen faster if you put up a patch).&lt;/p&gt;</comment>
                            <comment id="13188821" author="zhihyu@ebaysf.com" created="Thu, 19 Jan 2012 00:02:25 +0000"  >&lt;p&gt;@Flavio:&lt;br/&gt;
I searched Omid code and didn&apos;t find BookKeeperLogWriter being used.&lt;/p&gt;

&lt;p&gt;If you still prefer to add BookKeeperLog&lt;/p&gt;
{Writer,Reader}
&lt;p&gt; to HBase, please provide an updated patch (including pom.xml).&lt;/p&gt;</comment>
                            <comment id="13189041" author="fpj" created="Thu, 19 Jan 2012 10:27:25 +0000"  >&lt;p&gt;Hi Zhihong, Thanks for the bringing this up, and yes, I&apos;m still interested in providing an updated patch. I&apos;ll work on one.&lt;/p&gt;</comment>
                            <comment id="13399179" author="fpj" created="Fri, 22 Jun 2012 07:11:16 +0000"  >&lt;p&gt;Apologies for not being very active here. Yesterday we had a chat with Stack about moving forward with this jira. Here are a few key points I got from the discussion:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;To use the reader and writer interfaces, we need to implement a BookKeeper filesystem because the init method expects such an object;&lt;/li&gt;
	&lt;li&gt;The RegionServer maintains a list of existing logs and we need to map them to ledgers. The easiest way is probably to maintain this map in ZooKeeper and manage it through the Filesystem implementation;&lt;/li&gt;
	&lt;li&gt;Currently there is one single FileSystem object in the RegionServer and we would need to have at least two, one being used for BookKeeper;&lt;/li&gt;
	&lt;li&gt;Stack suggested that it would be a great idea to to have logs per region with BookKeeper. BookKeeper is designed exactly to serve use cases that have multiple concurrent logs being written to. According to Stack, this feature has the potential of reducing time to recover significantly. Enabling this feature might require some more changes to the wal interface, though, since we would need to separate the edits according to region. (Related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4529&quot; title=&quot;Make WAL Pluggable &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4529&quot;&gt;&lt;del&gt;HBASE-4529&lt;/del&gt;&lt;/a&gt; ?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Let us know if anyone has a comment, and otherwise we will push it forward.&lt;/p&gt;</comment>
                            <comment id="13399207" author="zhihyu@ebaysf.com" created="Fri, 22 Jun 2012 08:23:25 +0000"  >&lt;p&gt;Nice summary, Flavio.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;to have logs per region with BookKeeper&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4529&quot; title=&quot;Make WAL Pluggable &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4529&quot;&gt;&lt;del&gt;HBASE-4529&lt;/del&gt;&lt;/a&gt; has not been active.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; is the one receiving attention recently.&lt;br/&gt;
Does this JIRA depend on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="13399212" author="fpj" created="Fri, 22 Jun 2012 08:34:05 +0000"  >&lt;p&gt;Hi Ted, I don&apos;t fully understand what &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; is proposing. It mixes parallel writes and HDFS. In my perspective, we are able to perform efficient parallel writes out of the box and we can benefit from an interface that exposes enough information for us to separate the edits into different logs (e.g., one per region). If the outcome of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; is a set of changes to the interface that allow this to happen, then we could benefit from it. For a first cut of this issue, I don&apos;t think we need parallel writes, though, so I would say it is not dependent, but I&apos;d be happy to hear a different opinion.   &lt;/p&gt;</comment>
                            <comment id="13399220" author="zhihyu@ebaysf.com" created="Fri, 22 Jun 2012 08:48:59 +0000"  >&lt;p&gt;Here is the JIRA that tracks parallel hdfs writes:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; Allow parallel HDFS writes for HLogs&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; would allow multiple WALs per region server. Grouping of edits can be per-table or, one WAL per region.&lt;br/&gt;
WAL interface abstraction appears in the discussion there.&lt;/p&gt;

&lt;p&gt;Feel free to propose WAL interface for this JIRA.&lt;/p&gt;</comment>
                            <comment id="13400170" author="jmhsieh" created="Sun, 24 Jun 2012 15:50:57 +0000"  >&lt;p&gt;@flavio&lt;/p&gt;

&lt;p&gt;Li&apos;s been inactive for a while so I&apos;d suggest taking &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5937&quot; title=&quot;Refactor HLog into an interface.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5937&quot;&gt;&lt;del&gt;HBASE-5937&lt;/del&gt;&lt;/a&gt; and making the WAL an interface.  This would allow for experimentation here and with other alternate wal implementation related JIRA&apos;s &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6116&quot; title=&quot;Allow parallel HDFS writes for HLogs.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6116&quot;&gt;&lt;del&gt;HBASE-6116&lt;/del&gt;&lt;/a&gt; is about a potentially improving the latency when writing to a single hdfs file.  Normally writes to hdfs get pipelined to 2 other replicas on 2 different data nodes and this incurs some overhead.  The intuition behind that is that for small writes the overhead may cause more latency than the data transfer and the hope is that by having the client send in a parallel/broadcast we&apos;d reduce the overhead latency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5699&quot; title=&quot;Run with &amp;gt; 1 WAL in HRegionServer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5699&quot;&gt;&lt;del&gt;HBASE-5699&lt;/del&gt;&lt;/a&gt; is about improving throughput and improving latency by writing to multiple hdfs files.  This attacks the scenarios where a data node may be blocked (gc/swapping/etc) causing all other writes to be blocked.  Currently a region server serves many regions but all of a region server&apos;s writes go to a single WAL files with writes to different regions are intermingled.   &lt;/p&gt;

&lt;p&gt;Let&apos;s say these normally take 10ms but in a gc case it ends up taking 1000ms.  In one scenario with a wal per region, writes to another region could go a different hdfs file and not get blocked. &lt;/p&gt;

&lt;p&gt;In another, we&apos;d have two wal files for the region server and we could detect that one write was blocking (let&apos;s say after 20ms) then the retry against another set of data nodes.  Ideally we&apos;ve improved our worst case from 1000ms to 30-40ms.&lt;/p&gt;

</comment>
                            <comment id="13400406" author="fpj" created="Mon, 25 Jun 2012 10:53:56 +0000"  >&lt;p&gt;Hi Jonathan, A BK backend for logging can potentially deal with the issues you&apos;re raising. The main issue is that it doesn&apos;t expose a file system interface, and HBase currently seems to rely upon one. If we have an interface that matches more naturally the BK api, then we will possibly be able to deal with the issues you&apos;re raising without much effort.&lt;/p&gt;

&lt;p&gt;I was actually trying to implement a BookKeeperFS class that extends FileSystem, but it doesn&apos;t feel natural. I was wondering if we should try to work on the interface first instead of hacking it in by implementing an HLog.Reader and an HLog.Writer. &lt;/p&gt;</comment>
                            <comment id="13400616" author="jesse_yates" created="Mon, 25 Jun 2012 16:52:33 +0000"  >&lt;p&gt;personally, I&apos;d prefer to see a logical higher-level WAL interface and then have an FsWAL subclass that might have some more specifics to make an easier match to an underlying FS implementation (hadoop or otherwise). Maybe a bit more pain upfront, but will (likely) pay off in the end).&lt;/p&gt;</comment>
                            <comment id="13400722" author="jmhsieh" created="Mon, 25 Jun 2012 17:50:55 +0000"  >&lt;p&gt;+1 to what Jesse said.  The HLog class is relatively well encapsulated.  It already has its own Reader and Writer interfaces (I&apos;d probably ignore the FS argument or some how hide BK specific stuff into a constructor), and a Metric helper class.  Excluding its constructors, HLog only has a few hdfs-specific public methods (hsync, hflush, sync) and some public but static methods.&lt;/p&gt;</comment>
                            <comment id="13400864" author="fpj" created="Mon, 25 Jun 2012 20:38:00 +0000"  >&lt;p&gt;If I understand Jesse&apos;s proposal, the idea is to have a WAL interface and essentially bring all hdfs dependent WAL code under FsWAL? We would then implement an equivalent BkWAL for a BookKeeper backend? &lt;/p&gt;

&lt;p&gt;I didn&apos;t understand the point about the HLog class being relatively well encapsulated, Jonathan. Do you mean to say that it would be relatively easy to implement FsWAL by using HLog?&lt;/p&gt;
</comment>
                            <comment id="13400870" author="jesse_yates" created="Mon, 25 Jun 2012 20:41:44 +0000"  >&lt;p&gt;@Flavio  - yup, that&apos;s what I was getting at.&lt;/p&gt;</comment>
                            <comment id="13400891" author="jmhsieh" created="Mon, 25 Jun 2012 20:57:37 +0000"  >&lt;p&gt;@Flavio - with the encapsulation comment &amp;#8211; basically if you look at the interface there is very little that is hdfs-specific about it &amp;#8211; append, open, close, roll operations all basically take hbase constructs and could have any implementation behind it.   The hdfs-specifics have been contained in its constructor and the init functions of the reader/writer classes.&lt;/p&gt;</comment>
                            <comment id="13400908" author="fpj" created="Mon, 25 Jun 2012 21:14:03 +0000"  >&lt;p&gt;@Jonathan My issue has been exactly with the initialization of the log reader and writer. They take a FileSystem and a Path as input parameters. As I mentioned above, but I&apos;ve been looking at implementing a BK FileSystem, and it looks a bit hacky. &lt;/p&gt;

&lt;p&gt;If I get your suggestion, you&apos;re saying that we could reuse most of HLog and focus on generalizing its constructors and the init methods. Is it right?&lt;/p&gt;</comment>
                            <comment id="13400967" author="jmhsieh" created="Mon, 25 Jun 2012 22:15:18 +0000"  >&lt;p&gt;@Flavio&lt;/p&gt;

&lt;p&gt;I&apos;m suggesting you use the Factory pattern to create the HLog, and have in the config file that would have it instantiate a new BKHLog (doesn&apos;t even use FS or Path) that returns its own instances of BKReader/BKWriters. &lt;/p&gt;

&lt;p&gt;I only see a handful of non-test places where HLogs are constructed so this part shouldn&apos;t be too hard to make factory pluggable:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java#L1267&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java#L1267&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java#L3719&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java#L3719&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java#L161&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java#L161&lt;/a&gt;&lt;br/&gt;
(I think there are a few more)..&lt;/p&gt;

&lt;p&gt;My guess is that it will take a little bit of work but won&apos;t be too onerous to do.&lt;/p&gt;</comment>
                            <comment id="13411998" author="zhihyu@ebaysf.com" created="Wed, 11 Jul 2012 21:10:08 +0000"  >&lt;p&gt;@Flavio:&lt;br/&gt;
Looking at the attached patch:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void init(FileSystem fs, Path path, Configuration conf){
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Parameter fs isn&apos;t used.&lt;/p&gt;

&lt;p&gt;Further, you implemented HLog.Reader and HLog.Writer. I don&apos;t see where HLog is constructed.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13412065" author="fpj" created="Wed, 11 Jul 2012 22:20:26 +0000"  >&lt;p&gt;Hi Ted, fs is part of the issue I was discussing before. We don&apos;t have a filesystem implementation for bookkeeper, so we can&apos;t use the filesystem instance passed.&lt;/p&gt;

&lt;p&gt;About the reader and the writer, I was configuring them in the hbase-default configuration file:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.regionserver.hlog.reader.impl&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.hadoop.hbase.regionserver.wal.BookKeeperLogReader&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;The HLog file reader implementation.&amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hbase.regionserver.hlog.writer.impl&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.hadoop.hbase.regionserver.wal.BookKeeperLogWriter&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;The HLog file writer implementation.&amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I assumed previously that HLog was instantiated elsewhere.&lt;/p&gt;</comment>
                            <comment id="13412441" author="zhihyu@ebaysf.com" created="Thu, 12 Jul 2012 01:29:30 +0000"  >&lt;p&gt;The ctor of HLog takes a FileSystem parameter. Since the FileSystem isn&apos;t important to bookkeeper, my feeling is that the approach in previous patch makes sense.&lt;/p&gt;

&lt;p&gt;You can remodel that patch by introducing a new hbase module.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13412829" author="stack" created="Thu, 12 Jul 2012 14:22:10 +0000"  >&lt;p&gt;Flavio:&lt;/p&gt;

&lt;p&gt;You have what you need?  Jesse and Jon I think did a good job above filling in bits I waved hands over when we talked (I like Jesse&apos;s suggestion undoing even our dependency of FS).&lt;/p&gt;

&lt;p&gt;When we talked, we also figured that hbase would need access to bk or the bkfs or the bk interface at replay of edits time, on region open.  In particular, we&apos;d need to have a factory like the WAL log factory here abouts in the code: &lt;a href=&quot;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#2721&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/HRegion.html#2721&lt;/a&gt;  This is called when the region is opened by the regionserver.  It does all its init and as part of the init, it looks to see if there are edits to replay.  If edits-to-replay are out in bk, we need to be able to get at them at this very point.&lt;/p&gt;</comment>
                            <comment id="13428164" author="fpj" created="Fri, 3 Aug 2012 15:08:50 +0000"  >&lt;p&gt;Based on your feedback and our own observations from inspecting the code, here is a rough idea of what we would like to do.&lt;/p&gt;

&lt;p&gt;In the first step, we make HLog an interface exposing the public methods of the current HLog class and make the current HLog class an implementation of the interface. We also create an HLog factory to allow us to instantiate different HLog implementations. Eventually we will have this factory creating BKHLog when we tell it to do so via configuration. So far there is no new functionality.&lt;/p&gt;

&lt;p&gt;In the second step, we implement BKHLog and decide what to do with the splitter. It is still not entirely clear how to adapt the splitter to BK. Perhaps we don&apos;t need a splitter at all with BookKeeper?&lt;/p&gt;

&lt;p&gt;Let me know if there is any comment about these steps. Otherwise, I&apos;ll create two subtasks and start working on the first.&lt;/p&gt;</comment>
                            <comment id="13428172" author="zhihyu@ebaysf.com" created="Fri, 3 Aug 2012 15:15:34 +0000"  >&lt;p&gt;The plan is good.&lt;br/&gt;
Please use &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5937&quot; title=&quot;Refactor HLog into an interface.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5937&quot;&gt;&lt;del&gt;HBASE-5937&lt;/del&gt;&lt;/a&gt; for the first step.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13428299" author="lhofhansl" created="Fri, 3 Aug 2012 19:02:51 +0000"  >&lt;p&gt;Sounds like fine plan. If you write per region logs you do not need to split at all, but I don&apos;t know whether that is feasible to do in BK or not.&lt;/p&gt;</comment>
                            <comment id="14064059" author="stack" created="Wed, 16 Jul 2014 20:44:06 +0000"  >&lt;p&gt;Not being worked on.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12551766">HBASE-5843</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12553865">HBASE-5937</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12553865">HBASE-5937</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12438621" name="HBASE-2315.patch" size="14560" author="fpj" created="Fri, 12 Mar 2010 16:27:29 +0000"/>
                            <attachment id="12438680" name="bookkeeperOverview.pdf" size="142977" author="fpj" created="Sat, 13 Mar 2010 14:05:31 +0000"/>
                            <attachment id="12438622" name="zookeeper-dev-bookkeeper.jar" size="153733" author="fpj" created="Fri, 12 Mar 2010 16:30:55 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 12 Mar 2010 18:06:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32512</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 22 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hh7z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100052</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>