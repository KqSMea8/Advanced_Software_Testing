<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:41:26 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-13260/HBASE-13260.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-13260] Bootstrap Tables for fun and profit </title>
                <link>https://issues.apache.org/jira/browse/HBASE-13260</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Over at the ProcV2 discussions(&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12439&quot; title=&quot;Procedure V2&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-12439&quot;&gt;HBASE-12439&lt;/a&gt;) and elsewhere I was mentioning an idea where we may want to use regular old regions to store/persist some data needed for HBase master to operate. &lt;/p&gt;

&lt;p&gt;We regularly use system tables for storing system data. acl, meta, namespace, quota are some examples. We also store the table state in meta now. Some data is persisted in zk only (replication peers and replication state, etc). We are moving away from zk as a permanent storage. As any self-respecting database does, we should store almost all of our data in HBase itself. &lt;/p&gt;

&lt;p&gt;However, we have an &quot;availability&quot; dependency between different kinds of data. For example all system tables need meta to be assigned first. All master operations need ns table to be assigned, etc. &lt;/p&gt;

&lt;p&gt;For at least two types of data, (1) procedure v2 states, (2) RS groups in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6721&quot; title=&quot;RegionServer Group based Assignment&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6721&quot;&gt;&lt;del&gt;HBASE-6721&lt;/del&gt;&lt;/a&gt; we cannot depend on meta being assigned since &quot;assignment&quot; itself will depend on accessing this data. The solution in (1) is to implement a custom WAL format, and custom recover lease and WAL recovery. The solution in (2) is to have the table to store this data, but also cache it in zk for bootrapping initial assignments. &lt;/p&gt;

&lt;p&gt;For solving both of the above (and possible future use cases if any), I propose we add a &quot;boostrap table&quot; concept, which is: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A set of predefined tables hosted in a separate dir in HDFS.&lt;/li&gt;
	&lt;li&gt;A table is only 1 region, not splittable&lt;/li&gt;
	&lt;li&gt;Not assigned through regular assignment&lt;/li&gt;
	&lt;li&gt;Hosted only on 1 server (typically master)&lt;/li&gt;
	&lt;li&gt;Has a dedicated WAL.&lt;/li&gt;
	&lt;li&gt;A service does WAL recovery + fencing for these tables.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This has the benefit of using a region to keep the data, but frees us to re-implement caching and we can use the same WAL / Memstore / Recovery mechanisms that are battle-tested. &lt;/p&gt;



</description>
                <environment></environment>
        <key id="12782435">HBASE-13260</key>
            <summary>Bootstrap Tables for fun and profit </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="enis">Enis Soztutar</assignee>
                                    <reporter username="enis">Enis Soztutar</reporter>
                        <labels>
                    </labels>
                <created>Tue, 17 Mar 2015 01:23:44 +0000</created>
                <updated>Sat, 18 Jun 2016 00:10:16 +0000</updated>
                                                            <fixVersion>2.0.0</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>16</watches>
                                                                                                            <comments>
                            <comment id="14364383" author="enis" created="Tue, 17 Mar 2015 01:32:07 +0000"  >&lt;p&gt;Here is a patch to demonstrate what I propose. &lt;/p&gt;

&lt;p&gt;&lt;tt&gt;BootstrapTableService&lt;/tt&gt; implements a set of known tables, together with a separate WAL directory and data directory. WAL recovery is not implemented yet, but should be straightforward to implement. These regions are not &quot;assigned&quot;, but opened instead. We can make it so that they share the same resources with the rest of the RS running inside master (periodic memstore flusher, global memstore tracker, etc). &lt;/p&gt;

&lt;p&gt;&lt;tt&gt;RegionProcedureStore&lt;/tt&gt; is an implementation of &lt;tt&gt;ProcedureStore&lt;/tt&gt; and can be used instead of &lt;tt&gt;WALProcedureStore&lt;/tt&gt; on top of the patch in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13202&quot; title=&quot;Procedure v2 - core framework&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13202&quot;&gt;&lt;del&gt;HBASE-13202&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Let me know what do you think? I can pursue this approach, and polish / harden it for Procv2 and RS groups. If you think that this is not a good idea, now would be the best time to say so. &lt;/p&gt;</comment>
                            <comment id="14364423" author="apurtell" created="Tue, 17 Mar 2015 02:09:01 +0000"  >&lt;p&gt;This is also a nice idea for a time when we might not be backed by a filesystem but instead block chains. We&apos;d need similar properties then... single region/not splittable, a well known location, a bootstrap service.&lt;/p&gt;</comment>
                            <comment id="14364591" author="stack" created="Tue, 17 Mar 2015 05:11:59 +0000"  >&lt;p&gt;Seems heavyweight for keeping around edits but then, as you say, Region should just &apos;work&apos; by now.&lt;/p&gt;

&lt;p&gt;Looking at patch, I think I like it &amp;#8211; not much code and we have &apos;working&apos; store.  Would be interested in &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; take.&lt;/p&gt;

&lt;p&gt;Region has enough configuration knobs, we should be able to configure it to suit. Can we age out whole hfiles at a time or is that not done yet? Or will Master be doing heavyweight compacting when a fat assign dumps a bunch of hfiles under one of these system regions? I am against Master hosting Regions (because it puts Master in the write path, etc.) but having these little oddballs hosted in Master only seems like a nice boundary to put on them.&lt;/p&gt;

&lt;p&gt;On the patch, are we opening the region each time we scan it?&lt;/p&gt;

&lt;p&gt;Not you, but it is kinda odd that we have to have a &apos;table&apos; associated.&lt;/p&gt;
</comment>
                            <comment id="14365258" author="stack" created="Tue, 17 Mar 2015 14:54:59 +0000"  >&lt;p&gt;For consideration, lets say we have a master-private table to host assignment transitions &amp;#8211; we&apos;d write this internal table rather than update hbase:meta &amp;#8211; and say we have a 1M regions and we are starting up the cluster.  Let us say there are 4 transitions getting a region on line: OFFLINE, OPENING, OPENING, OPEN, done (after updating hbase:meta). So we&apos;ll need to do 4 million writes of at least a region name + its transition state &amp;#8211; lets call the serialization 128 bytes give or take (we could record just regionid....to cut it down &amp;#8211; in a short amount of time; about half a gigabyte of data. &lt;/p&gt;

&lt;p&gt;Would be worthwhile putting up a test on a single region to see how it does with a load like this, how many hfiles, what compactions are like.&lt;/p&gt;</comment>
                            <comment id="14499457" author="enis" created="Fri, 17 Apr 2015 08:22:56 +0000"  >&lt;p&gt;Here is a more complete patch for consideration: &lt;a href=&quot;https://reviews.apache.org/r/33293/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/33293/&lt;/a&gt;. It does some refactoring, and undos the WAL proc store in favor of the region-based one (the reason being we may not want an alternative WAL format). &lt;/p&gt;

&lt;p&gt;I think for DDL ops for master, what store is used for performance is irrelevant. For assignment, the store might become the bottleneck, but in case of fast procs, there will not be flushes for most of the time (since procs will get deleted from memstore). So WAL is the bottleneck. Between the WAL format in WALProcStore and the FSHLog, I did not check which one has more overhead vs write speed. But both will have to go thru wal IO.  &lt;/p&gt;</comment>
                            <comment id="14499474" author="mbertozzi" created="Fri, 17 Apr 2015 08:33:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;For assignment, the store might become the bottleneck, but in case of fast procs, there will not be flushes for most of the time (since procs will get deleted from memstore). So WAL is the bottleneck. Between the WAL format in WALProcStore and the FSHLog, I did not check which one has more overhead vs write speed. But both will have to go thru wal IO.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;aside compactions, I don&apos;t think the perf problem me and stack raised is around adding procs to the WAL, but it is on replay. With the current proc wal, we can throw the wal away at any point if we know that all the procs are deleted or all of them are in the latest wal. with the region wal as far as I know we have to keep all the wals around until we have a flush. to with the region the wal replay will be much more heavier. but maybe we can do some tricks there too. e.g. if the memstore is empty after deletes roll the wal and archive the others&lt;/p&gt;</comment>
                            <comment id="14500363" author="enis" created="Fri, 17 Apr 2015 18:26:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;not be flushes for most of the time (since procs will get deleted from memstore).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Devaraj, offline, pointed out that this is not true, duh! The deletes of procedures is still a tombstone which increases the memstore size. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;aside compactions, I don&apos;t think the perf problem me and stack raised is around adding procs to the WAL, but it is on replay&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Valid point. We have the regular region mechanics at play here. We can also throw away WALs, once the data is flushed, the flush seqId will be the way to skip replaying entries from WAL. I think if we enable periodic flushes, it might help with more regular flushes to get rid of WAL files. We can put another filter to the replay process to the same affect if needed. Let me spend some time in testing with millions of procs. &lt;/p&gt;</comment>
                            <comment id="14500624" author="stack" created="Fri, 17 Apr 2015 20:39:31 +0000"  >&lt;p&gt;Skimmed patch. EmbeddedDB freaked me out at first but the more I looked at it, the more sense it made &amp;#8211; smile (Implementing Admin might be OTT?). How do you think we will do memstore/WAL/compaction &apos;tricks&apos; if hosting RS provides memstore flushing and compaction services &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="14500993" author="enis" created="Sat, 18 Apr 2015 01:06:56 +0000"  >&lt;p&gt;Assuming regular normal operation, and no other bootstrap table, the WAL will only contain the proc region. On graceful HMaster stop, the WAL will be moved to the archive directory, so a backup master / new master will not have to replay anything. On HMaster abort or die, the next master will have to replay the WALs that is in the prev WAL directory. We do not keep lastFlushedSeqId or anything around so, whatever left in the WAL dir is replayed from beginning. However, since there is a single region, every time we flush, we can get rid of WAL files that is not needed anymore. We can have a WAL roll size of X, and a flush size of Y (assuming WAL and memstore sizes are similar), then we only have ceil(Y / X) many WAL files of size X around to read. &lt;/p&gt;</comment>
                            <comment id="14505951" author="enis" created="Tue, 21 Apr 2015 22:46:57 +0000"  >&lt;p&gt;I ran a super simple mini benchmark on my MBP to understand the differences between the WAL based store and region based store. Attaching a simple patch for the test. &lt;/p&gt;

&lt;p&gt;It is numThreads inserting a dummy procedure and deleting that procedure from the store (which is the mini clusters store). Below is the output. I was not able to insert 1M to WALProcStore, inserting 10K takes around  120sec with 50 threads. &lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;    num_procs     &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;                     5 &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;10 &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; 30 &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;50 &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;region proc store &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1M &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   78s  &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    ~68s &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   ~200 &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   ~300s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;wal proc store &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 10K &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  ?  &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   7s  &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   ~94s &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   ~120s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I have also observed these with WAL store with 50 threads: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2015-04-21 15:31:48,294 DEBUG [localhost,50957,1429655505598_splitLogManager__ChoreService_1] zookeeper.ZKSplitLog(184): Garbage collecting all recovering region znodes
java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker$BitSetNode.updateState(ProcedureStoreTracker.java:325)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker$BitSetNode.update(ProcedureStoreTracker.java:101)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.insert(ProcedureStoreTracker.java:357)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.insert(ProcedureStoreTracker.java:343)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.insert(WALProcedureStore.java:301)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 15:32:46,450 DEBUG [localhost,50957,1429655505598_ChoreService_1] compactions.PressureAwareCompactionThroughputController(148): compactionPressure is 0.0, tune compaction throughput to 10.00 MB/sec
2015-04-21 15:32:46,450 DEBUG [localhost,50959,1429655505784_ChoreService_1] compactions.PressureAwareCompactionThroughputController(148): compactionPressure is 0.0, tune compaction throughput to 10.00 MB/sec
java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker$BitSetNode.updateState(ProcedureStoreTracker.java:325)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker$BitSetNode.update(ProcedureStoreTracker.java:101)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.insert(ProcedureStoreTracker.java:357)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.insert(ProcedureStoreTracker.java:343)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.insert(WALProcedureStore.java:301)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker$BitSetNode.updateState(ProcedureStoreTracker.java:325)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker$BitSetNode.update(ProcedureStoreTracker.java:101)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.insert(ProcedureStoreTracker.java:357)
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.insert(ProcedureStoreTracker.java:343)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.insert(WALProcedureStore.java:301)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
...
2015-04-21 15:33:46,449 DEBUG [localhost,50957,1429655505598_ChoreService_1] compactions.PressureAwareCompactionThroughputController(148): compactionPressure is 0.0, tune compaction throughput to 10.00 MB/sec
2015-04-21 15:33:46,449 DEBUG [localhost,50959,1429655505784_ChoreService_1] compactions.PressureAwareCompactionThroughputController(148): compactionPressure is 0.0, tune compaction throughput to 10.00 MB/sec
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException
Wrote 10000 procedures in 166299 ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When run with &amp;lt;10 threads, I see a different behavior. The WAL gets rolled very frequently. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2015-04-21 15:42:21,976 INFO  [IPC Server handler 5 on 51193] blockmanagement.BlockManager(1074): BLOCK* addToInvalidates: blk_1073741835_1011 127.0.0.1:51194 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 15:42:21,989 INFO  [IPC Server handler 9 on 51193] blockmanagement.BlockManager(2383): BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:51194 is added to blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-5bb1bfab-5e56-49e0-ab98-e4d497d85c83:NORMAL|RBW]]} size 385
2015-04-21 15:42:21,989 INFO  [pool-57-thread-1] wal.WALProcedureStore(549): Roll &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; state log: 3
2015-04-21 15:42:21,990 INFO  [pool-57-thread-1] wal.WALProcedureStore(571): Remove all state logs with ID less then 2
2015-04-21 15:42:21,990 DEBUG [pool-57-thread-1] wal.WALProcedureStore(584): remove log: hdfs:&lt;span class=&quot;code-comment&quot;&gt;//localhost:51193/user/enis/test-data/de5f1aa9-4a7d-4871-88b5-7bb10b58c159/MasterProcWALs/state-00000000000000000002.log
&lt;/span&gt;2015-04-21 15:42:21,991 INFO  [IPC Server handler 0 on 51193] blockmanagement.BlockManager(1074): BLOCK* addToInvalidates: blk_1073741839_1015 127.0.0.1:51194 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.delete(ProcedureStoreTracker.java:373)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.delete(WALProcedureStore.java:369)
	at org.apache.hadoop.hbase.procedure2.ProcedureStoreTest$Worker.run(ProcedureStoreTest.java:109)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 15:42:22,012 DEBUG [localhost,51201,1429656139456_splitLogManager__ChoreService_1] zookeeper.ZKSplitLog(184): Garbage collecting all recovering region znodes
2015-04-21 15:42:22,384 INFO  [IPC Server handler 6 on 51193] blockmanagement.BlockManager(2383): BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:51194 is added to blk_1073741840_1016{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ddc64396-2e84-4e38-b3c0-fa10da1a1c44:NORMAL|RBW]]} size 36342
2015-04-21 15:42:22,787 INFO  [pool-57-thread-6] wal.WALProcedureStore(549): Roll &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; state log: 4
2015-04-21 15:42:22,788 INFO  [pool-57-thread-6] wal.WALProcedureStore(571): Remove all state logs with ID less then 3
2015-04-21 15:42:22,788 DEBUG [pool-57-thread-6] wal.WALProcedureStore(584): remove log: hdfs:&lt;span class=&quot;code-comment&quot;&gt;//localhost:51193/user/enis/test-data/de5f1aa9-4a7d-4871-88b5-7bb10b58c159/MasterProcWALs/state-00000000000000000003.log
&lt;/span&gt;...
2015-04-21 15:42:41,724 INFO  [IPC Server handler 4 on 51193] blockmanagement.BlockManager(2383): BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:51194 is added to blk_1073741888_1064{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-ddc64396-2e84-4e38-b3c0-fa10da1a1c44:NORMAL|RBW]]} size 385
2015-04-21 15:42:41,724 INFO  [pool-57-thread-6] wal.WALProcedureStore(549): Roll &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; state log: 52
2015-04-21 15:42:41,724 INFO  [pool-57-thread-6] wal.WALProcedureStore(571): Remove all state logs with ID less then 51
2015-04-21 15:42:41,725 DEBUG [pool-57-thread-6] wal.WALProcedureStore(584): remove log: hdfs:&lt;span class=&quot;code-comment&quot;&gt;//localhost:51193/user/enis/test-data/de5f1aa9-4a7d-4871-88b5-7bb10b58c159/MasterProcWALs/state-00000000000000000051.log
&lt;/span&gt;2015-04-21 15:42:41,725 INFO  [IPC Server handler 0 on 51193] blockmanagement.BlockManager(1074): BLOCK* addToInvalidates: blk_1073741888_1064 127.0.0.1:51194 

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; do you want to take a look at the above exceptions? &lt;/p&gt;

&lt;p&gt;I have also seen an unexpected state coming from FSHLog with 50 threads appending. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt; I think you are most familiar with this area, do you mind taking a look? The ring buffer queue is filling up (maybe due to 50 threads appending?). If the queue being full is a valid condition, we should be handling gracefully? &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2015-04-21 14:29:11,395 DEBUG [pool-57-thread-7] regionserver.HRegion(3609): rollbackMemstore rolled back 1
2015-04-21 14:29:11,396 DEBUG [pool-57-thread-10] regionserver.HRegion(3609): rollbackMemstore rolled back 1
2015-04-21 14:29:11,395 DEBUG [pool-57-thread-2] regionserver.HRegion(3609): rollbackMemstore rolled back 1
2015-04-21 14:29:11,395 DEBUG [pool-57-thread-41] regionserver.HRegion(3609): rollbackMemstore rolled back 1
2015-04-21 14:29:11,395 DEBUG [pool-57-thread-39] regionserver.HRegion(3609): rollbackMemstore rolled back 1
2015-04-21 14:29:11,396 WARN  [sync.2] wal.FSHLog$SyncRunner(1360): UNEXPECTED, continuing
java.lang.IllegalStateException
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.releaseSyncFuture(FSHLog.java:1261)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.releaseSyncFutures(FSHLog.java:1276)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run(FSHLog.java:1350)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 14:29:11,396 ERROR [localhost:49706.activeMasterManager.append-pool1-t1] wal.FSHLog$RingBufferEventHandler(1979): UNEXPECTED!!! syncFutures.length=5
java.lang.IllegalStateException: Queue full
	at java.util.AbstractQueue.add(AbstractQueue.java:98)
	at java.util.AbstractQueue.addAll(AbstractQueue.java:187)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.offer(FSHLog.java:1249)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1971)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 14:29:11,398 WARN  [sync.2] wal.FSHLog$SyncRunner(1360): UNEXPECTED, continuing
java.lang.IllegalStateException
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.releaseSyncFuture(FSHLog.java:1261)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run(FSHLog.java:1348)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 14:29:11,399 WARN  [sync.2] wal.FSHLog$SyncRunner(1360): UNEXPECTED, continuing
java.lang.IllegalStateException
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.releaseSyncFuture(FSHLog.java:1261)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run(FSHLog.java:1348)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 14:29:11,399 WARN  [sync.2] wal.FSHLog$SyncRunner(1360): UNEXPECTED, continuing
java.lang.IllegalStateException
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.releaseSyncFuture(FSHLog.java:1261)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run(FSHLog.java:1348)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 14:29:11,400 WARN  [sync.2] wal.FSHLog$SyncRunner(1360): UNEXPECTED, continuing
java.lang.IllegalStateException
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.releaseSyncFuture(FSHLog.java:1261)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner.run(FSHLog.java:1348)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2015-04-21 14:29:11,401 DEBUG [pool-57-thread-10] region.EmbeddedDatabase$EmbeddedTable(647): Received java.io.IOException: java.lang.IllegalStateException: Queue full  retrying, attempts:0/350
2015-04-21 14:29:11,401 DEBUG [pool-57-thread-41] region.EmbeddedDatabase$EmbeddedTable(647): Received java.io.IOException: java.lang.IllegalStateException: Queue full  retrying, attempts:0/350
2015-04-21 14:29:11,401 DEBUG [pool-57-thread-2] region.EmbeddedDatabase$EmbeddedTable(647): Received java.io.IOException: java.lang.IllegalStateException: Queue full  retrying, attempts:0/350
2015-04-21 14:29:11,401 DEBUG [pool-57-thread-39] region.EmbeddedDatabase$EmbeddedTable(647): Received java.io.IOException: java.lang.IllegalStateException: Queue full  retrying, attempts:0/350
2015-04-21 14:29:11,402 DEBUG [pool-57-thread-7] region.EmbeddedDatabase$EmbeddedTable(647): Received java.io.IOException: java.lang.IllegalStateException: Queue full  retrying, attempts:0/350
Wrote 89000 procedures in 27001 ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14506034" author="mbertozzi" created="Tue, 21 Apr 2015 23:46:32 +0000"  >&lt;p&gt;that number looks pretty good to me, &lt;br/&gt;
because you are basically executing only running availableProcessors() at the time.&lt;/p&gt;

&lt;p&gt;try to pass 50 as the WAL threads number (there is just 1 thread in the wal). the wal can be tuned based on how many threads will be pushing stuff into the system. the default is availableProcessors(), so you are doing a sync every 4 or 8 operation. if you bump that to the number of threads that are pushing stuff you&apos;ll be probably going way faster.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;-    ProcedureExecutor executor = util.getMiniHBaseCluster().getMaster().getMasterProcedureExecutor();
-    store = executor.getStore();
+    //util.getConfiguration().setLong(MasterProcedureConstants.MASTER_PROCEDURE_THREADS, 50);
+    //ProcedureExecutor executor = util.getMiniHBaseCluster().getMaster().getMasterProcedureExecutor();
+    //store = executor.getStore();
+
+    FileSystem fs = util.getMiniHBaseCluster().getMaster().getMasterFileSystem().getFileSystem();
+    final Path logDir = new Path(
+        util.getMiniHBaseCluster().getMaster().getMasterFileSystem().getRootDir(),
+        &quot;testLogs&quot;);
+    store = new WALProcedureStore(util.getConfiguration(), fs, logDir, new WALProcedureStore.LeaseRecovery() {
+      @Override
+      public void recoverFileLease(FileSystem fs, Path path) throws IOException {
+        // no-op
+      }
+    });
+
+    // YOU MUST SPECIFY THE NUMBER OF THREADS THAT ARE PUSHING STUFF TO MAKE THE WAL FAST
+    store.start(50);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but still, I don&apos;t think the focus should be comparison on write perf (also the region wal has stuff like compression/encryption and more that it will be cool to have). so to me it is just matter of how can we get the same wal shortcuts (on replay) that we can get with a simple wal using the current region code. can we throw away the logs when we remove everything from the memstore? can we avoid the delete markers and similar.&lt;/p&gt;</comment>
                            <comment id="14506155" author="enis" created="Wed, 22 Apr 2015 00:52:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;try to pass 50 as the WAL threads number (there is just 1 thread in the wal)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Let me try that. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;but still, I don&apos;t think the focus should be comparison on write perf &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;write perf comes into picture in case we will do assignments using procedures. In case of 100K/1M region clusters, we do not want to be bottlenecked by the proc store. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;can we throw away the logs when we remove everything from the memstore?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I am not sure I follow. Rolling the WAL is a very heavyweight operation. We should not roll the WAL everytime there is no more procedure references. We already have the region mechanics to optimize the WAL rolls vs flushes. We also have the periodic flushes to help with recovery (although the patch does not contain periodic flusher yet).  &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;can we avoid the delete markers and similar.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think this would be a bigger change. We want if memstore contains it remove, otherwise put a tombstone to optimize empty flushes (for this logic). Regardless of this, do you think flushes and compactions will be a perf problem? As I comment above, every time a flush happens, we will get rid of the WALs using the regular seqId tracking. Better yet, on graceful shutdown, we will not do any replay, since everything is flushed and WAL files already archived. &lt;/p&gt;

&lt;p&gt;One of the goals for this patch is not to just replace proc store, but have this as a general service for other use cases (storing cluster membership, RS groups, etc). The motivation is to also not have yet another WAL format. In case we do not want the rest of HRegion, I think we should at least use FSHLog as the proc wal store. &lt;/p&gt;</comment>
                            <comment id="14506349" author="stack" created="Wed, 22 Apr 2015 03:39:43 +0000"  >&lt;p&gt;The queue is bounded and its size is set by count of handlers (queue doesn&apos;t need to be larger than count of handlers). Is it possible that this config is warped in this context?&lt;/p&gt;</comment>
                            <comment id="14506851" author="mbertozzi" created="Wed, 22 Apr 2015 11:24:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; I think you didn&apos;t get from my last comment that I&apos;m pushing for having this patch in. and I&apos;d like to avoid wasting time on comparing perf on something that were not optimized for write and we are going to throw away. nonetheless, I looked into the code and found a couple of interesting things to fix see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13529&quot; title=&quot;Procedure v2 - WAL Improvements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13529&quot;&gt;&lt;del&gt;HBASE-13529&lt;/del&gt;&lt;/a&gt;. bug and optimization aside one major hit is on hsync used by default instead of hflush used by the region wal. and the times with the patch now looks much better (didn&apos;t compare with the region but without the patch which is infinitely slower).&lt;/p&gt;

&lt;p&gt;Wrote 1000000 procedures with 5 threads with hsync=false in 44.9360sec 44.936sec&lt;br/&gt;
Wrote 1000000 procedures with 10 threads with hsync=false in 27.9200sec 27.920sec&lt;br/&gt;
Wrote 1000000 procedures with 30 threads with hsync=false in 17.1160sec 17.116sec&lt;br/&gt;
Wrote 1000000 procedures with 50 threads with hsync=false in 14.7460sec 14.746sec&lt;/p&gt;

&lt;p&gt;Wrote 10000 procedures with 10 threads with hsync=true in 1mins, 47.52sec 107.520sec&lt;br/&gt;
Wrote 10000 procedures with 30 threads with hsync=true in 41.8420sec 41.842sec&lt;br/&gt;
Wrote 10000 procedures with 50 threads with hsync=true in 26.4210sec 26.421sec&lt;/p&gt;

&lt;p&gt;anway, going back to the real topic.&lt;br/&gt;
when I started with the wal there were a couple of obvious shortcut.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;procedure are short lived, we probably don&apos;t need compaction but a TTL expire should be ok.&lt;/li&gt;
	&lt;li&gt;if we don&apos;t have any procedure running, there is no need to replay on restart.&lt;/li&gt;
	&lt;li&gt;if we keep a tracker we can avoid loading completed/removed procedure and avoid serialize/deserialize.&lt;/li&gt;
	&lt;li&gt;we are able to start some procedure before reading all the wals. The replay is from the newest wal to the oldest. if we find the first entry (the user submit) of the procedure and it was in execution we know that we can start it without waiting on the rest.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;if we can, it will be nice to be able to get these behavior in, because they will be reducing the replay time. the last point for starting procedures before completing the replay it will be really nice to reduce the AM time on replay. the others are just shortcut to avoid reading data that we don&apos;t need.&lt;br/&gt;
of course it will be special logic just for the procedure case, but in theory we can extend the base Region and have the Procedure region and do this kind of tricks. We don&apos;t need this in now, and I don&apos;t want to block this jira for this stuff. I&apos;m just trying to point out what we can do to optimize this use case, and see if we can do it this reusing what we have.&lt;/p&gt;</comment>
                            <comment id="14507870" author="enis" created="Wed, 22 Apr 2015 20:51:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;The queue is bounded and its size is set by count of handlers (queue doesn&apos;t need to be larger than count of handlers). Is it possible that this config is warped in this context? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks, it makes sense. I was testing with 150 threads at some point, and the queue is initialized with numHandlers=5 * 3  = 15. 5 is coming from mini cluster test config. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;bug and optimization aside one major hit is on hsync used by default instead of hflush used by the region wal&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It is a valid point. I think we should do the hsync anyway for regular users. Since we are using hflush for meta already, procedures are not different than that. In any case, I&apos;ve added &lt;tt&gt;put.setDurability(Durability.FSYNC_WAL);&lt;/tt&gt; to the procedure store so that once we have it we will automatically start using it. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I looked into the code and found a couple of interesting things to fix see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13529&quot; title=&quot;Procedure v2 - WAL Improvements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13529&quot;&gt;&lt;del&gt;HBASE-13529&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Looks good. Numbers much better. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;when I started with the wal there were a couple of obvious shortcut.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With procs, we have a custom in-memory representation + WAL. For doing all those tricks, a custom memstore + custom flush and custom WAL replay is needed.  &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;procedure are short lived, we probably don&apos;t need compaction but a TTL expire should be ok.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Actually, a custom flusher which does not flush a deleted cell and delete tombstone should do the trick. The region will fill up its memstore, but the flush will write very little data (only the non-deleted procedures). &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;if we don&apos;t have any procedure running, there is no need to replay on restart.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is already the case whether procs are running or not. If clean shutdown, WALs are moved to archive after a flush. The next master start will not do any replay. If there is running procedures, the scan in load() will see these and start executing. All of the deleted procedures will be not be seen by the scan. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;if we keep a tracker we can avoid loading completed/removed procedure and avoid serialize/deserialize.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The procs which are deleted will not be deserialized since the scan in load() will not see them. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;we are able to start some procedure before reading all the wals. The replay is from the newest wal to the oldest. if we find the first entry (the user submit) of the procedure and it was in execution we know that we can start it without waiting on the rest.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;WAL replay works is by reading the oldest WAL first. Not sure whether we can change that without major surgery. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We don&apos;t need this in now, and I don&apos;t want to block this jira for this stuff. I&apos;m just trying to point out what we can do to optimize this use case, and see if we can do it this reusing what we have.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ok great. Let&apos;s get this in 1.1 then. Do you mind doing a review? &lt;/p&gt;


</comment>
                            <comment id="14510469" author="stack" created="Fri, 24 Apr 2015 05:07:05 +0000"  >&lt;p&gt;High-level, I was in favor of a region-based store (it is code that we know basically works and fixes in a region store will likely be generally applicable to all region deploys, etc.) but if a purposed WAL-based store is an order of magnitude faster and just generally costs less in cpu and i/o, I&apos;d change my mind. Would be good if store didn&apos;t get in the way of fast assign, etc.&lt;/p&gt;</comment>
                            <comment id="14513204" author="ndimiduk" created="Sun, 26 Apr 2015 19:24:16 +0000"  >&lt;p&gt;Where do we stand with this one. Clock is ticking on 1.1.&lt;/p&gt;</comment>
                            <comment id="14513220" author="mbertozzi" created="Sun, 26 Apr 2015 19:38:55 +0000"  >&lt;p&gt;code looks good, and there are some nice cleanups in region/wal code.&lt;/p&gt;

&lt;p&gt;but now that I&apos;m testing it, I&apos;m not sure if we should replace the proc-wal with this.&lt;br/&gt;
following what stack mentioned, there is too much overhead on the region path compared&lt;br/&gt;
to the simple wal. I was expecting the performance of the region to be even better&lt;br/&gt;
then the simple wal because it never had love, no optimization of any kind and similar.&lt;br/&gt;
but the region looks at least an of magnitude slower then the simple wal on write. &lt;br/&gt;
keep in mind that with the simple wal it is easy to do optimization on replay, &lt;br/&gt;
which is where I expect to get more benefit from using it.&lt;/p&gt;

&lt;p&gt;anyway, taking one step back and without looking too much into performance.&lt;br/&gt;
the question is, what benefit we have by using the region instead of a simple wal?&lt;br/&gt;
in theory the only benefit we have is that we can query it with the table interface,&lt;br/&gt;
but at this point with this patch we can probably wrap the proc-v2 wal in an EmbeddedTable or similar.&lt;br/&gt;
the other benefit of course is that the code is probably more tested than the proc-wal and we have already features like compression/encryption in.&lt;br/&gt;
the main disadvantage is that is that the overhead compared to the wal on the write side looks too much, and making optimization on the replay it may not be that simple.&lt;/p&gt;

&lt;p&gt;I was in favor of replacing the proc-wal with this before testing it, but now i&apos;m no longer sure about it. &lt;br/&gt;
so, if you can come up with a list of benefit and disadvantages of using the region vs proc-wal it will be nice to make everyone able to decide. &lt;br/&gt;
(I&apos;m still +1 on including this patch even without having it used as proc-wal replacement, because looks useful anyway, and it has some nice cleanups)&lt;/p&gt;</comment>
                            <comment id="14514771" author="enis" created="Mon, 27 Apr 2015 19:26:39 +0000"  >&lt;p&gt;Agreed with Matteo that I think we should commit this with or without replacing the WAL based proc store. The original motivation was to re-use region + WAL for dogfooding. Proc store and region server groups are the first candidates. &lt;/p&gt;

&lt;p&gt;The benefit of using the region internals and WAL is that it is well tested code and there should not be any reason to reinvent the wheel. We had to fix at least a couple of issues already in this code (see this issue and other threads) and honestly it is a bit scary. We can of course get it tested well enough and supported, but the question is why do we support yet another WAL format? Once we introduce it, we probably have to support it for life. &lt;br/&gt;
Agreed that the drawback is that doing proc-specific optimizations will be much harder (if at all possible) and there is the memstore + flush overhead. We can benefit from the region internals in get / scan kind of API. I did not see an order of magnitude difference, it was the other way around before we discovered &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13529&quot; title=&quot;Procedure v2 - WAL Improvements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13529&quot;&gt;&lt;del&gt;HBASE-13529&lt;/del&gt;&lt;/a&gt;. Let me verify with the latest code. &lt;/p&gt;
</comment>
                            <comment id="14514809" author="mbertozzi" created="Mon, 27 Apr 2015 19:44:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;I did not see an order of magnitude difference, it was the other way around before we discovered &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13529&quot; title=&quot;Procedure v2 - WAL Improvements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13529&quot;&gt;&lt;del&gt;HBASE-13529&lt;/del&gt;&lt;/a&gt;. Let me verify with the latest code.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;that&apos;s correct, before &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13529&quot; title=&quot;Procedure v2 - WAL Improvements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13529&quot;&gt;&lt;del&gt;HBASE-13529&lt;/del&gt;&lt;/a&gt; the proc-wal was way slower. I avoided to post my result to allow to have a run with tunings you may know and decide. &lt;br/&gt;
I&apos;ve a test with both proc-wal and region-store that you can run. it is based on your test, but you can run it with mvn clean test -Dsurefire.timeout=5400 -Dtest= ProcedureStoreTest and it will show you the result &lt;br/&gt;
&lt;a href=&quot;https://github.com/matteobertozzi/hbase/blob/7222e6a16430b8f1909b0197f21958ad2728d9e0/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureStoreTest.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/matteobertozzi/hbase/blob/7222e6a16430b8f1909b0197f21958ad2728d9e0/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureStoreTest.java&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14516045" author="enis" created="Tue, 28 Apr 2015 00:33:53 +0000"  >&lt;p&gt;I am still getting: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.NullPointerException
	at org.apache.hadoop.hbase.procedure2.util.ByteSlot.writeTo(ByteSlot.java:95)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.syncSlots(WALProcedureStore.java:515)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.syncSlots(WALProcedureStore.java:498)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.syncLoop(WALProcedureStore.java:480)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.access$1(WALProcedureStore.java:444)
	at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore$1.run(WALProcedureStore.java:142)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is using the master code with just the test. Do you see this? &lt;/p&gt;</comment>
                            <comment id="14516303" author="enis" created="Tue, 28 Apr 2015 04:07:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; am I missing anything? I&apos;ve ran the test, and here are the results. It is very little difference between the approaches: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.AssertionError: Wrote 1000000 procedures with 5 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 50.4700sec (50.470sec)
java.lang.AssertionError: Wrote 1000000 procedures with 5 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 14.9200sec (14.920sec)
java.lang.AssertionError: Wrote 1000000 procedures with 10 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 34.0240sec (34.024sec)
java.lang.AssertionError: Wrote 1000000 procedures with 10 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 15.4410sec (15.441sec)
java.lang.AssertionError: Wrote 1000000 procedures with 30 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 20.8070sec (20.807sec)
java.lang.AssertionError: Wrote 1000000 procedures with 30 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 24.7590sec (24.759sec)
java.lang.AssertionError: Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 18.2410sec (18.241sec)
java.lang.AssertionError: Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 20.1100sec (20.110sec)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14516424" author="mbertozzi" created="Tue, 28 Apr 2015 05:45:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; can I git url to clone the branch you are using with the test in it? I&apos;m getting really different result like 18sec vs 4min&lt;/p&gt;</comment>
                            <comment id="14516437" author="mbertozzi" created="Tue, 28 Apr 2015 05:55:40 +0000"  >&lt;p&gt;the other option is to attach the full patch with the test, and let QA run the test&lt;/p&gt;</comment>
                            <comment id="14516881" author="mbertozzi" created="Tue, 28 Apr 2015 11:38:26 +0000"  >&lt;p&gt;just did a rebase, &lt;a href=&quot;https://github.com/matteobertozzi/hbase/tree/hbase-12439&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/matteobertozzi/hbase/tree/hbase-12439&lt;/a&gt;&lt;br/&gt;
I&apos;m running directly from a fresh checkout: mvn clean test -Dsurefire.timeout=5400 -Dtest= ProcedureStoreTest &lt;br/&gt;
and I&apos;m getting the same result as before on different machines. I guess you have tuning changes or similar.&lt;br/&gt;
if you&apos;ll show me how to get that numbers I&apos;ll be back again to be in favor of having the region as proc-wal replacement.&lt;/p&gt;</comment>
                            <comment id="14516882" author="mbertozzi" created="Tue, 28 Apr 2015 11:38:28 +0000"  >&lt;p&gt;just did a rebase, &lt;a href=&quot;https://github.com/matteobertozzi/hbase/tree/hbase-12439&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/matteobertozzi/hbase/tree/hbase-12439&lt;/a&gt;&lt;br/&gt;
I&apos;m running directly from a fresh checkout: mvn clean test -Dsurefire.timeout=5400 -Dtest= ProcedureStoreTest &lt;br/&gt;
and I&apos;m getting the same result as before on different machines. I guess you have tuning changes or similar.&lt;br/&gt;
if you&apos;ll show me how to get that numbers I&apos;ll be back again to be in favor of having the region as proc-wal replacement.&lt;/p&gt;</comment>
                            <comment id="14516883" author="mbertozzi" created="Tue, 28 Apr 2015 11:38:30 +0000"  >&lt;p&gt;just did a rebase, &lt;a href=&quot;https://github.com/matteobertozzi/hbase/tree/hbase-12439&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/matteobertozzi/hbase/tree/hbase-12439&lt;/a&gt;&lt;br/&gt;
I&apos;m running directly from a fresh checkout: mvn clean test -Dsurefire.timeout=5400 -Dtest= ProcedureStoreTest &lt;br/&gt;
and I&apos;m getting the same result as before on different machines. I guess you have tuning changes or similar.&lt;br/&gt;
if you&apos;ll show me how to get that numbers I&apos;ll be back again to be in favor of having the region as proc-wal replacement.&lt;/p&gt;</comment>
                            <comment id="14517632" author="enis" created="Tue, 28 Apr 2015 18:41:22 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt; for pursuing this. Pushed the code that I am running at &lt;a href=&quot;https://github.com/enis/hbase/tree/hbase-13260-review&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/enis/hbase/tree/hbase-13260-review&lt;/a&gt;. Committed your test as TestProcedureStorePerf. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
mvn test -Dtest=TestProcedureStorePerf
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Results from my MBP. Maybe it is because of the relatively fast SSD in my laptop? Let me try a linux box without SSDs. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  TestProcedureStorePerf.runTestWith10ThreadsAndProcV2Wal:225-&amp;gt;runTest:207 Wrote 1000000 procedures with 10 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 36.8390sec (36.839sec)
  TestProcedureStorePerf.runTestWith10ThreadsAndRegionStore:250-&amp;gt;runTest:207 Wrote 1000000 procedures with 10 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 17.1810sec (17.181sec)
  TestProcedureStorePerf.runTestWith30ThreadsAndProcV2Wal:230-&amp;gt;runTest:207 Wrote 1000000 procedures with 30 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 22.3340sec (22.334sec)
  TestProcedureStorePerf.runTestWith30ThreadsAndRegionStore:255-&amp;gt;runTest:207 Wrote 1000000 procedures with 30 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 25.1180sec (25.118sec)
  TestProcedureStorePerf.runTestWith4ThreadsAndProcV2Wal:220-&amp;gt;runTest:207 Wrote 1000000 procedures with 5 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 53.6920sec (53.692sec)
  TestProcedureStorePerf.runTestWith50ThreadsAndProcV2Wal:235-&amp;gt;runTest:207 Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 20.8590sec (20.859sec)
  TestProcedureStorePerf.runTestWith50ThreadsAndRegionStore:260-&amp;gt;runTest:207 Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 19.1450sec (19.145sec)
  TestProcedureStorePerf.runTestWith5ThreadsAndRegionStore:245-&amp;gt;runTest:207 Wrote 1000000 procedures with 5 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 15.8760sec (15.876sec)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14517865" author="mbertozzi" created="Tue, 28 Apr 2015 19:37:11 +0000"  >&lt;p&gt;wait, I&apos;m testing your branch and I get the same result as you... but the region store is not used, or am I missing something?&lt;br/&gt;
&lt;a href=&quot;https://github.com/enis/hbase/blob/hbase-13260-review/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java#L1096&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/enis/hbase/blob/hbase-13260-review/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java#L1096&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;and in the test we are looking up the master store &lt;br/&gt;
&lt;a href=&quot;https://github.com/enis/hbase/blob/hbase-13260-review/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureStorePerf.java#L196&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/enis/hbase/blob/hbase-13260-review/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureStorePerf.java#L196&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14518262" author="enis" created="Tue, 28 Apr 2015 22:38:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;wait, I&apos;m testing your branch and I get the same result as you... but the region store is not used, or am I missing something?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ah, that maybe from some earlier commit for tests. The commit history is pretty messy. Let me run with the actual region based one. &lt;/p&gt;</comment>
                            <comment id="14518355" author="enis" created="Tue, 28 Apr 2015 23:34:21 +0000"  >&lt;p&gt;We need one more change to the test code. RegionStore.getNumThreads() always returns 1, it does not track the actual number of threads that it is started with. Thus, the region store tests are run with 1 Worker thread in the above case. &lt;/p&gt;

&lt;p&gt;With the fixes, I was able to get back to the previously reported numbers: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.AssertionError: Wrote 1000000 procedures with 5 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 1mins, 14.735sec (74.735sec)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not an order of magnitude, but this is still slow compared to the WAL based one. A couple of things are of note: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;With increased concurrency, the region performs worse (&amp;gt;10 threads)&lt;/li&gt;
	&lt;li&gt;Even with SKIP_WAL, the numbers does not look better. This is probably due to the region internals and memstore append being the bottleneck rather than the FSHLog. I did not yet spend time to understand why we are bottlenecked around 10K puts and deletes (20K ops per sec) per sec with SKIP_WAL.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If we cannot improve the region store perf, I would be also in favor of using the WAL based one. &lt;/p&gt;</comment>
                            <comment id="14518456" author="ndimiduk" created="Wed, 29 Apr 2015 00:41:52 +0000"  >&lt;p&gt;Had a chat with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; offline on this. Here&apos;s my understanding/summary:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;this patch cleans up region code in a way that everyone likes, +1 for that bit&lt;/li&gt;
	&lt;li&gt;procV2 is used for all DDL operations in 1.1. DDL is a relatively small number of edits to wal&lt;/li&gt;
	&lt;li&gt;procV2 is not used for region assignment in 1.1, the use-case that involves potentially lots of wal edits&lt;/li&gt;
	&lt;li&gt;proc-wal is a branch new file format, new code, &amp;amp;c.&lt;/li&gt;
	&lt;li&gt;proc-wal is probably faster than region-wal, but we now think it&apos;s less than an order of magnitude slower&lt;/li&gt;
	&lt;li&gt;proc-wal and region-wal are interchangeable for the purposes of procV2&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For branch-1.1, I&apos;m in favor of region-wal for procV2 because it&apos;s &lt;b&gt;NOT&lt;/b&gt; in a high throughput situation AND it means we can avoid supporting a new file format. Future improvements in performance to region wal help everyone. If we can&apos;t get it where we need perf-wise, we can always bring back proc-wal for region assignment operations &amp;#8211; that card is still up our sleeve.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mbertozzi&quot; class=&quot;user-hover&quot; rel=&quot;mbertozzi&quot;&gt;Matteo Bertozzi&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt; are you swayed?&lt;/p&gt;</comment>
                            <comment id="14518846" author="mbertozzi" created="Wed, 29 Apr 2015 06:48:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ndimiduk&quot; class=&quot;user-hover&quot; rel=&quot;ndimiduk&quot;&gt;Nick Dimiduk&lt;/a&gt; for 1.1 is probably ok having a slow proc-store (keep in mind that currently proc-wal is using hsync and the performance are really different from the bench above done with hflush only, instead of 15sec for 1M ops I get 200sec for 10k ops)&lt;/p&gt;

&lt;p&gt;what I&apos;d like to know from you is what do you have in mind for upgrading?&lt;br/&gt;
let say we can&apos;t get perf improvement on the region, and from the above &quot;With increased concurrency, the region performs worse (&amp;gt;10 threads)&quot;. for the assignment we need a better throughput and shortcut on the replay will help too.&lt;/p&gt;

&lt;p&gt;in my mind, I always had the different &quot;stores&quot; for groups of operations (e.g. the DDL goes on its own wal, the assignment for sys-ns goes on its own wal and so on). but using this we can even do DDL still uses the region-store and assignment uses the proc-wal, and avoid any migration.&lt;br/&gt;
but I&apos;d like to know what is your upgrade story in case the region patch can&apos;t be improved. (can we get the full bench with 10, 30, 50 threads too, just for reference?)&lt;/p&gt;</comment>
                            <comment id="14519668" author="ndimiduk" created="Wed, 29 Apr 2015 16:26:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;what I&apos;d like to know from you is what do you have in mind for upgrading?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure what you&apos;re asking me. What are the conditions for upgrade? What&apos;s window of support for handling the upgrade? What&apos;s the implementation look like?&lt;/p&gt;

&lt;p&gt;For the first, it would be some definitive decision that region based wal perf is insufficient for our assignment goals and we&apos;re unable to improve it. I don&apos;t know a specific target number in mind.&lt;/p&gt;

&lt;p&gt;For the second, it seems some detection mechanism would be necessary for new servers to be able to participate in old replay, shipping both code paths for a window of releases where compatibility is required (probably all future 1.x releases and 12-18 months&apos; worth of 2.x minor releases if that one ends up being rolling upgradeable).&lt;/p&gt;

&lt;p&gt;I have no recommendation on the third.&lt;/p&gt;</comment>
                            <comment id="14519678" author="mbertozzi" created="Wed, 29 Apr 2015 16:32:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Not sure what you&apos;re asking me. What are the conditions for upgrade? What&apos;s window of support for handling the upgrade? What&apos;s the implementation look like?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;my question was more on the line of: assuming we are going with region-wal for 1.1 and then we change mind in 1.2 and move to the proc-wal. how the 1.1 to 1.2 migration works? an offline tool to convert the region data to wal, both code present and the conversion done at master startup, draining the region and pushing stuff into the wal. something else? (there is always the option DDL keep using the region-store and AM will use the proc-wal, so no migration is necessary)&lt;/p&gt;</comment>
                            <comment id="14519698" author="ndimiduk" created="Wed, 29 Apr 2015 16:45:08 +0000"  >&lt;p&gt;Table DDL is low ops, so either implementation is sufficient. If we decide to role out proc-wal for assignment in 1.2, can both versions co-exist for some period? I assumed yes, but perhaps that&apos;s not correct. If not, we&apos;ll need a migration strategy to move to the new format, perhaps a one-time wal rewrite on startup or just support reading/honoring the old format for some period of compatibility releases.&lt;/p&gt;</comment>
                            <comment id="14519807" author="stack" created="Wed, 29 Apr 2015 17:41:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;...there is always the option DDL keep using the region-store and AM will use the proc-wal, so no migration is necessary&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Profiling the two stores, region-store is doing way more work than the WAL-store; it is sorting, writing blooms, resizing badly sized buffers and then on top of it all, flushing and compacting like a daemon during the 1M test run with much of this work unnecessary to the job at hand. Region-store is a misfit. Improvements making region-store work in this context may not always translate as general bread-and-butter improvement in our region engine.&lt;/p&gt;

&lt;p&gt;Assignment needs be as frictionless as we can make it; any lag impinges upon MTTR.&lt;/p&gt;

&lt;p&gt;To my mind, the above argues that we should go for WAL-store; harden it now while it carries low-numbers of DDL ops getting it ready for the big job.&lt;/p&gt;


</comment>
                            <comment id="14519954" author="enis" created="Wed, 29 Apr 2015 19:00:10 +0000"  >&lt;p&gt;This is useful exercise (although, Nick sorry to delay the RC). I did some more experiments yesterday. A pure FSHLog based proc store can do 1M procedures in ~20 seconds which is more or less on par with the current WAL based one. For figuring out where the bottleneck is, I&apos;ve changed the region based proc store to instead use &lt;tt&gt;numShards&lt;/tt&gt; tables and do simple sharding. &lt;/p&gt;

&lt;p&gt;With 8+ shards, the write throughput is 25-30 secs (compared to ~20secs). &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
4 shards:
java.lang.AssertionError: Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 2mins, 7.265sec (127.265sec)
8 shards:
java.lang.AssertionError: Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 31.0280sec (31.028sec)
16 shards:
java.lang.AssertionError: Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 25.4330sec (25.433sec)
32 shards: 
java.lang.AssertionError: Wrote 1000000 procedures with 50 threads with useProcV2Wal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; hsync=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; in 25.9470sec (25.947sec)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So it seems that the bottleneck is not on the CPU, but the HRegion&apos;s concurrency. I&apos;ve also done some basic testing with ASYNC_WAL and SKIP_WAL which surprisingly was slower than SYNC_WAL. I think it is an area worth digging into later. Maybe I am still missing something (code is at &lt;a href=&quot;https://github.com/enis/hbase/tree/hbase-13260-review&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/enis/hbase/tree/hbase-13260-review&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I am not suggesting that we do sharding for the store only to get around the region concurrency problem. Any improvement in this is definitely a big win for both regular data and proc metadata, but I am not sure whether we can get there soon enough. &lt;/p&gt;

&lt;p&gt;I like the idea of different stores for different kinds of procedures. We already keep (some) assignment state in meta (in zk-less AM) which is kind of like custom proc on a meta proc store. In an alternative world, we could have used the meta table for everything (table descriptors, table state, assignments, list of region files) and be done with it.&lt;/p&gt;

&lt;p&gt;For the less amount of work though, I think we should chose one and stick with it. Otherwise, we have to support two alternative code paths, migration code, upgrading etc. It is just wasted effort I think. Whether to go with the wal based one or region based one is a question of the design of proc-based assignment since for DDL ops it does not matter. Unfortunately it is not formalized yet. If we end up splitting meta, we can even do proc store on meta. &lt;/p&gt;

&lt;p&gt;If we think that assignment using procs will use the local proc store in master, we should go with the WAL based one since I don&apos;t think doing sharding for the region based one is right. Otherwise, we should go with the region based one. Sorry this is vague, but since we have yet to figure out the specifics of the new AM, it is hard to decide. &lt;/p&gt;</comment>
                            <comment id="14519988" author="mbertozzi" created="Wed, 29 Apr 2015 19:15:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we think that assignment using procs will use the local proc store in master, we should go with the WAL based one since I don&apos;t think doing sharding for the region based one is right. Otherwise, we should go with the region based one. Sorry this is vague, but since we have yet to figure out the specifics of the new AM, it is hard to decide.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;before the zkless assignment me, stack and jimmy had a chat on rewriting the AM to support 1M regions, splitting meta and so on. we still need the assignment state on co-located with the AssignmentManager (master) to avoid all the problems like &quot;I need to update the state on something that is not assigned or is not responding&quot; and similar, also for performance reason the assignment-state should be on master just to avoid the extra network round-trip. (The zk-less assignment moved the assignment-state in META, because meta is now co-located with the master).&lt;br/&gt;
Anyway, I have to dig up my notes from that chat and write up something but we can discuss that around hbasecon or the meetup the day before (it is a long conversation with drawings and similar).&lt;br/&gt;
but the bottom line is, the state must be on master. and that state for the procedure based assignment is the data we store in the procedure store.&lt;/p&gt;</comment>
                            <comment id="14520023" author="stack" created="Wed, 29 Apr 2015 19:30:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;A pure FSHLog based proc store ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is this? This sounds like a nice compromise where we get to reuse existing code.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;....but the HRegion&apos;s concurrency&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, need to fix that big time (Is this the &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eclark&quot; class=&quot;user-hover&quot; rel=&quot;eclark&quot;&gt;Elliott Clark&lt;/a&gt; lockless write work?)&lt;/p&gt;

&lt;p&gt;Going the region-store path, there would still be the millions of extra ancilliary objects created and their associated churn/GC that we&apos;d need to deal with.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Whether to go with the wal based one or region based one is a question of the design of proc-based assignment since for DDL ops it does not matter. Unfortunately it is not formalized yet. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True. But we do know that the less friction our store takes, the faster our assign will run.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we end up splitting meta, we can even do proc store on meta.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;An RPC to the store complicates consistency and will add lag. I suggest we not go this route and that an in-process store is actually required if we want fast assign (Just the final states are published in hbase:meta).&lt;/p&gt;

&lt;p&gt;Agree this is an interesting experiment to be pursued further (I like your sharding trick). What to do for 1.1 though? (I suggest we just go WAL-store).&lt;/p&gt;</comment>
                            <comment id="14520203" author="enis" created="Wed, 29 Apr 2015 20:46:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;What is this? This sounds like a nice compromise where we get to reuse existing code.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It was a quick hack to see the perf of pure HLog. It is here: &lt;a href=&quot;https://github.com/enis/hbase/blob/hbase-13260-review/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/FSHLogProcedureStore.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/enis/hbase/blob/hbase-13260-review/hbase-server/src/main/java/org/apache/hadoop/hbase/procedure2/store/FSHLogProcedureStore.java&lt;/a&gt;. Not sure how that plays with the rest of WALProcStore, some more work would be needed to handle rolling, and WAL deletion. Agreed that it is a valid approach (see my earlier comments suggesting so) to re-use FSHlog, but that also will have some unneeded stuff (WALKey, region, table name, cluster-uuids, etc). &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Anyway, I have to dig up my notes from that chat and write up something but we can discuss that around hbasecon or the meetup the day before (it is a long conversation with drawings and similar).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1. Lets gather around and do some brainstorming. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;(The zk-less assignment moved the assignment-state in META, because meta is now co-located with the master).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I thought that we abandoned co-location of meta. I think that we should not do that. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What to do for 1.1 though? (I suggest we just go WAL-store).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;My initial reasoning for this was to re-use what we have, and not support an additional WAL format with it&apos;s own fencing mechanism + rolling + disk format, etc. Adding all of these is just added complexity and needs maintenance. Now that at least we have fixed the perf issues in the impl, quantified and partly justified that having a pure WAL format for procs is better for performance, and the fact that we are unlikely to get that kind of write perf using a single region I think it is fine to go with the wal based approach. Whether it is custom WAL or FSHlog is another discussion though.&lt;/p&gt;</comment>
                            <comment id="14520209" author="ndimiduk" created="Wed, 29 Apr 2015 20:48:56 +0000"  >&lt;p&gt;So then this patch is deferred out of 1.1. I have that right?&lt;/p&gt;</comment>
                            <comment id="14520265" author="mbertozzi" created="Wed, 29 Apr 2015 21:15:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;I thought that we abandoned co-location of meta. I think that we should not do that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I haven&apos;t followed the discussion. but I can tell you that with the proc-based assignment, meta can be anywhere. the only thing required to be on the master is the wal by the procedure store.&lt;/p&gt;</comment>
                            <comment id="14520276" author="enis" created="Wed, 29 Apr 2015 21:22:36 +0000"  >&lt;p&gt;It is mostly in the 1M regions jira, and some offline discussion with Stack. &lt;/p&gt;</comment>
                            <comment id="14520282" author="stack" created="Wed, 29 Apr 2015 21:25:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;So then this patch is deferred out of 1.1. I have that right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As it is, yes, if only because it removes WAL-store. The posted patch has a load of goodness in it though so parts of it should get in. I don&apos;t think these improvements are needed in 1.1 though.&lt;/p&gt;</comment>
                            <comment id="14520287" author="enis" created="Wed, 29 Apr 2015 21:29:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;So then this patch is deferred out of 1.1. I have that right?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, I do not want to hold the RC for the proc store. Do you want the rest of the bits (WalContainer, RegionServices, and EmbeddedTable) in 1.1? It can come later as well.  &lt;br/&gt;
For the FSHlog based store, again the idea is to re-use what we have, but it will take some time to even see whether it is feasible (we would be faking flushes I guess for expiring WALs. I can spend some time on it, but not likely to have a committable patch today even if we agree that it is the right approach.  &lt;/p&gt;</comment>
                            <comment id="14520299" author="ndimiduk" created="Wed, 29 Apr 2015 21:34:22 +0000"  >&lt;p&gt;Last call was 2 day ago, I just wanted to give a chance for the file format questions to be sorted. Probably better not to take refactoring work on patch releases, just bug fixes. Work them up for 1.2.&lt;/p&gt;

&lt;p&gt;Thanks everyone.&lt;/p&gt;</comment>
                            <comment id="14520700" author="hudson" created="Thu, 30 Apr 2015 02:05:16 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.1 #451 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.1/451/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.1/451/&lt;/a&gt;)&lt;br/&gt;
drop &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13260&quot; title=&quot;Bootstrap Tables for fun and profit &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13260&quot;&gt;HBASE-13260&lt;/a&gt; from CHANGES (ndimiduk: rev e8e45ef8f2fb91a870399636b492d5cee58a4c39)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;CHANGES.txt&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14612226" author="stack" created="Thu, 2 Jul 2015 17:12:05 +0000"  >&lt;p&gt;Resolve as interesting experiment from which we learned a bunch?&lt;/p&gt;</comment>
                            <comment id="14612370" author="enis" created="Thu, 2 Jul 2015 18:52:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;Resolve as interesting experiment from which we learned a bunch?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I have a much smaller patch lying around that I still wanna get in. Basically the region / regionserver separation and cleanup and the embedded table stuff without the proc and bootstrap service changes. &lt;br/&gt;
I think better separation of the region library for the region server internals is still the direction we want to go, and the embedded service (although no direct users initially) will help with keeping concerns separately.  &lt;/p&gt;</comment>
                            <comment id="14612376" author="mbertozzi" created="Thu, 2 Jul 2015 18:55:25 +0000"  >&lt;p&gt;+1 there are nice things in this patch that can be applied anyway.&lt;br/&gt;
so, in my opinion you can go ahead and open other jiras for them&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12727016" name="hbase-13260_bench.patch" size="6451" author="enis" created="Tue, 21 Apr 2015 22:46:57 +0000"/>
                            <attachment id="12704968" name="hbase-13260_prototype.patch" size="34856" author="enis" created="Tue, 17 Mar 2015 01:32:07 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12844030">HBASE-14055</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 17 Mar 2015 02:09:01 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 24 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i26ufj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>