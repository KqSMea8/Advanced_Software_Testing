<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:19:41 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-4524/HBASE-4524.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-4524] support for more than one region in .META. table</title>
                <link>https://issues.apache.org/jira/browse/HBASE-4524</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;It seems there are some assumptions in the code that .META. table only has one region FIRST_META_REGIONINFO in the following areas:&lt;/p&gt;

&lt;p&gt;1) .META. table update with user region info.&lt;br/&gt;
2) .META. regions assignment.&lt;br/&gt;
3) .META. table split handling.&lt;/p&gt;

&lt;p&gt;Perhaps we don&apos;t have such requirement until we scale to really large number of regions like 1M.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12525375">HBASE-4524</key>
            <summary>support for more than one region in .META. table</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="7">Later</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="mingma">Ming Ma</reporter>
                        <labels>
                    </labels>
                <created>Fri, 30 Sep 2011 22:32:10 +0000</created>
                <updated>Sat, 11 Apr 2015 00:50:43 +0000</updated>
                            <resolved>Sat, 11 Apr 2015 00:50:43 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="13118543" author="stack" created="Fri, 30 Sep 2011 22:54:33 +0000"  >&lt;p&gt;This issue is where we turned off splitting (because it was broke)&lt;/p&gt;</comment>
                            <comment id="13118545" author="stack" created="Fri, 30 Sep 2011 22:56:35 +0000"  >&lt;p&gt;Reasons to let meta split other than because the cluster has millions of regions:&lt;/p&gt;

&lt;p&gt;+ Distribute the load on meta.&lt;br/&gt;
+ Its &apos;catastrophic&apos; if server carrying meta goes away.  It&apos;d be &lt;b&gt;less&lt;/b&gt; catastrophic if the regionserver were carrying only a piece of meta.&lt;/p&gt;</comment>
                            <comment id="13118590" author="tlipcon" created="Sat, 1 Oct 2011 00:15:13 +0000"  >&lt;p&gt;I&apos;ve spoken to some Bigtable engineers who said that the two-layer ROOT/META thing turned out to be a bit of a mistake given today&apos;s machines. The reason being that today&apos;s servers, if you put only META on a single server, easily have enough RAM and CPU horsepower to handle a several-GB meta region and hundreds of thousands of requests/second, since all the requests can hit cache.&lt;/p&gt;

&lt;p&gt;The recovery argument makes some sense, but I think if you deploy META alone on a server and configure it to flush often, the log recovery should be near-instantaneous and failover would be fast. If META is split across many regions, then it&apos;s likely to be intermingled on the same servers as user data, and hence would be slow to split logs.&lt;/p&gt;

&lt;p&gt;So I&apos;m not against doing this, but another alternate path might be:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add a feature so that the load balancer can be configured to keep META &quot;alone&quot; with no other regions on the server. This would be used on big clusters.&lt;/li&gt;
	&lt;li&gt;remove ROOT, and just store META directly in ZK  (ROOT is essentially vestigial these days)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Any thoughts on this? I&apos;d just like to consider the option of simplifying the META/ROOT situation rather than complicating it.&lt;/p&gt;</comment>
                            <comment id="13118603" author="mingma" created="Sat, 1 Oct 2011 00:43:30 +0000"  >&lt;p&gt;Agree on the &quot;remove ROOT&quot; suggestion for the current scale. I had this question when fixing &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4455&quot; title=&quot;Rolling restart RSs scenario, -ROOT-, .META. regions are lost in AssignmentManager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4455&quot;&gt;&lt;del&gt;HBASE-4455&lt;/del&gt;&lt;/a&gt;, why do we need such redirection in the current scale?&lt;/p&gt;

&lt;p&gt;Does anyone know if 1M regions is a realistic scenario any time soon? 1M regions implies 10K RSs if we have 100 regions per RS. That is a really large scale HBase cluster.&lt;/p&gt;

&lt;p&gt;Also, is there a chance we will put lot more meta data for each region to .META. table, thus we don&apos;t need 1M regions to make .META. table large? If we somehow need to put 100K bytes for each region in .META. table, we just need 100K regions to make .META. table reach 10GB.&lt;/p&gt;</comment>
                            <comment id="13118613" author="tlipcon" created="Sat, 1 Oct 2011 01:08:35 +0000"  >&lt;p&gt;My opinion is that we shouldn&apos;t necessarily design for the 10,000 node cluster yet &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; We&apos;ll hit other scalability issues somewhere along the line with that, too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we somehow need to put 100K bytes for each region in .META. table&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why would we need so much data per region? 1K seems much more reasonable.&lt;/p&gt;</comment>
                            <comment id="13118652" author="stack" created="Sat, 1 Oct 2011 03:53:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;add a feature so that the load balancer can be configured to keep META &quot;alone&quot; with no other regions on the server. This would be used on big clusters.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;When this server crashed, another server would have to be cleared of its regions to dedicate to meta. There&apos;d be some churn. Log splitting should be fast though.&lt;/p&gt;

&lt;p&gt;Changing the balancer would be less intrusive than, for example, having hlog write two logs, one for user space regions and another for meta edits.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;remove ROOT, and just store META directly in ZK (ROOT is essentially vestigial these days)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m +1 on killing root.  If we had to have multiple regions in meta table, we could store all locations in zk and then have client sort them whenever it read the list of metas from zk.&lt;/p&gt;

&lt;p&gt;On metadata size, 0.92 shrinks significantly the size per region entry because we remove the table descriptor per region entry.  We can shrink the meta entry size further if needs be.  That said, I can imagine we&apos;ll start to add data per region (e.g. your region history might be one such scenario). &lt;/p&gt;

&lt;p&gt;On 1M regions on a cluster, that was the fashion once.  Tendency seems to be toward less bigger regions per server now though as you say Ming, 10k regionservers makes for a 1M w/ only 100 regions per server.  But 1M rows is not that much though.  You could probably squeeze 1M rows each with a few versions each into a 256M single region I&apos;d say (Haven&apos;t checked).  I&apos;d doubt this will be the primary obstacle in the way of our getting to 10k cluster size.&lt;/p&gt;</comment>
                            <comment id="13119745" author="streamy" created="Mon, 3 Oct 2011 23:30:30 +0000"  >&lt;p&gt;+1 removing root and putting META location into zk.&lt;/p&gt;

&lt;p&gt;-1 on thinking about splittable META right now.&lt;/p&gt;

&lt;p&gt;+1 thinking about mirroring META in ZK, adding new locations/redirects in NSREs, and other optimizations and availability improvements&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12461337">HBASE-2415</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 30 Sep 2011 22:54:33 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>41676</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 11 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hr7z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>101672</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>