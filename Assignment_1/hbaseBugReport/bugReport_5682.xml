<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:30:00 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5682/HBASE-5682.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5682] Allow HConnectionImplementation to recover from ZK connection loss (for 0.94 only)</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5682</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Just realized that without this &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt; is broken.&lt;br/&gt;
I.e. there&apos;s no point keeping a persistent HConnection around if it can be rendered permanently unusable if the ZK connection is lost temporarily.&lt;br/&gt;
Note that this is fixed in 0.96 with &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5399&quot; title=&quot;Cut the link between the client and the zookeeper ensemble&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5399&quot;&gt;&lt;del&gt;HBASE-5399&lt;/del&gt;&lt;/a&gt; (but that seems to big to backport)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12548877">HBASE-5682</key>
            <summary>Allow HConnectionImplementation to recover from ZK connection loss (for 0.94 only)</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lhofhansl">Lars Hofhansl</assignee>
                                    <reporter username="lhofhansl">Lars Hofhansl</reporter>
                        <labels>
                    </labels>
                <created>Fri, 30 Mar 2012 21:45:42 +0000</created>
                <updated>Fri, 12 Oct 2012 05:35:09 +0000</updated>
                            <resolved>Mon, 2 Apr 2012 22:12:56 +0000</resolved>
                                                    <fixVersion>0.94.0</fixVersion>
                                    <component>Client</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="13242871" author="lhofhansl" created="Sat, 31 Mar 2012 00:04:20 +0000"  >&lt;p&gt;Here&apos;s a patch.&lt;br/&gt;
Please have a careful look. I can upload to RB too.&lt;/p&gt;

&lt;p&gt;The idea is that if this is an unmanaged Connection (see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt;), a ZK connection is re-establish whenever needed (if it was lost before).&lt;/p&gt;

&lt;p&gt;This patch is somewhat more complicated than I&apos;d like, because I did not want to change the behavior for managed (default) connections.&lt;br/&gt;
If we like I can make this the default behavior... Seems much more robust than the current behavior.&lt;/p&gt;

&lt;p&gt;I tested this manually, and the connection (if created with HConnectionManager.createConnection, and hence unmanaged) recovers from loosing both the HBase and ZK connections.&lt;/p&gt;

&lt;p&gt;(Interestingly in plain HBase 0.94 the client &lt;b&gt;never&lt;/b&gt; recovers from this - even with the default connection behavior.)&lt;/p&gt;</comment>
                            <comment id="13242873" author="lhofhansl" created="Sat, 31 Mar 2012 00:05:36 +0000"  >&lt;p&gt;I am willing to sink the 0.94.0rc for this.&lt;/p&gt;</comment>
                            <comment id="13242980" author="lhofhansl" created="Sat, 31 Mar 2012 03:01:16 +0000"  >&lt;p&gt;A little bit more background:&lt;br/&gt;
I put &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4805&quot; title=&quot;Allow better control of resource consumption in HTable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4805&quot;&gt;&lt;del&gt;HBASE-4805&lt;/del&gt;&lt;/a&gt; in place to be able to use the standard HBase client in long running app server processes. The idea is that one can manage an HConnection and a ThreadPool per app server and then cheaply create HTables when needed.&lt;br/&gt;
For that setup it is vital that the HConnection does not become unusable when there are temporary network outages, or that HBase cluster is temporarily taken down.&lt;br/&gt;
In that case the clients should timeout quickly to allow the application to react to it, and if the network/cluster has recovered the application should be able to recover.&lt;/p&gt;</comment>
                            <comment id="13243003" author="lhofhansl" created="Sat, 31 Mar 2012 04:01:28 +0000"  >&lt;p&gt;Slightly better patch.&lt;br/&gt;
No need to wait waitForRootRegion in ZK longer than RecoverableZookeeper tries.&lt;/p&gt;

&lt;p&gt;With recovery is clean. The client can control via timeouts how soon it would it would a connection problem up to the application layer.&lt;/p&gt;

&lt;p&gt;Since this patch is only against 0.94 I&apos;ll run tests locally.&lt;/p&gt;</comment>
                            <comment id="13243004" author="lhofhansl" created="Sat, 31 Mar 2012 04:05:13 +0000"  >&lt;p&gt;In fact this is what &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5153&quot; title=&quot;Add retry logic in HConnectionImplementation#resetZooKeeperTrackers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5153&quot;&gt;&lt;del&gt;HBASE-5153&lt;/del&gt;&lt;/a&gt; should have been.&lt;/p&gt;</comment>
                            <comment id="13243007" author="zhihyu@ebaysf.com" created="Sat, 31 Mar 2012 04:31:39 +0000"  >&lt;p&gt;For abort():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (managed) {
+              &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the connection is managed attempt to reconnect immediately
&lt;/span&gt;+              ensureZookeeperTrackers();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;the condition for calling ensureZookeeperTrackers() is different from other calls in the patch. Please explain.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void ensureZookeeperTrackers()
         &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; ZooKeeperConnectionException{
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Please add a space before the right curly brace.&lt;/p&gt;</comment>
                            <comment id="13243018" author="lhofhansl" created="Sat, 31 Mar 2012 05:06:51 +0000"  >&lt;p&gt;Thanks Ted.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;the condition is different, because that is what it did before. I.e. if the connection is managed the trackers are setup only at construction and during abort in the specific case of SessionExpiredException. If the connection is unmanaged on the other hand the trackers are rechecked before they are needed and hence abort becomes a no-op for any KeeperExcepion. Hence the condition is exactly reversed. This part is the key of the patch actually.&lt;/li&gt;
	&lt;li&gt;The space wasn&apos;t there before. I actually had the space added and then removed it again &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I&apos;ll add it back.&lt;/li&gt;
&lt;/ol&gt;

</comment>
                            <comment id="13243021" author="lhofhansl" created="Sat, 31 Mar 2012 05:19:23 +0000"  >&lt;p&gt;The gist of this change is that (1) the ZK connection is re-checked in all calls where it is needed and re-established if needed and (2) if the connection is down the client can find out quickly (by setting timeouts accordingly) and report via IOException to the calling thread.&lt;br/&gt;
This is only done for unmanaged HConnections (those that were created with HConnectionManager.createConnection(...) and are hence not reference counted. Reference counted HConnctions are treated as before.)&lt;/p&gt;

&lt;p&gt;This is needed to safely use the HConnection is a multithreaded long-lived AppServer setting.&lt;/p&gt;

&lt;p&gt;(In my tests I found that even 0.96 needs some more work here, but that&apos;s for a different jira.)&lt;/p&gt;</comment>
                            <comment id="13243052" author="lhofhansl" created="Sat, 31 Mar 2012 06:33:21 +0000"  >&lt;p&gt;v2 passes all tests locally.&lt;/p&gt;</comment>
                            <comment id="13243141" author="zhihyu@ebaysf.com" created="Sat, 31 Mar 2012 13:39:45 +0000"  >&lt;p&gt;+1 on patch.&lt;/p&gt;</comment>
                            <comment id="13243222" author="lhofhansl" created="Sat, 31 Mar 2012 16:42:06 +0000"  >&lt;p&gt;Thanks Ted.&lt;br/&gt;
The last question is: Should we do this for all HConnection (not just for unmanaged ones)? It means that HConnection would be able to recover from loss of ZK connection and the abort() method would only clear out the ZK trackers and never close or abort he connection. I&apos;d be in favor of that.&lt;/p&gt;

&lt;p&gt;@Ram and @Jieshan: Since would a more robust version of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5153&quot; title=&quot;Add retry logic in HConnectionImplementation#resetZooKeeperTrackers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5153&quot;&gt;&lt;del&gt;HBASE-5153&lt;/del&gt;&lt;/a&gt;, could you have a look at this?&lt;/p&gt;</comment>
                            <comment id="13243227" author="zhihyu@ebaysf.com" created="Sat, 31 Mar 2012 17:07:38 +0000"  >&lt;p&gt;Application to other HConnection makes sense.&lt;/p&gt;</comment>
                            <comment id="13243312" author="stack" created="Sat, 31 Mar 2012 22:11:50 +0000"  >&lt;p&gt;This is a perversion.&lt;/p&gt;

&lt;p&gt;If we pass in a connection from outside, down in the guts, do special handling that makes the connection and zookeeper handling do reconnect.  Its like we should be passing an Interface made at a higher-level of abstraction and then in the implementation, it did this fixup when connection breaks.&lt;/p&gt;

&lt;p&gt;With that out of the way, do whatever you need to make it work.  Patch looks fine. How did you test.  Would it be hard to make a unit test of it.  A unit test would be good codifying this perversion since it will be brittle being not whats expected.&lt;/p&gt;

&lt;p&gt;I&apos;m against changing the behavior of the default case in 0.92/0.94.   I&apos;m interested in problems you see in hbase-5153 or issues you have w/ the implementation there that being the 0.96 client.&lt;/p&gt;</comment>
                            <comment id="13243584" author="lhofhansl" created="Sat, 31 Mar 2012 22:33:02 +0000"  >&lt;p&gt;Here&apos;s a patch that always attempts reconnecting to ZK when a ZK connection is needed.&lt;/p&gt;</comment>
                            <comment id="13243589" author="lhofhansl" created="Sat, 31 Mar 2012 22:46:58 +0000"  >&lt;p&gt;&quot;perversion&quot; is hard word. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; It is just rechecking before each use whether the trackers are still usable. The timeout is handled through the HConnection&apos;s abort().&lt;/p&gt;

&lt;p&gt;The testing I&apos;ve done:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;ZK down, HBase down, start a client. Then start ZK, then HBase.&lt;/li&gt;
	&lt;li&gt;ZK up, HBase down, start client. Then start HBase&lt;/li&gt;
	&lt;li&gt;both ZK and HBase up, start client, kill HBase, restart HBase&lt;/li&gt;
	&lt;li&gt;both ZK and HBase up, start client, kill ZK and HBase restart&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The client just create a new HTable and then tries to get some rows in a loop.&lt;br/&gt;
In all cases the client should successfully be able to reconnect when both ZK and HBase are up.&lt;/p&gt;

&lt;p&gt;The problem I have seen in 0.94/0.92 without this patch even with managed connections is that after HConnection times out, it is unusable and even getting a new HTable does not fix the problem since behind the scenes the same HConnection is retrieved.&lt;/p&gt;

&lt;p&gt;Will think about an automated test. Do you like the version better that always does the recheck (and hence all the conditional for &quot;managed&quot; go away)?&lt;/p&gt;</comment>
                            <comment id="13243605" author="lhofhansl" created="Sat, 31 Mar 2012 23:34:26 +0000"  >&lt;p&gt;The more I look at, the more I do like the patch that changes the behavior in all cases.&lt;br/&gt;
It&apos;s simple and low risk: Just recheck the ZK trackers before they are needed.&lt;/p&gt;</comment>
                            <comment id="13243619" author="stack" created="Sun, 1 Apr 2012 00:26:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;The problem I have seen in 0.94/0.92 without this patch even with managed connections is that after HConnection times out, it is unusable and even getting a new HTable does not fix the problem since behind the scenes the same HConnection is retrieved.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Didn&apos;t we add a check for if the connection is bad?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Will think about an automated test. Do you like the version better that always does the recheck (and hence all the conditional for &quot;managed&quot; go away)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How does this work in trunk?  In trunk the work has been done so we don&apos;t really keep open a zk session any more.  For the sake of making tests run smoother, we&apos;ll do keep alive on zk session and hold it open 5 minutes and let it go if unused.&lt;/p&gt;

&lt;p&gt;I&apos;m +1 on making our stuff more resilient.  Resusing a dud hconnection either because the connection is dead or zk session died is hard to figure.&lt;/p&gt;

&lt;p&gt;How will this change a users&apos;s perception about how this stuff is used?  If your answer is that it helps in the extreme where the connection goes dead, and thats the only change a user percieves, then lets commit.  But we should include a test?  If you describe one, I can try help write it?&lt;/p&gt;

&lt;p&gt;You think this should go into 0.92?&lt;/p&gt;</comment>
                            <comment id="13243620" author="stack" created="Sun, 1 Apr 2012 00:28:31 +0000"  >&lt;p&gt;I looked at the &apos;all&apos; patch.  Looks good to me.  Am interested in how it changes API usage (if at all).&lt;/p&gt;</comment>
                            <comment id="13243637" author="lhofhansl" created="Sun, 1 Apr 2012 03:27:43 +0000"  >&lt;p&gt;I am not envisioning any API changes, just that the HConnection would no longer be ripped from under any HTables where there is a ZK connection loss.&lt;/p&gt;

&lt;p&gt;I ran all tests again, and TestReplication and TestZookeeper have some failures that are related. Looking.&lt;/p&gt;</comment>
                            <comment id="13243662" author="lhofhansl" created="Sun, 1 Apr 2012 05:42:07 +0000"  >&lt;p&gt;Found the problem.&lt;br/&gt;
The ClusterId could remain null permanently if HConnection.getZookeeperWatcher() was called. That would initialize HConnectionImplementation.zookeeper, and hence not reset clusterid in ensureZookeeperTrackers.&lt;br/&gt;
TestZookeeper.testClientSessionExpired does that.&lt;/p&gt;

&lt;p&gt;Also in TestZookeeper.testClientSessionExpired the state might be CONNECTING rather than CONNECTED depending on timing.&lt;/p&gt;

&lt;p&gt;Upon inspection I also made clusterId, rootRegionTracker, masterAddressTracker, and zooKeeper volatile, because they can be modified by a different thread, but are not exclusively accessed in a synchronized block (existing problem).&lt;/p&gt;

&lt;p&gt;New patch that fixes the problem, passes all tests.&lt;/p&gt;

&lt;p&gt;TestZookeeper seems to have good coverage. If I can think of more tests, I&apos;ll add them there.&lt;/p&gt;</comment>
                            <comment id="13243663" author="lhofhansl" created="Sun, 1 Apr 2012 05:44:37 +0000"  >&lt;p&gt;Upped to &quot;critical&quot;. Without this the HBase client is pretty much useless in an AppServer setting where client can outlive the HBase cluster and ZK ensemble.&lt;br/&gt;
(Testing within the Salesforce AppServer is how I noticed the problem initially.)&lt;/p&gt;</comment>
                            <comment id="13243665" author="lhofhansl" created="Sun, 1 Apr 2012 05:55:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;You think this should go into 0.92?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Probably. I guess most folks have clients that they restart frequently, use thrift, or asynchhbase. But in its current form using the standard HBase client in an app server is very error prone if the HBase/ZK cluster is ever serviced without bringing the app server down in lock step.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Didn&apos;t we add a check for if the connection is bad?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah with hbase-5153 but in 0.90 only. At some point we decided the fix there wasn&apos;t good and Ram patched it up for 0.90.&lt;br/&gt;
This should subsime &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5153&quot; title=&quot;Add retry logic in HConnectionImplementation#resetZooKeeperTrackers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5153&quot;&gt;&lt;del&gt;HBASE-5153&lt;/del&gt;&lt;/a&gt;. I&apos;m happy to even put this in 0.90, but that&apos;s up to Ram.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m interested in problems you see in hbase-5153 or issues you have w/ the implementation there that being the 0.96 client.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What I saw in 0.96 is that the client was blocked for a very long time (gave up after a few minutes), even though I had set all timeouts to low values. This is also deadly in an app server setting. Might be a simple fix there, didn&apos;t dig deeper.&lt;/p&gt;</comment>
                            <comment id="13243874" author="stack" created="Sun, 1 Apr 2012 21:50:38 +0000"  >&lt;p&gt;On commit, change this &apos;+      LOG.debug(&quot;Abort&quot;, t);&apos; to include the passed in msg?&lt;/p&gt;

&lt;p&gt;Else, +1 on the patch.  Let me ask N if he thinks TRUNK can pick up anything from this patch (maybe his keepalive should do this auto-reconnect but maybe it doesn&apos;t need it).  What were you doing w/ it was taking a long time to recover?&lt;/p&gt;

</comment>
                            <comment id="13243991" author="lhofhansl" created="Mon, 2 Apr 2012 06:08:47 +0000"  >&lt;p&gt;Patch that removes the log statement Stack mentioned (had it in there for earlier debugging, forgot to remove it).&lt;/p&gt;

&lt;p&gt;Also adds a simple test with an HConnection that is created before the mini-cluster is started to prove that initialization is indeed lazy.&lt;br/&gt;
(can&apos;t test with stopping and restarting the minicluster as new random ports are used each time).&lt;/p&gt;</comment>
                            <comment id="13243992" author="lhofhansl" created="Mon, 2 Apr 2012 06:09:21 +0000"  >&lt;p&gt;all-v3 is what I like to commit tomorrow if there are no objections.&lt;/p&gt;</comment>
                            <comment id="13244093" author="jeason" created="Mon, 2 Apr 2012 09:39:44 +0000"  >&lt;p&gt;Everything seems good to me. Only a minor doubt, is it necessary to close zooKeeper before set it as null?&lt;br/&gt;
If HConnectionImplementation#managed is true, HConnectionImplementation#abort doesn&apos;t set closed to true, just calls close method. It makes sense to me&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. So the retry logic introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5153&quot; title=&quot;Add retry logic in HConnectionImplementation#resetZooKeeperTrackers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5153&quot;&gt;&lt;del&gt;HBASE-5153&lt;/del&gt;&lt;/a&gt; seems redundant.&lt;br/&gt;
If one want to manage the connection by himself. If the connection is aborted. We should suggest to recreate the HConnection and HTable, right? &lt;/p&gt;</comment>
                            <comment id="13244359" author="lhofhansl" created="Mon, 2 Apr 2012 17:30:54 +0000"  >&lt;p&gt;Presumably close it not needed since the connection is known to be down in this case. To be save, I&apos;ll add that, and make sure it doesn&apos;t cause another hang.&lt;/p&gt;

&lt;p&gt;I think this is better than &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5153&quot; title=&quot;Add retry logic in HConnectionImplementation#resetZooKeeperTrackers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5153&quot;&gt;&lt;del&gt;HBASE-5153&lt;/del&gt;&lt;/a&gt;, because it attempts to reconnect when the connection is needed and not when it was lost (in which case it is likely that the next retry will fail as well, leading to long hangs with no change for the caller to notice).&lt;/p&gt;</comment>
                            <comment id="13244363" author="lhofhansl" created="Mon, 2 Apr 2012 17:35:18 +0000"  >&lt;p&gt;Oh, and thanks for taking a look Jieshan &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13244367" author="stack" created="Mon, 2 Apr 2012 17:45:19 +0000"  >&lt;p&gt;@Nkeywal  Hows&apos; this relate to your TRUNK work (if at all)?&lt;/p&gt;</comment>
                            <comment id="13244420" author="lhofhansl" created="Mon, 2 Apr 2012 18:35:53 +0000"  >&lt;p&gt;One other strangeness I found is that none of ZKUtil methods actually throw exceptions. They retry (via RecoverableZooKeeper) and then just log a message if there is a failure. This is especially annoying with ZooKeeperWatcher, because there is no way of telling whether the connection succeeded of not from the outside.&lt;/p&gt;</comment>
                            <comment id="13244436" author="stack" created="Mon, 2 Apr 2012 18:47:24 +0000"  >&lt;p&gt;Can we add an isAlive to ZKW?&lt;/p&gt;</comment>
                            <comment id="13244440" author="nkeywal" created="Mon, 2 Apr 2012 18:50:48 +0000"  >&lt;p&gt;.bq none of ZKUtil methods actually throw exceptions&lt;br/&gt;
From what is see on 0.96 it should, as the return is not reached: the pattern is too call keeperException, and keeperException throws an exception.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  public void keeperException(KeeperException ke)
  throws KeeperException {
    LOG.error(prefix(&quot;Received unexpected KeeperException, re-throwing exception&quot;), ke);
    throw ke;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13244468" author="lhofhansl" created="Mon, 2 Apr 2012 19:09:00 +0000"  >&lt;p&gt;Yeah, my comment was wrong. It&apos;s not generally doing that.&lt;/p&gt;

&lt;p&gt;What I do find is if the ZK quorum is down, none of getZookeeperWatcher(), masterAddressTracker.start(), and rootRegionTracker.start() actually fail. They just retry and then happily return, which is as designed, because they are asynchronous.&lt;br/&gt;
Would be nice to have a isAlive or waitForConnect method on ZKW that would throw if the connection could not be established.&lt;/p&gt;

&lt;p&gt;The attached patch is still a vast improvement, but it could be made better (even with zk timeout set to 100ms and retries to 3, it still take 22s for ensureZookeeperTrackers to finish).&lt;/p&gt;</comment>
                            <comment id="13244470" author="lhofhansl" created="Mon, 2 Apr 2012 19:11:03 +0000"  >&lt;p&gt;Even isAlive or waitForConnect would need to rely on a timeout, so we wouldn&apos;t have won anything really.&lt;/p&gt;</comment>
                            <comment id="13244475" author="lhofhansl" created="Mon, 2 Apr 2012 19:15:38 +0000"  >&lt;p&gt;I think this is as good as we can get in 0.94.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Removed the exception handling from ensureZookeeperTrackers none of these methods throw.&lt;/li&gt;
	&lt;li&gt;added getZookeeperWatcher to two methods that just need a ZKW.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The key is that an HConnection will never be left in a permanently useless state. Can file another jira for better timeouts.&lt;/p&gt;</comment>
                            <comment id="13244484" author="nkeywal" created="Mon, 2 Apr 2012 19:24:06 +0000"  >&lt;p&gt;In 0.96 this should work, with the restriction that the logic is that you can get a non working connection, that will get fixed when you try to use it. It&apos;s a different mechanism than the one for HBaseAdmin, as HBaseAdmin first check the connection. Thz ZK mechanism is more efficient (you save a remote call to check that the connection is really working), but is more complex.&lt;/p&gt;

&lt;p&gt;However it seems it does not work at the end:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What I saw in 0.96 is that the client was blocked for a very long time (gave up after a few minutes), even though I had set all timeouts to low values. This is also deadly in an app server setting. Might be a simple fix there, didn&apos;t dig deeper.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;@lars What did you exactly do? I can do the fix it on 0.96.&lt;/p&gt;</comment>
                            <comment id="13244577" author="lhofhansl" created="Mon, 2 Apr 2012 20:37:35 +0000"  >&lt;p&gt;Let me dig into 0.96 after I get this into 0.94... Wanna cut RC1 soon.&lt;/p&gt;

&lt;p&gt;From the past comments here I see no objections to posted patch... Will commit soon. Please speak up if you disagree.&lt;/p&gt;</comment>
                            <comment id="13244724" author="lhofhansl" created="Mon, 2 Apr 2012 22:12:56 +0000"  >&lt;p&gt;Committed to 0.94 only&lt;/p&gt;</comment>
                            <comment id="13244806" author="hudson" created="Mon, 2 Apr 2012 23:07:58 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #80 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/80/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/80/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5682&quot; title=&quot;Allow HConnectionImplementation to recover from ZK connection loss (for 0.94 only)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5682&quot;&gt;&lt;del&gt;HBASE-5682&lt;/del&gt;&lt;/a&gt; Allow HConnectionImplementation to recover from ZK connection loss (Revision 1308596)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13246904" author="hudson" created="Thu, 5 Apr 2012 00:53:33 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #7 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/7/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/7/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5682&quot; title=&quot;Allow HConnectionImplementation to recover from ZK connection loss (for 0.94 only)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5682&quot;&gt;&lt;del&gt;HBASE-5682&lt;/del&gt;&lt;/a&gt; Allow HConnectionImplementation to recover from ZK connection loss (Revision 1308596)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
larsh : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/branches/0.94/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java&lt;/li&gt;
	&lt;li&gt;/hbase/branches/0.94/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13284327" author="ram_krish" created="Mon, 28 May 2012 08:50:54 +0000"  >&lt;p&gt;@Lars&lt;br/&gt;
See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6115&quot; title=&quot;NullPointerException is thrown when root and meta table regions are assigning to another RS.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6115&quot;&gt;&lt;del&gt;HBASE-6115&lt;/del&gt;&lt;/a&gt;.  As we are not waiting for the root location to come up we get NPE now.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12520813" name="5682-all-v2.txt" size="7590" author="lhofhansl" created="Sun, 1 Apr 2012 05:42:06 +0000"/>
                            <attachment id="12520873" name="5682-all-v3.txt" size="12285" author="lhofhansl" created="Mon, 2 Apr 2012 06:08:44 +0000"/>
                            <attachment id="12521017" name="5682-all-v4.txt" size="12486" author="lhofhansl" created="Mon, 2 Apr 2012 19:15:38 +0000"/>
                            <attachment id="12520804" name="5682-all.txt" size="5632" author="lhofhansl" created="Sat, 31 Mar 2012 22:33:02 +0000"/>
                            <attachment id="12520762" name="5682-v2.txt" size="5972" author="lhofhansl" created="Sat, 31 Mar 2012 04:01:27 +0000"/>
                            <attachment id="12520696" name="5682.txt" size="5498" author="lhofhansl" created="Sat, 31 Mar 2012 00:04:20 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 31 Mar 2012 04:31:39 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>233974</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 29 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02guv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12332</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>