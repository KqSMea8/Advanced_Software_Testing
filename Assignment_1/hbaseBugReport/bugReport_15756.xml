<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:06:53 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15756/HBASE-15756.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15756] Pluggable RpcServer</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15756</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Current we use a simple RpcServer, and can not configure and use other implementation.This issue is to make the RpcServer pluggable, so we can make other implementation for example netty rpc server. Patch will upload laterly&lt;/p&gt;</description>
                <environment></environment>
        <key id="12964560">HBASE-15756</key>
            <summary>Pluggable RpcServer</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="aoxiang">binlijin</assignee>
                                    <reporter username="aoxiang">binlijin</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 May 2016 08:21:57 +0000</created>
                <updated>Tue, 13 Dec 2016 02:29:42 +0000</updated>
                                                            <fixVersion>2.0.0</fixVersion>
                                    <component>Performance</component>
                    <component>rpc</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>16</watches>
                                                                                                            <comments>
                            <comment id="15270186" author="stack" created="Wed, 4 May 2016 06:29:03 +0000"  >&lt;p&gt;I&apos;d suggest you not waste your time making it pluggable &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;. Just change our rpcserver be netty posting some numbers that show it has same basic performance and we&apos;ll get it in?&lt;/p&gt;

&lt;p&gt;One kink is what to do w/ the &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; work? These lads are busy making our write pipeline offheap. It starts with a pool of fixed-sized offheap buffers to take the request (and thereby avoid an alloccation by the socket). Going netty, IIRC, you get netty ByteBufs to work with as opposed to ByteBuffers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; Any comment in here sir?&lt;/p&gt;</comment>
                            <comment id="15270209" author="aoxiang" created="Wed, 4 May 2016 07:04:08 +0000"  >&lt;p&gt;@stack&#65292;i test with some quick implements, and test it with ycsb:&lt;br/&gt;
1 Server machine start with hdfs, and hbase.&lt;br/&gt;
1 Client, 200 thread, value=128B.&lt;br/&gt;
First create a test table and write 1w record, and random read the 1w record.&lt;br/&gt;
With 4 connection(hbase.client.ipc.pool.size=4), i can see the throughput is the best, with the current RpcServer the result is almost 25W QPS But with NettyRpcServer, it is has 35W QPS.&lt;/p&gt;</comment>
                            <comment id="15270241" author="aoxiang" created="Wed, 4 May 2016 07:25:17 +0000"  >&lt;p&gt;I only test the read performance.&lt;/p&gt;</comment>
                            <comment id="15270753" author="apurtell" created="Wed, 4 May 2016 14:45:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;d suggest you not waste your time making it pluggable binlijin. Just change our rpcserver be netty posting some numbers that show it has same basic performance and we&apos;ll get it in?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;However, it might be nice, like with Thrift, if we can compose some request handling and protocol features at runtime via configuration or command line switches. Specifically, since we&apos;d have built on the Netty stack, and recent versions of Netty ship with HTTP/2 support, I&apos;d like to have the option of tunneling the HBase protocol through HTTP/2 &quot;almost for free&quot;. &lt;/p&gt;</comment>
                            <comment id="15271325" author="stack" created="Wed, 4 May 2016 19:52:43 +0000"  >&lt;p&gt;I forgot to say, yeah, justify why we should do netty instead of what we currently have. It seems like you&apos;ve started in on that.&lt;/p&gt;

&lt;p&gt;I&apos;ve been doing some perf work trying to get our random read throughput up. Currently I&apos;ve been focused on the handoff from Socket/Listener to Readers and then via scheduling Queues to Handlers. If I have the Handler doing the read of the request, I can double throughput (IIRC). How you think Netty improves stuff in here &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;? Thanks.&lt;/p&gt;</comment>
                            <comment id="15271327" author="stack" created="Wed, 4 May 2016 19:53:49 +0000"  >&lt;p&gt;The argument for Netty could be the built-in features it has that we don&apos;t have in our hadoop-derived RPC: e.g. HTTP/2 support.&lt;/p&gt;</comment>
                            <comment id="15271727" author="aoxiang" created="Thu, 5 May 2016 00:52:54 +0000"  >&lt;p&gt;I also test the situation: Readers read ByteBuffer data, and handoff to Decoders, Decoders decode the ByteBuffer data to Call and then handoff to Handlers via scheduling Queues, and i find this will have 10% improvements and not as better as NettyRpcServer. &lt;/p&gt;</comment>
                            <comment id="15271731" author="aoxiang" created="Thu, 5 May 2016 00:57:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrew.purtell%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andrew.purtell@gmail.com&quot;&gt;Andrew Purtell&lt;/a&gt;, i have a question with the thrift client, i find the thrift client is not thread safe, and so the connection can not share by multiple thread?&lt;br/&gt;
And i also test thrift with yscb and find it is not better as netty.&lt;/p&gt;</comment>
                            <comment id="15271732" author="aoxiang" created="Thu, 5 May 2016 01:00:03 +0000"  >&lt;p&gt;So if we can make the RpcServer pluggable, and if we do not want to have HTTP/2, so can we use the NettyRpcServer?&lt;br/&gt;
If we want the HTTP/2, we can use the current RpcServer?&lt;/p&gt;</comment>
                            <comment id="15271750" author="aoxiang" created="Thu, 5 May 2016 01:14:34 +0000"  >&lt;p&gt;Use Netty, we can keep the scheduling Queues, we still have Scheduling&apos;s advantage (&lt;a href=&quot;http://blog.cloudera.com/blog/2014/12/new-in-cdh-5-2-improvements-for-running-multiple-workloads-on-a-single-hbase-cluster/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://blog.cloudera.com/blog/2014/12/new-in-cdh-5-2-improvements-for-running-multiple-workloads-on-a-single-hbase-cluster/&lt;/a&gt;).&lt;/p&gt;
</comment>
                            <comment id="15271889" author="ram_krish" created="Thu, 5 May 2016 05:01:37 +0000"  >&lt;p&gt;Even the NettyRpcServer will do the read to the NettyByteBuffs and handoffs to the Decoder and the decoder reads the NettyByteBuff data and handoffs to the handlers via the queues right?&lt;br/&gt;
So you see more improvement in terms of NettyRpcServer because of the Netty&apos;s efficiency?&lt;br/&gt;
In terms of offheap/onheap ByteBuffers when we try to use Unsafe way of accessing we could see that the performance was better than netty. Hence while working on the offheap read path we concluded in going with nio ByteBuffers only. Like to know more here and should be pretty much useful too.&lt;/p&gt;</comment>
                            <comment id="15272141" author="carp84" created="Thu, 5 May 2016 09:55:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; is my workmate so allow me to give some supplement here (it seems you forgot to add me into subscribers buddy &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; and I just noticed it... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;)&lt;/p&gt;

&lt;p&gt;As found in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15619&quot; title=&quot;Performance regression observed: Empty random read(get) performance of branch-1 worse than 0.98&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15619&quot;&gt;HBASE-15619&lt;/a&gt;, read against empty table shows perf regression on RPC layer, so we are trying to improve this part. And since 2.0 is still way to go, we chose to start from trying netty to replace our current RpcServer, and got the number &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; posted above.&lt;/p&gt;

&lt;p&gt;It&apos;s really good to know your thoughts &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt;, and thanks for sharing the information that we even see a better performance than netty when using ByteBuffers (Unsafe way of accessing), but I&apos;m still wondering whether there&apos;s other parts that netty does good besides ByteBufs, and we will dig deeper into this part (and please let us know if you already did some and had a conclusion, thanks).&lt;/p&gt;

&lt;p&gt;Meanwhile, if offheap stuff won&apos;t be available in branch-1 and NettyRpcServer truly does better than our current native one, maybe it&apos;s still useful for branch-1 users? Mind share your thoughts sir &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoopsamjohn&quot; class=&quot;user-hover&quot; rel=&quot;anoopsamjohn&quot;&gt;Anoop Sam John&lt;/a&gt;? Thanks.&lt;/p&gt;</comment>
                            <comment id="15272196" author="jurmous" created="Thu, 5 May 2016 10:52:45 +0000"  >&lt;p&gt;If you have some questions on the Netty Client which I built earlier I am here. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Possibly you can be helped by some of the work of the client like the SaslClientHandlers and more.&lt;/p&gt;

&lt;p&gt;A Netty RpcServer would make it easy to quickly experiment with other transport methods like HTTP2. Although our current wire format already has the same advantages like multiple requests over the same connection and more. But I think to move our transport mechanism closer to HTTP/2 would make it more accessible to create new clients.&lt;/p&gt;

&lt;p&gt;A pluggable RpcServer would make it easy to have it stabilize and compared against the current RpcServer.&lt;/p&gt;</comment>
                            <comment id="15272589" author="stack" created="Thu, 5 May 2016 16:33:22 +0000"  >&lt;p&gt;Pluggable for branch-1 could be a good way to go. I was suggesting not bothering w/ pluggable aspect because when options, users near always go with whatever is the default. If netty superior &amp;#8211; perf, options &amp;#8211; then lets just move there wholesale.&lt;/p&gt;

&lt;p&gt;I&apos;ve been messing with random read workloads trying to overrun a machine where all data is in cache so no i/o (working with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=danielpol&quot; class=&quot;user-hover&quot; rel=&quot;danielpol&quot;&gt;Daniel Pol&lt;/a&gt;). I am having difficulty driving all CPUs. There seems to be always plenty of idle. After removing and temporarily hacking out the points at which we have contention, I&apos;m still unable to use all CPU.&lt;/p&gt;

&lt;p&gt;If you have a patch for netty, put it up and I&apos;ll give it a go.&lt;/p&gt;</comment>
                            <comment id="15272823" author="ghelmling" created="Thu, 5 May 2016 18:47:02 +0000"  >&lt;p&gt;We had a pluggable RpcServer back when security was first implemented, in order to make security completely optional.  What it typically meant was that patches were only tested against the default RPC server and bugs or regressions would show up in the non-default server due to other changes and have to be fixed up after the fact.  Let&apos;s not go back to that.&lt;/p&gt;

&lt;p&gt;If a netty-based server shows clear gains in perf/flexibility/maintainability over our current implementation, let&apos;s just switch to that.  But of course we can&apos;t break the wire format itself.&lt;/p&gt;

&lt;p&gt;If we need to stabilize an implementation, do it in a branch while it is being developed.  In my opinion, having been through this, making the server pluggable just adds complexity without adding any real value there.&lt;/p&gt;

&lt;p&gt;I&apos;m happy to help review a patch with some numbers behind it as well.&lt;/p&gt;</comment>
                            <comment id="15273504" author="carp84" created="Fri, 6 May 2016 02:04:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;I was suggesting not bothering w/ pluggable aspect because when options, users near always go with whatever is the default. If netty superior &#8211; perf, options &#8211; then lets just move there wholesale.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I see, but it seems offheap work would be affected if we completely move to netty? Collaboration needed here with Anoop and Ram, right?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If you have a patch for netty, put it up and I&apos;ll give it a go.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No problem. We were testing with branch-1, so the patch is also for branch-1, is this ok or prefer a master-branch one?&lt;/p&gt;</comment>
                            <comment id="15273536" author="carp84" created="Fri, 6 May 2016 02:18:44 +0000"  >&lt;p&gt;Thanks for chim in &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ghelmling&quot; class=&quot;user-hover&quot; rel=&quot;ghelmling&quot;&gt;Gary Helmling&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What it typically meant was that patches were only tested against the default RPC server and bugs or regressions would show up in the non-default server due to other changes and have to be fixed up after the fact... making the server pluggable just adds complexity without adding any real value there.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Understood the concerns, but maybe we could use Parameterized way to cover UT? And it seems we already made RpcClient pluggable... One advantage of making it pluggable may be that we could work in parallel with offheap work? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we need to stabilize an implementation, do it in a branch while it is being developed&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I see, thanks for the note sir.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m happy to help review a patch with some numbers behind it as well.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks for offering the help sir, will let you know when we&apos;re well prepared. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15273540" author="carp84" created="Fri, 6 May 2016 02:24:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;If you have some questions on the Netty Client which I built earlier I am here. Possibly you can be helped by some of the work of the client like the SaslClientHandlers and more.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks for chim in sir, and yes we already refer to AsyncRpcClient, thanks for the note anyway. Will definitely reach for you if we have any question later. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15273615" author="stack" created="Fri, 6 May 2016 04:12:15 +0000"  >&lt;p&gt;Yes. Need to make sure we are good w/ &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; work. Branch-1 works better for me.&lt;/p&gt;</comment>
                            <comment id="15273671" author="carp84" created="Fri, 6 May 2016 05:51:41 +0000"  >&lt;p&gt;Ok, got it. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; please upload our draft patch for branch-1 so stack could try it out. Thanks.&lt;/p&gt;</comment>
                            <comment id="15273739" author="aoxiang" created="Fri, 6 May 2016 07:16:51 +0000"  >&lt;p&gt;@stack, upload the patch for branch-1, and this patch is a draft for fast perf test.&lt;br/&gt;
I test it with 1.1.2 and netty3 &lt;/p&gt;</comment>
                            <comment id="15273743" author="aoxiang" created="Fri, 6 May 2016 07:20:06 +0000"  >&lt;p&gt;I will test it with netty4 this days.&lt;/p&gt;</comment>
                            <comment id="15273831" author="jurmous" created="Fri, 6 May 2016 09:17:04 +0000"  >&lt;p&gt;I would highly prefer a Netty 4 implementation like the Client. (io.netty instead of org.jboss.netty) A lot of improvements where added to 4.x versions, HTTP/2 is for example only available from 4.1 which will go final soon. &lt;/p&gt;</comment>
                            <comment id="15273832" author="aoxiang" created="Fri, 6 May 2016 09:19:20 +0000"  >&lt;p&gt;@Jurriaan Mous, thanks for you advice, we will change it to Netty4. &lt;/p&gt;</comment>
                            <comment id="15275981" author="anoop.hbase" created="Mon, 9 May 2016 05:54:25 +0000"  >&lt;p&gt;Sorry for being late here.&lt;br/&gt;
I had a go at the new NettRpcServer class.. I have some concerns/Qs&lt;br/&gt;
In request side I can see that to HBaseDecoder, we pass a ChannelBuffer and from that again read the request into an on heap NIO BB.  In response part, we have our cells in an off heap BB and we have a BufferChain.  This is again written into a ChannelBuffer. That is DynamicChannelBuffer.  So we have again a copy from off heap area.  And then this is passed to down layers of netty..  Am not sure how that area being implemented.&lt;br/&gt;
In our current RpcServer it is very less copy across now.  While response, we will accumulate the cells into off heap BB (this is a pooled BB from BBBPool) and that is passed to SocketChannel.  When we pass a DBB to it, there is no further copy and given to OS layers.  When request processing, as of now, we pass an on heap on demand created BB. The socket layer will 1st read the request bytes into a off heap BB and copy back that into passed on heap BB.  In write path offheaping work we try to change.  We will pass DBBs obtained from pool and socket layer will directy read into it and we will make Cells directly over this DBBs and later the cells are data copied into MSLAB area just before adding to memstore.&lt;/p&gt;

&lt;p&gt;AFAIK, the netty ByteBuff is having all support to work with NIO BB.  We can just make wrapper both side with out any data copy.  So can u tell the details abt the down layers of actual n/w movement.Can we do like above with just 1 copy?&lt;/p&gt;</comment>
                            <comment id="15276139" author="ram_krish" created="Mon, 9 May 2016 09:12:28 +0000"  >&lt;p&gt;Seeing the patch the operations seems to be done as it was previously in branch-1. The exact number of copies or handovers occur between onheap to offheap buffers. So where does netty ByteBufs do wel? Can that be seen clearly to understand things better?&lt;/p&gt;</comment>
                            <comment id="15400127" author="stack" created="Fri, 29 Jul 2016 22:24:17 +0000"  >&lt;p&gt;I tried the patch. There are three runs: current state of branch-1, then patch-as-is, and then patch with max i/o workers of 6. The latter change was to make it so netty config more resembled the first runs config (6 reader threads).&lt;/p&gt;

&lt;p&gt;Looking at the charts, there is a bit of an improvement. We are doing 110k ops/second vs about 105k ops/second. This is not a large difference but at the same time, given how little of an incursion has been made &amp;#8211; just swapping the Listern/Readers for netty boss thread and &apos;i/o workers&apos; &amp;#8211; it is pretty nice. No tuning was done.&lt;/p&gt;

&lt;p&gt;How hard to do a netty4 &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; in here? I could try that and we could just switch?&lt;/p&gt;

&lt;p&gt;What you think of the questions &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ramkrishna.s.vasudevan&lt;/a&gt; are asking? It&apos;d be critical that the rpcserver does not get in the way of their offheaping (offheap netty BBuff as far as the MSLAB blocks?) &lt;/p&gt;</comment>
                            <comment id="15400386" author="aoxiang" created="Sat, 30 Jul 2016 02:30:45 +0000"  >&lt;p&gt;First let me try to explain Netty&apos;s Thread Model first: &lt;br/&gt;
Netty&apos;s NioWork read request from channel, decode into Call and handoff it to HBase Handlers via Queue, when HBase Handlers done all the work, it will handoff the result to the channel&apos;s WriteBufferQueue, then Netty&apos;s NioWork batch write the result to the client.&lt;br/&gt;
So i think the big different Between the NettyRpcServer with the current RpcServer is the write response.&lt;br/&gt;
@Jurriaan Mous, if there is wrong, please correct me.&lt;/p&gt;</comment>
                            <comment id="15400398" author="aoxiang" created="Sat, 30 Jul 2016 02:58:35 +0000"  >&lt;p&gt;Netty&apos;s NioWork read request from channel to OffHeap, current we copy to heap to decode into the Call.&lt;br/&gt;
HBaseProtocolEncoder write a CompositeChannelBuffer result which a wrap with BufferChain&apos;s buffers.&lt;br/&gt;
Netty&apos;s NioWork write result to channel via ((GatheringByteChannel) ch).write(buffers);&lt;br/&gt;
So i think there is no further copy.&lt;br/&gt;
This is the NettyRpcServer_forperf.patch with hbase branch-1.&lt;br/&gt;
If there is wrong, please correct me.&lt;br/&gt;
So i think  Anoop Sam John&apos;s concern can be resolved.&lt;/p&gt;</comment>
                            <comment id="15400400" author="aoxiang" created="Sat, 30 Jul 2016 03:02:04 +0000"  >&lt;p&gt;The result is much different with my&apos;s result, i am curious how you do the test, my dear sir.&lt;/p&gt;</comment>
                            <comment id="15400410" author="aoxiang" created="Sat, 30 Jul 2016 03:17:13 +0000"  >&lt;p&gt;The more client connections, the result will be more different. So i think i know you benchmark test.&lt;/p&gt;</comment>
                            <comment id="15400416" author="aoxiang" created="Sat, 30 Jul 2016 03:26:26 +0000"  >&lt;p&gt;you can change the hbase.client.ipc.pool.size to see how the result is different.&lt;/p&gt;</comment>
                            <comment id="15400712" author="stack" created="Sat, 30 Jul 2016 15:37:39 +0000"  >&lt;p&gt;Change it to what? I have 24 * 8 * 32 clients running against a single server.&lt;/p&gt;

&lt;p&gt;It is ycsb doing the random read test. 8 machines going against a single server that has all data cached.&lt;/p&gt;</comment>
                            <comment id="15400713" author="stack" created="Sat, 30 Jul 2016 15:42:49 +0000"  >&lt;p&gt;Yes. It is the same as we have except in regard to response being possible multithreaded.&lt;/p&gt;

&lt;p&gt;Our Listener thread == Boss Worker&lt;br/&gt;
Our Reader threads == Worker&lt;br/&gt;
Our single Responder == Worker (but we only have one Responder but profiling, the Handler thread usually writes direct on the socket)&lt;/p&gt;

&lt;p&gt;I didn&apos;t expect any speedup but perhaps netty does Select handling better and responding, we may clash more often and netty does a better job doing the reply. I could test more extensively. Any chance of a patch with netty4?  We don&apos;t want to commit a netty3-based patch.&lt;/p&gt;</comment>
                            <comment id="15400715" author="stack" created="Sat, 30 Jul 2016 15:46:20 +0000"  >&lt;p&gt;We&apos;ll have to dig. We need to have Netty make use of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;&apos;s nice pool of fixed-size offheap buffers.&lt;/p&gt;</comment>
                            <comment id="15400860" author="aoxiang" created="Sat, 30 Jul 2016 22:28:19 +0000"  >&lt;p&gt;I think handlers will have more lock contention when trying to write response to a same client.&lt;/p&gt;

&lt;p&gt;I will do a patch with Netty4, wo wait for a moment.&lt;/p&gt;</comment>
                            <comment id="15400867" author="aoxiang" created="Sat, 30 Jul 2016 22:46:12 +0000"  >&lt;p&gt;Looks like i am guessing wrong.&lt;br/&gt;
But a region server have only 105k ops/second is too little.&lt;br/&gt;
When i test, i can see the throughput is 300k ops/second(Current RpcServer) and 420k ops/second(NettyRpcServer) for a single region server, use ycsb doing random read test, every row have only one qualifier, and the valueLength=128B, all data cache in LruBlockCache.&lt;/p&gt;</comment>
                            <comment id="15400868" author="aoxiang" created="Sat, 30 Jul 2016 22:46:41 +0000"  >&lt;p&gt;Yes.&lt;/p&gt;</comment>
                            <comment id="15400929" author="stack" created="Sun, 31 Jul 2016 02:56:38 +0000"  >&lt;p&gt;I have defaults which is ten columns with value of 128 bytes?&lt;/p&gt;</comment>
                            <comment id="15401477" author="aoxiang" created="Mon, 1 Aug 2016 03:28:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; Upload Netty4RpcServer_forperf.patch which base on Netty4.&lt;/p&gt;</comment>
                            <comment id="15401512" author="stack" created="Mon, 1 Aug 2016 04:25:27 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; Will try it in morning boss.&lt;/p&gt;</comment>
                            <comment id="15406946" author="stack" created="Thu, 4 Aug 2016 01:18:26 +0000"  >&lt;p&gt;Here are a few graphs &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As is, we are slower ... 120k vs ~105k with the patch as is. Looking at thread dump, the nioEventLoopGroup for workers is spinning up lots of threads.... 40 or 50? Trying to do apples to apples, again I put a bound on threads created making netty worker count == readers count. When I do this, I get closer... 120k vs 115k or so?  I then set hbase.rpc.server.nativetransport to true so we use the alternative epoll and then I get almost the same: 120k vs ~199k.&lt;/p&gt;

&lt;p&gt;Looking at the thread stack, I see this:&lt;/p&gt;

&lt;p&gt;3683 &quot;epollEventLoopGroup-3-5&quot; #40 prio=10 os_prio=0 tid=0x000000000260f520 nid=0xe09b runnable &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f5ac437e000&amp;#93;&lt;/span&gt;&lt;br/&gt;
3684    java.lang.Thread.State: RUNNABLE&lt;br/&gt;
3685   at sun.misc.Cleaner.add(Cleaner.java:79)&lt;br/&gt;
3686   - locked &amp;lt;0x00007f5bde303070&amp;gt; (a java.lang.Class for sun.misc.Cleaner)&lt;br/&gt;
3687   at sun.misc.Cleaner.create(Cleaner.java:133)&lt;br/&gt;
3688   at java.nio.DirectByteBuffer.&amp;lt;init&amp;gt;(DirectByteBuffer.java:139)&lt;br/&gt;
3689   at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)&lt;br/&gt;
3690   at io.netty.buffer.UnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledUnsafeDirectByteBuf.java:108)&lt;br/&gt;
3691   at io.netty.buffer.UnpooledUnsafeDirectByteBuf.&amp;lt;init&amp;gt;(UnpooledUnsafeDirectByteBuf.java:69)&lt;br/&gt;
3692   at io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:50)&lt;br/&gt;
3693   at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:155)&lt;br/&gt;
3694   at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:146)&lt;br/&gt;
3695   at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:107)&lt;br/&gt;
3696   at io.netty.channel.AdaptiveRecvByteBufAllocator$HandleImpl.allocate(AdaptiveRecvByteBufAllocator.java:104)&lt;br/&gt;
3697   at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:712)&lt;br/&gt;
3698   at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)&lt;br/&gt;
3699   at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)&lt;br/&gt;
3700   at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)&lt;br/&gt;
3701   at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)&lt;br/&gt;
3702   at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;We seem to be doing direct allocations each time. That&apos;ll slow us down (also explains a slightly higher GC time). I and (@appy) messed around trying to use a buffer pool enabling this...&lt;/p&gt;

&lt;p&gt;  bootstrap.option(ChannelOption.ALLOCATOR,    PooledByteBufAllocator.DEFAULT);&lt;/p&gt;

&lt;p&gt;... and messing in code but our server hangs. I can mess more but thought I&apos;d ask you first since you are probably coming on line now why you had the above commented out.&lt;/p&gt;

&lt;p&gt;Hopefully we can put our own allocator in here... one that does &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;&apos;s fixed size pool of buffers... hmmmm... or this might take some work... We&apos;ll see. Anyways, any thoughts on above &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; appreciated. If we can make netty as fast &amp;#8211; or faster &amp;#8211; and we can make it so it plays nicely with the offheaping of the write path, lets slot it in. Thanks.&lt;/p&gt;
</comment>
                            <comment id="15406987" author="aoxiang" created="Thu, 4 Aug 2016 02:04:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; What about patch with the PooledByteBufAllocator.patch? &lt;/p&gt;</comment>
                            <comment id="15407240" author="stack" created="Thu, 4 Aug 2016 06:09:20 +0000"  >&lt;p&gt;Thank you for fast turnaround &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;. I&apos;m pretty sure I got your patch in but it still shows this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;epollEventLoopGroup-3-3&quot;&lt;/span&gt; #37 prio=10 os_prio=0 tid=0x00007fb95421c470 nid=0x10e3d runnable [0x00007fb153d6e000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
        at sun.misc.Unsafe.freeMemory(Native Method)
        at java.nio.DirectByteBuffer$Deallocator.run(DirectByteBuffer.java:94)
        at sun.misc.Cleaner.clean(Cleaner.java:143)
        at io.netty.util.internal.Cleaner0.freeDirectBuffer(Cleaner0.java:66)
        at io.netty.util.internal.PlatformDependent0.freeDirectBuffer(PlatformDependent0.java:147)
        at io.netty.util.internal.PlatformDependent.freeDirectBuffer(PlatformDependent.java:274)
        at io.netty.buffer.UnpooledUnsafeDirectByteBuf.freeDirect(UnpooledUnsafeDirectByteBuf.java:115)
        at io.netty.buffer.UnpooledUnsafeDirectByteBuf.deallocate(UnpooledUnsafeDirectByteBuf.java:507)
        at io.netty.buffer.AbstractReferenceCountedByteBuf.release(AbstractReferenceCountedByteBuf.java:106)
        at org.apache.hadoop.hbase.ipc.Netty4RpcServer$NettyProtocolDecoder.channelRead(Netty4RpcServer.java:1161)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
        at io.netty.channel.epoll.EpollSocketChannel$EpollSocketUnsafe.epollInReady(EpollSocketChannel.java:722)
        at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:326)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:264)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.... and throughput is still below default. Do you see Pooled buffer use w/ your changes?&lt;/p&gt;</comment>
                            <comment id="15407682" author="aoxiang" created="Thu, 4 Aug 2016 12:43:42 +0000"  >&lt;p&gt;Sorry sir for my mistake. PooledByteBufAllocator2.patch should be right. I have run and check with it.&lt;/p&gt;</comment>
                            <comment id="15408233" author="stack" created="Thu, 4 Aug 2016 18:04:43 +0000"  >&lt;p&gt;Thanks for the patch &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; (I didn&apos;t realize it had to go as last option on bootstrap &amp;#8211; thanks).&lt;/p&gt;

&lt;p&gt;Here is a graph with three peaks. The first is with the patch in place. Looking in jstacks, allocations no longer show so we must be getting from the pool now. We seem to be close to the unpatched version but slightly slower still.&lt;/p&gt;

&lt;p&gt;Turns out all data has not been fitting in cache (I&apos;d messed w/ configs). I made the configs so all data does fit and when I run this way, I see that the netty server is a good bit slower than what we have: 300k ops/sec vs 400k ops/sec... This is after my constraining netty not to run away with workers making it so only 6.&lt;/p&gt;

&lt;p&gt;I&apos;ll try and look and see if anything obvious in the difference but maybe you have ideas &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;? Thanks.&lt;/p&gt;</comment>
                            <comment id="15408706" author="aoxiang" created="Fri, 5 Aug 2016 00:46:51 +0000"  >&lt;p&gt;The unpatched version can reach 400k ops/sec? This is huge.&lt;br/&gt;
Or your may try different worker numbers.&lt;br/&gt;
I do not have machine to test now, i need to wait to next week for the machine to be ready.&lt;/p&gt;</comment>
                            <comment id="15408816" author="stack" created="Fri, 5 Aug 2016 03:32:06 +0000"  >&lt;p&gt;Yeah, unpatched 400k and the patched does 300k. I played with worker threads. I made them same as the count of Readers that I have in the unpatched version. If more we seem to slow down. Let me try again.... cluster is idle. I can try stuff if you want me to. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15408881" author="stack" created="Fri, 5 Aug 2016 05:05:17 +0000"  >&lt;p&gt;Hmm... I upped the worker thread count from 6 to 12 and the netty rpc server went faster. Let me up it again. It seems like it plays by different rules to those I understood from playing with our current rpcserver. I am currently at 450k/second. Setting the workers to 24 seems to make it go a bit slower. Let me do more careful tests tomorrow. I&apos;ll report back.&lt;/p&gt;</comment>
                            <comment id="15408909" author="aoxiang" created="Fri, 5 Aug 2016 05:36:04 +0000"  >&lt;p&gt;Yeah, Netty&apos;s NioWork read request from client but also write response to client.&lt;br/&gt;
But our current rpcserver, the reader only read request from client, write response to client is done by handler or Responder.&lt;/p&gt;</comment>
                            <comment id="15408913" author="aoxiang" created="Fri, 5 Aug 2016 05:38:08 +0000"  >&lt;p&gt;Yeah, if i find other things can impact the performance, i will tell you.&lt;/p&gt;</comment>
                            <comment id="15409365" author="anoop.hbase" created="Fri, 5 Aug 2016 12:06:30 +0000"  >&lt;p&gt;Sorry I missed all the conversation here. So which is the patch ready to be read?  May be one patch and another on top of that?  Wanted to read fully and understand what is going on in the NettyServer. Tks.&lt;/p&gt;</comment>
                            <comment id="15409401" author="aoxiang" created="Fri, 5 Aug 2016 12:44:52 +0000"  >&lt;p&gt;Netty4RpcServer_forperf.patch with PooledByteBufAllocator2.patch, they are just for test now.&lt;/p&gt;</comment>
                            <comment id="15409574" author="stack" created="Fri, 5 Aug 2016 15:22:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; It is the two patches &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; says above and then I&apos;ve put limits on the boss and worker allocations... this bit:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 208     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (useEpoll) {
 209       bossGroup = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; EpollEventLoopGroup(3);
 210       workerGroup = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; EpollEventLoopGroup(24);
 211     } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
 212       bossGroup = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; NioEventLoopGroup(3);
 213       workerGroup = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; NioEventLoopGroup(24);
 214     }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also, using epoll ... instead of the nio event loop group seems to go faster... setting  hbase.rpc.server.nativetransport to true.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; &quot;But our current rpcserver, the reader only read request from client, write response to client is done by handler or Responder.&quot;&lt;/p&gt;

&lt;p&gt;In my observation, the handler seems to be able to put the response on the wire itself. Rare do we go via the queue and then the single Responder thread. So I am not sure why netty is doing better (but am happy it does).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; How to go from netty direct bytebuffers to MSLAB? Netty has native refcounting IIRC but not sure it does your trick of a pool of fixed sized buffers.&lt;/p&gt;


</comment>
                            <comment id="15409578" author="stack" created="Fri, 5 Aug 2016 15:24:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; Have you seen an issue around server shutdown? I am seeing this. Haven&apos;t dug in....&lt;/p&gt;</comment>
                            <comment id="15410348" author="aoxiang" created="Sat, 6 Aug 2016 00:59:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt; &quot;But our current rpcserver, the reader only read request from client, write response to client is done by handler or Responder.&quot;&lt;br/&gt;
In my observation, the handler seems to be able to put the response on the wire itself. Rare do we go via the queue and then the single Responder thread. So I am not sure why netty is doing better (but am happy it does).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; Yes, i am just point out the different.&lt;/p&gt;</comment>
                            <comment id="15410352" author="aoxiang" created="Sat, 6 Aug 2016 01:01:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; i am not use the Netty4RpcServer much, so may be i encounter, but also not dig in.&lt;/p&gt;</comment>
                            <comment id="15410355" author="aoxiang" created="Sat, 6 Aug 2016 01:04:31 +0000"  >&lt;p&gt;So the Netty&apos;s NioWork doing more work, and the number may be should be more then current rpcserver&apos;s reader number.&lt;/p&gt;</comment>
                            <comment id="15410412" author="stack" created="Sat, 6 Aug 2016 02:47:45 +0000"  >&lt;p&gt;Testing today, the results are less conclusive. I can do about 380k/sec w/ current rpcserver and then just over with 12 workers on the netty server (415k), so a difference of almost 10%. I can&apos;t get to 450 today for some reason... not sure what I was doing yesterday evening.&lt;/p&gt;</comment>
                            <comment id="15411614" author="ram_krish" created="Mon, 8 Aug 2016 10:05:26 +0000"  >&lt;p&gt;Seeing the code here - the Netty4RpcServer tries to create some buffers internally thro which the requests are read (now that is getting pooled and hence the above mentioned stack trace where every allocated buffer is cleared up to avoid full GC ) is now gone. Once we read from the netty buffers again we create onheap NIO buffers and hand over them to process the requests?  Am I missing anything here?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// There&apos;s enough bytes in the buffer. Read it.
&lt;/span&gt;1329	      &lt;span class=&quot;code-comment&quot;&gt;// ByteBuffer data = buf.toByteBuffer(buf.readerIndex(), length);
&lt;/span&gt;1330	      ByteBuffer data = ByteBuffer.allocate(length);
1331	      buf.readBytes(data);
1332	      data.flip();
1333	      &lt;span class=&quot;code-comment&quot;&gt;// buf.skipBytes(length);
&lt;/span&gt;1334	      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; data;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is where our pooling was done. So what we do here is to ensure that the incoming requests are accepted much faster than the existing RPC Server and then go ahead with our way of pooled BBs. &lt;/p&gt;</comment>
                            <comment id="15411621" author="aoxiang" created="Mon, 8 Aug 2016 10:24:52 +0000"  >&lt;p&gt;@ramkrishna.s.vasudevan, We current test the Netty4RpcServer on branch-1. Netty read data to offHeap then decode to onheap,  this is like the current RpcServer on branch-1. Because i test it with branch-1, so i do not optimize it as much as possible.&lt;/p&gt;</comment>
                            <comment id="15411630" author="ram_krish" created="Mon, 8 Aug 2016 10:31:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;&lt;br/&gt;
I think I got that it is for branch-1. But my point was that the pooling of the ByteBuffers was done after the layer were the data is read from Netty. That is to answer Stack&apos;s question. When I read that Pooling was enabled I thought the Netty&apos;s pooling itself comes into play for process the requests. Hence went through the patch. Thank &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15724589" author="aoxiang" created="Tue, 6 Dec 2016 06:35:28 +0000"  >&lt;p&gt;We have use NettyRpcServer on production for two months in Alibaba search, and this have show big improvement in our cluster.&lt;br/&gt;
Discuss with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; NettyRpcServer can work with write pipeline off heap, so start continue this work.&lt;/p&gt;

&lt;p&gt;1.&#160; Refactoring in the RpcServer so as to make it extendable and/or pluggable&lt;br/&gt;
2. The new Rpc server impl which is netty based.&lt;/p&gt;</comment>
                            <comment id="15724616" author="ram_krish" created="Tue, 6 Dec 2016 06:51:07 +0000"  >&lt;p&gt;Yes. It can be done with offheap  because from the netty buffers we tend to copy the requests to onheap buffers. Instead now you will need to just copy to the buffers from the pool. seeing the private impl of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt; - that is what was being done. So should work fine along with the benefits that they have seen with netty by running in prod for 2 months.&lt;/p&gt;</comment>
                            <comment id="15724630" author="aoxiang" created="Tue, 6 Dec 2016 06:59:03 +0000"  >&lt;p&gt;Thanks, man!&lt;/p&gt;</comment>
                            <comment id="15725854" author="stack" created="Tue, 6 Dec 2016 15:44:25 +0000"  >&lt;p&gt;Lets NOT make it pluggable. Lets just switch to netty. Can you say how netty rpc server has been better for you in production &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aoxiang&quot; class=&quot;user-hover&quot; rel=&quot;aoxiang&quot;&gt;binlijin&lt;/a&gt;? Thanks.&lt;/p&gt;</comment>
                            <comment id="15725857" author="stack" created="Tue, 6 Dec 2016 15:45:15 +0000"  >&lt;p&gt;Oops. Repeating myself. From top of this issue: &quot;I&apos;d suggest you not waste your time making it pluggable binlijin. Just change our rpcserver be netty posting some numbers that show it has same basic performance and we&apos;ll get it in?&quot;&lt;/p&gt;</comment>
                            <comment id="15743826" author="aoxiang" created="Tue, 13 Dec 2016 01:59:18 +0000"  >&lt;p&gt;We run NettyRpcServer on production for two months in Alibaba search. It use netty3 not nettey4.&lt;br/&gt;
The performance improvements you can see it at Cluser_total_QPS.png, which come from our online A/B test cluster (with 450 physical machines, and each with 256G memory + 64 core) with real world workloads.&lt;br/&gt;
When use SimpleRpcServer the total qps is less than 20M/s, when use NettyRpcServer, the total qps is more than 30M/s.&lt;/p&gt;</comment>
                            <comment id="15743861" author="aoxiang" created="Tue, 13 Dec 2016 02:13:06 +0000"  >&lt;p&gt;And i also test the Netty4RpcServer perf on master version, the Netty4RpcServer can get at  &lt;a href=&quot;https://github.com/binlijin/hbase/tree/NettyRpcServer&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/binlijin/hbase/tree/NettyRpcServer&lt;/a&gt;.&lt;br/&gt;
1 regionserver, 1 client machine, client and rs on different machine.&lt;br/&gt;
Write a table with 10M rows, then split into 32 regions, every row have only one cell, keyLength=10B, valueLength=256B.&lt;br/&gt;
Only test random read, and all data cache in LruBlockCache.&lt;br/&gt;
Client&#65306; YCSB + hbase-1.1.2 client, start 16 YCSB process on client machine, and every process have 32 threads.&lt;/p&gt;

&lt;p&gt;share connections:  use YCSB build with &lt;a href=&quot;https://github.com/brianfrankcooper/YCSB/commit/57e1ab5a0cae13d1766c8fa6bdbf9d9117ee50d0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/brianfrankcooper/YCSB/commit/57e1ab5a0cae13d1766c8fa6bdbf9d9117ee50d0&lt;/a&gt;&lt;br/&gt;
No share connections: use YCSB build without &lt;a href=&quot;https://github.com/brianfrankcooper/YCSB/commit/57e1ab5a0cae13d1766c8fa6bdbf9d9117ee50d0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/brianfrankcooper/YCSB/commit/57e1ab5a0cae13d1766c8fa6bdbf9d9117ee50d0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The result is: &lt;br/&gt;
(Master + Netty)    +  ( RpcClientImpl   &#65291; No share connections ) = 600K/S&lt;br/&gt;
(Master + SingleResponder)     +  ( RpcClientImpl   &#65291; No share connections ) = 600K/S&lt;/p&gt;

&lt;p&gt;(Master + Netty)    +  ( RpcClientImpl   &#65291; share connections ) = 500K/S&lt;br/&gt;
(Master + MultiResponder)    +  ( RpcClientImpl   &#65291; share connections ) = 500K/S&lt;br/&gt;
(Master + SingleResponder)     +  ( RpcClientImpl   &#65291; share connections ) = 295K/S&lt;/p&gt;

&lt;p&gt;(Master + MultiResponder)  +   ( AsyncRpcClient  &#65291; share connections ) = 570K/S   (Netty client)&lt;br/&gt;
(Master + Netty)  +  (AsyncRpcClient  &#65291; share connections ) =  620K/S  (Netty client)&lt;/p&gt;

&lt;p&gt;MultiResponder:  see patch MultiResponder.master.patch&lt;/p&gt;</comment>
                            <comment id="15743880" author="aoxiang" created="Tue, 13 Dec 2016 02:22:09 +0000"  >&lt;p&gt;The current RpcServer is complicated, it support plain data/security/encryption data...&lt;br/&gt;
We only use plain data, do not use security/encryption. I think it need time to stabilize the Netty4RpcServer with all function and do not break the wire format. And i think make RpcServer pluggable can make it easy to implement Netty4RpcServer and switch from RpcServer to Netty4RpcServer. When Netty4RpcServer is stable than we can delete the current RpcServer.&lt;/p&gt;</comment>
                            <comment id="15743889" author="aoxiang" created="Tue, 13 Dec 2016 02:26:17 +0000"  >&lt;p&gt;I add metrics to record how many responses handled by Handlers and Responder.&lt;br/&gt;
When share connections:&lt;br/&gt;
    &quot;numResponsesWriteByResponder&quot; : 79995359,&lt;br/&gt;
    &quot;numResponsesWriteByHandler&quot; : 578055,&lt;/p&gt;

&lt;p&gt;When not share connections:&lt;br/&gt;
    &quot;numResponsesWriteByResponder&quot; : 88855,&lt;br/&gt;
    &quot;numResponsesWriteByHandler&quot; : 137469140,&lt;/p&gt;

&lt;p&gt;So when share connections, most of the response write to client by Responder. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12842912" name="Cluster_total_QPS.png" size="47802" author="aoxiang" created="Tue, 13 Dec 2016 01:57:06 +0000"/>
                            <attachment id="12842465" name="MultiResponder.branch-1.patch" size="7201" author="aoxiang" created="Fri, 9 Dec 2016 01:28:49 +0000"/>
                            <attachment id="12842466" name="MultiResponder.master.patch" size="8014" author="aoxiang" created="Fri, 9 Dec 2016 01:28:49 +0000"/>
                            <attachment id="12821256" name="Netty4RpcServer_forperf.patch" size="141952" author="aoxiang" created="Mon, 1 Aug 2016 03:26:49 +0000"/>
                            <attachment id="12802623" name="NettyRpcServer.patch" size="76882" author="aoxiang" created="Fri, 6 May 2016 07:13:24 +0000"/>
                            <attachment id="12820903" name="NettyRpcServer_forperf.patch" size="77886" author="aoxiang" created="Fri, 29 Jul 2016 07:39:21 +0000"/>
                            <attachment id="12821980" name="PooledByteBufAllocator.patch" size="2447" author="aoxiang" created="Thu, 4 Aug 2016 02:03:02 +0000"/>
                            <attachment id="12822052" name="PooledByteBufAllocator2.patch" size="2936" author="aoxiang" created="Thu, 4 Aug 2016 12:42:45 +0000"/>
                            <attachment id="12821974" name="gc.png" size="24558" author="stack" created="Thu, 4 Aug 2016 01:18:26 +0000"/>
                            <attachment id="12821973" name="gets.png" size="24022" author="stack" created="Thu, 4 Aug 2016 01:18:26 +0000"/>
                            <attachment id="12821100" name="gets.png" size="23310" author="stack" created="Fri, 29 Jul 2016 22:24:17 +0000"/>
                            <attachment id="12821101" name="idle.png" size="25781" author="stack" created="Fri, 29 Jul 2016 22:24:17 +0000"/>
                            <attachment id="12822123" name="patched.vs.patched_and_cached.vs.no_patch.png" size="17982" author="stack" created="Thu, 4 Aug 2016 18:04:43 +0000"/>
                            <attachment id="12821102" name="queue.png" size="29657" author="stack" created="Fri, 29 Jul 2016 22:24:17 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="13025861">HBASE-17262</subtask>
                            <subtask id="13025862">HBASE-17263</subtask>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>14.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 4 May 2016 06:29:03 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2x347:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>