<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:52:30 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-14383/HBASE-14383.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-14383] Compaction improvements</title>
                <link>https://issues.apache.org/jira/browse/HBASE-14383</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Still major issue in many production environments. The general recommendation - disabling region splitting and major compactions to reduce unpredictable IO/CPU spikes, especially during peak times and running them manually during off peak times. Still do not resolve the issues completely.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;Flushstorms&quot;&gt;&lt;/a&gt;Flush storms&lt;/h3&gt;

&lt;ul&gt;
	&lt;li&gt;rolling WAL events across cluster can be highly correlated, hence flushing memstores, hence triggering minor compactions, that can be promoted to major ones. These events are highly correlated in time if there is a balanced write-load on the regions in a table.&lt;/li&gt;
	&lt;li&gt;the same is true for memstore flushing due to periodic memstore flusher operation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Both above may produce &lt;b&gt;flush storms&lt;/b&gt; which are as bad as &lt;b&gt;compaction storms&lt;/b&gt;. &lt;/p&gt;

&lt;p&gt;What can be done here. We can spread these events over time by randomizing (with jitter) several  config options:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;hbase.regionserver.optionalcacheflushinterval&lt;/li&gt;
	&lt;li&gt;hbase.regionserver.flush.per.changes&lt;/li&gt;
	&lt;li&gt;hbase.regionserver.maxlogs&lt;/li&gt;
&lt;/ol&gt;


&lt;h3&gt;&lt;a name=&quot;ExploringCompactionPolicymaxcompactionsize&quot;&gt;&lt;/a&gt;ExploringCompactionPolicy max compaction size&lt;/h3&gt;
&lt;p&gt;One more optimization can be added to ExploringCompactionPolicy. To limit size of a compaction there is a config parameter one could use hbase.hstore.compaction.max.size. It would be nice to have two separate limits: for peak and off peak hours.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;ExploringCompactionPolicyselectionevaluationalgorithm&quot;&gt;&lt;/a&gt;ExploringCompactionPolicy selection evaluation algorithm&lt;/h3&gt;

&lt;p&gt;Too simple? Selection with more files always wins, selection of smaller size wins if number of files is the same. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12862595">HBASE-14383</key>
            <summary>Compaction improvements</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="vrodionov">Vladimir Rodionov</assignee>
                                    <reporter username="vrodionov">Vladimir Rodionov</reporter>
                        <labels>
                    </labels>
                <created>Wed, 9 Sep 2015 00:08:48 +0000</created>
                <updated>Wed, 21 Oct 2015 02:01:03 +0000</updated>
                                                            <fixVersion>2.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>18</watches>
                                                                <comments>
                            <comment id="14736029" author="chenheng" created="Wed, 9 Sep 2015 02:41:55 +0000"  >&lt;blockquote&gt;
&lt;p&gt;One more optimization can be added to ExploringCompactionPolicy. To limit size of a compaction there is a config parameter one could use hbase.hstore.compaction.max.size. It would be nice to have two separate limits: for peak and off peak hours.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This issue meets your needs?  &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8329&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-8329&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14736108" author="vrodionov" created="Wed, 9 Sep 2015 03:49:44 +0000"  >&lt;p&gt;No, it is different. This is compaction throughput limit.  &lt;/p&gt;</comment>
                            <comment id="14746564" author="vrodionov" created="Wed, 16 Sep 2015 00:23:14 +0000"  >&lt;p&gt;Need a feedback on usefulness of &lt;b&gt;hbase.regionserver.maxlogs&lt;/b&gt; configuration setting.&lt;/p&gt;

&lt;p&gt;LogRoller runs periodically (1h by default) and does two things:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Archive old logs (WAL files which have all WALEdits already flushed)&lt;/li&gt;
	&lt;li&gt;Then checks number of active WAL files and if it exceeds hbase.regionserver.maxlogs then all regions which have edits from the oldest WAL file will be flushed.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;rollWriter from FSHLog:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; [][] rollWriter(&lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; force) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; FailedLogCloseException, IOException {
    rollWriterLock.lock();
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      &lt;span class=&quot;code-comment&quot;&gt;// Return &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; nothing to flush.
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!force &amp;amp;&amp;amp; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.writer != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.numEntries.get() &amp;lt;= 0)) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
      &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; [][] regionsToFlush = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.closed) {
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;WAL closed. Skipping rolling of writer&quot;&lt;/span&gt;);
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; regionsToFlush;
      }
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!closeBarrier.beginOp()) {
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;WAL closing. Skipping rolling of writer&quot;&lt;/span&gt;);
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; regionsToFlush;
      }
      TraceScope scope = Trace.startSpan(&lt;span class=&quot;code-quote&quot;&gt;&quot;FSHLog.rollWriter&quot;&lt;/span&gt;);
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        Path oldPath = getOldPath();
        Path newPath = getNewPath();
        &lt;span class=&quot;code-comment&quot;&gt;// Any exception from here on is catastrophic, non-recoverable so we currently abort.
&lt;/span&gt;        Writer nextWriter = &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.createWriterInstance(newPath);
        FSDataOutputStream nextHdfsOut = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (nextWriter &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; ProtobufLogWriter) {
          nextHdfsOut = ((ProtobufLogWriter)nextWriter).getStream();
          &lt;span class=&quot;code-comment&quot;&gt;// If a ProtobufLogWriter, go ahead and &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; and sync to force setup of pipeline.
&lt;/span&gt;          &lt;span class=&quot;code-comment&quot;&gt;// If &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; fails, we just keep going.... it is an optimization, not the end of the world.
&lt;/span&gt;          preemptiveSync((ProtobufLogWriter)nextWriter);
        }
        tellListenersAboutPreLogRoll(oldPath, newPath);
        &lt;span class=&quot;code-comment&quot;&gt;// NewPath could be equal to oldPath &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; replaceWriter fails.
&lt;/span&gt;        newPath = replaceWriter(oldPath, newPath, nextWriter, nextHdfsOut);
        tellListenersAboutPostLogRoll(oldPath, newPath);
        &lt;span class=&quot;code-comment&quot;&gt;// Can we delete any of the old log files?
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (getNumRolledLogFiles() &amp;gt; 0) {
          cleanOldLogs();
          regionsToFlush = findRegionsToForceFlush();
        }
      } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
        closeBarrier.endOp();
        &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt; scope == NullScope.INSTANCE || !scope.isDetached();
        scope.close();
      }
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; regionsToFlush;
    } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
      rollWriterLock.unlock();
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is a clear duplication in functionality between LogRoller (LR) and PeriodicMemstoreFlsuher (PMF). PMF already takes care of old memstores and flushes them - no need to call regionsToFlush = findRegionsToForceFlush() in a rollWriter call and hence there is no need in &lt;b&gt;hbase.regionserver.maxlogs&lt;/b&gt; config option. PMF flushes periodically oldest memstores and LogRoller archives periodically old WAL files. That is it.   &lt;/p&gt;</comment>
                            <comment id="14746573" author="vrodionov" created="Wed, 16 Sep 2015 00:26:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enis&quot; class=&quot;user-hover&quot; rel=&quot;enis&quot;&gt;Enis Soztutar&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lhofhansl&quot; class=&quot;user-hover&quot; rel=&quot;lhofhansl&quot;&gt;Lars Hofhansl&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apurtell&quot; class=&quot;user-hover&quot; rel=&quot;apurtell&quot;&gt;Andrew Purtell&lt;/a&gt; what do you think? Can we retire &lt;b&gt;hbase.regionserver.maxlogs&lt;/b&gt;?&lt;/p&gt;</comment>
                            <comment id="14746597" author="enis" created="Wed, 16 Sep 2015 00:48:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can we retire hbase.regionserver.maxlogs?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I am in favor of that, or keeping it as a safety net, but with a much higher default (128?). &lt;/p&gt;

&lt;p&gt;With default settings&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hbase.regionserver.maxlogs=32
hbase.regionserver.hlog.blocksize=128MB
hbase.regionserver.logroll.multiplier=0.95
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can only have 32*128*0.95 = 3.9GB of WAL entries. So, if you are running with 32GB heap and 0.4 memstore size, the memstore space is just left unused. &lt;/p&gt;

&lt;p&gt;Also, not related to compactions, but I have seen cases where there are not enough regions per region server to fill the whole memstore space with the 128MB flush size, a few active regions and big heaps. We do not allow a memstore to grow beyond the flush limit to guard against long flushes and long MTTR times. But my feeling is that, maybe we can have a dynamically adjustable flush size taking into account a min and max flush size and delay triggering the flush if there is more space. &lt;/p&gt;</comment>
                            <comment id="14746608" author="eclark" created="Wed, 16 Sep 2015 00:57:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;Also, not related to compactions, but I have seen cases where there are not enough regions per region server to fill the whole memstore space with the 128MB flush size, a few active regions and big heaps. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m hopeful that we can up that per memstore limit a lot and just let the auto-tuning/periodic flusher work together and ensure that all space is more used while MTTR is not too bad.&lt;/p&gt;</comment>
                            <comment id="14746749" author="stack" created="Wed, 16 Sep 2015 03:22:41 +0000"  >&lt;p&gt;I&apos;d be for upping the max logs number (have seen cases where it ran away up to the thousands so some guard would be good)&lt;/p&gt;</comment>
                            <comment id="14746783" author="vrodionov" created="Wed, 16 Sep 2015 03:48:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=saint.ack%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;saint.ack@gmail.com&quot;&gt;Stack&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I&apos;d be for upping the max logs number (have seen cases where it ran away up to the thousands so some guard would be good)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That is very degenerate case. I have thought about this, it is possible to have many CF in a table and very small flush files. By default, flush policy ignores all files less than 15MB. Imagine that all your files in a region&apos;s memstores  selected for flushing less than 15MB =&amp;gt; there will be no flush and WAL numbers will continue growing (indefinitely, by the way).&lt;/p&gt;

&lt;p&gt;We probably need  &lt;b&gt;hbase.regionserver.maxlogs&lt;/b&gt; as a safeguard against runaway wals during prolonged burst load, when ingested data per RS  in a PMF flush interval (1h) is much greater than overall memstore capacity. I agree we have to up default value of &lt;b&gt;hbase.regionserver.maxlogs&lt;/b&gt; but set during RS init and not statically. We have to make sure that overall WAL capacity is not less than overall memstore capacity. Ideally it should be large enough to make the event (max number exceeded) very rare.&lt;/p&gt;

&lt;p&gt;MTTR depends not a max number of WAL files but on a current load and PMF interval.      &lt;/p&gt;</comment>
                            <comment id="14746786" author="vrodionov" created="Wed, 16 Sep 2015 03:50:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt;MTTR depends not a max number of WAL files but on a current load and PMF interval.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;and on memstore size, of course .&lt;/p&gt;</comment>
                            <comment id="14746788" author="vrodionov" created="Wed, 16 Sep 2015 03:52:46 +0000"  >&lt;blockquote&gt;
&lt;p&gt;flush policy ignores all files less than 15MB. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Correction: memstores, not files.&lt;/p&gt;</comment>
                            <comment id="14746847" author="stack" created="Wed, 16 Sep 2015 04:37:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;That is very degenerate case.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I do not disagree. Bitch to fix after the fact though.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;By default, flush policy ignores all files less than 15MB. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This seems wrong if one of these memstores has an old edit that is holding up our freeing/GC&apos;ing WALs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I agree we have to up default value of hbase.regionserver.maxlogs but set during RS init and not statically. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That sounds reasonable...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;MTTR depends not a max number of WAL files but on a current load and PMF interval.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Disagree. Many WALs has longer MTTR than few WALs&lt;/p&gt;
</comment>
                            <comment id="14804474" author="lhofhansl" created="Thu, 17 Sep 2015 20:47:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can we retire hbase.regionserver.maxlogs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think so. maxlogs is really a function of heap available for the memstores and the HDFS block size used. Something like: &lt;tt&gt;maxlogs = memstore heap / (HDFS blocksize * 0.95)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Can we just default it to that? Maybe with 10% padding.&lt;/p&gt;</comment>
                            <comment id="14804682" author="enis" created="Thu, 17 Sep 2015 23:03:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;flush policy ignores all files less than 15MB.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Where is this code? I could not find anything in the periodic or non-periodic flush requests that prevents flush requests. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;maxlogs is really a function of heap available for the memstores and the HDFS block size used. Something like: maxlogs = memstore heap / (HDFS blocksize * 0.95)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This assumes that all memstores are getting updates. In case a memstore stops getting updates, it will not flush for ~0.5 hour (expected) unless it is the biggest memstore left. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Can we just default it to that? Maybe with 10% padding.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Maybe we can instead do the limit as 2x or 3x. &lt;/p&gt;</comment>
                            <comment id="14804687" author="vrodionov" created="Thu, 17 Sep 2015 23:08:41 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Where is this code? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In a FlushLargeStoresPolicy, We flush only stores with 16MB in size and greater, otherwise we check if if it is old enough to be flushed.&lt;/p&gt;</comment>
                            <comment id="14804741" author="enis" created="Fri, 18 Sep 2015 00:32:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;In a FlushLargeStoresPolicy, We flush only stores with 16MB in size and greater, otherwise we check if if it is old enough to be flushed.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Hmm, does this policy mean that we may end up not flushing data even with periodic flusher? The periodic flusher should be like a force flush to be affective. &lt;/p&gt;</comment>
                            <comment id="14876121" author="apurtell" created="Fri, 18 Sep 2015 18:37:33 +0000"  >&lt;p&gt;A couple of suggestions above to dynamically set maxlogs. I concur. Then, there&apos;s little (or no) point then to leave it be something a user can set to a fixed value. Remove it. One config knob down thanks to autotuning, a ton more to go. (smile)&lt;/p&gt;</comment>
                            <comment id="14876130" author="vrodionov" created="Fri, 18 Sep 2015 18:44:18 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Hmm, does this policy mean that we may end up not flushing data even with periodic flusher? The periodic flusher should be like a force flush to be affective.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For small stores flush happens only if they have data which is older than periodic flush interval. This is how it works today. In theory, if you have small heap and large number of regions you won&apos;t be able to load data fast w/o being totally blocked periodically. ALl memstores &amp;lt; 16MB and they will be flushed once an hour.&lt;/p&gt;</comment>
                            <comment id="14909570" author="vrodionov" created="Sun, 27 Sep 2015 02:31:36 +0000"  >&lt;p&gt;Patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14467&quot; title=&quot;Compaction improvements: DefaultCompactor should not compact TTL-expired files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14467&quot;&gt;&lt;del&gt;HBASE-14467&lt;/del&gt;&lt;/a&gt; was submitted. Can somebody take a look? Thanks. &lt;/p&gt;</comment>
                            <comment id="14910049" author="vrodionov" created="Mon, 28 Sep 2015 05:25:26 +0000"  >&lt;p&gt;Patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14468&quot; title=&quot;Compaction improvements: FIFO compaction policy&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14468&quot;&gt;&lt;del&gt;HBASE-14468&lt;/del&gt;&lt;/a&gt; (FIFO compaction policy) was submitted. Includes &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14467&quot; title=&quot;Compaction improvements: DefaultCompactor should not compact TTL-expired files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14467&quot;&gt;&lt;del&gt;HBASE-14467&lt;/del&gt;&lt;/a&gt; (and sub-task).&lt;/p&gt;</comment>
                            <comment id="14933979" author="vrodionov" created="Mon, 28 Sep 2015 21:01:14 +0000"  >&lt;p&gt;Can somebody take a look at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14468&quot; title=&quot;Compaction improvements: FIFO compaction policy&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14468&quot;&gt;&lt;del&gt;HBASE-14468&lt;/del&gt;&lt;/a&gt;? It is totally separate feature - new compaction policy and it has passed QA.&lt;/p&gt;</comment>
                            <comment id="14966112" author="vrodionov" created="Wed, 21 Oct 2015 02:01:03 +0000"  >&lt;p&gt;Some patches have been submitted recently, including &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14388&quot; title=&quot;Compaction improvements: Avoid flush storms by jittering flush interval and max log files &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14388&quot;&gt;HBASE-14388&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12895998">HBASE-14477</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12896639">HBASE-14496</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12906101">HBASE-14651</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12862646">HBASE-14389</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12862641">HBASE-14387</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12895668">HBASE-14467</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12895669">HBASE-14468</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12862642">HBASE-14388</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 9 Sep 2015 02:41:55 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 8 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2jx4v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>