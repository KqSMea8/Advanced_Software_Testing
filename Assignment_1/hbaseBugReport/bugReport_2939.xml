<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:06:02 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2939/HBASE-2939.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2939] Allow Client-Side Connection Pooling</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2939</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;By design, the HBase RPC client multiplexes calls to a given region server (or the master for that matter) over a single socket, access to which is managed by a connection thread defined in the HBaseClient class. While this approach may suffice for most cases, it tends to break down in the context of a real-time, multi-threaded server, where latencies need to be lower and throughputs higher. &lt;/p&gt;

&lt;p&gt;In brief, the problem is that we dedicate one thread to handle all client-side reads and writes for a given server, which in turn forces them to share the same socket. As load increases, this is bound to serialize calls on the client-side. In particular, when the rate at which calls are submitted to the connection thread is greater than that at which the server responds, then some of those calls will inevitably end up sitting idle, just waiting their turn to go over the wire.&lt;/p&gt;

&lt;p&gt;In general, sharing sockets across multiple client threads is a good idea, but limiting the number of such sockets to one may be overly restrictive for certain cases. Here, we propose a way of defining multiple sockets per server endpoint, access to which may be managed through either a load-balancing or thread-local pool. To that end, we define the notion of a SharedMap, which maps a key to a resource pool, and supports both of those pool types. Specifically, we will apply that map in the HBaseClient, to associate multiple connection threads with each server endpoint (denoted by a connection id). &lt;/p&gt;

&lt;p&gt; Currently, the SharedMap supports the following types of pools:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;A ThreadLocalPool, which represents a pool that builds on the ThreadLocal class. It essentially binds the resource to the thread from which it is accessed.&lt;/li&gt;
	&lt;li&gt;A ReusablePool, which represents a pool that builds on the LinkedList class. It essentially allows resources to be checked out, at which point it is (temporarily) removed from the pool. When the resource is no longer required, it should be returned to the pool in order to be reused.&lt;/li&gt;
	&lt;li&gt;A RoundRobinPool, which represents a pool that stores its resources in an ArrayList. It load-balances access to its resources by returning a different resource every time a given key is looked up.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To control the type and size of the connection pools, we give the user a couple of parameters (viz. &quot;hbase.client.ipc.pool.type&quot; and &quot;hbase.client.ipc.pool.size&quot;). In case the size of the pool is set to a non-zero positive number, that is used to cap the number of resources that a pool may contain for any given key. A size of Integer#MAX_VALUE is interpreted to mean an unbounded pool.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12472787">HBASE-2939</key>
            <summary>Allow Client-Side Connection Pooling</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="karthick">Karthick Sankarachary</assignee>
                                    <reporter username="karthick">Karthick Sankarachary</reporter>
                        <labels>
                    </labels>
                <created>Sat, 28 Aug 2010 19:51:06 +0000</created>
                <updated>Fri, 20 Nov 2015 12:41:14 +0000</updated>
                            <resolved>Tue, 19 Apr 2011 18:32:49 +0000</resolved>
                                    <version>0.89.20100621</version>
                                    <fixVersion>0.92.0</fixVersion>
                                    <component>Client</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                <comments>
                            <comment id="12903906" author="tlipcon" created="Sat, 28 Aug 2010 22:50:00 +0000"  >&lt;p&gt;Hi Karthick. Have you had a chance to run any benchmarks with this patch? As is, we do use a single TCP connection to each backend region server, but we can have multiple outstanding calls, so we should really be constrained by bandwidth, not by waiting for the previous call to return.&lt;/p&gt;</comment>
                            <comment id="12904047" author="karthick" created="Sun, 29 Aug 2010 22:51:13 +0000"  >&lt;p&gt;Hi Todd, &lt;/p&gt;

&lt;p&gt;We observed significantly lower latencies (and consequently higher throughputs) when running our load test, especially with low think times. While it is true that we can having multiple outstanding calls, we do synchronize the send and receive legs of the call (see snippets below), and I believe that serializes the calls to some extent.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;Synchronization Of Send Request&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
&lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; void sendParam(Call call) {
  ...
  &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.out) {
  ...
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;Synchronization Of Receive Response&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void run() {
...
&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (waitForWork()) {&lt;span class=&quot;code-comment&quot;&gt;//wait here &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; work - read or close connection
&lt;/span&gt;  receiveResponse();
}
...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12904051" author="ryanobjc" created="Sun, 29 Aug 2010 23:18:50 +0000"  >&lt;p&gt;I can see how using a single thread to read and write and serialize all data&lt;br/&gt;
to a single regionserver could make things slow. I +1 the general approach&lt;br/&gt;
here. Thanks for doing this!&lt;/p&gt;

&lt;p&gt;On Aug 29, 2010 3:52 PM, &quot;Karthick Sankarachary (JIRA)&quot; &amp;lt;jira@apache.org&amp;gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2939?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12904047#action_12904047&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-2939?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12904047#action_12904047&lt;/a&gt;]&lt;br/&gt;
throughputs) when running our load test, especially with low think times.&lt;br/&gt;
While it is true that we can having multiple outstanding calls, we do&lt;br/&gt;
synchronize the send and receive legs of the call (see snippets below), and&lt;br/&gt;
I believe that serializes the calls to some extent.&lt;br/&gt;
server (or the master for that matter) over a single socket, access to which&lt;br/&gt;
is managed by a connection thread defined in the HBaseClient class. While&lt;br/&gt;
this approach may suffice for most cases, it tends to break down in the&lt;br/&gt;
context of a real-time, multi-threaded server, where latencies need to be&lt;br/&gt;
lower and throughputs higher.&lt;br/&gt;
client-side reads and writes for a given server, which in turn forces them&lt;br/&gt;
to share the same socket. As load increases, this is bound to serialize&lt;br/&gt;
calls on the client-side. In particular, when the rate at which calls are&lt;br/&gt;
submitted to the connection thread is greater than that at which the server&lt;br/&gt;
responds, then some of those calls will inevitably end up sitting idle, just&lt;br/&gt;
waiting their turn to go over the wire.&lt;br/&gt;
idea, but limiting the number of such sockets to one may be overly&lt;br/&gt;
restrictive for certain cases. Here, we propose a way of defining multiple&lt;br/&gt;
sockets per server endpoint, access to which may be managed through either a&lt;br/&gt;
load-balancing or thread-local pool. To that end, we define the notion of a&lt;br/&gt;
SharedMap, which maps a key to a resource pool, and supports both of those&lt;br/&gt;
pool types. Specifically, we will apply that map in the HBaseClient, to&lt;br/&gt;
associate multiple connection threads with each server endpoint (denoted by&lt;br/&gt;
a connection id).&lt;br/&gt;
ThreadLocal class. It essentially binds the resource to the thread from&lt;br/&gt;
which it is accessed.&lt;br/&gt;
class. It essentially allows resources to be checked out, at which point it&lt;br/&gt;
is (temporarily) removed from the pool. When the resource is no longer&lt;br/&gt;
required, it should be returned to the pool in order to be reused.&lt;br/&gt;
an ArrayList. It load-balances access to its resources by returning a&lt;br/&gt;
different resource every time a given key is looked up.&lt;br/&gt;
couple of parameters (viz. &quot;hbase.client.ipc.pool.type&quot; and&lt;br/&gt;
&quot;hbase.client.ipc.pool.size&quot;). In case the size of the pool is set to a&lt;br/&gt;
non-zero positive number, that is used to cap the number of resources that a&lt;br/&gt;
pool may contain for any given key. A size of Integer#MAX_VALUE is&lt;br/&gt;
interpreted to mean an unbounded pool.&lt;/p&gt;</comment>
                            <comment id="12904072" author="ryanobjc" created="Mon, 30 Aug 2010 03:36:43 +0000"  >&lt;p&gt;On the thread local, do the sockets get closed when the thread goes away?&lt;/p&gt;</comment>
                            <comment id="12904076" author="karthick" created="Mon, 30 Aug 2010 04:09:10 +0000"  >&lt;p&gt;Actually, no, the sockets won&apos;t get closed the way it is right now. The ThreadLocalPool#values method needs to return all of the thread-local connections in it, which doesn&apos;t happen right now.  Please stay tuned for an updated patch.&lt;/p&gt;
</comment>
                            <comment id="12904489" author="ryanobjc" created="Tue, 31 Aug 2010 01:39:27 +0000"  >&lt;p&gt;i gave this a shot using ycsb and 50 threads of parallelism and it bit the dust and got nothing done.  it also seems to be spending a lot of time expiring entries and looking them up in meta again, which is strange considering i have a 1 region table that hasnt moved.&lt;/p&gt;</comment>
                            <comment id="12904490" author="ryanobjc" created="Tue, 31 Aug 2010 01:41:35 +0000"  >&lt;p&gt;ok looks like i hit FD limits on my machines (1024, doh!).&lt;/p&gt;
</comment>
                            <comment id="12904498" author="ryanobjc" created="Tue, 31 Aug 2010 01:59:38 +0000"  >&lt;p&gt;another issue testing this patch - with the config set to use roundrobin and a limited number of threads (20,50), the client won&apos;t exist normally anymore.  The HCM shutdown hook won&apos;t finish up.&lt;/p&gt;</comment>
                            <comment id="12904499" author="ryanobjc" created="Tue, 31 Aug 2010 02:07:12 +0000"  >&lt;p&gt;more feedback:&lt;/p&gt;

&lt;p&gt;With threadlocal I get npes on client shutdown:&lt;br/&gt;
Exception in thread &quot;HCM.shutdownHook&quot; java.lang.NullPointerException&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HBaseClient.stop(HBaseClient.java:724)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HBaseRPC$ClientCache.stopClient(HBaseRPC.java:219)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.close(HBaseRPC.java:265)&lt;br/&gt;
        at org.apache.hadoop.hbase.ipc.HBaseRPC.stopProxy(HBaseRPC.java:441)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.close(HConnectionManager.java:1386)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager.deleteAllConnections(HConnectionManager.java:156)&lt;br/&gt;
        at org.apache.hadoop.hbase.client.HConnectionManager$1.run(HConnectionManager.java:83)&lt;/p&gt;

&lt;p&gt;Without an explicit pool type configuration, something BAD happens and the client seems to unlimitedly spawn sockets and takes down my low socket regionservers.  Setting the config to threadlocal fixes it.  &lt;/p&gt;

&lt;p&gt;And last but not least, the speed of round robin is about as fast as without the patch if not slower even.  The threadlocal is much faster (20%+) than the pre-patch version.&lt;/p&gt;
</comment>
                            <comment id="12904950" author="karthick" created="Wed, 1 Sep 2010 07:01:57 +0000"  >&lt;p&gt;Ryan,  &lt;/p&gt;

&lt;p&gt;Thanks for the feedback - it is much appreciated. Some of the concerns you raised should be addressed by the second version of the patch which:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Modifies the &quot;threadlocal&quot; pool&apos;s #values method so that it return all connections across all threads, instead of the one local to the current thread that could potentially be null, which should resolve the NPEs on client shutdown.&lt;/li&gt;
	&lt;li&gt;Defaults the pool type to a bounded &quot;roundrobin&quot; of size 1, which would emulate pre-patch behavior. On the other hand, the &quot;threadlocal&quot; pool&apos;s size is bounded only by the clients&apos; thread pool size.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Additionally, it:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Renames the SharedMap class to PoolMap, for lack of a better name.&lt;/li&gt;
	&lt;li&gt;Adds a test case for the PoolMap, which demonstrates how pools of different types and sizes behave under different loads.&lt;/li&gt;
	&lt;li&gt;Constrains the value for &quot;hbase.client.ipc.pool.type&quot; to one of the following: &quot;roundrobin&quot;, and &quot;threadlocal&quot;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To your point about file descriptor limits, the prescribed workaround for that is described &lt;a href=&quot;http://www.cloudera.com/blog/2009/03/configuration-parameters-what-can-you-just-ignore/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;. We found out about that limit the hard way too.&lt;/p&gt;

&lt;p&gt;The 20% speed up in &quot;threadlocal&quot; is good, although it may be even better if the benchmark runs at high throughputs. The &quot;roundrobin&quot; pool should not be unbounded, which is no longer the default - in general, it should be somewhere between 1 and the number of user-defined threads.&lt;/p&gt;</comment>
                            <comment id="12921194" author="tao.xie" created="Fri, 15 Oct 2010 01:16:17 +0000"  >&lt;p&gt;I&apos;m using 0.20.6 version, I checked the patch and found it just modifies the one connection thread to a pool strategy so I applied it to my 0.20.6 hbase code. But now master cannot recognizes RegionServers. Maybe this patch can only be applied to 0.89 version?&lt;/p&gt;

&lt;p&gt;HBase Master log:&lt;/p&gt;


&lt;p&gt;2010-10-15 17:10:18,225 INFO org.apache.hadoop.hbase.master.BaseScanner: All 0 .META. region(s) scanned&lt;br/&gt;
2010-10-15 17:11:18,069 INFO org.apache.hadoop.hbase.master.ServerManager: 0 region servers, 0 dead, average load NaN&lt;br/&gt;
2010-10-15 17:11:18,229 INFO org.apache.hadoop.hbase.master.BaseScanner: All 0 .META. region(s) scanned&lt;br/&gt;
2010-10-15 17:12:18,073 INFO org.apache.hadoop.hbase.master.ServerManager: 0 region servers, 0 dead, average load NaN&lt;br/&gt;
2010-10-15 17:12:18,233 INFO org.apache.hadoop.hbase.master.BaseScanner: All 0 .META. region(s) scanned&lt;/p&gt;</comment>
                            <comment id="12922836" author="karthick" created="Wed, 20 Oct 2010 03:26:10 +0000"  >&lt;p&gt;Hi Tao,&lt;/p&gt;

&lt;p&gt;Judging by the master log messages, it seems that your region server has not started. &lt;/p&gt;

&lt;p&gt;AFAIK, this patch should work with the 0.20.6 branch as well (note that it was originally based on the trunk). To validate that, I back-ported the patch to the 0.20.6 tag (see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2939&quot; title=&quot;Allow Client-Side Connection Pooling&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2939&quot;&gt;&lt;del&gt;HBASE-2939&lt;/del&gt;&lt;/a&gt;-0.20.6.patch) , packaged it, started hbase, and sure enough it starts (see status below).&lt;/p&gt;

&lt;p&gt;hbase(main):001:0&amp;gt; status&lt;br/&gt;
1 servers, 0 dead, 2.0000 average load&lt;br/&gt;
hbase(main):002:0&amp;gt; version&lt;br/&gt;
Version: 0.20.6, r1609fbbdc0e76921eba3dfaeffd140a3606c4da0, Tue Oct 19 22:53:37 EDT 2010&lt;br/&gt;
hbase(main):003:0&amp;gt;&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Karthick&lt;/p&gt;</comment>
                            <comment id="12968891" author="stack" created="Tue, 7 Dec 2010 19:08:18 +0000"  >&lt;p&gt;Making this issue critical &amp;#8211; We owe Karthik feedback &amp;#8211; and assigning Ryan since he was looking into this (Can you take a look RR?  Karthik updated his patch... thanks).&lt;/p&gt;</comment>
                            <comment id="12968892" author="stack" created="Tue, 7 Dec 2010 19:08:31 +0000"  >&lt;p&gt;Oh, I brought it into 0.92 too.&lt;/p&gt;</comment>
                            <comment id="12987454" author="ryanobjc" created="Thu, 27 Jan 2011 09:28:30 +0000"  >&lt;p&gt;btw the recent patch applies with no major issues, some import jazz but intellij fixes that.&lt;/p&gt;

&lt;p&gt;I&apos;m working on perf testing this, but I&apos;m having problems with my test env right now.&lt;/p&gt;</comment>
                            <comment id="12987745" author="karthick" created="Thu, 27 Jan 2011 20:42:04 +0000"  >&lt;p&gt;Thanks for the update. Please let me know how your benchmarking goes.&lt;/p&gt;</comment>
                            <comment id="13000661" author="ryanobjc" created="Tue, 1 Mar 2011 01:47:38 +0000"  >&lt;p&gt;I ran this on a little cluster test and my results were a little mixed.  This is due to my set up and my benchmarking setup.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;using YCSB and a small working set I loaded data on Regionserver R1&lt;/li&gt;
	&lt;li&gt;ran YCSB with small 300 row scans with 100 columns on Master M&lt;/li&gt;
	&lt;li&gt;ran with both this patch, and without.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The test runs with this patch did not indicate significant improvement.  I think this was due to the fact that I was saturating the network between M and R1, and opening more sockets gave slight but not significant improvement.  I didn&apos;t capture the numbers, but it was something like 390 ms without and 340 ms with.&lt;/p&gt;

&lt;p&gt;I ran in a fairly interesting case since I was trying to test the contention of the single socket, and it seems like adding more sockets to a saturated network did not improve like I had hoped to.&lt;/p&gt;

&lt;p&gt;Could you paste in your test scenario?  I could see that for some scenarios involving small to medium gets, that this could provide an improvement to latency.&lt;/p&gt;</comment>
                            <comment id="13009383" author="stack" created="Mon, 21 Mar 2011 21:10:35 +0000"  >&lt;p&gt;Moving out of 0.92.&lt;/p&gt;</comment>
                            <comment id="13012204" author="kinger" created="Mon, 28 Mar 2011 20:28:05 +0000"  >&lt;p&gt;Attached is the client code we developed in house.  This helped alleviate the HTable bottleneck when fetching 10K+ rows from HBase.&lt;/p&gt;

&lt;p&gt;It&apos;s a bit of a hack at the moment.&lt;/p&gt;</comment>
                            <comment id="13012240" author="karthick" created="Mon, 28 Mar 2011 21:45:21 +0000"  >&lt;p&gt;Hello Jon,&lt;/p&gt;

&lt;p&gt;What branch is your client code based on? It shows a lot of changes w.r.t the trunk, so I take it you&apos;re working off of an older version?&lt;/p&gt;

&lt;p&gt;Regards, &lt;br/&gt;
Karthick&lt;/p&gt;</comment>
                            <comment id="13012256" author="kinger" created="Mon, 28 Mar 2011 22:10:27 +0000"  >&lt;p&gt;Hi Karthick,&lt;/p&gt;

&lt;p&gt;We are running CDH3B3.&lt;/p&gt;</comment>
                            <comment id="13012259" author="karthick" created="Mon, 28 Mar 2011 22:37:51 +0000"  >&lt;p&gt;Hi Jon,&lt;/p&gt;

&lt;p&gt;It looks like your patch hard codes the pool type to be round-robin and the pool size to be 50, which seems reasonable to me. Also, I noticed that you&apos;re removing the connection in the event it is marked for closure, which seems reasonable as well. The latter change doesn&apos;t seem to exist in the 0.89 branch, but perhaps it came from CDH3B3 distro. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; void markClosed(IOException e) {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (shouldCloseConnection.compareAndSet(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)) {
+    	  connections.remove(&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.remoteId, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In any case, your findings do corroborate the theory that the default singleton (or multiton) client connection model may break down for high throughput scenarios. I maintain that, when connection-level multiplexing starts to become a bottleneck, it is best to load-balance requests across multiple (non-overlapping) connection sockets.&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Karthick&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Karthick&lt;/p&gt;</comment>
                            <comment id="13019966" author="stack" created="Thu, 14 Apr 2011 18:21:40 +0000"  >&lt;p&gt;Bringing into 0.92.0 since Karthik got support for his claim from photobucket jon.  How do you suggest we proceed Karthik?  Clean up Jons patch or go forward with yours.  What about Ryans&apos; query about how you are testing your patch and what you are seeing?  Thanks.&lt;/p&gt;</comment>
                            <comment id="13020084" author="karthick" created="Thu, 14 Apr 2011 23:03:21 +0000"  >&lt;p&gt;I believe Jon&apos;s patch is based more or less on the one dated &quot;03/Sep/10 21:14&quot;, and hard codes the pool type to &quot;round-robin&quot; and size to &quot;50&quot;. In general, I think this change ought to be backward compatible w.r.t. the existing behavior, which is to map a given &lt;tt&gt;ConnectionId&lt;/tt&gt; to a unique &lt;tt&gt;Connection&lt;/tt&gt;. To that end, we should default the pool type to &quot;round-robin&quot; and its size to 1. At any rate, considering that my patch is more generic, I vote for going forward with that, with these changes (see patch attached today):&lt;/p&gt;

&lt;p&gt;a) Change the default pool size to 1.&lt;br/&gt;
b) Clean up all trailing white spaces.&lt;/p&gt;

&lt;p&gt;About Ryan&apos;s query, I had a hard time rewriting the YCSB workload in terms of our (homegrown) test scenario, which involved sending 5-15K &lt;tt&gt;HTable.get&lt;/tt&gt; per second. The problem with the existing YCSB workload is that it assumes a scan, hence hard to refactor. At a minimum, I am going to run the &quot;mvn test&quot; suite and make sure no regressions were introduced, before I take another shot at YCSB, time permitting.&lt;/p&gt;


</comment>
                            <comment id="13020244" author="stack" created="Fri, 15 Apr 2011 09:10:52 +0000"  >&lt;p&gt;I&apos;d suggest you not try reworking YCSB.  It will be hard to bring it around (Its hard code to work with).  Let us know if your patch passes all tests and I&apos;ll give it a once-over.  Thanks Karthik.&lt;/p&gt;</comment>
                            <comment id="13020835" author="karthick" created="Sun, 17 Apr 2011 18:09:04 +0000"  >&lt;p&gt;Please review the latest patch, which by default, exhibits the existing behavior (i.e., each connection id maps to precisely one connection). &lt;/p&gt;

&lt;p&gt;As far as testing is concerned, all but five tests passed. FWIW, those failures occur even without the patch, so in that sense, no regressions were found. For good measure, I added a couple of test cases in &lt;tt&gt;TestFromClientSide&lt;/tt&gt;, which also serve to illustrate how the connection pool may be configured.&lt;/p&gt;</comment>
                            <comment id="13021239" author="stack" created="Mon, 18 Apr 2011 20:39:51 +0000"  >&lt;p&gt;I took a look at the patch.  The changes to HBase seem innocuous enough.  Looks like nice addition and would make it easy to try out different poolings.&lt;/p&gt;

&lt;p&gt;Whats a little odd is your adding of new functionality but w/o unit tests.  It would help build confidence if you had a unit test to prove that the RoundRobinPool was indeed RoundRobin.... Seems like tests would be easy enough to do since these take generics.&lt;/p&gt;

&lt;p&gt;I see the Pool Interface.  Seems to be straight subset of Map Interface?  Do we need PoolMap then?  Or should PoolMap be non-public?&lt;/p&gt;

&lt;p&gt;Should the pool implementations be inner classes of PoolMap because PoolMap refers to them explicitly in enum and in its little factory for creating them.&lt;/p&gt;

&lt;p&gt;Why does javadoc talk about SharedMap?  Should that be PoolMap?&lt;/p&gt;

&lt;p&gt;Good stuff Karthik.&lt;/p&gt;



</comment>
                            <comment id="13021435" author="karthick" created="Tue, 19 Apr 2011 07:12:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;It would help build confidence if you had a unit test to prove that the RoundRobinPool was indeed RoundRobin&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Point well taken. I&apos;ve added a suite of tests to cover all types of pool maps.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I see the Pool Interface. Seems to be straight subset of Map Interface? Do we need PoolMap then? Or should PoolMap be non-public?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The primary role of the &lt;tt&gt;PoolMap&lt;/tt&gt; is to associate a pool of values with every key. The first time a key is inserted, it creates a pool of the specified type, and puts the value into the that pool. Subsequent inserts into the same key will put the value into the pre-existing pool. By the same token, when a key is removed, it&apos;s corresponding pool is cleared. &lt;/p&gt;

&lt;p&gt;The &lt;tt&gt;Pool&lt;/tt&gt; interface, while it may seem like a subset of &lt;tt&gt;Map&lt;/tt&gt;, was meant to be generic enough that it could represent not just a bounded list (a.k.a round-robin pool), or a bounded queue (a.k.a. reusable pool), but also a thread-local object (a.k.a. thread-local pool).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Should the pool implementations be inner classes of PoolMap because PoolMap refers to them explicitly in enum and in its little factory for creating them.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed. I made the &lt;tt&gt;PoolMap&lt;/tt&gt; a self-contained entity.&lt;/p&gt;

&lt;p&gt;{quote{Why does javadoc talk about SharedMap? Should that be PoolMap?&lt;blockquote&gt;&lt;/blockquote&gt;&lt;br/&gt;
Fixed.&lt;/p&gt;

&lt;p&gt;FYI, the version of the updated patch is V6 (sorry about the poor patch naming convention).&lt;/p&gt;
</comment>
                            <comment id="13021657" author="stack" created="Tue, 19 Apr 2011 16:53:33 +0000"  >&lt;p&gt;Patch looks good.  Let me run tests.&lt;/p&gt;</comment>
                            <comment id="13021706" author="stack" created="Tue, 19 Apr 2011 18:32:49 +0000"  >&lt;p&gt;Applied to TRUNK.  Thanks for the patch Karthik (S).  I assigned you the issue.  Tests passed for me.&lt;/p&gt;</comment>
                            <comment id="13021737" author="karthick" created="Tue, 19 Apr 2011 19:07:44 +0000"  >&lt;p&gt;Thanks much! Hopefully, this patch will address some of the concerns raised in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1849&quot; title=&quot;HTable doesn&amp;#39;t work well at the core of a multi-threaded server; e.g. webserver&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1849&quot;&gt;&lt;del&gt;HBASE-1849&lt;/del&gt;&lt;/a&gt; as well. On a related note, the &lt;tt&gt;PoolMap&lt;/tt&gt; may also be applied to generalize the &lt;tt&gt;HTablePool&lt;/tt&gt;, as described in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2938&quot; title=&quot;Add Thread-Local Behavior To HTable Pool&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2938&quot;&gt;&lt;del&gt;HBASE-2938&lt;/del&gt;&lt;/a&gt; - would you like me to revisit/update that issue?&lt;/p&gt;</comment>
                            <comment id="13021866" author="stack" created="Wed, 20 Apr 2011 00:05:55 +0000"  >&lt;p&gt;For sure go ahead and update hbase-2938 and note PoolMap in hbase-1849.&lt;/p&gt;</comment>
                            <comment id="13436951" author="nmarasoi" created="Fri, 17 Aug 2012 18:35:51 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am not sure why multiplexing does not work so well with multiple tcp connections. &lt;br/&gt;
When using one tcp connection, the multiplexing pays off peaking at 16 threads using same connection in my particular tests (remote client, 4kb batching of 2k puts).&lt;br/&gt;
While when using 16 tcp connections, the multiplexing benefit peaks at 2 threads sharing one tcp connection.&lt;br/&gt;
The relative benefit of sharing/multiplexing seems about the same when 2 htables share one socket. However when using hbase.client.ipc.pool.size=16, the benefit degrades rapidly when the multiplexing factor is above 2, and goes below the line when above 4.&lt;br/&gt;
I would like to understand the underlying reason. Perhaps there are locking and contention mechanisms preventing us loading the multiple connections in the same way we can multiplex a single one.&lt;/p&gt;

&lt;p&gt;Here are my times with batched puts, n htable instances (threads), m connections in the robin pool:&lt;/p&gt;

&lt;p&gt;1 HTable:  1400 records in a fixed timeframe&lt;br/&gt;
2 HTables sharing the tcp socket:  1566 records in a fixed timeframe&lt;br/&gt;
8 Htables sharing the tcp socket: 6500 records&lt;br/&gt;
16 HTables sharing the tcp socket: 9200 records&lt;br/&gt;
32 HTables sharing the tcp socket: 6500 records&lt;/p&gt;

&lt;p&gt;256 Htables sharing the tcp socket: 1340 recs&lt;br/&gt;
16 HTables on 16 connections: 16753 recs&lt;br/&gt;
32 Htables on 16 connections: 18661 recs&lt;br/&gt;
64 Htables on 16 connections: 16800 recs&lt;br/&gt;
128 Htables on 16 connections: 4300 recs&lt;br/&gt;
256 Htables on 16 connections: 2434 recs&lt;/p&gt;

&lt;p&gt;when saying 16 htables i mean 16 threads using a HTablePool&lt;/p&gt;

&lt;p&gt;You can see that it seems that the multiplexing performance degrades when using multiple connections much faster than when using just one (without pooling, default).&lt;/p&gt;</comment>
                            <comment id="13436961" author="stack" created="Fri, 17 Aug 2012 18:49:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;Perhaps there are locking and contention mechanisms preventing us loading the multiple connections in the same way we can multiplex a single one.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you see contention in thread dumps?&lt;/p&gt;</comment>
                            <comment id="15016919" author="lars_francke" created="Fri, 20 Nov 2015 12:41:14 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12435909">HBASE-1849</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12472725">HBASE-2938</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12457640" name="HBASE-2939-0.20.6.patch" size="28527" author="karthick" created="Wed, 20 Oct 2010 03:26:35 +0000"/>
                            <attachment id="12476565" name="HBASE-2939-LATEST.patch" size="28920" author="karthick" created="Sun, 17 Apr 2011 17:59:31 +0000"/>
                            <attachment id="12476694" name="HBASE-2939-V6.patch" size="34708" author="karthick" created="Tue, 19 Apr 2011 07:13:44 +0000"/>
                            <attachment id="12476385" name="HBASE-2939.patch" size="19511" author="karthick" created="Thu, 14 Apr 2011 23:03:39 +0000"/>
                            <attachment id="12453828" name="HBASE-2939.patch" size="28625" author="karthick" created="Fri, 3 Sep 2010 21:14:38 +0000"/>
                            <attachment id="12453360" name="HBASE-2939.patch" size="19690" author="karthick" created="Sat, 28 Aug 2010 19:53:22 +0000"/>
                            <attachment id="12474808" name="HBaseClient.java" size="31538" author="kinger" created="Mon, 28 Mar 2011 20:28:05 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 28 Aug 2010 22:50:00 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32842</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0dbqv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>75832</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>