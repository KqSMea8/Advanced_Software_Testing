<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:27:00 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-5353/HBASE-5353.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-5353] HA/Distributed HMaster via RegionServers</title>
                <link>https://issues.apache.org/jira/browse/HBASE-5353</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently, the HMaster node(s) must be considered a &apos;special&apos; node (though not a single point of failover), meaning that the node must be protected more than the other cluster machines or at least specially monitored. Minimally, we always need to ensure that the master is running, rather than letting the system handle that internally. It should be possible to instead have the HMaster be much more available, either in a distributed sense (meaning a bit rewrite) or multiple, dynamically created instances combined with the hot fail-over of masters. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12541847">HBASE-5353</key>
            <summary>HA/Distributed HMaster via RegionServers</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="7">Later</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="jesse_yates">Jesse Yates</reporter>
                        <labels>
                    </labels>
                <created>Wed, 8 Feb 2012 18:32:18 +0000</created>
                <updated>Wed, 22 Apr 2015 00:45:03 +0000</updated>
                            <resolved>Wed, 22 Apr 2015 00:45:03 +0000</resolved>
                                    <version>0.94.0</version>
                                                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="13203829" author="jesse_yates" created="Wed, 8 Feb 2012 18:40:50 +0000"  >&lt;p&gt;I was thinking about this and it seems like it wouldn&apos;t be that hard to have each of the regionservers doing leader election via ZK to select the one (or top &apos;n&apos; rs) that would spin up master instances on their local machine. Those new masters could do their own leader election in ZK to determine who is the current &apos;official&apos; HMaster, and the others would act as hot failovers. If a master dies, the next rs in the list would spin up a master instance, ensuring that we always have a certain number of hot masters (clearly cascading failure here is a problem, but if that happens, you have bigger problems). Clearly, running the master from the same JVM is probably a bad idea, but you could potentially even use the startup scripts to spin up a separate jvm with the master.&lt;/p&gt;

&lt;p&gt;This also means some modification to the client, to keep track of the current master, but that should be fairly trivial, as it already has the zk connection (or can do a fail and lookup). &lt;/p&gt;</comment>
                            <comment id="13203872" author="stack" created="Wed, 8 Feb 2012 19:24:26 +0000"  >&lt;p&gt;I&apos;d say just run the master in-process w/ the regionserver.  Master doesn&apos;t do much (It used to be heavily loaded when we did log splitting but thats distributed now or on startup... but even then, should be fine).&lt;/p&gt;

&lt;p&gt;Client already tracks master location as you say though we need to undo this...and just have the client do a read of zk to find master location when it needs it.&lt;/p&gt;

&lt;p&gt;Regards UI, we&apos;d collapse it so that there&apos;d be a single webapp rather than the two we have now.  There&apos;d be a &apos;master&apos; link.  If the current regionserver were not the master, the master link would redirect you to current master.&lt;/p&gt;</comment>
                            <comment id="13203892" author="jesse_yates" created="Wed, 8 Feb 2012 19:38:53 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I&apos;d say just run the master in-process w/ the regionserver. Master doesn&apos;t do much (It used to be heavily loaded when we did log splitting but thats distributed now or on startup... but even then, should be fine).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I worry about putting too much in the same jvm, especially when you ahve a heavily loaded RS, you could be seriously killed with jvm pauses when you up the size to accomodate the master (could be bad too when you have the larger jvm, but no master running). Since, initially, this is going to be enabled via a configuration option, another option would just be to start it in JVM vs. outside the jvm; seems to me it would work either way.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Client already tracks master location as you say though we need to undo this...and just have the client do a read of zk to find master location when it needs it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Talked wtih Lars H about doing this fix too - the client really doesn&apos;t need the long running zk connection, but should just zk when it needs the master info. So that could be part of this ticket too. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Regards UI, we&apos;d collapse it so that there&apos;d be a single webapp rather than the two we have now.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Only works if we have same jvm stuff. I would argue that it should have a RS link rather than a master link (smile). But for the initial patch I would say the ui stuff should be on hold, until the actual implementation gets worked out.&lt;/p&gt;</comment>
                            <comment id="13203909" author="jxiang" created="Wed, 8 Feb 2012 19:55:36 +0000"  >&lt;p&gt;Another option is not to have a master, every region server can do the work a master currently does.  Just uses the ZK to coordinate them.&lt;br/&gt;
For example, once a region server dies, all other region server knows about it, all try to run the dead server clean up, but only one will actually do it.  The drawback here is too much zk interaction.&lt;/p&gt;</comment>
                            <comment id="13204036" author="tlipcon" created="Wed, 8 Feb 2012 21:53:33 +0000"  >&lt;p&gt;Given that we already have failover support for the master, I&apos;m skeptical that adding any complexity here is a good idea. If you want to colocate masters and RS, you can simply run a master process on a few of your RS nodes, and basically have the same behavior.&lt;/p&gt;

&lt;p&gt;What&apos;s the compelling use case? The master is &lt;em&gt;not&lt;/em&gt; a SPOF since we already have hot failover support.&lt;/p&gt;</comment>
                            <comment id="13204043" author="jesse_yates" created="Wed, 8 Feb 2012 21:59:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;have failover support for the master,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;oh right. But this means you don&apos;t need to worry about where you run your master - the system takes care of all of that for you, making the startup process easier. Works well here b/c we have the master down to such a lightweight process.&lt;/p&gt;</comment>
                            <comment id="13204048" author="tlipcon" created="Wed, 8 Feb 2012 22:05:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;But this means you don&apos;t need to worry about where you run your master&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Except it opens a new can of worms: where do you find the master UI? how do you monitor your master if it moves around? how do you easily find the master logs when it could be anywhere in the cluster?&lt;/p&gt;

&lt;p&gt;If your goal is to automatically pick a system to run a master on, you could have your cluster management software do that, but I only see additional complexity being introduced if you add this to HBase proper.&lt;/p&gt;</comment>
                            <comment id="13204051" author="jesse_yates" created="Wed, 8 Feb 2012 22:13:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;where do you find the master UI? how do you monitor your master if it moves around? how do you easily find the master logs when it could be anywhere in the cluster?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The cluster knows about it, so you can have a link on the webui to the master or any of the region servers. As stack was saying above, the region server page would have a link to the master page. Same deal with the logs (or using something like the hbscan stuff from fbook).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;if your goal is to automatically pick a system to run a master on, you could have your cluster management software do that&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True, but if those masters fail over, then your cluster management needs to be aware enough of that to provision more, on different servers; afaik, thisis a pain to do in a really &apos;cluster aware&apos; sense. This way, its all handled under the covers&lt;/p&gt;</comment>
                            <comment id="13204058" author="tlipcon" created="Wed, 8 Feb 2012 22:22:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;The cluster knows about it, so you can have a link on the webui to the master or any of the region servers&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And each of the potential masters publishes metrics to ganglia, so if you want to find the master metrics, you have to hunt around in the ganglia graphs for which master was active at that time?&lt;br/&gt;
And any cron jobs or nagios alerts you write need to first call some HBase utility to find the active master&apos;s IP via ZK in order to get to it?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;True, but if those masters fail over, then your cluster management needs to be aware enough of that to provision more, on different servers&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you have two masters on separate racks, and you have any reasonable monitoring, then your ops team will restart or provision a new one when they fail. I&apos;ve never ever heard of this kind of scenario being a major cause of downtime.&lt;/p&gt;


&lt;p&gt;The whole thing seems like a bad idea to me. I won&apos;t -1 but consider me -0.5&lt;/p&gt;</comment>
                            <comment id="13204113" author="stack" created="Wed, 8 Feb 2012 23:34:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;Except it opens a new can of worms: where do you find the master UI? how do you monitor your master if it moves around? how do you easily find the master logs when it could be anywhere in the cluster?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its not a new can of worms, right?  We have the above (mostly unsolved) problems now if you run with more than one master.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And any cron jobs or nagios alerts you write need to first call some HBase utility to find the active master&apos;s IP via ZK in order to get to it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They should be doing this now, if multiple masters?&lt;/p&gt;

&lt;p&gt;If the master function were lightweight enough, it&apos;d be kinda sweet having one daemon type only I&apos;d think; there&apos;d be no longer need for special treatment of master.  Might be tricky having them running in the same JVM what w/ all the executors afloat and RPCs (I&apos;d rather do all in the one JVM then have RS start/stop separate Master processes if we were going to go this route).&lt;/p&gt;</comment>
                            <comment id="13204123" author="tlipcon" created="Wed, 8 Feb 2012 23:48:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;We have the above (mostly unsolved) problems now if you run with more than one master.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;They should be doing this now, if multiple masters?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sort of - except when you have two masters, you just set up nagios alerts and metrics to point to both, and you only need to look two places if you have an issue. If you have no idea where the master is, you have to hunt around the cluster to find it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If the master function were lightweight enough, it&apos;d be kinda sweet having one daemon type only I&apos;d think&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Except we&apos;d still have multiple daemon types, logically, it&apos;s just that they&apos;d be collocated inside the same process, making logs harder to de-interleave, etc.&lt;/p&gt;


&lt;p&gt;Plus, if your RS are are all collocated with TTs and heavily loaded, then I wouldn&apos;t want to see the master running on one of them. I&apos;d rather just tell ops &quot;these nodes run the important master daemons, please monitor them and any high utilization is problematic&quot;.&lt;/p&gt;</comment>
                            <comment id="13204204" author="eli2" created="Thu, 9 Feb 2012 02:31:42 +0000"  >&lt;p&gt;@Jesse, how is the HMaster machine a single point of failure? A SPOF is a part of the system that, if it fails, sstop the entire system from working. Because there&apos;s automatic HMaster failover that&apos;s not the case for HBase.&lt;/p&gt;</comment>
                            <comment id="13204215" author="jesse_yates" created="Thu, 9 Feb 2012 03:14:18 +0000"  >&lt;p&gt;@Eli - yeah, I already mentioned that that is correct in response to Todd; I&apos;d forgotten we had added that to HBase&lt;/p&gt;</comment>
                            <comment id="13204220" author="eli2" created="Thu, 9 Feb 2012 03:33:26 +0000"  >&lt;p&gt;Cool, thanks for the clarification. Mind updating the description to match the latest understanding?&lt;/p&gt;</comment>
                            <comment id="13204241" author="jesse_yates" created="Thu, 9 Feb 2012 04:18:12 +0000"  >&lt;p&gt;@Eli - done.&lt;/p&gt;</comment>
                            <comment id="13204268" author="stack" created="Thu, 9 Feb 2012 05:18:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;...you only need to look two places if you have an issue.   If you have no idea where the master is, you have to hunt around the cluster to find it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d imagine it&apos;d be hard getting this patch in if no idea where the master is (And, again, don&apos;t we have this problem now if you start up three masters and one fails?  You have to hunt around.  We need to build the redirect piece regardless such as a link to master on each server page which redirects to current master and such as a history of who was master when in zk).&lt;/p&gt;

&lt;p&gt;You could even make the combined master+regionserver daemon work like our current multimaster system by having there be affinity for a certain set of servers.&lt;/p&gt;

&lt;p&gt;What kind of nagios alerts would be master particular?  We need to add indirection to these now anyways &amp;#8211; ask zk who the master is &amp;#8211; if more than one master running.  Metrics could be a little complicated especially if master moved servers over the period of interest but generally aren&apos;t master metrics of less interest since they are generally just aggregates and ganglia or opentsdb do it better job of this anyways?&lt;/p&gt;

&lt;p&gt;Logs don&apos;t have to be interleaved.  Thats just a bit of log4j config?&lt;/p&gt;

&lt;p&gt;Yes, could be issue if the daemon is bogged down.  The master would be less responsive which should be fine for short periods but if sustained it could be issue.&lt;/p&gt;

&lt;p&gt;I&apos;m not going to work on this.  I do see it as something that could simplify our deploy story.&lt;/p&gt;








</comment>
                            <comment id="13207994" author="nspiegelberg" created="Tue, 14 Feb 2012 20:32:51 +0000"  >&lt;p&gt;There&apos;s a lot of discussion here, so I might have missed something... How is searching for the master a problem?  We already have bin/get-active-master.rb , a jruby script that will search ZK for the current owner of the master lock.  Combining that with &apos;bin/hbase start master --backup&apos;, and you have a full solution to let your monitoring scripts ping the correct server for UI information.  The downtime for the master is dominated by the ZK ephemeral node timeout more than process setup.&lt;/p&gt;

&lt;p&gt;The common &apos;annoyance&apos; that we see internally is on the developer side (not monitoring side).  You see that servers are down, so you checkout the web ui.  The master went down, backup master took over, then autoremediation restarted the downed server in backup mode.  This means that the UI is inaccessible from the normal location.  DNS propagation takes a lot longer than restarting a process, so that&apos;s not really an option for us.  Because of this, I think a more important feature is to have the backup masters setup a web server with an HTTP redirect to the active master&apos;s UI.  &lt;/p&gt;</comment>
                            <comment id="13208006" author="tlipcon" created="Tue, 14 Feb 2012 20:56:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Because of this, I think a more important feature is to have the backup masters setup a web server with an HTTP redirect to the active master&apos;s UI.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13209944" author="jesse_yates" created="Fri, 17 Feb 2012 00:57:27 +0000"  >&lt;p&gt;To follow up on Nicholas&apos;s comment, this would also be &lt;em&gt;just a configuration option&lt;/em&gt;, not changing the way you have to run HBase. If it makes more sense for your setup to have a couple known masters that you monitor (the current impl), then you can do it that way. Alternatively, if you just want to have 1 daemon running for hbase - a region/masterServer - then you can do that too. A lot of the motivation around this jira was to try and make lives&apos; easier by having less stuff to worry about. &lt;/p&gt;</comment>
                            <comment id="13225079" author="jmhsieh" created="Thu, 8 Mar 2012 08:04:14 +0000"  >&lt;p&gt;@Todd @Nicholas A months back I&apos;ve started some work on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5083&quot; title=&quot;Backup HMaster should have http infoport open with link to the active master&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5083&quot;&gt;&lt;del&gt;HBASE-5083&lt;/del&gt;&lt;/a&gt; but never haven&apos;t posted the patch yet since it has a pretty nasty hack in it (adding 10 to the master port to get the info/http port).   There has been some cluster status things changing recently &amp;#8211; I&apos;ll wait for that to settle before I finish that patch.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 8 Feb 2012 19:24:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>227134</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 41 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02f8n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12070</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>