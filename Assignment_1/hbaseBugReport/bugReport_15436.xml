<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:03:33 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15436/HBASE-15436.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15436] BufferedMutatorImpl.flush() appears to get stuck</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15436</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;We noticed an instance where the thread that was executing a flush (&lt;tt&gt;BufferedMutatorImpl.flush()&lt;/tt&gt;) got stuck when the (local one-node) cluster shut down and was unable to get out of that stuck state.&lt;/p&gt;

&lt;p&gt;The setup is a single node HBase cluster, and apparently the cluster went away when the client was executing flush. The flush eventually logged a failure after 30+ minutes of retrying. That is understandable.&lt;/p&gt;

&lt;p&gt;What is unexpected is that thread is stuck in this state (i.e. in the &lt;tt&gt;flush()&lt;/tt&gt; call). I would have expected the &lt;tt&gt;flush()&lt;/tt&gt; call to return after the complete failure.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12948590">HBASE-15436</key>
            <summary>BufferedMutatorImpl.flush() appears to get stuck</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="sjlee0">Sangjin Lee</reporter>
                        <labels>
                    </labels>
                <created>Wed, 9 Mar 2016 22:56:04 +0000</created>
                <updated>Wed, 30 Mar 2016 18:37:30 +0000</updated>
                                            <version>1.0.2</version>
                                                    <component>Client</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="15188260" author="sjlee0" created="Wed, 9 Mar 2016 22:59:26 +0000"  >&lt;p&gt;The hbaseException.log file shows the exception and the failure during &lt;tt&gt;flush()&lt;/tt&gt;. The threaddump.log file shows the full thread stack trace dump after the shutdown mechanism was unable to shut down the thread that was stuck in the &lt;tt&gt;flush()&lt;/tt&gt; call.&lt;/p&gt;</comment>
                            <comment id="15188261" author="sjlee0" created="Wed, 9 Mar 2016 22:59:40 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-4736&quot; title=&quot;Issues with HBaseTimelineWriterImpl in single node hadoop &amp;amp; hbase cluster &quot; class=&quot;issue-link&quot; data-issue-key=&quot;YARN-4736&quot;&gt;YARN-4736&lt;/a&gt; for more details.&lt;/p&gt;</comment>
                            <comment id="15190717" author="anoop.hbase" created="Fri, 11 Mar 2016 09:41:40 +0000"  >&lt;p&gt;So you say after u see the log abt failure (after some 30+ mins, in fact 36 mins I guess, as 1 min seems socket time out and 36 attempts there), still the flush is not coming out. So after seeing this log how long u wait?&lt;br/&gt;
So this is an async way of write to table.. Ya when the size of accumulated puts become some configured size, we will do a flush. Till then puts are accumulated at client side.&lt;br/&gt;
I believe I got the issue. This is not a dead lock or so.  &lt;br/&gt;
To this flush we will pass all the Rows to flush (Write to RS).  Rows I mean Mutations.&lt;br/&gt;
It will try to group the mutations per server and will contact each of the server with List of mutations to go there.&lt;br/&gt;
Well to group this it checks the region locations for each of the row. And the scan happens to META (as shown in logs) and it fails.  For the 1st Mutation in this list itself, it took 36 mins.  Because the scan to META has retries.  Each of the trial fails after the SocketTimeout&lt;/p&gt;

&lt;p&gt;See in AsyncProcess#submit&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; {
      .......
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; posInList = -1;
      Iterator&amp;lt;? &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Row&amp;gt; it = rows.iterator();
      &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (it.hasNext()) {
        Row r = it.next();
        HRegionLocation loc;
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (r == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalArgumentException(&lt;span class=&quot;code-quote&quot;&gt;&quot;#&quot;&lt;/span&gt; + id + &lt;span class=&quot;code-quote&quot;&gt;&quot;, row cannot be &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&quot;&lt;/span&gt;);
          &lt;span class=&quot;code-comment&quot;&gt;// Make sure we get 0-s replica.
&lt;/span&gt;          RegionLocations locs = connection.locateRegion(
              tableName, r.getRow(), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, RegionReplicaUtil.DEFAULT_REPLICA_ID);
          ........
        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (IOException ex) {
          locationErrors = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Exception&amp;gt;();
          locationErrorRows = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;&amp;gt;();
          LOG.error(&lt;span class=&quot;code-quote&quot;&gt;&quot;Failed to get region location &quot;&lt;/span&gt;, ex);
          &lt;span class=&quot;code-comment&quot;&gt;// This action failed before creating ars. Retain it, but &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not add to submit list.
&lt;/span&gt;          &lt;span class=&quot;code-comment&quot;&gt;// We will then add it to ars in an already-failed state.
&lt;/span&gt;          retainedActions.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Action&amp;lt;Row&amp;gt;(r, ++posInList));
          locationErrors.add(ex);
          locationErrorRows.add(posInList);
          it.remove();
          &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;; &lt;span class=&quot;code-comment&quot;&gt;// Backward compat: we stop considering actions on location error.
&lt;/span&gt;        }

       .........
      }
    } &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (retainedActions.isEmpty() &amp;amp;&amp;amp; atLeastOne &amp;amp;&amp;amp; (locationErrors == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The List &apos;rows&apos; is the same List which BufferedMutatorImpl hold. (ie. writeAsyncBuffer).   So for the 1st Mutation the region location lookup failed and that Mutation got removed from this List also as u can see.  This will eventually marked as failed op. And the flow comes back to BufferedMutatorImpl#backgroundFlushCommits&lt;br/&gt;
Here we can see&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (synchronous || ap.hasError()) {
        &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (!writeAsyncBuffer.isEmpty()) {
          ap.submit(tableName, writeAsyncBuffer, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The loop continues till writeAsyncBuffer is non empty.  So in this 36 mins we could remove only one item from the list.  Again it goes on and removes the  2nd and so on.   So if there are 100 Mutation in the list when we called flush(), it would get over after  36 * 100 mins  !!!!!&lt;/p&gt;

&lt;p&gt;Am not much knowing the design consideration of this AsyncProcess etc.   May be we should narrow down the lock on close() method from method level and set some thing like a closing state to true, the retries within the flows should check for this state and early out with a fat WARN log saying we will loose some of the mutations applied till now. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15191553" author="sjlee0" created="Fri, 11 Mar 2016 21:59:14 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; for your comments. To answer your question,&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So after seeing this log how long u wait?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe the user tried to shut it down about 30 minutes after this failure:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Fri Feb 26 00:39:03 IST 2016, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68065: row &apos;timelineservice.entity,naga!yarn_cluster!flow_1456425026132_1!&#127;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;!&#127;&#65533;&#65533;&#65533;&#65533;M&#65533;&#65533;&#127;&#65533;&#65533;&#65533;!YARN_CONTAINER!container_1456425026132_0001_01_000001,99999999999999&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=localhost,16201,1456365764939, seqNum=0
...
2016-02-26 01:09:19,799 ERROR org.apache.hadoop.yarn.server.nodemanager.NodeManager: RECEIVED SIGNAL 15: SIGTERM
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also, I&apos;m not too sure it is the case that flush is still going through the mutations. This is the stack trace of the thread that was in the &lt;tt&gt;flush()&lt;/tt&gt; call (taken &lt;b&gt;after&lt;/b&gt; this exception was seen):&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;pool-14-thread-1&quot; prio=10 tid=0x00007f4215268000 nid=0x46e6 waiting on condition [0x00007f41fe75d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  &amp;lt;0x00000000eeb5a010&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
	at org.apache.hadoop.hbase.util.BoundedCompletionService.take(BoundedCompletionService.java:75)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:190)
	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:56)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)
	at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache(ClientSmallReversedScanner.java:211)
	at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next(ClientSmallReversedScanner.java:185)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1200)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1109)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:369)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:320)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:206)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:183)
	- locked &amp;lt;0x00000000c246f268&amp;gt; (a org.apache.hadoop.hbase.client.BufferedMutatorImpl)
	at org.apache.hadoop.yarn.server.timelineservice.storage.common.BufferedMutatorDelegator.flush(BufferedMutatorDelegator.java:66)
	at org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl.flush(HBaseTimelineWriterImpl.java:457)
	at org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager$WriterFlushTask.run(TimelineCollectorManager.java:230)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The stack trace strongly indicates that it is waiting for more tasks to be completed and &lt;b&gt;is idle&lt;/b&gt;. I wasn&apos;t the one who observed this, and don&apos;t have any more thread dumps around that time.&lt;/p&gt;</comment>
                            <comment id="15192411" author="anoop.hbase" created="Sun, 13 Mar 2016 16:28:22 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;pool-14-thread-1&quot;&lt;/span&gt; prio=10 tid=0x00007f4215268000 nid=0x46e6 waiting on condition [0x00007f41fe75d000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;  &amp;lt;0x00000000eeb5a010&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
        at org.apache.hadoop.hbase.util.BoundedCompletionService.take(BoundedCompletionService.java:75)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:190)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:56)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)
        at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache(ClientSmallReversedScanner.java:211)
        at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next(ClientSmallReversedScanner.java:185)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1200)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1109)
        at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:369)
        at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:320)
        at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:206)
        at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:183)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I say the flush is continuing with each of the Mutation and you dont see, the thread doing flush op doing nothing, u say it looks not. But the issue is the thread doing the flush op works in a loop and that op in turn given a Meta table scan.  This u can see that the scan op is given to another thread in a pool. The original flush thread is waiting for the completion of that scan thread.  This u can clearly see in above trace.&lt;br/&gt;
So it is like this thread will wait for the result and that result is an Exception (SocketTimeout) which it will see after mins. Then the flush thread again comes back to life and continue that loop and again wil go into this wait mode..!!&lt;/p&gt;</comment>
                            <comment id="15195735" author="sjlee0" created="Tue, 15 Mar 2016 17:32:33 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;. That sounds plausible.&lt;/p&gt;

&lt;p&gt;This does represent a pretty critical issue then, no? If a region server is in a state where a socket timeout is thrown in this manner, flush will be stuck for a LONG time. In a high throughput situation, this would imply a pretty severe consequence and induce huge instability on the client.&lt;/p&gt;

&lt;p&gt;For the background, we are working on using HBase for the timeline service v.2 (see &lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-4736&quot; title=&quot;Issues with HBaseTimelineWriterImpl in single node hadoop &amp;amp; hbase cluster &quot; class=&quot;issue-link&quot; data-issue-key=&quot;YARN-4736&quot;&gt;YARN-4736&lt;/a&gt;) and node managers will be HBase clients. If a region server or region servers are in an unhealthy state, this issue would cause a pretty big cascading effect on the client cluster, correct?&lt;/p&gt;

&lt;p&gt;Does this behavior exist in all other later releases?&lt;/p&gt;</comment>
                            <comment id="15196668" author="anoop.hbase" created="Wed, 16 Mar 2016 02:55:03 +0000"  >&lt;p&gt;The HBase cluster fully went down.  So this kind of a scenario the application should take care? I mean shutdown the clients ( The NMs in this case) before HBase cluster down &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Another thing is when the flush in BufferedMutator not able to do (this can be because of temp unavailability of HBase RS(s) do the put ops on it getting blocked? I dont think so. That will make the client side (NM) to go out of memory at some point?  This we need to fix.&lt;/p&gt;

&lt;p&gt;When close() is called on BufferedMutator what is the expectation? All the prior writes (async) should get synced with HBase RS?&lt;/p&gt;</comment>
                            <comment id="15196669" author="anoop.hbase" created="Wed, 16 Mar 2016 02:55:31 +0000"  >&lt;p&gt;And Ya, I believe all 1.0+ releases having similar code&lt;/p&gt;</comment>
                            <comment id="15196687" author="naganarasimha" created="Wed, 16 Mar 2016 03:24:23 +0000"  >&lt;p&gt;Thanks for looking into it &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoopsamjohn&quot; class=&quot;user-hover&quot; rel=&quot;anoopsamjohn&quot;&gt;Anoop Sam John&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;So this kind of a scenario the application should take care? I mean shutdown the clients ( The NMs in this case) before HBase cluster down &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Well this could be a OM/admin operation which i think YARN/platform will have less control off. And also in my case it was happening due HBASE master and Region server going down abruptly due to connectivity problems with zookeeper. I have attached the HBase logs last when i faced in 1.0.3 in &lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-4736&quot; title=&quot;Issues with HBaseTimelineWriterImpl in single node hadoop &amp;amp; hbase cluster &quot; class=&quot;issue-link&quot; data-issue-key=&quot;YARN-4736&quot;&gt;YARN-4736&lt;/a&gt;. &lt;br/&gt;
I faced this issue when trying to test ATS Next Gen with Hbase in Pseudo cluster and it was easily reproduced when zookeeper data folder was set to default &lt;tt&gt;tmp/hbase-&amp;lt;username&amp;gt;&lt;/tt&gt;. Not sure whether its coincidence or the cause.&lt;/p&gt;</comment>
                            <comment id="15196723" author="anoop.hbase" created="Wed, 16 Mar 2016 04:07:01 +0000"  >&lt;p&gt;When close() is called on BufferedMutator what is the expectation? All the prior writes (async) should get synced with HBase RS?&lt;br/&gt;
Ya when data is pumped in in async way, it is expected that there may be data loss. If client went down abruptly.  So we can say if close is called, we can try flush data and gracefully down. If flush is not happening normally  we may need to close it with out flush of remaining mutations &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  How abt the decision making?  cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tedyu&quot; class=&quot;user-hover&quot; rel=&quot;tedyu&quot;&gt;Ted Yu&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15196846" author="anoop.hbase" created="Wed, 16 Mar 2016 06:23:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;And also in my case it was happening due HBASE master and Region server going down abruptly due to connectivity problems with zookeeper.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;So the HBase cluster will come back after some time?  The normal usages the OM will make it up again?  What is ur use case&lt;/p&gt;</comment>
                            <comment id="15197343" author="naganarasimha" created="Wed, 16 Mar 2016 13:54:05 +0000"  >&lt;p&gt;Valid point let me discuss on this more with ATS team...&lt;/p&gt;</comment>
                            <comment id="15198866" author="anoop.hbase" created="Thu, 17 Mar 2016 07:11:24 +0000"  >&lt;p&gt;There are some must fix things&lt;br/&gt;
1.  The BufferedMutator flush is keep on trying and taking more time.  It kicked as the size of all Mutations accumulated so far, met the flush size. (Say 2 MB).  The flush takes time and we keep on accepting new mutations into the list. This may lead to client side OOME !.. We may need to accept more mutations after a background started. Normally things will get moving faster. But this cannot be infinite.  There should be a cap size for the size above which we should block the writes. We should not take more than this limit. May be some thing like 1.5 times of what is the flush size.&lt;br/&gt;
2. The row lookups into META happening for one row at a time. So this makes its such that one row lookup failed after 36 retries and each having 1 min timeout.  The 1 min time out itself is so high? And even after that it just fails this one Mutation and continue with remaining.  What if we were doing multi Get to META table to know the region location for N mutations at a time.&lt;br/&gt;
3. When close() is explicitly called on BufferedMutator, we try for graceful down (ie. wait for a flush if one is there in progress and/or call flush before close).  In such case what if the cluster is down and it takes too long. How long we should wait?  Whether we should come out faster?  (May be loosing some Mutations, but that is any way known) &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15217714" author="nkeywal" created="Wed, 30 Mar 2016 09:23:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;There should be a cap size for the size above which we should block the writes. We should not take more than this limit. May be some thing like 1.5 times of what is the flush size.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We definitively want to take more than this limit, but may be not as much as what we&apos;re taking today (or maybe we want to be clearer on what these settings mean)&lt;br/&gt;
There is a limit, given by the number of task executed in parallel (hbase.client.max.total.tasks). If I understand correctly, this setting is now per client (and not per htable).&lt;br/&gt;
Ideally these parameters should be hidden to the user (i.e. the defaults are ok for a standard client w/o too much memory constraints). &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How long we should wait? Whether we should come out faster? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;iirc, A long time ago, the buffer was attached to the Table object, so the policy (or at least the objective &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) when one of the puts had failed (i.e. reached the max retry number) was simple: all the operations currently in the buffer were considered as failed as well, even if we had not even tried to send them. As a consequence the buffer was empty after the failure of a single put. It was then up to the client to continue or not. May be we should do the same with the buffered mutator, for all  cases, close or not? I haven&apos;t looked at the bufferedMutator code, but I can have a look it you whish &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What if we were doing multi Get to META table to know the region location for N mutations at a time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It seems like a good idea. There are many possible optimisation on how we use meta, and this is one of them.&lt;/p&gt;

</comment>
                            <comment id="15218554" author="anoop.hbase" created="Wed, 30 Mar 2016 18:37:30 +0000"  >&lt;p&gt;Thanks Nicholas&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;iirc, A long time ago, the buffer was attached to the Table object, so the policy (or at least the objective &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) when one of the puts had failed (i.e. reached the max retry number) was simple: all the operations currently in the buffer were considered as failed as well, even if we had not even tried to send them. As a consequence the buffer was empty after the failure of a single put. It was then up to the client to continue or not. May be we should do the same with the buffered mutator, for all  cases, close or not? I haven&apos;t looked at the bufferedMutator code, but I can have a look it you whish &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Both BufferedMutator and normal Table uses same AycnProcess path.  Am not remembering our old way of fail all when one failed(after max retries).&lt;br/&gt;
Also I feel, we need to add the closed check in the loop of retry..  Some how user called close on the BufferedMutator.  Ya it has to be a graceful close.  But not like mins user has to wait for the close..   We are in a trial and that failed, and at least before the next retry, we need to see the close flag.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12944636">YARN-4736</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12792371" name="hbaseException.log" size="14224" author="sjlee0" created="Wed, 9 Mar 2016 22:59:26 +0000"/>
                            <attachment id="12792372" name="threaddump.log" size="43733" author="sjlee0" created="Wed, 9 Mar 2016 22:59:26 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 11 Mar 2016 09:41:40 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            37 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2ufmf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>