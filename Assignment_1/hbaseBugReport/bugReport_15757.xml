<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:06:54 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15757/HBASE-15757.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15757] Reverse scan fails with no obvious cause</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15757</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;related issue on stackoverflow: &lt;a href=&quot;http://stackoverflow.com/questions/37001169/hbase-reverse-scan-error?noredirect=1#comment61558097_37001169&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/37001169/hbase-reverse-scan-error?noredirect=1#comment61558097_37001169&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;this works well:&lt;/p&gt;

&lt;p&gt;    scan = new Scan(startRow, stopRow);&lt;/p&gt;

&lt;p&gt;this throws exception sometimes:&lt;/p&gt;

&lt;p&gt;    scan = new Scan(stopRow, startRow);&lt;br/&gt;
	scan.setReversed(true);&lt;/p&gt;

&lt;p&gt;throwing exception while traffic is at least 100 req/s. there are actually no timeouts, exception is fired immediately for 1-10% requests&lt;/p&gt;

&lt;p&gt;hbase: 0.98.12-hadoop2;&lt;br/&gt;
hadoop: 2.7.0;&lt;br/&gt;
cluster in AWS, 10 datanodes: d2.4xlarge&lt;/p&gt;

&lt;p&gt;I think it&apos;s maybe related with this issue but I&apos;m not using any filters &lt;a href=&quot;http://apache-hbase.679495.n3.nabble.com/Exception-during-a-reverse-scan-with-filter-td4069721.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://apache-hbase.679495.n3.nabble.com/Exception-during-a-reverse-scan-with-filter-td4069721.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    	java.lang.RuntimeException: org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?&lt;br/&gt;
			at org.apache.hadoop.hbase.client.AbstractClientScanner$1.hasNext(AbstractClientScanner.java:94)&lt;br/&gt;
			at com.socialbakers.broker.client.hbase.htable.AbstractHtableListScanner.scanToList(AbstractHtableListScanner.java:30)&lt;br/&gt;
			at com.socialbakers.broker.client.hbase.htable.AbstractHtableListSingleScanner.invokeOperation(AbstractHtableListSingleScanner.java:23)&lt;br/&gt;
			at com.socialbakers.broker.client.hbase.htable.AbstractHtableListSingleScanner.invokeOperation(AbstractHtableListSingleScanner.java:11)&lt;br/&gt;
			at com.socialbakers.broker.client.hbase.AbstractHbaseApi.endPointMethod(AbstractHbaseApi.java:40)&lt;br/&gt;
			at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)&lt;br/&gt;
			at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
			at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
			at com.socialbakers.broker.client.Route.invoke(Route.java:241)&lt;br/&gt;
			at com.socialbakers.broker.client.handler.EndpointHandler.invoke(EndpointHandler.java:173)&lt;br/&gt;
			at com.socialbakers.broker.client.handler.EndpointHandler.process(EndpointHandler.java:69)&lt;br/&gt;
			at com.thetransactioncompany.jsonrpc2.server.Dispatcher.process(Dispatcher.java:196)&lt;br/&gt;
			at com.socialbakers.broker.client.RejectableRunnable.run(RejectableRunnable.java:38)&lt;br/&gt;
			at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
			at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
			at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
			Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: Failed after retry of OutOfOrderScannerNextException: was there a rpc timeout?&lt;br/&gt;
			at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:430)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:333)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.AbstractClientScanner$1.hasNext(AbstractClientScanner.java:91)&lt;br/&gt;
			... 15 more&lt;br/&gt;
			Caused by: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 2 But the nextCallSeq got from client: 1; request=scanner_id: 27700695 number_of_rows: 100 close_scanner: false next_call_seq: 1&lt;br/&gt;
			at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3231)&lt;br/&gt;
			at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:30946)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2093)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)&lt;br/&gt;
			at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;			at sun.reflect.GeneratedConstructorAccessor16.newInstance(Unknown Source)&lt;br/&gt;
			at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
			at java.lang.reflect.Constructor.newInstance(Constructor.java:422)&lt;br/&gt;
			at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)&lt;br/&gt;
			at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)&lt;br/&gt;
			at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:287)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:214)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:58)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:115)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:91)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:375)&lt;br/&gt;
			... 17 more&lt;br/&gt;
			Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException): org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 2 But the nextCallSeq got from client: 1; request=scanner_id: 27700695 number_of_rows: 100 close_scanner: false next_call_seq: 1&lt;br/&gt;
			at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3231)&lt;br/&gt;
			at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:30946)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2093)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)&lt;br/&gt;
			at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;			at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1457)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)&lt;br/&gt;
			at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)&lt;br/&gt;
			at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:31392)&lt;br/&gt;
			at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:173)&lt;br/&gt;
			... 21 more&lt;/p&gt;</description>
                <environment>&lt;p&gt;ubuntu 14.04, amazon cloud; 10 datanodes d2.4xlarge - 16cores, 12x200GB HDD, 122GB RAM&lt;/p&gt;</environment>
        <key id="12964636">HBASE-15757</key>
            <summary>Reverse scan fails with no obvious cause</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="fiserro">Robert Fiser</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 May 2016 11:57:57 +0000</created>
                <updated>Fri, 6 May 2016 14:14:52 +0000</updated>
                                            <version>0.98.12</version>
                                                    <component>Client</component>
                    <component>Scanners</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="15273640" author="anoop.hbase" created="Fri, 6 May 2016 05:12:25 +0000"  >&lt;p&gt;Can u capture the RS log at this time around and pass it here?&lt;/p&gt;</comment>
                            <comment id="15273799" author="fiserro" created="Fri, 6 May 2016 08:36:51 +0000"  >&lt;p&gt;There is RS log:&lt;br/&gt;
		16/05/06 08:19:46 INFO regionserver.HRegion: Starting compaction on o in region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,156483584365269_775211065825848,1452787869225.f4a36bfadc9903043191313f85d9c99e.&lt;br/&gt;
		16/05/06 08:19:46 INFO regionserver.HStore: Starting compaction of 3 file(s) in o of &amp;lt;REPLACED_TABLE_NAME&amp;gt;,156483584365269_775211065825848,1452787869225.f4a36bfadc9903043191313f85d9c99e. into tmpdir=hdfs://sencha/hbase/data/default/&amp;lt;REPLACED_TABLE_NAME&amp;gt;/f4a36bfadc9903043191313f85d9c99e/.tmp, totalSize=159.4 M&lt;br/&gt;
		16/05/06 08:19:50 INFO regionserver.HRegionServer: Scanner 42329759 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,76469923274_199526018274\x00\x7F\xFF\xFE\xADb\xE9\xFB\x1F,1454587404590.3a2da3ba070db86dc666337cf6c94f38.&lt;br/&gt;
		16/05/06 08:19:51 INFO regionserver.HRegionServer: Scanner 42329808 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,76469923274_199526018274\x00\x7F\xFF\xFE\xADb\xE9\xFB\x1F,1454587404590.3a2da3ba070db86dc666337cf6c94f38.&lt;br/&gt;
		16/05/06 08:19:52 INFO regionserver.HRegionServer: Scanner 42329872 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,76469923274_199526018274\x00\x7F\xFF\xFE\xADb\xE9\xFB\x1F,1454587404590.3a2da3ba070db86dc666337cf6c94f38.&lt;br/&gt;
		16/05/06 08:19:54 INFO regionserver.HRegionServer: Scanner 42330004 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,76469923274_199526018274\x00\x7F\xFF\xFE\xADb\xE9\xFB\x1F,1454587404590.3a2da3ba070db86dc666337cf6c94f38.&lt;br/&gt;
		16/05/06 08:19:54 INFO regionserver.HRegionServer: Scanner 42329973 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,156185069843_10153620394269844\x00\x7F\xFF\xFE\xB0\x05\xF9t\xD7,1454594676584.7f2688ce2f34b378485bc4376d83de66.&lt;br/&gt;
		16/05/06 08:19:55 INFO regionserver.HRegionServer: Scanner 42330016 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,156185069843_10153620394269844\x00\x7F\xFF\xFE\xB0\x05\xF9t\xD7,1454594676584.7f2688ce2f34b378485bc4376d83de66.&lt;br/&gt;
		16/05/06 08:19:56 INFO regionserver.HRegionServer: Scanner 42330057 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,76469923274_199526018274\x00\x7F\xFF\xFE\xADb\xE9\xFB\x1F,1454587404590.3a2da3ba070db86dc666337cf6c94f38.&lt;br/&gt;
		16/05/06 08:19:57 INFO regionserver.HRegionServer: Scanner 42330039 lease expired on region &amp;lt;REPLACED_TABLE_NAME&amp;gt;,156185069843_10153620394269844\x00\x7F\xFF\xFE\xB0\x05\xF9t\xD7,1454594676584.7f2688ce2f34b378485bc4376d83de66.&lt;/p&gt;

&lt;p&gt;We have a time in rowkey and if the timerange is very large it may hit a thousands of rows even if we have implemented a limit(5000) to stop scanning there might be much more rows.&lt;br/&gt;
I&apos;ve tried to set caching from 100 to 5000 with no result.&lt;br/&gt;
Now I&apos;ve found that the shorten range between start and stop row causes no error at all. All requests pass. If the range is large it cause error for scans even if they hits a less than 100 rows.&lt;/p&gt;</comment>
                            <comment id="15273936" author="ram_krish" created="Fri, 6 May 2016 11:40:42 +0000"  >&lt;p&gt;Usual questions, how big is the row?  Even after reducing the start and stop row (though you get result) does it take more time to get back those results?&lt;/p&gt;</comment>
                            <comment id="15273978" author="fiserro" created="Fri, 6 May 2016 12:29:43 +0000"  >&lt;p&gt;Row is not big. One column family and max 10 columns with Integer or double value. After reducing the start and stop row getting results is pretty fast.&lt;/p&gt;

&lt;p&gt;Maybe I&apos;ve forget to tell one important thing. It is a phoenix table and first implementation read data through a phoenix client. It cause the same OutOfOrderScannerNextException error but much more often. The second implementation is the current one with simple scan. Maybe it has something to do with phoenix coprocessors which are still presents on table.&lt;/p&gt;

&lt;p&gt;{TABLE_ATTRIBUTES =&amp;gt; &lt;/p&gt;
{coprocessor$1 =&amp;gt; &apos;|org.apache.phoenix.coprocessor.ScanRegionObserver|805306366|&apos;, coprocessor$2 =&amp;gt; &apos;|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|805306366|&apos;, coprocessor$3 =&amp;gt; &apos;|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|805306366|&apos;, coprocessor$4 =&amp;gt; &apos;|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|805306366|&apos;, coprocessor$5 =&amp;gt; &apos;|org.apache.phoenix.hbase.index.Indexer|805306366|index.builder=org.apache.phoenix.index.PhoenixIndexBuilder,org.apache.hadoop.hbase.index.codec.class=org.apache.phoenix.index.PhoenixIndexCodec&apos;, coprocessor$6 =&amp;gt; &apos;|org.apache.hadoop.hbase.regionserver.LocalIndexSplitter|805306366|&apos;}</comment>
                            <comment id="15274083" author="anoop.hbase" created="Fri, 6 May 2016 14:14:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;After reducing the start and stop row getting results is pretty fast.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Means when u have less rows to scan, there is no issue..  So it is because of time outs only.  Another comment u said abt increasing the scan caching.  That is not correct in this respect.  U should reduce it.. Every RPC will try to fetch more rows.. And reverse scan is not same performing as the normal forward scan. It will take more time and when u have more rows to be scanned in one RPC, it is sure that the time out happening at client side and it gives a retry.  When retry comes, at server, already the old scan request moved the row position and so we throw seqNo out of sync exception. Either you have to increase the timeout or decrease the #rows to scan in one RPC..  You dont have to reduce the range. You have to reduce the scan caching value,&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 6 May 2016 05:12:25 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            32 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2x3l3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>