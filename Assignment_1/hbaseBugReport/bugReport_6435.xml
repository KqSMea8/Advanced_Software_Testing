<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:36:44 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6435/HBASE-6435.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6435] Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6435</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;HBase writes a Write-Ahead-Log to revover from hardware failure. This log is written on hdfs.&lt;br/&gt;
Through ZooKeeper, HBase gets informed usually in 30s that it should start the recovery process. &lt;br/&gt;
This means reading the Write-Ahead-Log to replay the edits on the other servers.&lt;/p&gt;

&lt;p&gt;In standards deployments, HBase process (regionserver) are deployed on the same box as the datanodes.&lt;/p&gt;

&lt;p&gt;It means that when the box stops, we&apos;ve actually lost one of the edits, as we lost both the regionserver and the datanode.&lt;/p&gt;

&lt;p&gt;As HDFS marks a node as dead after ~10 minutes, it appears as available when we try to read the blocks to recover. As such, we are delaying the recovery process by 60 seconds as the read will usually fail with a socket timeout. If the file is still opened for writing, it adds an extra 20s + a risk of losing edits if we connect with ipc to the dead DN.&lt;/p&gt;


&lt;p&gt;Possible solutions are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;shorter dead datanodes detection by the NN. Requires a NN code change.&lt;/li&gt;
	&lt;li&gt;better dead datanodes management in DFSClient. Requires a DFS code change.&lt;/li&gt;
	&lt;li&gt;NN customisation to write the WAL files on another DN instead of the local one.&lt;/li&gt;
	&lt;li&gt;reordering the blocks returned by the NN on the client side to put the blocks on the same DN as the dead RS at the end of the priority queue. Requires a DFS code change or a kind of workaround.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The solution retained is the last one. Compared to what was discussed on the mailing list, the proposed patch will not modify HDFS source code but adds a proxy. This for two reasons:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Some HDFS functions managing block orders are static (MD5MD5CRC32FileChecksum). Implementing the hook in the DFSClient would require to implement partially the fix, change the DFS interface to make this function non static, or put the hook static. None of these solution is very clean.&lt;/li&gt;
	&lt;li&gt;Adding a proxy allows to put all the code in HBase, simplifying dependency management.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Nevertheless, it would be better to have this in HDFS. But this solution allows to target the last version only, and this could allow minimal interface changes such as non static methods.&lt;/p&gt;

&lt;p&gt;Moreover, writing the blocks to the non local DN would be an even better solution long term.&lt;/p&gt;



</description>
                <environment></environment>
        <key id="12599648">HBASE-6435</key>
            <summary>Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nkeywal">Nicolas Liochon</assignee>
                                    <reporter username="nkeywal">Nicolas Liochon</reporter>
                        <labels>
                    </labels>
                <created>Fri, 20 Jul 2012 17:07:14 +0000</created>
                <updated>Tue, 15 Oct 2013 04:46:44 +0000</updated>
                            <resolved>Tue, 21 Aug 2012 10:59:49 +0000</resolved>
                                    <version>0.95.2</version>
                                    <fixVersion>0.95.0</fixVersion>
                                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>15</watches>
                                                                <comments>
                            <comment id="13419414" author="nkeywal" created="Fri, 20 Jul 2012 18:26:12 +0000"  >&lt;p&gt;The patch is not finished. Actually, it contains for code for the hdfs hook and the related test, but not the code for defining the location order from the file name. But as it is different from what we initially discussed, I post it here in case someone sees something I missed.&lt;/p&gt;

&lt;p&gt;It does not mean it should not be fixed in hdfs as well, just that this is likely to be much simpler than patching the 1.0 branch...&lt;/p&gt;</comment>
                            <comment id="13419446" author="tlipcon" created="Fri, 20 Jul 2012 19:02:30 +0000"  >&lt;p&gt;I&apos;m -1 on this kind of hack going into HBase before we add the feature to HDFS. I agree that adding to HDFS proper means we have to wait for a release, but this kind of code is likely to be really fragile. Also, without HBase driving requirements of HDFS, it will never evolve to natively have these kind of features, and HBase will devolve into a mess of reflection hacks to change around the HDFS internals.&lt;/p&gt;</comment>
                            <comment id="13419551" author="stack" created="Fri, 20 Jul 2012 20:50:04 +0000"  >&lt;p&gt;Yeah, we should do both (I&apos;d think that whats added to HDFS is more general than just this workaround scheme where local gets moved to the end of the list; i.e. we add being able to intercept the order returned by the NN and let a client-side policy alter it based on &quot;local knowledge&quot; if wanted.... Could add other customizations like being able to set timeout per DFSInput/OutputStream as you&apos;ve suggested up on dev list N).  Would be sweet if the &apos;hack&apos; were available meantime while we wait on an hdfs release.&lt;/p&gt;

&lt;p&gt;Looking at patch, looks like inventive hackery; good on you.&lt;/p&gt;

&lt;p&gt;Do we have to do this in both master and regionserver?  Can&apos;t do it in HFileSystem constructor assuming it takes a Conf (or that&apos;d be too late?)&lt;/p&gt;

&lt;p&gt;+      HFileSystem.addLocationOrderHack(conf);&lt;/p&gt;

&lt;p&gt;Rather than have it called a reorderProxy, call it an HBaseDFSClient?  Might want to add more customizations while waiting on HDFS fix to arrive.&lt;/p&gt;</comment>
                            <comment id="13419581" author="nkeywal" created="Fri, 20 Jul 2012 21:20:09 +0000"  >&lt;p&gt;My thinking was it could make it on a hdfs release that accepts changing public interfaces. I fully agree with you Todd, we need to do our homeworks and push hdfs to ensure that what we need is understood and makes it to a release. On the other hand, if I look at how it worked for much simpler stuff like JUnit and surefire, our changes are in theie trunk for a few months and we&apos;re still waiting. These things take time. But I will do my homeworks on hdfs, I promise (I may need your help actually). The Jira will be created next week and if I have enough feedback I will propose a patch.&lt;/p&gt;

&lt;p&gt;I was also wondering if proposing natively to have interceptors would not be interesting for hdfs. It was available a long time in an orb called orbix and was great to use. But they would need to be per conf, so cannot be available with static stuff.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Do we have to do this in both master and regionserver? Can&apos;t do it in HFileSystem constructor assuming it takes a Conf (or that&apos;d be too late?)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It can be put pretty late, basically before we start a recovery process. But we don&apos;t want it client side, so I will check this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Rather than have it called a reorderProxy, call it an HBaseDFSClient? Might want to add more customizations while waiting on HDFS fix to arrive.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve intercepted a lower level call: I&apos;m between the DFSClient and the namenode. This because the DFSClient does more than just transferring calls: it contains some logic. Hence going in front of the namenode. But yes, I could make it more generic.&lt;/p&gt;</comment>
                            <comment id="13419588" author="tlipcon" created="Fri, 20 Jul 2012 21:32:34 +0000"  >&lt;p&gt;I think there&apos;s a good motivation to add these kind of APIs generally to DFSInputStream. In particular, I think something like the following:&lt;/p&gt;

&lt;p&gt;public List&amp;lt;Replica&amp;gt; getAvailableReplica(long pos); // return the list of available replicas at given file offset, in priority order&lt;br/&gt;
public void prioritizeReplica(Replica r); // move given replica to front of list&lt;br/&gt;
public void blacklistReplica(Replica r); // move replica to back of list&lt;br/&gt;
(or something of this sort)&lt;/p&gt;

&lt;p&gt;The Replica API would then expose the datanode IDs (and after &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3672&quot; title=&quot;Expose disk-location information for blocks to enable better scheduling&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3672&quot;&gt;&lt;del&gt;HDFS-3672&lt;/del&gt;&lt;/a&gt;, the disk ID).&lt;br/&gt;
So, in HBase we could simply open the file, enumerate the replicas, deprioritize the one on the suspected node, and move on with the normal code paths.&lt;/p&gt;</comment>
                            <comment id="13419608" author="nkeywal" created="Fri, 20 Jul 2012 22:02:20 +0000"  >&lt;p&gt;I understand that you don&apos;t want to expose the internal nor something like the DatanodeInfo. The same type of API would be useful for the outputstream, putting priorities on nodes (and so reusing some knowledge for the dead nodes, or, for the wal, remove the local writes). It simple and efficient.&lt;/p&gt;

&lt;p&gt;With the current DFSClient implementation, a callback would ease cases like opening a file already opened for writing, or when a node list is cleared when they all failed. But may be it can be changed as well.&lt;/p&gt;
</comment>
                            <comment id="13419623" author="tlipcon" created="Fri, 20 Jul 2012 22:10:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;With the current DFSClient implementation, a callback would ease cases like opening a file already opened for writing, or when a node list is cleared when they all failed. But may be it can be changed as well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you explain further what you mean here? What would you use these callbacks for?&lt;/p&gt;</comment>
                            <comment id="13419646" author="nkeywal" created="Fri, 20 Jul 2012 22:52:10 +0000"  >&lt;p&gt;If I can to keep the existing interface&lt;/p&gt;


&lt;p&gt;Today, when you open a file, there is a call to a datanode if the file is also opened for writing somewhere. In HBase, we want the priorities to be taken into account during this opening, as we have a guess that one of these datanode may be dead.&lt;/p&gt;

&lt;p&gt;So either I register a callback that the DFSClient will call before using its list, either I change the &apos;open&apos; interface to add the possibility to provide the list of replicas. Same thing for chooseDataNode called from blockSeekTo: even if we have a list at the beginning, this list is recreated during a read as a part of the retry process (in case the NN discovered new replicas on new datanodes).&lt;/p&gt;

&lt;p&gt;if we put a callback like&lt;/p&gt;

&lt;p&gt;We would offer this service.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;class  ReplicaSet {
  public List&amp;lt;Replica&amp;gt; getAvailableReplica(long pos); // return the list of available replicas at given file offset, in priority order
  public void prioritizeReplica(Replica r); // move given replica to front of list
  public void blacklistReplica(Replica r); // move replica to back of list
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;The client would need to implement this interface:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;// Implement this interface and provide it to the DFSClient during its construction to manage the replica ordering
interface OrganizeReplicaSet{
 void organize(String fileName, ReplicaSet rs); 
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the DFSClient code would become:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;LocatedBlocks callGetBlockLocations(ClientProtocol namenode,
      String src, long start, long length) throws IOException {
    try {
        LocatedBlocks lbs = namenode.getBlockLocations(src, start, length);
        if (organizeReplicaSet != null){
            ReplicaSet rs = LocatedBlocks.getAsReplicaSet()
            try {
                organizeReplicaSet.organize(src, rs);
            }catch (Throwable t){
                throw new IOException(&quot;ClientBlockReordorer failed. class=&quot;+reorderer.getClass(), t);
            }
            return new LocatedBlocks(rs);
        } else
          return lbs;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is called from the DFSInputStream constructor in openInfo today.&lt;/p&gt;

&lt;p&gt;In real life I would try to use the class ReplicaSet as an interface on the internal LocatedBlock(s) to limit the number of objects created. The callback could also be given as a parameter to the DFSInputStream constructor if a there is a specific rule to apply...&lt;/p&gt;</comment>
                            <comment id="13419871" author="stack" created="Sat, 21 Jul 2012 16:30:31 +0000"  >&lt;p&gt;@Todd Given suggested Interface, how we map from an hbase session expiration to a Replica?  What if the DN died but RS didn&apos;t?  Won&apos;t the fact that DFSClient under the wraps is banging its head timingout against a dead DN &amp;#8211; once per DFSInputStream &amp;#8211; be hidden from the RS since its being handled down in DFSClient? Don&apos;t we need more knowledge on DFSClient workings than suggested API exposes if we are to avoid dead DNs?  If we do figure we have a bad DN, do we then per open DFSInputStream iterate updating priorities?&lt;/p&gt;</comment>
                            <comment id="13420346" author="tlipcon" created="Sun, 22 Jul 2012 21:51:01 +0000"  >&lt;p&gt;Good points. We should probably move this discussion over to an HDFS JIRA. Having a global DFSClient-wide ability to mark nodes un-preferred is probably advantageous.&lt;/p&gt;</comment>
                            <comment id="13423280" author="nkeywal" created="Thu, 26 Jul 2012 17:45:06 +0000"  >&lt;p&gt;v2. May need some clean up on logs + a check to unactivate it for hadoop 2 for example.&lt;/p&gt;</comment>
                            <comment id="13423282" author="nkeywal" created="Thu, 26 Jul 2012 17:46:23 +0000"  >&lt;p&gt;+ I need to test it on a real cluster (emulating locations on a mini cluster can be dangerous...)&lt;/p&gt;</comment>
                            <comment id="13425878" author="nkeywal" created="Tue, 31 Jul 2012 16:11:19 +0000"  >&lt;p&gt;Tested on a real cluster by adding validation code on a region server, went ok. I don&apos;t have a real idea on how to activate it just for some hadoop versions, so I will do a last clean-up on the logs and propose a final version.&lt;/p&gt;</comment>
                            <comment id="13425959" author="nkeywal" created="Tue, 31 Jul 2012 17:51:50 +0000"  >&lt;p&gt;Ok for review...&lt;/p&gt;</comment>
                            <comment id="13425980" author="zhihyu@ebaysf.com" created="Tue, 31 Jul 2012 18:15:26 +0000"  >&lt;p&gt;Just started to look at the patch.&lt;br/&gt;
It doesn&apos;t compile against hadoop 2.0:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:compile (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-compile) on project hbase-server: Compilation failure: Compilation failure:
[ERROR] /Users/zhihyu/trunk-hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java:[214,12] namenode is not &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; in org.apache.hadoop.hdfs.DFSClient; cannot be accessed from outside &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;
[ERROR] 
[ERROR] /Users/zhihyu/trunk-hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java:[221,52] namenode is not &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; in org.apache.hadoop.hdfs.DFSClient; cannot be accessed from outside &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;
[ERROR] 
[ERROR] /Users/zhihyu/trunk-hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java:[289,81] cannot find symbol
[ERROR] symbol  : method getHost()
[ERROR] location: class org.apache.hadoop.hdfs.protocol.DatanodeInfo
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Can we give the following a more meaningful name ?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!conf.getBoolean(&lt;span class=&quot;code-quote&quot;&gt;&quot;hbase.hdfs.jira6435&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)){  &lt;span class=&quot;code-comment&quot;&gt;// activated by &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Comment from Todd would be appreciated.&lt;/p&gt;</comment>
                            <comment id="13425988" author="nkeywal" created="Tue, 31 Jul 2012 18:21:02 +0000"  >&lt;p&gt;I will have a look at the hadoop2 stuff.&lt;/p&gt;

&lt;p&gt;for &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Can we give the following a more meaningful name ?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Do you have an idea?&lt;/p&gt;</comment>
                            <comment id="13425995" author="zhihyu@ebaysf.com" created="Tue, 31 Jul 2012 18:28:04 +0000"  >&lt;p&gt;How about &apos;hbase.filesystem.reorder.blocks&apos; ?&lt;/p&gt;

&lt;p&gt;BTW replacing &apos;Hack&apos; with some form of &apos;Intercept&apos; would be better IMHO.&lt;/p&gt;</comment>
                            <comment id="13426015" author="nkeywal" created="Tue, 31 Jul 2012 18:52:06 +0000"  >&lt;p&gt;Ok.&lt;br/&gt;
I wanted to make clear it was a temporary workaround.&lt;/p&gt;</comment>
                            <comment id="13426102" author="nkeywal" created="Tue, 31 Jul 2012 20:46:09 +0000"  >&lt;p&gt;v8 works ok with hadoop 1 &amp;amp; hadoop 2 and other Ted&apos;s comments. I tried the v3 profile, but got errors in the pom.xml.&lt;/p&gt;</comment>
                            <comment id="13426151" author="zhihyu@ebaysf.com" created="Tue, 31 Jul 2012 21:31:17 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; ClientProtocol createReordoringProxy(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; ClientProtocol cp,
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Usually spelling would be nit. But this spelling mistake was in method name &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; ServerName getServerNameFromHLogDirectoryName(Configuration conf, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; path) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The above line is too long.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+              LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Moved the location &quot;&lt;/span&gt;+toLast.getHostName()+&lt;span class=&quot;code-quote&quot;&gt;&quot; to the last place.&quot;&lt;/span&gt; +
+                  &lt;span class=&quot;code-quote&quot;&gt;&quot; locations size was &quot;&lt;/span&gt;+dnis.length);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I think the above log may appear many times.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+            LOG.fatal(&lt;span class=&quot;code-quote&quot;&gt;&quot;AAAA REORDER&quot;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The above can be made a debug log.&lt;/p&gt;</comment>
                            <comment id="13426158" author="hadoopqa" created="Tue, 31 Jul 2012 21:46:40 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538610/6435.v8.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538610/6435.v8.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 8 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 6 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.fs.TestBlockReorder&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2464//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13426225" author="zhihyu@ebaysf.com" created="Wed, 1 Aug 2012 00:00:32 +0000"  >&lt;p&gt;For the test failure:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.junit.ComparisonFailure: expected:&amp;lt;[localhost]&amp;gt; but was:&amp;lt;[host2]&amp;gt;
	at org.junit.Assert.assertEquals(Assert.java:125)
	at org.junit.Assert.assertEquals(Assert.java:147)
	at org.apache.hadoop.hbase.fs.TestBlockReorder.testFromDFS(TestBlockReorder.java:320)
	at org.apache.hadoop.hbase.fs.TestBlockReorder.testHBaseCluster(TestBlockReorder.java:271)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;testFromDFS() should have utilized the done flag for the while loop below:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; y = 0; y &amp;lt; l.getLocatedBlocks().size() &amp;amp;&amp;amp; done; y++) {
+          done = (l.get(y).getLocations().length == 3);
+        }
+      } &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (l.get(0).getLocations().length != 3);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When l.getLocatedBlocks().size() is greater than 1, the above loop may exit prematurely.&lt;/p&gt;</comment>
                            <comment id="13426371" author="nkeywal" created="Wed, 1 Aug 2012 08:11:08 +0000"  >&lt;p&gt;Thanks for the review and the test failure analysis, Ted. v9 takes the comments into account.&lt;/p&gt;</comment>
                            <comment id="13426595" author="zhihyu@ebaysf.com" created="Wed, 1 Aug 2012 13:10:54 +0000"  >&lt;p&gt;From PreCommit build #2470, look like compilation against Hadoop 2.0 failed. &lt;/p&gt;</comment>
                            <comment id="13426656" author="nkeywal" created="Wed, 1 Aug 2012 14:17:06 +0000"  >&lt;p&gt;Is there a way to have more info on the failure?&lt;br/&gt;
Locally&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;mvn test -Dhadoop.profile=2.0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;says&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Tests in error:
  testSimpleCase(org.apache.hadoop.hbase.mapreduce.TestImportExport)
  testWithDeletes(org.apache.hadoop.hbase.mapreduce.TestImportExport)

Tests run: 719, Failures: 0, Errors: 2, Skipped: 2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and .TestBlockReorder is ok (executed 5 times)&lt;/p&gt;</comment>
                            <comment id="13426682" author="zhihyu@ebaysf.com" created="Wed, 1 Aug 2012 14:58:58 +0000"  >&lt;p&gt;The compilation in PreCommit build was aborted.&lt;br/&gt;
I couldn&apos;t reproduce the issue.&lt;/p&gt;

&lt;p&gt;Suggest re-attaching patch v9.&lt;/p&gt;</comment>
                            <comment id="13426683" author="nkeywal" created="Wed, 1 Aug 2012 15:02:40 +0000"  >&lt;p&gt;done &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13426714" author="hadoopqa" created="Wed, 1 Aug 2012 16:03:06 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538789/6435.v9.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538789/6435.v9.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 8 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 6 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.client.TestAdmin&lt;br/&gt;
                  org.apache.hadoop.hbase.fs.TestBlockReorder&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2472//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13426738" author="nkeywal" created="Wed, 1 Aug 2012 16:31:16 +0000"  >&lt;p&gt;I was expecting the name to be locahost, but it&apos;s not the case on hadoop-qa env:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;/asf011.sp2.ygridcore.net,43631,1343836299404/asf011.sp2.ygridcore.net%2C43631%2C1343836299404.1343836318993 is an HLog file, so reordering blocks, last hostname will be:asf011.sp2.ygridcore.net
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So the trick used to check location ordering on a mini cluster does not work. I will find another way...&lt;/p&gt;</comment>
                            <comment id="13427302" author="zhihyu@ebaysf.com" created="Thu, 2 Aug 2012 13:20:47 +0000"  >&lt;p&gt;From &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2479/console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2479/console&lt;/a&gt;, it looks like compilation didn&apos;t pass for hadoop 2.0&lt;/p&gt;</comment>
                            <comment id="13427348" author="hadoopqa" created="Thu, 2 Aug 2012 14:33:49 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538906/6435.v10.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538906/6435.v10.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 8 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 6 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.fs.TestBlockReorder&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2481//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13427356" author="nkeywal" created="Thu, 2 Aug 2012 14:45:14 +0000"  >&lt;p&gt;ok, 127.0.0.1 is localhost, but it&apos;s not the name used by the minicluster... Will try again to trick the test...&lt;/p&gt;</comment>
                            <comment id="13427640" author="nkeywal" created="Thu, 2 Aug 2012 21:36:08 +0000"  >&lt;p&gt;I had to change the test to make it more hadoop-qa friendly. In one of my numerous attempts, I added the possibility to start a miniCluster with a specific HMaster or HRegionServer class. I finally didn&apos;t use it here, but I kept it in the patch as it may be useful later...&lt;/p&gt;</comment>
                            <comment id="13427752" author="zhihyu@ebaysf.com" created="Fri, 3 Aug 2012 00:07:53 +0000"  >&lt;p&gt;PreCommit build #2490 got aborted:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/trunk/dev-support/test-patch.sh: line 353: 14017 Aborted                 $MVN clean test help:active-profiles -X -DskipTests -Dhadoop.profile=2.0 -D${PROJECT_NAME}PatchProcess &amp;gt; $PATCH_DIR/trunk2.0JavacWarnings.txt 2&amp;gt;&amp;amp;1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13427757" author="zhihyu@ebaysf.com" created="Fri, 3 Aug 2012 00:11:27 +0000"  >&lt;p&gt;Patch from N.&lt;/p&gt;</comment>
                            <comment id="13427816" author="hadoopqa" created="Fri, 3 Aug 2012 02:51:09 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538981/6435.v12.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538981/6435.v12.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 11 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.client.TestFromClientSide&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2494//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13427850" author="nkeywal" created="Fri, 3 Aug 2012 05:54:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;org.apache.hadoop.hbase.client.TestFromClientSide&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think it&apos;s unrelated. Let&apos;s retry.&lt;/p&gt;</comment>
                            <comment id="13427867" author="hadoopqa" created="Fri, 3 Aug 2012 06:54:24 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12538999/6435.v12.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12538999/6435.v12.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 11 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 10 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed unit tests in .&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2496//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13428612" author="zhihyu@ebaysf.com" created="Sat, 4 Aug 2012 14:28:23 +0000"  >&lt;p&gt;@N:&lt;br/&gt;
Can you put patch on review board ?&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13428638" author="nkeywal" created="Sat, 4 Aug 2012 17:12:53 +0000"  >&lt;p&gt;Ok. Tried. &quot;Something broke! (Error 500)&quot;, I will retry later.&lt;/p&gt;</comment>
                            <comment id="13428667" author="zhihyu@ebaysf.com" created="Sat, 4 Aug 2012 19:42:45 +0000"  >&lt;p&gt;Same as N&apos;s patch v12.&lt;/p&gt;

&lt;p&gt;I was able to generate review on review board from this patch.&lt;/p&gt;</comment>
                            <comment id="13432715" author="nkeywal" created="Fri, 10 Aug 2012 12:11:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://reviews.apache.org/r/6522/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/6522/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13435102" author="nkeywal" created="Wed, 15 Aug 2012 13:39:09 +0000"  >&lt;p&gt;v13 takes into account the comments from the review board.&lt;/p&gt;</comment>
                            <comment id="13435164" author="hadoopqa" created="Wed, 15 Aug 2012 14:38:50 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12541051/6435.v13.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12541051/6435.v13.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 11 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 9 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster&lt;br/&gt;
                  org.apache.hadoop.hbase.master.TestMasterNoCluster&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2586//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13435392" author="zhihyu@ebaysf.com" created="Wed, 15 Aug 2012 18:41:11 +0000"  >&lt;p&gt;+1 on v13 if TestSplitTransactionOnCluster#testSplitBeforeSettingSplittingInZK passes.&lt;/p&gt;</comment>
                            <comment id="13437131" author="stack" created="Fri, 17 Aug 2012 23:15:21 +0000"  >&lt;p&gt;+1 on commit.  Some notes below that you can address on commit.  Tests look good on cursory glance.  Comprehensive.  Nice hacking N.&lt;/p&gt;

&lt;p&gt;HMaster does an import of this:&lt;/p&gt;

&lt;p&gt;+import org.apache.hadoop.hbase.fs.HFileSystem;&lt;/p&gt;

&lt;p&gt;... but not used.  Fix on commit.&lt;/p&gt;

&lt;p&gt;Not important, but I&apos;d check its DFS before I&apos;d check reorder enabled flag.  Next time.&lt;/p&gt;

&lt;p&gt;Whats this about?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+      modifiersField.setInt(nf, nf.getModifiers() &amp;amp; ~Modifier.FINAL);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some of this patch probably belongs in compatibility layers.  One day the reorder will be in hadoop....  We can address in new issue.&lt;/p&gt;

&lt;p&gt;What does this mean?&lt;/p&gt;

&lt;p&gt;+    // We have a rack to get always the same location order but it does not work.&lt;/p&gt;


</comment>
                            <comment id="13437295" author="nkeywal" created="Sat, 18 Aug 2012 12:17:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;... but not used. Fix on commit.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ok&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;modifiersField.setInt(nf, nf.getModifiers() &amp;amp; ~Modifier.FINAL);&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The field is final, we&apos;re changing this as we&apos;re changing its value.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;+ // We have a rack to get always the same location order but it does not work.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I could remove it on commit... I wanted to use racks to have always the same order, but it does not work; the racks are not taken into account in this case, I don&apos;t know why...&lt;/p&gt;

&lt;p&gt;Thanks for the review Ted and Stack, I will commit it beginning of next week if I don&apos;t have another feedback.&lt;/p&gt;</comment>
                            <comment id="13437419" author="stack" created="Sat, 18 Aug 2012 21:50:47 +0000"  >&lt;p&gt;+1 on commit w/ above two edits.  Needs nice fat release note.  Good on you N.&lt;/p&gt;</comment>
                            <comment id="13437485" author="nkeywal" created="Sun, 19 Aug 2012 08:56:56 +0000"  >&lt;p&gt;release notes done.&lt;/p&gt;</comment>
                            <comment id="13437590" author="stack" created="Sun, 19 Aug 2012 21:51:04 +0000"  >&lt;p&gt;@N Nice note.  You should write a blog on it and your other findings.  You going to commit?&lt;/p&gt;</comment>
                            <comment id="13438551" author="nkeywal" created="Tue, 21 Aug 2012 09:15:44 +0000"  >&lt;p&gt;v14: version I&apos;m going to commit as soon as the local tests (in progress) are ok.&lt;/p&gt;</comment>
                            <comment id="13438563" author="nkeywal" created="Tue, 21 Aug 2012 09:48:29 +0000"  >&lt;p&gt;Ok, local tests said:&lt;br/&gt;
Tests in error:&lt;br/&gt;
  testGetRowVersions(org.apache.hadoop.hbase.TestMultiVersions): Shutting down&lt;br/&gt;
  testScanMultipleVersions(org.apache.hadoop.hbase.TestMultiVersions): org.apache.hadoop.hbase.MasterNotRunningException: Can create a proxy to master, but it is not running&lt;/p&gt;

&lt;p&gt;Not reproduced (tried once).&lt;/p&gt;

&lt;p&gt;Committed revision 1375451.&lt;/p&gt;</comment>
                            <comment id="13438567" author="nkeywal" created="Tue, 21 Aug 2012 09:54:27 +0000"  >&lt;p&gt;+ Committed revision 1375454.&lt;br/&gt;
As I forgot to add the new test in svn initially.&lt;/p&gt;</comment>
                            <comment id="13438575" author="hadoopqa" created="Tue, 21 Aug 2012 10:10:56 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12541734/6435.v14.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12541734/6435.v14.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 11 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    -1 javac.  The applied patch generated 5 javac compiler warnings (more than the trunk&apos;s current 4 warnings).&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 6 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;br/&gt;
                       org.apache.hadoop.hbase.io.encoding.TestUpgradeFromHFileV1ToEncoding&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2638//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13438599" author="hudson" created="Tue, 21 Aug 2012 10:52:48 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3247 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3247/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3247/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes - addendum TestBlockReorder.java (Revision 1375454)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes (Revision 1375451)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/ServerName.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13438605" author="nkeywal" created="Tue, 21 Aug 2012 10:59:30 +0000"  >&lt;p&gt;R&#233;sultats des tests (2 &#233;checs / &#177;0)&lt;br/&gt;
    org.apache.hadoop.hbase.TestMultiVersions.testGetRowVersions&lt;br/&gt;
    org.apache.hadoop.hbase.TestMultiVersions.testScanMultipleVersions&lt;/p&gt;

&lt;p&gt;Hum. It&apos;s the same error as the one I had in my fist local test. But it&apos;s so unrelated, and moreover we had this error in build #3242 as well; so I think it&apos;s ok. Marking as resolved.&lt;/p&gt;</comment>
                            <comment id="13438623" author="hudson" created="Tue, 21 Aug 2012 11:45:04 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #140 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/140/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/140/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes - addendum TestBlockReorder.java (Revision 1375454)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes (Revision 1375451)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/ServerName.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13451887" author="hudson" created="Mon, 10 Sep 2012 10:57:17 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3320 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3320/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3320/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6746&quot; title=&quot;Impacts of HBASE-6435 vs. HDFS 2.0 trunk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6746&quot;&gt;&lt;del&gt;HBASE-6746&lt;/del&gt;&lt;/a&gt; Impacts of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; vs. HDFS 2.0 trunk (Revision 1382723)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13451914" author="hudson" created="Mon, 10 Sep 2012 11:59:34 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #168 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/168/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/168/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6746&quot; title=&quot;Impacts of HBASE-6435 vs. HDFS 2.0 trunk&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6746&quot;&gt;&lt;del&gt;HBASE-6746&lt;/del&gt;&lt;/a&gt; Impacts of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6435&quot; title=&quot;Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6435&quot;&gt;&lt;del&gt;HBASE-6435&lt;/del&gt;&lt;/a&gt; vs. HDFS 2.0 trunk (Revision 1382723)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/fs/HFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13463678" author="nkeywal" created="Wed, 26 Sep 2012 10:11:28 +0000"  >&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3701&quot; title=&quot;HDFS may miss the final block when reading a file opened for writing if one of the datanode is dead&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3701&quot;&gt;&lt;del&gt;HDFS-3701&lt;/del&gt;&lt;/a&gt; (dataloss) is into the branch 1.1 as &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; (helps to minimize data reads errors), I think it implies that we should target 1.1 for 0.96 as the recommended minimal version. If it&apos;s the case, we can remove this fix, as it contains a dependency on hdfs internals. If we keep it, I need to fix the filename analysis and to add &quot;-splitting&quot; on the directories managed. In both cases, it should be done in a separate jiras, but let&apos;s have the discussion here.&lt;/p&gt;</comment>
                            <comment id="13463714" author="yuzhihong@gmail.com" created="Wed, 26 Sep 2012 11:18:43 +0000"  >&lt;p&gt;I think we can poll dev@hbase for minimal hadoop version requirement.&lt;br/&gt;
If 1.1 passes as the minimal version, we should remove this fix.&lt;/p&gt;</comment>
                            <comment id="13463718" author="nkeywal" created="Wed, 26 Sep 2012 11:24:47 +0000"  >&lt;p&gt;I suppose we won&apos;t want to put it as minimum, at least to ease migration. But someone considering the mttr as important would have to migrate to 1.1.&lt;/p&gt;</comment>
                            <comment id="13464100" author="stack" created="Wed, 26 Sep 2012 19:44:48 +0000"  >&lt;p&gt;So not a requirement but a strong suggestion?&lt;/p&gt;

&lt;p&gt;Yeah, we should discuss on dev.&lt;/p&gt;</comment>
                            <comment id="13642685" author="nkeywal" created="Fri, 26 Apr 2013 08:38:44 +0000"  >&lt;p&gt;During the tests on the impact of waiting for the end of hdfs recoverLease, it appeared:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;there is a bug, and somes files are not detected.&lt;/li&gt;
	&lt;li&gt;we have a dependency on the machine name (issue if a machine has multiple names).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4754&quot; title=&quot;Add an API in the namenode to mark a datanode as stale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4754&quot;&gt;HDFS-4754&lt;/a&gt; supercedes this, so, to keep things simple and limit the number of possible configuration my plan is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;make sure that &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4754&quot; title=&quot;Add an API in the namenode to mark a datanode as stale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4754&quot;&gt;HDFS-4754&lt;/a&gt;  makes it to a reasonable number of hdfs branches.&lt;/li&gt;
	&lt;li&gt;revert this.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13775358" author="stack" created="Mon, 23 Sep 2013 18:31:20 +0000"  >&lt;p&gt;Marking closed.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12599111">HBASE-6401</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12599839">HDFS-3705</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                                                <inwardlinks description="is required by">
                                        <issuelink>
            <issuekey id="12551766">HBASE-5843</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12539158" name="6435-v12.txt" size="39674" author="zhihyu@ebaysf.com" created="Sat, 4 Aug 2012 19:42:45 +0000"/>
                            <attachment id="12537380" name="6435.unfinished.patch" size="14514" author="nkeywal" created="Fri, 20 Jul 2012 18:22:00 +0000"/>
                            <attachment id="12538906" name="6435.v10.patch" size="35110" author="nkeywal" created="Thu, 2 Aug 2012 13:32:57 +0000"/>
                            <attachment id="12538880" name="6435.v10.patch" size="35110" author="nkeywal" created="Thu, 2 Aug 2012 08:13:04 +0000"/>
                            <attachment id="12538999" name="6435.v12.patch" size="40262" author="nkeywal" created="Fri, 3 Aug 2012 05:53:11 +0000"/>
                            <attachment id="12538981" name="6435.v12.patch" size="40262" author="nkeywal" created="Fri, 3 Aug 2012 00:09:57 +0000"/>
                            <attachment id="12538958" name="6435.v12.patch" size="40262" author="nkeywal" created="Thu, 2 Aug 2012 21:33:44 +0000"/>
                            <attachment id="12541051" name="6435.v13.patch" size="40345" author="nkeywal" created="Wed, 15 Aug 2012 13:38:06 +0000"/>
                            <attachment id="12541734" name="6435.v14.patch" size="39473" author="nkeywal" created="Tue, 21 Aug 2012 09:14:41 +0000"/>
                            <attachment id="12538032" name="6435.v2.patch" size="32197" author="nkeywal" created="Thu, 26 Jul 2012 17:42:48 +0000"/>
                            <attachment id="12538580" name="6435.v7.patch" size="33689" author="nkeywal" created="Tue, 31 Jul 2012 17:49:55 +0000"/>
                            <attachment id="12538610" name="6435.v8.patch" size="35295" author="nkeywal" created="Tue, 31 Jul 2012 20:44:36 +0000"/>
                            <attachment id="12538789" name="6435.v9.patch" size="34882" author="nkeywal" created="Wed, 1 Aug 2012 15:02:18 +0000"/>
                            <attachment id="12538658" name="6435.v9.patch" size="34882" author="nkeywal" created="Wed, 1 Aug 2012 08:09:03 +0000"/>
                            <attachment id="12538919" name="6535.v11.patch" size="36461" author="nkeywal" created="Thu, 2 Aug 2012 16:26:34 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>15.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 20 Jul 2012 19:02:30 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>256611</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 12 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0huzj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>102282</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>This JIRA adds a hook in the HDFS client to reorder the replica locations for HLog files. The default ordering in HDFS is rack aware + random. When reading a HLog file, we prefer not to use the replica on the same server as the region server that wrote the HLog: this server is likely to be not available, and this will delay the HBase recovery by one minute. This occurs because the recovery starts sooner in HBase than in HDFS: 3 minutes by default in HBase vs. 10:30 minutes in HDFS. This will be changed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;strike&gt;HDFS-3703&lt;/strike&gt;&lt;/a&gt;. Moreover, when a HDFS file is already opened for writing, a read triggers another call to get the file size, leading to another timeout (see &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3704&quot; title=&quot;In the DFSClient, Add the node to the dead list when the ipc.Client calls fails&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3704&quot;&gt;HDFS-3704&lt;/a&gt;), but as well a wrong file size value (see &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3701&quot; title=&quot;HDFS may miss the final block when reading a file opened for writing if one of the datanode is dead&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3701&quot;&gt;&lt;strike&gt;HDFS-3701&lt;/strike&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6401&quot; title=&quot;HBase may lose edits after a crash if used with HDFS 1.0.3 or older&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6401&quot;&gt;&lt;strike&gt;HBASE-6401&lt;/strike&gt;&lt;/a&gt;). Technically:&lt;br/&gt;
- this hook won&amp;#39;t be useful anymore when &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3702&quot; title=&quot;Add an option for NOT writing the blocks locally if there is a datanode on the same box as the client&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3702&quot;&gt;&lt;strike&gt;HDFS-3702&lt;/strike&gt;&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3705&quot; title=&quot;Add the possibility to mark a node as &amp;#39;low priority&amp;#39; for read in the DFSClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3705&quot;&gt;&lt;strike&gt;HDFS-3705&lt;/strike&gt;&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3706&quot; title=&quot;Add the possibility to mark a node as &amp;#39;low priority&amp;#39; for writes in the DFSClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3706&quot;&gt;HDFS-3706&lt;/a&gt; is available and used in HBase.&lt;br/&gt;
- the hook intercepts the calls to the nanemode and reorder the locations it returned, extracting the region server name from the HLog file. This server is put at the end of the list, ensuring it will be tried only if all the others fail.&lt;br/&gt;
- It has been tested with HDFS 1.0.3. of HDFS 2.0 apha.&lt;br/&gt;
- It can be deactivated (at master &amp;amp; region server start-up) by setting &amp;quot;hbase.filesystem.reorder.blocks&amp;quot; to false in the HBase configuration.&lt;br/&gt;
</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_12310230" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Tags</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.96notable</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>