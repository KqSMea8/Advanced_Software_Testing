<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:00:14 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2244/HBASE-2244.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2244] META gets inconsistent in a number of crash scenarios</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2244</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;(Forking this issue off from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2235&quot; title=&quot;Mechanism that would not have -ROOT- and .META. on same server caused failed assign of .META.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2235&quot;&gt;&lt;del&gt;HBASE-2235&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;During load testing, in a number of failure scenarios (unexpected region server deaths) etc., we notice that META can get inconsistent. This primarily happens for regions which are in the process of being split. Manually running add_table.rb seems to fix the tables meta data just fine. &lt;/p&gt;

&lt;p&gt;But it would be good to do automatic cleansing (as part of META scanners work) and/or avoid these inconsistent states altogether.&lt;/p&gt;

&lt;p&gt;For example, for a particular startkey, I see all these entries:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
test1,1204765,1266569946560 column=info:regioninfo, timestamp=1266581302018, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1204765,1266569946560&apos;, STARTKEY =&amp;gt; &apos;1204765&apos;, ENDKEY =&amp;gt; &apos;1441091&apos;, ENCODED =&amp;gt; 18
                             19368969, OFFLINE =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, SPLIT =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt;
                              [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;
                             , BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,1204765,1266569946560 column=info:server, timestamp=1266570029133, value=10.129.68.212:60020
 test1,1204765,1266569946560 column=info:serverstartcode, timestamp=1266570029133, value=1266562597546
 test1,1204765,1266569946560 column=info:splitB, timestamp=1266581302018, value=\x00\x071441091\x00\x00\x00\x0
                             1\x26\xE6\x1F\xDF\x27\x1Btest1,1290703,1266581233447\x00\x071290703\x00\x00\x00\x
                             05\x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x
                             00\x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00
                             \x00\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSI
                             ON\x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TT
                             L\x00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00
                             \x00\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04t
                             rueh\x0FQ\xCF
 test1,1204765,1266581233447 column=info:regioninfo, timestamp=1266609172177, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1204765,1266581233447&apos;, STARTKEY =&amp;gt; &apos;1204765&apos;, ENDKEY =&amp;gt; &apos;1290703&apos;, ENCODED =&amp;gt; 13
                             73493090, OFFLINE =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, SPLIT =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt;
                              [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;
                             , BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,1204765,1266581233447 column=info:server, timestamp=1266604768670, value=10.129.68.213:60020
 test1,1204765,1266581233447 column=info:serverstartcode, timestamp=1266604768670, value=1266562597511
 test1,1204765,1266581233447 column=info:splitA, timestamp=1266609172177, value=\x00\x071226169\x00\x00\x00\x0
                             1\x26\xE7\xCA,\x7D\x1Btest1,1204765,1266609171581\x00\x071204765\x00\x00\x00\x05\
                             x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\
                             x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x0
                             0\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\
                             x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x
                             00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x0
                             0\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true
                             \xB9\xBD\xFEO
 test1,1204765,1266581233447 column=info:splitB, timestamp=1266609172177, value=\x00\x071290703\x00\x00\x00\x0
                             1\x26\xE7\xCA,\x7D\x1Btest1,1226169,1266609171581\x00\x071226169\x00\x00\x00\x05\
                             x05test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\
                             x00\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x0
                             0\x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\
                             x00\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x
                             00\x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x0
                             0\x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true
                             \xE1\xDF\xF8p
 test1,1204765,1266609171581 column=info:regioninfo, timestamp=1266609172212, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1204765,1266609171581&apos;, STARTKEY =&amp;gt; &apos;1204765&apos;, ENDKEY =&amp;gt; &apos;1226169&apos;, ENCODED =&amp;gt; 21
                             34878372, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =
                             &amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMOR
                             Y =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</description>
                <environment></environment>
        <key id="12456875">HBASE-2244</key>
            <summary>META gets inconsistent in a number of crash scenarios</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="kannanm">Kannan Muthukkaruppan</reporter>
                        <labels>
                    </labels>
                <created>Sat, 20 Feb 2010 02:08:10 +0000</created>
                <updated>Fri, 12 Oct 2012 06:15:00 +0000</updated>
                            <resolved>Thu, 4 Mar 2010 18:24:04 +0000</resolved>
                                                    <fixVersion>0.20.4</fixVersion>
                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                <comments>
                            <comment id="12836100" author="kannanm" created="Sat, 20 Feb 2010 02:09:18 +0000"  >&lt;p&gt;Stack wrote &amp;lt;&amp;lt;&amp;lt;The report of split is not atomic; we are sending 3 separate puts. We don&apos;t have means of making a cross-row transaction out of this operation&amp;gt;&amp;gt;&amp;gt;.&lt;/p&gt;

&lt;p&gt;Would keeping all region info for a table as columns under a single row key in .META. be such a crazy idea? With that you can do atomic mutations to a given table&apos;s region info. The cons would be that entire &quot;rowkey&quot; would be hosted on a single server... but for most tables that&apos;d probably already be the case unless it has an exorbitant number of regions.&lt;/p&gt;

&lt;p&gt;To which Stack responded with: &amp;lt;&amp;lt;&amp;lt;&amp;lt; For sure, lets do something in 0.20. I think metascanner when it runs can check an offlined region. If its not followed by two daughters - it has the necessary info as columns splitA and splitB - it can add them. Let me take a closer look. Will report back.&lt;/p&gt;

&lt;p&gt;Having all in the one row is a bit radical. Gets and puts would each take out a row lock. I think this might slow down all .META. lookups. We also have a mechanism for getting the row closest to the asked for one. Its used for figuring out which region a row sits in. This would have to be recast if all was in the one row.&amp;gt;&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="12836280" author="stack" created="Sat, 20 Feb 2010 23:39:25 +0000"  >&lt;p&gt;I&apos;m on it.  Patch soon.&lt;/p&gt;</comment>
                            <comment id="12836322" author="stack" created="Sun, 21 Feb 2010 04:59:06 +0000"  >&lt;p&gt;Within the scope of this issue we should a couple of things for the 0.20 branch specifically.&lt;/p&gt;

&lt;p&gt;First, add fixup to the metascanner for case where offlined parent but no daughters present because HRS crashed and didn&apos;t add daughters... or HRS carrying .META. crashed and we only recovered the parent offlining edit, but not the daughter additions.   This inconsistency is clean so easy to recognize.  Also, data needed to do the repair is in the offlined parent as columns splitA and splitB.&lt;/p&gt;

&lt;p&gt;Here are a few notes.&lt;/p&gt;

&lt;p&gt;+ On split, the HRS does three updates: 1. offline parent and add splitA and splitB columns that hold the HRegionInfo of daughter split regions, 2. add daughter A, and 3., add daughter B.  The updates are not done atomically.  Before we send the messages, the HRS has offlined (closed) the parent and created two new daughters.  The parent is already unavailable.&lt;br/&gt;
++ Reading code, there are issues to address in here.  If we crash after parent close, thats ok.  The parent will be assigned to a new HRS.  But subsequently, as the split goes forward, we do an open of the new daughter regions BEFORE we add them to the .META..  This seems like it could be avoided (speeding the split); only open once assigned in new location (Moving the location of where we do the split work should be all that is needed).  Also, if already a daughter region of same name in the FS, we&apos;ll fail the split rather than overwrite as it seems we should do (only reason for a pre-existing daughter is a split failed mid-way).  I can add a check of .META.  If daughter not there, its for sure a failed split.  Let me see if I can improve stuff in here in general for 0.20 as part of this patch.  I need to study  some more.&lt;br/&gt;
+ The HRS, after making updates in the .META., then sends a message to the master telling it about the split.  The master adds the new daughters to his assignment list and they are assigned out on next report-in by a cluster-member.  If this message is missed, the daughters are assigned the next time the metascanner runs.&lt;/p&gt;

&lt;p&gt;In the .META. listing posted above, there are some interesting issues.  We still have a reference to a daughter, splitB, in the first offlined (row) region, yet the next row is a daughter that has been offlined itself.  There may be a race in here if we&apos;re splitting fast.  Let me check it out and see if a fix.&lt;/p&gt;

&lt;p&gt;The other inconsistency is that there seems to be a row missing of the end, the splitB from test1,1204765,1266581233447.  Is that possible?&lt;/p&gt;</comment>
                            <comment id="12836390" author="kannanm" created="Sun, 21 Feb 2010 16:17:17 +0000"  >&lt;p&gt;Stack: The splitB from test1,1204765,1266581233447, namely  &quot;test1,1226169,1266609171581&quot; was probably there. Not entirely sure. I should have cut-pasted more from the scan of .META. &lt;/p&gt;

&lt;p&gt;I just wanted to add that during this state, the client was receiving errors of the form:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.129.68.212:60020 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region\
 test1,1204765,1266581233447, row &apos;1232762&apos;, but failed after 10 attempts. 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: key &quot;1232762&quot; would fall in the &quot;splitB&quot; range for the original test1,1204765,1266581233447.&lt;/p&gt;

&lt;p&gt;(BTW, are the JIRA time zones always in GMT? Is there a way to configure it to local timezones?)&lt;/p&gt;

&lt;p&gt;When I look at the same cluster&apos;s .META. now, the state for the related META regions is as follows:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;

 test1,1204765,1266616432091 column=info:regioninfo, timestamp=1266616432230, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1204765,1266616432091&apos;, STARTKEY =&amp;gt; &apos;1204765&apos;, ENDKEY =&amp;gt; &apos;1215466&apos;, ENCODED =&amp;gt; 41
                             6976676, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt;
                              &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY
                              =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,1204765,1266616432091 column=info:server, timestamp=1266616433943, value=10.129.68.213:60020
 test1,1204765,1266616432091 column=info:serverstartcode, timestamp=1266616433943, value=1266562597511
 test1,1215466,1266616432091 column=info:regioninfo, timestamp=1266616432232, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1215466,1266616432091&apos;, STARTKEY =&amp;gt; &apos;1215466&apos;, ENDKEY =&amp;gt; &apos;1226169&apos;, ENCODED =&amp;gt; 40
                             3995950, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt;
                              &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY
                              =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,1215466,1266616432091 column=info:server, timestamp=1266616434963, value=10.129.68.213:60020
 test1,1215466,1266616432091 column=info:serverstartcode, timestamp=1266616434963, value=1266562597511
 test1,1226169,1266609171581 column=info:regioninfo, timestamp=1266621116341, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1226169,1266609171581&apos;, STARTKEY =&amp;gt; &apos;1226169&apos;, ENDKEY =&amp;gt; &apos;1290703&apos;, ENCODED =&amp;gt; 45
                             9318323, OFFLINE =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, SPLIT =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt;
                             [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;,
                              BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,1226169,1266609171581 column=info:server, timestamp=1266613546335, value=10.129.68.214:60020
 test1,1226169,1266609171581 column=info:serverstartcode, timestamp=1266613546335, value=1266562596451
 test1,1226169,1266609171581 column=info:splitA, timestamp=1266621116341, value=\x00\x0512790\x00\x00\x00\x01\
                             x26\xE8\x80l\xCD\x1Btest1,1226169,1266621115597\x00\x071226169\x00\x00\x00\x05\x0
                             5test1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x0
                             0\x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x00\
                             x07\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\x0
                             0\x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00
                             \x00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\
                             x09IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04trueb\
                             xD0\x21\xBC
 test1,1226169,1266609171581 column=info:splitB, timestamp=1266621116341, value=\x00\x071290703\x00\x00\x00\x0
                             1\x26\xE8\x80l\xCD\x19test1,12790,1266621115597\x00\x0512790\x00\x00\x00\x05\x05t
                             est1\x00\x00\x00\x00\x00\x02\x00\x00\x00\x07IS_ROOT\x00\x00\x00\x05false\x00\x00\
                             x00\x07IS_META\x00\x00\x00\x05false\x00\x00\x00\x01\x07\x07actions\x00\x00\x00\x0
                             7\x00\x00\x00\x0BBLOOMFILTER\x00\x00\x00\x05false\x00\x00\x00\x0BCOMPRESSION\x00\
                             x00\x00\x04NONE\x00\x00\x00\x08VERSIONS\x00\x00\x00\x013\x00\x00\x00\x03TTL\x00\x
                             00\x00\x0A2147483647\x00\x00\x00\x09BLOCKSIZE\x00\x00\x00\x0565536\x00\x00\x00\x0
                             9IN_MEMORY\x00\x00\x00\x05false\x00\x00\x00\x0ABLOCKCACHE\x00\x00\x00\x04true\x5B
                             \x7D\x1A1
 test1,1226169,1266621115597 column=info:regioninfo, timestamp=1266621116358, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             1226169,1266621115597&apos;, STARTKEY =&amp;gt; &apos;1226169&apos;, ENDKEY =&amp;gt; &apos;12790&apos;, ENCODED =&amp;gt; 1988
                             459280, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt;
                             &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY
                             =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,1226169,1266621115597 column=info:server, timestamp=1266768596714, value=10.129.68.212:60020
 test1,1226169,1266621115597 column=info:serverstartcode, timestamp=1266768596714, value=1266768408023
 test1,12790,1266621115597   column=info:regioninfo, timestamp=1266621116361, value=REGION =&amp;gt; {NAME =&amp;gt; &apos;test1,
                             12790,1266621115597&apos;, STARTKEY =&amp;gt; &apos;12790&apos;, ENDKEY =&amp;gt; &apos;1290703&apos;, ENCODED =&amp;gt; 179081
                             1553, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test1&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;actions&apos;, VERSIONS =&amp;gt; &apos;3
                             &apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt;
                              &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
 test1,12790,1266621115597   column=info:server, timestamp=1266768555610, value=10.129.68.214:60020
 test1,12790,1266621115597   column=info:serverstartcode, timestamp=1266768555610, value=1266768408015
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Note: that both the daughters of  the parent region test1,1226169,1266609171581 have been installed. But the offlined parent row itself (for test1,1226169,1266609171581) is still present. Not sure if it is in these situations that the client starts seeing errors... but curious why the offline parent row hasn&apos;t been reaped yet from .META.&lt;/p&gt;

&lt;p&gt;I repro&apos;ed a similar problem yesterday. Might have more detailed logs and errors. Will share them shortly.&lt;/p&gt;

</comment>
                            <comment id="12836422" author="kannanm" created="Sun, 21 Feb 2010 18:53:49 +0000"  >&lt;p&gt;Stack wrote: &amp;lt;&amp;lt;&amp;lt;&amp;lt; In the .META. listing posted above, there are some interesting issues. We still have a reference to a daughter, splitB, in the first offlined (row) region, yet the next row is a daughter that has been offlined itself. There may be a race in here if we&apos;re splitting fast. Let me check it out and see if a fix.&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;Yes, I see several times that nested splits are happening, but the offlined parent row hasn&apos;t been reaped. But perhaps that in itself isn&apos;t an issue.  For example, corresponding to my first .META. snippet in this JIRA:&lt;/p&gt;

&lt;p&gt;The split of test1,1204765,1266569946560 was announced @4:08:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-02-19 04:08:23,764 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT: test1,1204765,1266569946560: Daughters; test1,1204765,1266581233447, test1,1290703,1266581233447 from test013.abcxyz.com,60020,1266562597546; 1 of 3
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But reclaiming the offlined parent row from .META. took time. First we  detected one of the daughters no longer reference it @ about 11:53:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-02-19 11:53:46,673 DEBUG org.apache.hadoop.hbase.master.BaseScanner: test1,1204765,1266581233447/1373493090 no longer has references to test1,1204765,1266569946560
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the second daughter at about 14:01. It is only at this point we delete the offlined parent row:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-02-19 14:01:48,283 DEBUG org.apache.hadoop.hbase.master.BaseScanner: test1,1290703,1266581233447/580635726 no longer has references to test1,1204765,1266569946560
2010-02-19 14:01:48,299 INFO org.apache.hadoop.hbase.master.BaseScanner: Deleting region test1,1204765,1266569946560 (encoded=1819368969) because daughter splits no longer hold references
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Naturally, given this wide window it is not uncommon to see rows corresponding to nested splits in .META. In most of these cases, eventually the .META. seems to fix itself. But it still seems odd to me that it takes so much time. &lt;/p&gt;

&lt;p&gt;During one of these situations, I saw the client get errors of the form:&lt;/p&gt;

&lt;p&gt;10/02/19 09:09:37 INFO tests.MultiThreadedWriter: &lt;span class=&quot;error&quot;&gt;&amp;#91;22&amp;#93;&lt;/span&gt; Users = 1052116, mails = 1M, time = 10:10:53&lt;br/&gt;
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.129.68.212:60020 for region\&lt;br/&gt;
 test1,1204765,1266581233447, row &apos;1232785&apos;, but failed after 10 attempts.&lt;/p&gt;

&lt;p&gt;and assumed that this was related to the .META. being in a wierd state (i.e. offlined parent not being deleted). But looking at the logs, these client errors happened during a smaller period (8:49 to 9:09). And were likely due to other load issues on the particular region server. I will post any findings from that RS&apos;es logs shortly.&lt;/p&gt;



</comment>
                            <comment id="12836425" author="kannanm" created="Sun, 21 Feb 2010 19:06:16 +0000"  >&lt;p&gt;(continued)&lt;/p&gt;

&lt;p&gt;Looking at the logs for the concerned RS, yes, at 8:49 due to some DFS errors the region server shut itself down. Basically this confirms that META&apos;s state wasn&apos;t the cause for the client errors. It would still be good to understand why parent row deletion takes time and fix any known inconsistencies in this area. But the original snippet I posted of .META. in and of itself doesn&apos;t seem to indicate a problem.&lt;/p&gt;

&lt;p&gt;But the comment Stack made about: &amp;lt;&amp;lt;&amp;lt;Also, if already a daughter region of same name in the FS, we&apos;ll fail the split rather than overwrite as it seems we should do (only reason for a pre-existing daughter is a split failed mid-way). I can add a check of .META. If daughter not there, its for sure a failed split. Let me see if I can improve stuff in here in general for 0.20 as part of this patch.&amp;gt;&amp;gt;&amp;gt;&amp;gt; seems would be worth addressing.&lt;/p&gt;</comment>
                            <comment id="12836426" author="stack" created="Sun, 21 Feb 2010 19:08:51 +0000"  >&lt;p&gt;The offlined parent is deleted only after daughters have let go of all references to the parent.  If you want a quick lesson on how references work just say.   So yes the offlined parent can stick around for a while.  It&apos;s the metascanner that does the parent removal. On the retried issue grep the region in the master logs to an idea of it&apos;s history.  What was happening during the time the client failed?&lt;/p&gt;</comment>
                            <comment id="12836428" author="jdcryans" created="Sun, 21 Feb 2010 19:10:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;It would still be good to understand why parent row deletion takes time and fix any known inconsistencies in this area.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s by design, we only clear the parent when the daughters no longer have references since we don&apos;t split the HFiles during the split process but after during compactions.&lt;/p&gt;</comment>
                            <comment id="12836478" author="kannanm" created="Mon, 22 Feb 2010 00:48:47 +0000"  >&lt;p&gt;@stack, jdcryans:  The exact implication of the &quot;references&quot; &amp;#8211; i.e. the &lt;br/&gt;
daughter regions still sharing HFile with the parent for a period of &lt;br/&gt;
time &amp;#8211; hadn&apos;t sunk in.  But it makes sense now. I think we can close &lt;br/&gt;
this issue as not a bug.&lt;/p&gt;

&lt;p&gt;@stack: If you found other opportunities (as part of your recent code walk &lt;br/&gt;
through) for making the general handling of splits more bullet proof,&lt;br/&gt;
perhaps we should file that as a separate issue so that it doesn&apos;t get&lt;br/&gt;
muddled with this non-issue.&lt;/p&gt;

&lt;p&gt;@ What was happening during the time the client failed? &lt;/p&gt;

&lt;p&gt;In this particular case the RS was shutting down. The DNs were getting overwhelmed in my small cluster test case, and not able to keep up. It started with a lot of:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-02-19 08:48:57,462 WARN org.apache.hadoop.hdfs.DFSClient: NotReplicatedYetException sleeping /hbase-kannan1/test1/580635726/actions/133921297096924993\
7 retries left 1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Followed by:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-02-19 08:49:07,102 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block blk_9144926768183088527_186431 bad datanode[0] nodes == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
2010-02-19 08:49:07,102 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Source file &quot;/hbase-kannan1/test1/580635726/actions/133921297\
0969249937&quot; - Aborting...
2010-02-19 08:49:07,117 FATAL org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Replay of hlog required. Forcing server shutdown
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


</comment>
                            <comment id="12836897" author="stack" created="Mon, 22 Feb 2010 20:52:11 +0000"  >&lt;p&gt;Patch that skips opening of new daughter regions post creation.  Its not necessary.  Should make splits run a little faster and take a little load off hdfs/nn.&lt;/p&gt;</comment>
                            <comment id="12836906" author="tlipcon" created="Mon, 22 Feb 2010 21:12:09 +0000"  >&lt;p&gt;Kannan: have you applied &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-630&quot; title=&quot;In DFSOutputStream.nextBlockOutputStream(), the client can exclude specific datanodes when locating the next block.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-630&quot;&gt;&lt;del&gt;HDFS-630&lt;/del&gt;&lt;/a&gt; on your hadoop test cluster? There&apos;s a patch on that JIRA that&apos;s compatible with 0.20.&lt;br/&gt;
I also recommend &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-793&quot; title=&quot;DataNode should first receive the whole packet ack message before it constructs and sends its own ack message for the packet&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-793&quot;&gt;&lt;del&gt;HDFS-793&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-101&quot; title=&quot;DFS write pipeline : DFSClient sometimes does not detect second datanode failure &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-101&quot;&gt;&lt;del&gt;HDFS-101&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-927&quot; title=&quot;DFSInputStream retries too many times for new block locations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-927&quot;&gt;&lt;del&gt;HDFS-927&lt;/del&gt;&lt;/a&gt; which are in 0.20.2&apos;s release candidate but perhaps not in your version.&lt;/p&gt;</comment>
                            <comment id="12836907" author="stack" created="Mon, 22 Feb 2010 21:14:21 +0000"  >&lt;p&gt;I took a look at our split code.  Its not as bad as I thought in that the fail I mention above should never happen (on open of a region, we clean up any old split working dirs so there should never be a clash creating daughter dirs &amp;#8211; if there is then something very seriously wrong is going on and we should fail).&lt;/p&gt;

&lt;p&gt;@Kannan Do you think hdfs-826 would have helped wih &quot;Error Recovery for block blk_9144926768183088527_186431 bad datanode&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; nodes == null&quot;?  Looks like we ran out of replicas?  Is that your take?&lt;/p&gt;

&lt;p&gt;I&apos;d like to keep this issue open for the fixup I&apos;d like to add to .META. that will defend against case where we get the parent offlining edit but somehow the daughter additions don&apos;t go in.  Let me post a fuller patch soon.&lt;/p&gt;</comment>
                            <comment id="12837658" author="stack" created="Wed, 24 Feb 2010 07:19:58 +0000"  >&lt;p&gt;This version adds to the above fixup in BaseScanner.  Currently BaseScanner, checks offlined parents.  If no references by daughters, then we let the parent go.   This patch adds a bit of code in here to check &amp;#8211; if there are references stil &amp;#8211; that the daughters of the split are in the metatable.  If not, it adds them and marks the parent row so we don&apos;t do expensive check each time through metascan.&lt;/p&gt;

&lt;p&gt;Need to add a test and run on cluster yet.  Just posting what I have so far.&lt;/p&gt;</comment>
                            <comment id="12837674" author="kannanm" created="Wed, 24 Feb 2010 08:18:38 +0000"  >&lt;p&gt;@Todd/@Stack: So my previous run didn&apos;t have &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-927&quot; title=&quot;DFSInputStream retries too many times for new block locations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-927&quot;&gt;&lt;del&gt;HDFS-927&lt;/del&gt;&lt;/a&gt;. But we have now applied the fix for that. We are also in the process of applying/testing a fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2234&quot; title=&quot;Roll Hlog if any datanode in the write pipeline dies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2234&quot;&gt;&lt;del&gt;HBASE-2234&lt;/del&gt;&lt;/a&gt; (which in turn depends on &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-836&quot; title=&quot;Make dot a valid HDFS path?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-836&quot;&gt;&lt;del&gt;HDFS-836&lt;/del&gt;&lt;/a&gt;). Will look into your other recommened HDFS patches as well.&lt;/p&gt;

&lt;p&gt;Stack wrote: &amp;lt;&amp;lt;&amp;lt; Do you think hdfs-826 would have helped wih &quot;Error Recovery for block blk_9144926768183088527_186431 bad datanode&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; nodes == null&quot;? Looks like we ran out of replicas? Is that your take?&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt;

&lt;p&gt;Analyzed the various logs with Dhruba today.  Turns out this was happening because the region server started getting NotReplicatedYetException from HDFS for a particular block. When the client (region server) requests the HDFS namenode for a new block, the namenode first checks to see if datanodes for the penultimate block have sent in their &quot;blocks received&quot; confirmation. If not, the NameNode rejects the new block request with a NotReplicatedYetException.  The client retries a configurable number of times... (I think the default is 4), and in our case  eventually gave up after about 10 seconds.  The data nodes in question took about 30 seconds to send in their block received confirmation for the penultimate block. We don&apos;t yet have a good theory on why the data nodes were running slow. We have put in some more instrumentation on the HDFS side that might give us some clues if this happens again.&lt;/p&gt;

</comment>
                            <comment id="12838323" author="stack" created="Thu, 25 Feb 2010 12:34:45 +0000"  >&lt;p&gt;Can&apos;t add unit test for this explicit case.  The actors are the BaseScanner up in Master and a failing split over in a RegionServer.  The split region needs to be completely coherent; that is, after split it has to have references in place else fixup won&apos;t cut in (that there are references and there is no entry for a daughter is how we identify this pathology).  Then we need to have regionserver fail at an explicit location in a cluster test, after offlining parent but but somewhere in the&lt;br/&gt;
middle of adding the daughters to meta.  Our current testing vocabulary doesn&apos;t allow us do this kind of detailed fail (Needs to be fixed but not as part of this patch).&lt;/p&gt;

&lt;p&gt;So, I can see the repair cutting in if I doctor the RegionServer with the following patch to CompactSplitThread:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Index: src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java       (revision 915869)
+++ src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java       (working copy)
@@ -199,6 +199,13 @@
         Writables.getBytes(newRegions[1].getRegionInfo()));
     t.put(put);
     
+    &lt;span class=&quot;code-comment&quot;&gt;// If we crash here, then the daughters will not be added and we&apos;ll have
&lt;/span&gt;+    &lt;span class=&quot;code-comment&quot;&gt;// and offlined parent but no daughters to take up the slack.  hbase-2244
&lt;/span&gt;+    &lt;span class=&quot;code-comment&quot;&gt;// adds fixup to the metascanners.
&lt;/span&gt;+
+    &lt;span class=&quot;code-comment&quot;&gt;// REMOVE
&lt;/span&gt;+    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Fail to add other regions and to send split to master&quot;&lt;/span&gt;);
+    
     &lt;span class=&quot;code-comment&quot;&gt;// Add &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; regions to META
&lt;/span&gt;     &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; newRegions.length; i++) {
       put = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Put(newRegions[i].getRegionName());

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice the throw IOE after parent has been offlined.  This makes it so meta does not get daughter entries and the master does not get split message.&lt;/p&gt;

&lt;p&gt;In TestForceSplit, you&apos;ll see the following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2010-02-24 19:45:16,685 ERROR [RegionServer:0.compactor] regionserver.CompactSplitThread(104): Compaction/Split failed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region test,,1267069500020
java.io.IOException: Fail to add other regions and to send split to master
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:207)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:95)
2010-02-24 19:45:17,101 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:18,105 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:19,111 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:20,117 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:21,122 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:22,127 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:23,133 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:24,140 DEBUG [WaitOnSplit] client.TestForceSplit$WaitOnSplit(72): Cycle waiting on split
2010-02-24 19:45:24,655 INFO  [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-103] master.ServerManager$ServerMonitor(129): 1 region servers, 0 dead, average load 2.0
2010-02-24 19:45:24,920 INFO  [RegionManager.metaScanner] master.BaseScanner(163): RegionManager.metaScanner scanning meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: &amp;lt;&amp;gt;}
2010-02-24 19:45:24,921 INFO  [RegionManager.rootScanner] master.BaseScanner(163): RegionManager.rootScanner scanning meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: &amp;lt;&amp;gt;}
2010-02-24 19:45:24,941 INFO  [RegionManager.rootScanner] master.BaseScanner(242): RegionManager.rootScanner scan of 1 row(s) of meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: &amp;lt;&amp;gt;} complete
2010-02-24 19:45:24,942 WARN  [RegionManager.metaScanner] master.BaseScanner(379): Fixup broke split: Add missing split daughter to meta, daughter=REGION =&amp;gt; {NAME =&amp;gt; &apos;test,,1267069516539&apos;, STARTKEY =&amp;gt; &apos;&apos;, ENDKEY =&amp;gt; &apos;lqg&apos;, ENCODED =&amp;gt; 904299994, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;a&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}, parent=REGION =&amp;gt; {NAME =&amp;gt; &apos;test,,1267069500020&apos;, STARTKEY =&amp;gt; &apos;&apos;, ENDKEY =&amp;gt; &apos;&apos;, ENCODED =&amp;gt; 970247455, OFFLINE =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, SPLIT =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;a&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
2010-02-24 19:45:24,947 WARN  [RegionManager.metaScanner] master.BaseScanner(379): Fixup broke split: Add missing split daughter to meta, daughter=REGION =&amp;gt; {NAME =&amp;gt; &apos;test,lqg,1267069516539&apos;, STARTKEY =&amp;gt; &apos;lqg&apos;, ENDKEY =&amp;gt; &apos;&apos;, ENCODED =&amp;gt; 1813617937, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;a&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}, parent=REGION =&amp;gt; {NAME =&amp;gt; &apos;test,,1267069500020&apos;, STARTKEY =&amp;gt; &apos;&apos;, ENDKEY =&amp;gt; &apos;&apos;, ENCODED =&amp;gt; 970247455, OFFLINE =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, SPLIT =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;test&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;a&apos;, VERSIONS =&amp;gt; &apos;3&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;65536&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;}]}}
2010-02-24 19:45:24,948 INFO  [RegionManager.metaScanner] master.BaseScanner(242): RegionManager.metaScanner scan of 1 row(s) of meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: &amp;lt;&amp;gt;} complete
2010-02-24 19:45:24,949 INFO  [RegionManager.metaScanner] master.MetaScanner(132): All 1 .META. region(s) scanned
2010-02-24 19:45:25,145 DEBUG [WaitOnSplit] client.HConnectionManager$TableServers(780): Cache hit &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; row &amp;lt;&amp;gt; in tableName .META.: location server 127.0.0.1:59090, location region name .META.,,1
2010-02-24 19:45:25,149 DEBUG [WaitOnSplit] client.HTable$ClientScanner(1832): Creating scanner over .META. starting at key &apos;&apos;
2010-02-24 19:45:25,151 DEBUG [WaitOnSplit] client.HTable$ClientScanner(1928): Advancing internal scanner to startKey at &apos;&apos;
2010-02-24 19:45:25,151 DEBUG [WaitOnSplit] client.HConnectionManager$TableServers(780): Cache hit &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; row &amp;lt;&amp;gt; in tableName .META.: location server 127.0.0.1:59090, location region name .META.,,1
2010-02-24 19:45:25,157 INFO  [WaitOnSplit] client.TestForceSplit$WaitOnSplit(79): keyvalues={test,,1267069500020/info:regioninfo/1267069516682/Put/vlen=254, test,,1267069500020/info:server/1267069501165/Put/vlen=15, test,,1267069500020/info:serverstartcode/1267069501165/Put/vlen=8, test,,1267069500020/info:splitA/1267069516682/Put/vlen=257, test,,1267069500020/info:splitA_checked/1267069524944/Put/vlen=1, test,,1267069500020/info:splitB/1267069516682/Put/vlen=260, test,,1267069500020/info:splitB_checked/1267069524948/Put/vlen=1}
2010-02-24 19:45:25,158 INFO  [WaitOnSplit] client.TestForceSplit$WaitOnSplit(79): keyvalues={test,,1267069516539/info:regioninfo/1267069524943/Put/vlen=257}
2010-02-24 19:45:25,158 INFO  [WaitOnSplit] client.TestForceSplit$WaitOnSplit(79): keyvalues={test,lqg,1267069516539/info:regioninfo/1267069524948/Put/vlen=260}
2010-02-24 19:45:25,159 DEBUG [WaitOnSplit] client.HTable$ClientScanner(1915): Finished with scanning at REGION =&amp;gt; {NAME =&amp;gt; &apos;.META.,,1&apos;, STARTKEY =&amp;gt; &apos;&apos;, ENDKEY =&amp;gt; &apos;&apos;, ENCODED =&amp;gt; 1028785192, TABLE =&amp;gt; {{NAME =&amp;gt; &apos;.META.&apos;, IS_META =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;, MEMSTORE_FLUSHSIZE =&amp;gt; &apos;16384&apos;, FAMILIES =&amp;gt; [{NAME =&amp;gt; &apos;historian&apos;, VERSIONS =&amp;gt; &apos;2147483647&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;604800&apos;, BLOCKSIZE =&amp;gt; &apos;8192&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;}, {NAME =&amp;gt; &apos;info&apos;, VERSIONS =&amp;gt; &apos;10&apos;, COMPRESSION =&amp;gt; &apos;NONE&apos;, TTL =&amp;gt; &apos;2147483647&apos;, BLOCKSIZE =&amp;gt; &apos;8192&apos;, IN_MEMORY =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;, BLOCKCACHE =&amp;gt; &apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;}]}}
2010-02-24 19:45:25,160 ERROR [IPC Server handler 4 on 59090] regionserver.HRegionServer(862): 
org.apache.hadoop.hbase.NotServingRegionException: test,,1267069500020 is closing=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; or closed=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:1787)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1907)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
2010-02-24 19:45:25,175 DEBUG [main] client.HTable$ClientScanner(1928): Advancing internal scanner to startKey at &apos;aaa&apos;
2010-02-24 19:45:25,177 ERROR [IPC Server handler 2 on 59090] regionserver.HRegionServer(864): Failed openScanner
org.apache.hadoop.hbase.NotServingRegionException: test,,1267069500020
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2279)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1858)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
2010-02-24 19:45:30,186 DEBUG [main] client.HConnectionManager$TableServers(857): Removed test,,1267069500020 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; tableName=test from cache because of aaa
2010-02-24 19:45:30,188 DEBUG [main] client.HConnectionManager$TableServers(734): locateRegionInMeta attempt 0 of 3 failed; retrying after sleep of 5000 because: No server address listed in .META. &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region test,,1267069516539
2010-02-24 19:45:34,656 INFO  [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-103] master.ServerManager$ServerMonitor(129): 1 region servers, 0 dead, average load 2.0
2010-02-24 19:45:34,920 INFO  [RegionManager.rootScanner] master.BaseScanner(163): RegionManager.rootScanner scanning meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: &amp;lt;&amp;gt;}
2010-02-24 19:45:34,920 INFO  [RegionManager.metaScanner] master.BaseScanner(163): RegionManager.metaScanner scanning meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: &amp;lt;&amp;gt;}
2010-02-24 19:45:34,932 DEBUG [RegionManager.metaScanner] master.BaseScanner(572): Current assignment of test,,1267069516539 is not valid;  serverAddress=, startCode=0 unknown.
2010-02-24 19:45:34,934 DEBUG [RegionManager.metaScanner] master.BaseScanner(572): Current assignment of test,lqg,1267069516539 is not valid;  serverAddress=, startCode=0 unknown.
2010-02-24 19:45:34,940 INFO  [RegionManager.rootScanner] master.BaseScanner(242): RegionManager.rootScanner scan of 1 row(s) of meta region {server: 127.0.0.1:59090, regionname: -ROOT-,,0, startKey: &amp;lt;&amp;gt;} complete
2010-02-24 19:45:34,941 INFO  [RegionManager.metaScanner] master.BaseScanner(242): RegionManager.metaScanner scan of 3 row(s) of meta region {server: 127.0.0.1:59090, regionname: .META.,,1, startKey: &amp;lt;&amp;gt;} complete
2010-02-24 19:45:34,941 INFO  [RegionManager.metaScanner] master.MetaScanner(132): All 1 .META. region(s) scanned
2010-02-24 19:45:35,189 DEBUG [main] client.HConnectionManager$TableServers(734): locateRegionInMeta attempt 1 of 3 failed; retrying after sleep of 5000 because: No server address listed in .META. &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; region test,,1267069516539
2010-02-24 19:45:35,315 INFO  [IPC Server handler 4 on 60000] master.RegionManager(337): Assigning region test,,1267069516539 to localhost,59090,1267069494948
2010-02-24 19:45:35,315 INFO  [IPC Server handler 4 on 60000] master.RegionManager(337): Assigning region test,lqg,1267069516539 to localhost,59090,1267069494948
2010-02-24 19:45:35,316 INFO  [RegionServer:0] regionserver.HRegionServer(499): MSG_REGION_OPEN: test,,1267069516539
2010-02-24 19:45:35,317 INFO  [RegionServer:0] regionserver.HRegionServer(499): MSG_REGION_OPEN: test,lqg,1267069516539
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2010-02-24 19:26:50,203 and 2010-02-24 19:26:50,209 are the new stuff.  See how subsequently the daughter rows added by the fixup are assigned and the verification scan &amp;#8211; the test is to prove scans can ride across splits &amp;#8211; completes and the unit tests fails.&lt;/p&gt;

&lt;p&gt;I&apos;m going to commit the patch unless objection.&lt;/p&gt;

&lt;p&gt;This patch adds some load on .META.: two new Gets after every split.  Shouldn&apos;t be too bad.  Splits are rare enough.&lt;/p&gt;</comment>
                            <comment id="12838325" author="stack" created="Thu, 25 Feb 2010 12:35:29 +0000"  >&lt;p&gt;Marking patch available; review would be ok.  It ain&apos;t too big.  Mostly its comments.&lt;/p&gt;</comment>
                            <comment id="12838326" author="stack" created="Thu, 25 Feb 2010 12:39:18 +0000"  >&lt;p&gt;@Kannan I&apos;ve seen NotReplicatedYetException.  Adding instrumentation to figure why datanode is slow reporting sounds excellent.&lt;/p&gt;</comment>
                            <comment id="12838472" author="jdcryans" created="Thu, 25 Feb 2010 18:38:04 +0000"  >&lt;p&gt;Patch itself looks good. Could Kannan try to destroy it before you commit?&lt;/p&gt;</comment>
                            <comment id="12838536" author="kannanm" created="Thu, 25 Feb 2010 20:53:01 +0000"  >&lt;p&gt;@Stack, JD: I haven&apos;t had a chance to review the patch. I&apos;ll do that as well as try it out on my test setup. So perhaps just hold off on committing it for a few days...&lt;/p&gt;</comment>
                            <comment id="12840302" author="kannanm" created="Tue, 2 Mar 2010 19:58:20 +0000"  >&lt;p&gt;Stack: The changes look very good. Like all the helpful comments as well.&lt;/p&gt;

&lt;p&gt;Some small comments on the patch:&lt;/p&gt;

&lt;p&gt;Previously, the first time hasReferences() finds that there are no references&lt;br/&gt;
from a daughter region, it would delete the corresponding &quot;info:splitA or B&quot; row&lt;br/&gt;
from the parent. On subsequent calls, hasReferences would bail early because &quot;split&quot;&lt;br/&gt;
would be null.&lt;/p&gt;

&lt;p&gt;Now, the deletion of the daughter column (info:splitA or B) in the parent row&lt;br/&gt;
happens in checkDaughter() (when it calls removeDaughterFromParent()). hasReferences()&lt;br/&gt;
will keep returning false for a daughter that no longer has references, and it seems&lt;br/&gt;
we&apos;ll unnecessarily be calling removeDaughterFromParent() on more than one&lt;br/&gt;
occasion. Delete of a non-existent column should be a no-op-- so this is probably&lt;br/&gt;
not a major issue, but it would be good to avoid introducing these wasteful deletes.&lt;/p&gt;

&lt;p&gt;Minor/Cosmetic:&lt;br/&gt;
~~~~~~~~~~~~~~~&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;HRegion.java:createDaughterDirInSplitDir()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Doesn&apos;t actually create the daughter dir, but only constructs a path&lt;br/&gt;
to the daughter dir, correct? Perhaps the function could be name&lt;br/&gt;
getSplitDirForDaughter() or something.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;typo: BaseScanner.java&lt;/li&gt;
	&lt;li&gt;the filesystem, then a daughters was not added _&lt;em&gt;o&lt;/em&gt;_ .META. &amp;#8211; must have been&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;typo: HRegion.java&lt;br/&gt;
      // _&lt;em&gt;Crate&lt;/em&gt;_ a region instance and then move the splits into place under&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12840324" author="stack" created="Tue, 2 Mar 2010 21:05:59 +0000"  >&lt;p&gt;@Kannan Sweet! Thanks for great review.  Attached patch has all your changes.  Your first observation is particularly egregious.  Thanks for pointing it out.  I&apos;m running tests to make sure it still works as its supposed.... will commit if all pass.&lt;/p&gt;</comment>
                            <comment id="12840427" author="kannanm" created="Wed, 3 Mar 2010 02:23:18 +0000"  >&lt;p&gt;There is one minor issue still with the patch that:&lt;/p&gt;

&lt;p&gt;in hasReferences(), this condition:&lt;/p&gt;

&lt;p&gt;      if (ps != null &amp;amp;&amp;amp; ps.length &amp;gt; 0) &lt;/p&gt;
{
        result = true;
        break;
      }

&lt;p&gt;should be changed to:&lt;/p&gt;

&lt;p&gt;      if (ps != null &amp;amp;&amp;amp; ps.length &amp;gt; 0) &lt;/p&gt;
{
        return true;
      }

&lt;p&gt;We don&apos;t want to fall into removeDaughterFromParent() under the above. The older version of the code had a:&lt;/p&gt;

&lt;p&gt;   if (result) &lt;/p&gt;
{
      return result;
    }

&lt;p&gt;to handle the same.&lt;/p&gt;
</comment>
                            <comment id="12840428" author="kannanm" created="Wed, 3 Mar 2010 02:40:08 +0000"  >&lt;p&gt;Also,&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!verifyDaughterRowPresent(rowContent, qualifier, srvr, metaRegionName,
        hri, parent)) {
      &lt;span class=&quot;code-comment&quot;&gt;// If we got here, we added a daughter region to metatable. Update
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// parent row that daughter has been verified present so we don&apos;t check
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; it by doing a get each time through here.
&lt;/span&gt;      addDaughterRowChecked(metaRegionName, srvr, parent.getRegionName(), hri,
        qualifier);
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the comment above isn&apos;t strictly correct. We&apos;ll do a addDaughterRowChecked() even if the daughter region was already present in the meta table, correct?&lt;/p&gt;

&lt;p&gt;Similarly, the description for the &quot;return&quot; param of verifyDaughterRowPresent() needs to be tweaked accordingly. The function returns TRUE if we have already checked this daughter and FALSE otherwise.&lt;/p&gt;

</comment>
                            <comment id="12840443" author="stack" created="Wed, 3 Mar 2010 04:00:45 +0000"  >&lt;p&gt;@Kannan Really appreciate the review.  What I committed actually does as you suggest up in &quot;03/Mar/10 02:23 AM&quot; (My extra testing unveiled the condition you found by reading code).  I did what old code did to avoid return in middle of a method.  On the incorrect comment.  Fixing now.  Thanks.&lt;/p&gt;</comment>
                            <comment id="12840458" author="stack" created="Wed, 3 Mar 2010 04:51:46 +0000"  >&lt;p&gt;@Kannan I committed below to address your last comment.  Let me know if insufficient.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Index: core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java
===================================================================
--- core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java  (revision 918324)
+++ core/src/main/java/org/apache/hadoop/hbase/master/BaseScanner.java  (working copy)
@@ -342,9 +342,8 @@
     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!references) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; references;
     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!verifyDaughterRowPresent(rowContent, qualifier, srvr, metaRegionName,
         hri, parent)) {
-      &lt;span class=&quot;code-comment&quot;&gt;// If we got here, we added a daughter region to metatable. Update
&lt;/span&gt;-      &lt;span class=&quot;code-comment&quot;&gt;// parent row that daughter has been verified present so we don&apos;t check
&lt;/span&gt;-      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; it by doing a get each time through here.
&lt;/span&gt;+      &lt;span class=&quot;code-comment&quot;&gt;// If we got here, then the parent row does not yet have the
&lt;/span&gt;+      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-quote&quot;&gt;&quot;daughter row verified present&quot;&lt;/span&gt; marker present. Add it.
&lt;/span&gt;       addDaughterRowChecked(metaRegionName, srvr, parent.getRegionName(), hri,
         qualifier);
     }
@@ -360,8 +359,8 @@
    * @param metaRegionName
    * @param daughterHRI
    * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException
-   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; True, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the daughter row is present in meta.  If &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;
-   * method just added it to meta.
+   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; True, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; parent row has marker &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;daughter row verified present&quot;&lt;/span&gt;
+   * &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; (and will &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; fixup adding daughter &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; daughter not present).
    */
   &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; verifyDaughterRowPresent(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Result rowContent,
       &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; [] daughter, &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; HRegionInterface srvr,
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12840485" author="kannanm" created="Wed, 3 Mar 2010 05:48:45 +0000"  >&lt;p&gt;Looks good to me. Thanks. &lt;/p&gt;</comment>
                            <comment id="12840488" author="apurtell" created="Wed, 3 Mar 2010 06:02:32 +0000"  >&lt;p&gt;This has been committed, and I updated CHANGES.txt to reflect that.&lt;/p&gt;</comment>
                            <comment id="12841425" author="stack" created="Thu, 4 Mar 2010 18:24:04 +0000"  >&lt;p&gt;Applied branch and trunk.  Resolving.  Please reopen if you think this issue not done (Kannan suggested it a non-issue a good while back)&lt;/p&gt;

&lt;p&gt;That said, there is still good stuff/discussion up in this issue.  Lets open new issues for the still unresolve for stuff like figuring why notreplicatedyet is happening, etc.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12436826" name="2244-v3.patch" size="19844" author="stack" created="Wed, 24 Feb 2010 07:19:58 +0000"/>
                            <attachment id="12436996" name="2244-v5.patch" size="23916" author="stack" created="Thu, 25 Feb 2010 12:34:45 +0000"/>
                            <attachment id="12437642" name="2244-v6.patch" size="23976" author="stack" created="Tue, 2 Mar 2010 21:05:59 +0000"/>
                            <attachment id="12436626" name="2244.patch" size="5489" author="stack" created="Mon, 22 Feb 2010 20:52:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Sat, 20 Feb 2010 23:39:25 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26222</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 42 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i08sq7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>49254</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>