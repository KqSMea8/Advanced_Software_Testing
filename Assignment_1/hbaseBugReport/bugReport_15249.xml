<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:01:27 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15249/HBASE-15249.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15249] Provide lower bound on number of regions in region normalizer for pre-split tables</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15249</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;AMS (Ambari Metrics System) developer found the following scenario:&lt;/p&gt;

&lt;p&gt;Metrics table was pre-split with many regions on large cluster (1600 nodes).&lt;br/&gt;
After some time, AMS stopped working because region normalizer merged the regions into few big regions which were not able to serve high read / write load.&lt;br/&gt;
This is a big problem since the write requests flood the regions faster than the splits can happen resulting in poor performance.&lt;/p&gt;

&lt;p&gt;We should consider setting reasonable lower bound on region count.&lt;br/&gt;
If the table is pre-split, we can use initial region count as the lower bound.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12938272">HBASE-15249</key>
            <summary>Provide lower bound on number of regions in region normalizer for pre-split tables</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="5">Cannot Reproduce</resolution>
                                        <assignee username="yuzhihong@gmail.com">Ted Yu</assignee>
                                    <reporter username="yuzhihong@gmail.com">Ted Yu</reporter>
                        <labels>
                            <label>normalization</label>
                    </labels>
                <created>Wed, 10 Feb 2016 18:41:32 +0000</created>
                <updated>Fri, 4 Mar 2016 22:14:19 +0000</updated>
                            <resolved>Fri, 4 Mar 2016 22:14:19 +0000</resolved>
                                                                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                <comments>
                            <comment id="15141513" author="yuzhihong@gmail.com" created="Wed, 10 Feb 2016 19:40:59 +0000"  >&lt;p&gt;Draft patch&lt;/p&gt;</comment>
                            <comment id="15141788" author="yuzhihong@gmail.com" created="Wed, 10 Feb 2016 22:08:04 +0000"  >&lt;p&gt;Patch v2 responds to table disablement and removes entry from internal map for the table&lt;/p&gt;</comment>
                            <comment id="15141875" author="yuzhihong@gmail.com" created="Wed, 10 Feb 2016 22:48:00 +0000"  >&lt;p&gt;Alternatively, we can retrieve write request counts from RegionLoad for the regions.&lt;br/&gt;
If the two regions to be merged happen to receive the most write requests, don&apos;t merge them in the current iteration.&lt;/p&gt;</comment>
                            <comment id="15142293" author="anoop.hbase" created="Thu, 11 Feb 2016 06:06:36 +0000"  >&lt;p&gt;IMO when 2 regions getting write requests ( not necessarily the most count) it may not be good to merge them.. They are any way growing.  So merging based on region size wont be a good idea.&lt;/p&gt;</comment>
                            <comment id="15142821" author="yuzhihong@gmail.com" created="Thu, 11 Feb 2016 14:43:25 +0000"  >&lt;p&gt;If some regions are empty, they wouldn&apos;t get write requests.&lt;br/&gt;
But if regions receive even access, it is likely that regions would get write requests.&lt;/p&gt;

&lt;p&gt;How about giving regions whose write requests are in the bottom 10% (can make this configurable) ?&lt;/p&gt;</comment>
                            <comment id="15142935" author="stack" created="Thu, 11 Feb 2016 16:03:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;How about giving regions whose write requests are in the bottom 10% (can make this configurable) ?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Above is totally arbitrary. If a region is getting 100 hits a second, thats 10% of 1k &amp;#8211; you&apos;d merge it because it is getting too little load?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;After some time, AMS stopped working because region normalizer merged the regions into few big regions which were not able to serve high read / write load. This is a big problem since the write requests flood the regions faster than the splits can happen resulting in poor performance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Numbers? How many regions was too little? What was the hit rate that overwhelmed? What did the normalizer do? It merged up how many regions in how much time?&lt;/p&gt;

&lt;p&gt;The problem here is that the time between presplit and the load coming on was too long? The normalizer merged up the regions  before the load came on? &lt;/p&gt;</comment>
                            <comment id="15143049" author="anoop.hbase" created="Thu, 11 Feb 2016 17:02:14 +0000"  >&lt;p&gt;Normalizer just checks the region size and decide on merge?   Ya when a region is getting write requests (even if very less than 10%) we should not allow to merge it IMHO.. This means the region is growing.    When 2 regions stop getting any write reqs and may be only have read request/or even that also not, it can be considered for merge.&lt;/p&gt;</comment>
                            <comment id="15143541" author="yuzhihong@gmail.com" created="Thu, 11 Feb 2016 21:30:26 +0000"  >&lt;p&gt;To better answer the questions above, we are trying to reproduce the problem with DEBUG logging on.&lt;br/&gt;
Previously metrics table ended up with 2 regions.&lt;/p&gt;

&lt;p&gt;Will be back with details.&lt;/p&gt;</comment>
                            <comment id="15149247" author="swagle" created="Tue, 16 Feb 2016 20:26:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; AMS (Ambari Metrics system), creates tables with pre-splits based on the knowledge of how many daemons will be writing metrics to HBase and the memory available to RS.&lt;br/&gt;
What we saw is that on a large cluster we defined close to 10 initial pre-split Regions for the table that gets heavy Read/Write load and this count dropped shortly after the system came online. This was during Ambari performance test runs.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Note&lt;/b&gt;: The metrics system is a constant write load system, however there is a bootstrap lag as the cluster comes online. This is the grey area where although Regions are not getting as many writes, merging them would be a bad idea. We enable normalizer specifically because the intial splits might not be optimal, however, the Region count is certainly critical for us to support the volume of writes that would eventually settle downs to a consistent number.&lt;/p&gt;

&lt;p&gt;We will try to get the numbers for you, since normalizer has DEBUG as the log level, we could not capture this on our intial run.&lt;/p&gt;</comment>
                            <comment id="15149636" author="stack" created="Wed, 17 Feb 2016 01:12:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;AMS (Ambari Metrics system), creates tables with pre-splits based on the knowledge of how many daemons will be writing metrics to HBase and the memory available to RS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What does the math look like &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=swagle&quot; class=&quot;user-hover&quot; rel=&quot;swagle&quot;&gt;Siddharth Wagle&lt;/a&gt;?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;....and this count dropped shortly after the system came online. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You need to run normalizer? Its a new feature that is not yet in any shipping version of hbase and it is off by default. An important service like AMS might try and do without it, at least at first? Could you settle for a less aggressive set of initial splits that is somewhat a factor of how many servers there are involved? e.g. cluster node count/ 10? As is, the default is to split aggressively at first so regions fan out over the cluster. That was not working for you?&lt;/p&gt;


</comment>
                            <comment id="15149801" author="anoop.hbase" created="Wed, 17 Feb 2016 03:50:58 +0000"  >&lt;p&gt;Thanks for the explanation of ur usage &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=swagle&quot; class=&quot;user-hover&quot; rel=&quot;swagle&quot;&gt;Siddharth Wagle&lt;/a&gt;.  Yes what I was saying is even if write reqs to a region is very small in numbers, still it may not be correct to merge it with another.  As there is clear indication that this region is growing. May be after some time it might be getting much more write load.   When 2 regions are done with all its writes and the data will be used only for read purpose, it may get merged.. The challenge is how we know whether region is done with its writes &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15150949" author="swagle" created="Wed, 17 Feb 2016 18:39:38 +0000"  >&lt;blockquote&gt;&lt;p&gt; What does the math look like for region splits &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ref: &lt;a href=&quot;https://issues.apache.org/jira/browse/AMBARI-13039&quot; title=&quot;Optimize precision table Region split policy for AMS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;AMBARI-13039&quot;&gt;&lt;del&gt;AMBARI-13039&lt;/del&gt;&lt;/a&gt;. We use the &lt;em&gt;memstore.lowerLimit&lt;/em&gt; and &lt;em&gt;memstore.flush.size&lt;/em&gt; to calculate memory available to the memstore and number of max-value on regions. Then we calculate lexically equidistant split points based on the services deployed by Ambari (from a static list of metrics that we mined from a deployed cluster) for the large tables.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You need to run normalizer?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In a stable state it seems normalizer works well for us managing the region boundaries. We do give user the option to disable this with a configuration setting in AMS (precautionary tactic from our end). All in all, we can definitely live without the normalizer this was not available to us until very recently, the pre-splitting pre-dates normalizer setting in AMS. The best use case for normalizer use for us is this: Ambari user can lets say add a service example: KAFKA that starts writing a ton of metrics and introduces a skew where previous splits become irrelevant.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; Thanks for feedback.&lt;/p&gt;</comment>
                            <comment id="15172206" author="eclark" created="Mon, 29 Feb 2016 17:40:35 +0000"  >&lt;p&gt;This whole normalizer seems to be running off the rails. We can&apos;t add a new config every time there&apos;s a new use case that the normalizer doesn&apos;t behave the ideal way. That leads to a feature that is so complex that everyone gets it wrong. It seems like the normalizer is currently using incorrect logic and incorrect signals. Are we sure this is a feature that will ever be complete?&lt;/p&gt;</comment>
                            <comment id="15180623" author="avijayan" created="Fri, 4 Mar 2016 22:05:02 +0000"  >&lt;p&gt;On further investigation, we find that the normalizer is not causing an issue for the AMS use case.&lt;/p&gt;

&lt;p&gt;We ran the normalizer in the following scenarios and verified that it respects the initial splits recommended by AMS.&lt;/p&gt;

&lt;p&gt;1. Small cluster (~10 Nodes)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-02-29 23:22:29,372 DEBUG [avijayan-hbase-3.novalocal,56004,1456787532496_ChoreService_1] normalizer.SimpleRegionNormalizer: Computing normalization plan &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; table: METRIC_RECORD, number of regions: 10
2016-02-29 23:22:29,372 DEBUG [avijayan-hbase-3.novalocal,56004,1456787532496_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, total aggregated regions size: 0
2016-02-29 23:22:29,372 DEBUG [avijayan-hbase-3.novalocal,56004,1456787532496_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, average region size: 0.0
2016-02-29 23:22:29,372 DEBUG [avijayan-hbase-3.novalocal,56004,1456787532496_ChoreService_1] normalizer.SimpleRegionNormalizer: No normalization needed, regions look good &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; table: METRIC_RECORD
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2. Large cluster (~900 Nodes) with hbase.normalizer.period = 10 minutes&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-03-04 20:55:14,168 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457124301027_ChoreService_1] normalizer.SimpleRegionNormalizer: Computing normalization plan &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; table: METRIC_RECORD, number of r
egions: 10
2016-03-04 20:55:14,168 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457124301027_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, total aggregated regions size: 157
2016-03-04 20:55:14,168 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457124301027_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, average region size: 15.7
2016-03-04 20:55:14,169 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457124301027_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, largest region [B@6901ddd has size 37, more t
han 2 times than avg size, splitting
2016-03-04 20:55:14,170 INFO  [perf-a-2.c.pramod-thangali.internal,61300,1457124301027_ChoreService_1] normalizer.SplitNormalizationPlan: Executing splitting normalization plan: SplitNormalizationPlan{reg
ionInfo={ENCODED =&amp;gt; 318e17da72832e72005e202f09a7ee55, NAME =&amp;gt; &apos;METRIC_RECORD,regionserver.Server.Increment_num_ops\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00,1457124324424.318e17da72832e72005e202f09a
7ee55.&apos;, STARTKEY =&amp;gt; &apos;regionserver.Server.Increment_num_ops\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00&apos;, ENDKEY =&amp;gt; &apos;rpc.rpc.CallQueueLength\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00&apos;}, splitPoi
nt=&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3. Large cluster with hbase.normalizer.period = 1 minute, to test normalizer behavior for close to empty regions&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2016-03-04 21:50:50,465 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457127998192_ChoreService_1] normalizer.SimpleRegionNormalizer: Computing normalization plan &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; table: METRIC_RECORD, number of r
egions: 10
2016-03-04 21:50:50,467 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457127998192_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, total aggregated regions size: 0
2016-03-04 21:50:50,467 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457127998192_ChoreService_1] normalizer.SimpleRegionNormalizer: Table METRIC_RECORD, average region size: 0.0
2016-03-04 21:50:50,467 DEBUG [perf-a-2.c.pramod-thangali.internal,61300,1457127998192_ChoreService_1] normalizer.SimpleRegionNormalizer: No normalization needed, regions look good &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; table: METRIC_RECOR
D
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12787318" name="HBASE-15249.v1.txt" size="2382" author="yuzhihong@gmail.com" created="Wed, 10 Feb 2016 19:40:59 +0000"/>
                            <attachment id="12787359" name="HBASE-15249.v2.txt" size="3720" author="yuzhihong@gmail.com" created="Wed, 10 Feb 2016 22:08:04 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 11 Feb 2016 06:06:36 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            40 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2sojr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>