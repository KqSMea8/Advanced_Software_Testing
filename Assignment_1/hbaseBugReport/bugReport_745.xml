<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:47:07 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-745/HBASE-745.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-745] scaling of one regionserver, improving memory and cpu usage</title>
                <link>https://issues.apache.org/jira/browse/HBASE-745</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;after weeks testing hbase 0.1.3 and hadoop(0.16.4, 0.17.1), i found there are many works to do,  before a particular regionserver can handle data about 100G, or even more. i&apos;d share my opions here with stack, and other developers.&lt;/p&gt;

&lt;p&gt;first, the easiest way improving scalability of regionserver is upgrading hardware, use 64bit os and 8G memory for the regionserver process, and speed up disk io. &lt;/p&gt;

&lt;p&gt;besides hardware, following are software bottlenecks i found in regionserver:&lt;br/&gt;
1. as data increasing, compaction was eating cpu(with io) times, the total compaction time is basicly linear relative to whole data size, even worse, sometimes square relavtive to that size.&lt;br/&gt;
2. memory usage are depends on opened mapfiles&lt;br/&gt;
3. network connection are depends on opened mapfiles, see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-2341&quot; title=&quot;Datanode active connections never returns to 0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-2341&quot;&gt;&lt;del&gt;HADOOP-2341&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-24&quot; title=&quot;Scaling: Too many open file handles to datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-24&quot;&gt;&lt;del&gt;HBASE-24&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</description>
                <environment>&lt;p&gt;hadoop 0.17.1&lt;/p&gt;</environment>
        <key id="12400273">HBASE-745</key>
            <summary>scaling of one regionserver, improving memory and cpu usage</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="ln@webcate.net">Luo Ning</reporter>
                        <labels>
                    </labels>
                <created>Tue, 15 Jul 2008 07:57:44 +0000</created>
                <updated>Fri, 22 Aug 2008 21:13:20 +0000</updated>
                            <resolved>Tue, 12 Aug 2008 21:34:25 +0000</resolved>
                                    <version>0.1.3</version>
                    <version>0.2.0</version>
                                    <fixVersion>0.2.0</fixVersion>
                                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                <comments>
                            <comment id="12613555" author="ln@webcate.net" created="Tue, 15 Jul 2008 08:33:58 +0000"  >&lt;p&gt;compaction time caculating:&lt;br/&gt;
1. suppose we are keep writing data to regionserver, and rowid of data is hashed to all regions.&lt;br/&gt;
2. according to default optionalcacheflushinterval(30min) and threshold(3), all HStore will create a flushed storefile in 30min, after 1 hour, each HStore will have 3 storefile(include original 1), so a compaction will taken. that is, all HStore in the regionserver will do a compaction in 1 hour.&lt;br/&gt;
3. a compaction of HStore will read all data in mapfiles of the HStore, i&apos;d suppose the time of compcating is depends on total file size of mapfiles holding by the HStore. so the whole compacting time(caused by optionalcacheflushinterval) of a regionserver, depends on data size  the regionserver serving.&lt;br/&gt;
4. now we can see, the default optionalcacheflushinterval is not suitable for most env., i&apos;ve found my hardware(Xeon 3.2*2, dualcore, scsi ) can compacting 10M data per second, this mean it can compact 36G in 1 hour, so a regionserver can only holding data size less than 36G?&lt;br/&gt;
5. how about increasing optionalcacheflushinterval? to 12hours, even 24hours? unfortunatly, i found it useless. because globalMemcacheLimit, it default 512M, when reached, memcache will flushed(storefile created), until total size of memcache lower than 256M, since inserted rowids are distributed to all regions, nearly half of all regions will have a new storefile too. then when inserted data reach 1G(4 times of flushing global memcache), all data of the regionserver need compaction. no setting can adjust this behavor.&lt;/p&gt;</comment>
                            <comment id="12613557" author="ln@webcate.net" created="Tue, 15 Jul 2008 08:56:04 +0000"  >&lt;p&gt;compaction improvement:&lt;/p&gt;

&lt;p&gt;compaction has very poor efficiency in current hbase release(0.1.3), suppose 3 mapfile in a HStore, the 1 orginal is 128M, and newly flushed 2 is smaller than 1M(this is the most common situation where regionserver carrying 512 hstore or more, flushing 256M global mamcache each time), we compacted 2M data, but read and write 120M!&lt;/p&gt;

&lt;p&gt;my suggestion:&lt;br/&gt;
1. set threshold larger, this will cause lower compaction times, but more mapfiles(will discuss later in this issue about memory usage)&lt;br/&gt;
2. implementing incremental compaction, that&apos;s mean: don&apos;t compact to 1 file each time, compact small files only, &lt;br/&gt;
do a whole compaction when file size large enough. in HStore#compact(boolean), we can use a alorighm to select hstorefiles for compacting. (will attach my impl for review later.)&lt;/p&gt;</comment>
                            <comment id="12613566" author="ln@webcate.net" created="Tue, 15 Jul 2008 09:34:24 +0000"  >&lt;p&gt;memory calculating:&lt;br/&gt;
memory usage of a regionserver is determined by 3 things:&lt;br/&gt;
#1. the mapfile index read into memory(io.map.index.skip can adjust it, buf allwill stay in mem weather u need it or not)&lt;br/&gt;
#2. data output buffer used by each SequenceFile$Reader(each can measured as the largest value size in the file)&lt;br/&gt;
#3. memcache, controlled by &apos;globalMemcacheLimit&apos; and &apos;globalMemcacheLimitLowMark&apos;&lt;/p&gt;

&lt;p&gt;that is, beside already controlled #3,  memory is determined by &apos;concurrent opening&apos; mapfiles(in fact, opening SequenceFiles of mapfile data).&lt;/p&gt;

&lt;p&gt;in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-24&quot; title=&quot;Scaling: Too many open file handles to datanodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-24&quot;&gt;&lt;del&gt;HBASE-24&lt;/del&gt;&lt;/a&gt;, stack advicing control open region number or open mapfile reader number, i&apos;d prefer contorlling opened mapfile reader directly, the core of regionserver resource usage. &lt;/p&gt;

&lt;p&gt;my suggestions of regionserver memory:&lt;br/&gt;
1. upgrade to hadoop 0.17.1(there&apos;s only one line incompatible with hadoop 0.17.1 in hbase 0.1.3, i&apos;ll file a issue seprately.), &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-2346&quot; title=&quot;DataNode should have timeout on socket writes.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-2346&quot;&gt;&lt;del&gt;HADOOP-2346&lt;/del&gt;&lt;/a&gt; resolved out of connection/thread in DataNode, using read/write timeout.&lt;br/&gt;
2. set globalMemcacheLimit to a lower size, if ur application didn&apos;t read recently inserted records frequently.&lt;br/&gt;
3. implment a MonitoredMapFileReader, it extends MapFile.reader, control cocurrent opening instances use LRU, checkin/checkout in every MapFile.Reader method. make HStoreFile.HbaseMapFile.HbaseReader extends MonitoredMapFileReader.&lt;/p&gt;

&lt;p&gt;further more the release 0.1.3, i think hbase need a interface like HStoreFileReader for abstracting file reading method, that will make open reader controlling more easier.&lt;/p&gt;</comment>
                            <comment id="12613627" author="viper799" created="Tue, 15 Jul 2008 14:55:26 +0000"  >&lt;p&gt;I agree on your idea of a incremental compaction&lt;/p&gt;

&lt;p&gt;My two ideas for increased efficiency in compaction while under load&lt;/p&gt;

&lt;p&gt;1. compact only the newest threshold(3) of mapfiles&lt;/p&gt;

&lt;p&gt;This will allow a region server to compact the lastest 3 map files created lowering the number of mapfile by 2 per compaction&lt;br/&gt;
the newest mapfile will not store the bulk of the data for a region if we are under load they will be small memcache flushes and compact fast.&lt;/p&gt;

&lt;p&gt;By doing the newest ones when the load reduces and there is only 3 map files left 1 will be the largest and oldest mapfile &lt;br/&gt;
and all old data and new data will get compacted together.&lt;/p&gt;

&lt;p&gt;2. The compaction queue&lt;br/&gt;
Currently we only add the region to a queued list of regions needing compaction check and compact in that order.&lt;/p&gt;

&lt;p&gt;My suggestion would be to have the queued list store how many times a region has been added to the compaction queued(memcache flushes)&lt;br/&gt;
That way we can sort the list and compact the hot spots under load and compact them first and reduce the number of map files the fastest with the above idea implemented.&lt;br/&gt;
When it is done with the compaction reduce the number in the queued by how many files we compacted or remove it if  left to compact and sort the list again start over.&lt;/p&gt;

&lt;p&gt;these are my ideas on how we can reduce the number of mapfiles we have while we are under a write load.&lt;/p&gt;</comment>
                            <comment id="12613641" author="ln@webcate.net" created="Tue, 15 Jul 2008 15:42:46 +0000"  >&lt;p&gt;incremental compaction patch for 0.1.3 release. i use a simple algorithm for automate selecting compacting files, described in source.&lt;/p&gt;

&lt;p&gt;sorry for no unit test case for this patch, i haven&apos;t learn how to prepare unit test data for such issues:&amp;lt;. in fact, this patch has worked about a week in my test env. most of compation time reduced to less than 5sec from 1min before.&lt;/p&gt;

&lt;p&gt;btw, i removed some modification to HStore.java 0.1.3 release version in this patch manually, those for hadoop 0.17.1 compatible.&lt;/p&gt;</comment>
                            <comment id="12613650" author="jimk" created="Tue, 15 Jul 2008 16:03:52 +0000"  >&lt;p&gt;With respect to MapFile extensions in HBase, see HStoreFile$HBaseMapFile, HStoreFile$BloomFilterMapFile and HStoreFile$HalfMapFileReader&lt;/p&gt;</comment>
                            <comment id="12613652" author="jimk" created="Tue, 15 Jul 2008 16:07:29 +0000"  >&lt;p&gt;I would also suggest that with respect to performance, you should focus on trunk and not 0.1.x because trunk has changed the internals of flushing and compaction quite a bit, and it is unlikely that performance improvements for 0.1.x will port easily to trunk.&lt;/p&gt;
</comment>
                            <comment id="12614291" author="ln@webcate.net" created="Thu, 17 Jul 2008 10:23:02 +0000"  >&lt;p&gt;&amp;gt; With respect to MapFile extensions in HBase, see HStoreFile$HBaseMapFile, HStoreFile$BloomFilterMapFile and HStoreFile$HalfMapFileReader&lt;/p&gt;

&lt;p&gt;i have noticed the inheritance between HStoreFile$xxxMapFile classes, since all xxxReader inheriting HStoreFile$HbaseMapFile$HbaseReader, it is a good point for us controlling all reading operations in HbaseReader, so my suggestion is let HbaseReader extends a new class(extends MapFile.Reader), we do limitations in there.&lt;/p&gt;</comment>
                            <comment id="12614292" author="ln@webcate.net" created="Thu, 17 Jul 2008 10:38:30 +0000"  >&lt;p&gt;about version: forget my code(patch) here, i want providing more infomations about hbase running(include patched results), that may helpful for further design and coding.  however, i think users of hbase need more stable and scaling on current release, if it can.&lt;/p&gt;

&lt;p&gt;i will read codes from trunk. is there any other discussion about memory and compaction i can read first, in jira or wiki?  &lt;/p&gt;</comment>
                            <comment id="12614357" author="stack" created="Thu, 17 Jul 2008 14:26:36 +0000"  >&lt;p&gt;LN: I&apos;m not sure I follow the above comment.  What you thinking?  Yes, hbase users need stability in 0.1.3 and in 0.2.  Lets experiment in 0.3.&lt;/p&gt;

&lt;p&gt;No discussion of memory or compaction other than what is in JIRAs.  Want to start up a wiki page that we can all hack on?&lt;/p&gt;

&lt;p&gt;FYI, Izaak is working on upgrading your patch so it works against TRUNK.&lt;/p&gt;</comment>
                            <comment id="12614429" author="ln@webcate.net" created="Thu, 17 Jul 2008 16:45:08 +0000"  >&lt;p&gt;maybe i&apos;m hungering for hbase stronger&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; i know Robustness and Scalabilit(in order) are focused by 0.2 release. and &quot;3TB of data on about ~50 nodes&quot; means 60G per regionserver, not very hard, each (default config) regionserver can handle 30G data on my testing server, by 0.1.3.&lt;/p&gt;

&lt;p&gt;i&apos;m trying to make regionserver handling more data, 1T? because i think the resource(memory, cpu) usage of a regionserver should not depends on existing data size, but active data size(read/write throughput). &lt;/p&gt;

&lt;p&gt;i think i found the bottlenecks(compaction eating cpu, open mapfiles eating memory), but NOT SURE my solution, so i paste here for review, esp. from Jim and Stack. &lt;/p&gt;

&lt;p&gt;here my &apos;total solution&apos;, i named it &apos;0.1.3/0.17.1 scalability pack&apos;:&lt;br/&gt;
1. patch &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-749&quot; title=&quot;make 0.1.x compatible with hadoop 0.17&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-749&quot;&gt;&lt;del&gt;HBASE-749&lt;/del&gt;&lt;/a&gt; for 0.17.1 compatible&lt;br/&gt;
2. patch &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-3778&quot; title=&quot;seek(long) in DFSInputStream should catch socket exception for retry later&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-3778&quot;&gt;&lt;del&gt;HADOOP-3778&lt;/del&gt;&lt;/a&gt; for a socket exception bug&lt;br/&gt;
3. &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-285&quot; title=&quot;limit concurrent connections(data serving thread) in one datanode&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-285&quot;&gt;&lt;del&gt;HADOOP-3779&lt;/del&gt;&lt;/a&gt; for concurrent connection limitation of datanode(patch not attached)&lt;br/&gt;
4. attached incremental compaction patch&lt;br/&gt;
5. a &quot;open mapfile reader&quot; limitaion patch, implemented my suggestion above, but looks not good, so havn&apos;t attach.&lt;/p&gt;

&lt;p&gt;with above and adjusting some config properties, i have my regionserver handling about 400G data now, with about 15G testing write throughput per day.&lt;/p&gt;

</comment>
                            <comment id="12614511" author="irubin" created="Thu, 17 Jul 2008 19:54:43 +0000"  >&lt;p&gt;I&apos;ve been looking over the issue, and I (and Stack) agree with LN and the changes proposed in his patch.  However, as Jim noted, we want to be focusing on 0.2 instead of 0.1.3.  I&apos;ve taken LN&apos;s patch and modified it slightly to fit into trunk (hbase-745-for-0.2.patch).  I&apos;ve also added several additional assertions to TestCompaction to account for the changes.&lt;/p&gt;

&lt;p&gt;All HBase tests passed successfully.  However, this patch SHOULD NOT be applied until after HBase-720 is resolved and it&apos;s patch (hbase-720.patch) is applied.  Both of these patches modify the same two files (HStore, TestCompaction), and they must be committed in the correct order (first 720, then 745).  &lt;/p&gt;</comment>
                            <comment id="12614537" author="viper799" created="Thu, 17 Jul 2008 21:36:08 +0000"  >&lt;p&gt;I tried to apply this patch for 0.2 to trunk but got an error I applied hbase-720 patch successfully first but this one failed.&lt;/p&gt;</comment>
                            <comment id="12615420" author="irubin" created="Mon, 21 Jul 2008 20:58:32 +0000"  >&lt;p&gt;Hi Billy,&lt;/p&gt;

&lt;p&gt;I can&apos;t seem to replicate this problem - I removed my local copies of HStore and TestCompaction, updated, and then applied hbase-745-for-0.2.patch (successfully).  The patch for HBase-720 was committed before you made your comment on Thursday (although the issue was only closed today) - is it possible that when you tried to apply the hbase-720 patch, you actually removed it by accident?  Maybe try what I did (remove the files, update, and re-apply 745) and see if it still doesn&apos;t work - let me know.&lt;/p&gt;</comment>
                            <comment id="12615487" author="viper799" created="Tue, 22 Jul 2008 01:09:59 +0000"  >&lt;p&gt;now that I thank about it I thank I tryed a older version of trunk my mistake it applys to trunk.&lt;/p&gt;

&lt;p&gt;I will run some bulk import test soon on it and see if the compaction&apos;s work out ok&lt;/p&gt;</comment>
                            <comment id="12615510" author="stack" created="Tue, 22 Jul 2008 04:29:45 +0000"  >&lt;p&gt;Billy, I&apos;m running tests too.  So far, it looks like the LN patch is an improvement.  Will report back when more data.  If its good &amp;#8211; should know tonight &amp;#8211; I&apos;ll apply it.&lt;/p&gt;</comment>
                            <comment id="12615531" author="stack" created="Tue, 22 Jul 2008 06:01:15 +0000"  >&lt;p&gt;I applied hbase-745-for-0.2.patch, Izaak&apos;s fixup of LN&apos;s original patch though little discernible improvement.&lt;/p&gt;

&lt;p&gt;Running the PerformanceEvaluation with the patch, we spent about 20% less time compacting in total but on test completion, there were 79 data files in the filesystem as opposed to 72 when I ran without the patch.  My guess is that after the 79 files became 72, there wouldn&apos;t be much of the 20% difference left over.&lt;/p&gt;

&lt;p&gt;Test ran for about 30 minutes running 8 concurrent MR clients writing 8M rows.&lt;/p&gt;</comment>
                            <comment id="12615532" author="stack" created="Tue, 22 Jul 2008 06:08:04 +0000"  >&lt;p&gt;Hmm.  Took another look.  Comparison is a little more complicated than I above suppose.   I did a recheck of the number of data files post completion of the without patch run, about ten minutes after it ended; about the same amount of time that had elapsed when I went to check the withpatch test.  The number of data files is rising as is the aggregate of all time spent compacting.  Would seem then that the patch cuts time spent compacting by some 10-20% or so in the test I just ran.&lt;/p&gt;</comment>
                            <comment id="12615538" author="viper799" created="Tue, 22 Jul 2008 07:01:32 +0000"  >&lt;p&gt;I lost my data I was running to test large import so re downloading I will be able to run my test on the patch in about 24 hours when I get done processing my dataset again.&lt;br/&gt;
Your last post sounds better and more correct if the patch is working correct we pick up efficiency when we do not have to compact the larger mapfiles with every compaction.&lt;br/&gt;
I would assume this will help with the users that still have 32bit server keep the region server under the 2gb limit by flushing a little more often if needed under load.&lt;/p&gt;</comment>
                            <comment id="12616918" author="viper799" created="Fri, 25 Jul 2008 15:01:05 +0000"  >&lt;p&gt;can not test patch will not apply to trunk.&lt;/p&gt;

&lt;p&gt;we should get this in to 2.0 f its showing good results like stack reported above&lt;/p&gt;</comment>
                            <comment id="12616937" author="viper799" created="Fri, 25 Jul 2008 16:34:54 +0000"  >&lt;p&gt;looks like this has been committed to trunk &lt;br/&gt;
seams to be improving my import speed because I am spending less time on compaction so I get more cpu time for transactions.&lt;br/&gt;
I use compression on my table so it improving my speed on compaction&apos;s by not havening to uncompress and re compress all the map files each compaction.&lt;br/&gt;
+1&lt;/p&gt;

&lt;p&gt;so we need to mark this issue done.&lt;/p&gt;</comment>
                            <comment id="12616962" author="stack" created="Fri, 25 Jul 2008 17:40:44 +0000"  >&lt;p&gt;Hey Billy:  Yeah, it was committed a while back.   In my comments above, I&apos;m not very enthusiastic because I did not see BIG gains in our simple PerformanceEvaluation.  But thinking on it more, Luo Ning&apos;s simple rule is kinda elegant and in real-life situations is probably saving truckloads of CPU and I/O.&lt;/p&gt;</comment>
                            <comment id="12621992" author="stack" created="Tue, 12 Aug 2008 21:34:25 +0000"  >&lt;p&gt;Bulk of this work was applied to 0.2.0.  I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-823&quot; title=&quot;Concurrent &amp;quot;open mapfile reader&amp;quot; limit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-823&quot;&gt;&lt;del&gt;HBASE-823&lt;/del&gt;&lt;/a&gt; to do Luo Ning&apos;s &apos;&quot;open mapfile reader&quot; limitation patch.  Thanks for the patch Luo (and Izaak).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12386395">HBASE-70</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12386068" name="HBASE-745.compact.patch" size="2594" author="ln@webcate.net" created="Tue, 15 Jul 2008 15:42:46 +0000"/>
                            <attachment id="12386341" name="hbase-745-for-0.2.patch" size="4421" author="irubin" created="Thu, 17 Jul 2008 19:54:43 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 15 Jul 2008 14:55:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31832</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 19 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0h96v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>98751</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>