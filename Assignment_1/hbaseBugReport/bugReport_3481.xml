<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:10:34 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3481/HBASE-3481.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3481] max seq id in flushed file can be larger than its correct value causing data loss during recovery</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3481</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;While doing some cluster kill tests, I noticed some missing data after log recovery. Upon investigating further, and pretty printing contents of HFiles and recovered logs, this is my analysis of the situation/bug. Please confirm the theory and pitch in with suggestions.&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;When memstores are flushed, the max sequence id recorded in  the HFile should be the max sequence id of all KVs in the memstore. However, we seem to simply obtain the current sequence id from the HRegion, and stamp the HFile&apos;s MAX_SEQ_ID with it.&lt;/p&gt;

&lt;p&gt;From HRegion.java:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    sequenceId = (wal == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;)? myseqid: wal.startCacheFlush();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where, startCacheFlush() is:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; startCacheFlush() {
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.cacheFlushLock.lock();
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; obtainSeqNum();
 }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where, obtainSeqNum() is simply: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; obtainSeqNum() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.logSeqNum.incrementAndGet();
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So let&apos;s say a memstore contains edits with sequence number 1..10.&lt;/p&gt;

&lt;p&gt;Meanwhile, say more Puts come along, and are going through this flow (in pseudo-code)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
1. HLog.append();
       1.1  obtainSeqNum()
       1.2 writeToWAL()

2 updateMemStore()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So it is possible that the sequence number has already been incremented to say 15 if there are 5 more outstanding puts. Say the writeToWAL() is still in progress for these puts. In this case, none of these edits (11..15) would have been written to memstore yet.&lt;/p&gt;

&lt;p&gt;At this point if a cache flush of the memstore happens, then we&apos;ll record its MAX_SEQ_ID as 16 in the store file instead of 10 (because that&apos;s what obtainSeqNum() would return as the next sequence number to use, right?).&lt;/p&gt;

&lt;p&gt;Assume that the edits 11..15 eventually complete. And so HLogs do contain the data for edits 11..15.&lt;/p&gt;

&lt;p&gt;Now, at this point if the region server were to crash, and we run log recovery, the splits all go through correctly, and a correct recovered.edits file is generated with the edits 11..15. &lt;/p&gt;

&lt;p&gt;Next, when the region is opened, the HRegion notes that one of the store file says MAX_SEQ_ID is 16. So, when it replays the recovered.edits file, it  skips replaying edits 11..15. Or in other words, data loss.&lt;/p&gt;

&lt;hr /&gt;


</description>
                <environment></environment>
        <key id="12496775">HBASE-3481</key>
            <summary>max seq id in flushed file can be larger than its correct value causing data loss during recovery</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ryanobjc">ryan rawson</assignee>
                                    <reporter username="kannanm">Kannan Muthukkaruppan</reporter>
                        <labels>
                    </labels>
                <created>Wed, 26 Jan 2011 08:46:20 +0000</created>
                <updated>Fri, 20 Nov 2015 12:40:38 +0000</updated>
                            <resolved>Thu, 27 Jan 2011 09:27:41 +0000</resolved>
                                                    <fixVersion>0.90.1</fixVersion>
                    <fixVersion>0.92.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                <comments>
                            <comment id="12986899" author="ryanobjc" created="Wed, 26 Jan 2011 08:58:50 +0000"  >&lt;p&gt;Your analysis sounds correct.  The correct thing to do here is to provide the MAX_SEQ_ID from the memstore KVs, not from the &quot;current&quot; HLog seqid.&lt;/p&gt;

&lt;p&gt;good find!&lt;/p&gt;</comment>
                            <comment id="12986900" author="kannanm" created="Wed, 26 Jan 2011 09:01:02 +0000"  >&lt;p&gt;Maybe the quickest fix is to avoid the &quot;skip&quot; optimization during replaying of recovered.edits.&lt;/p&gt;

&lt;p&gt;I think this should restore correctness. &lt;/p&gt;

&lt;p&gt;And with regards to HLog reclamation (i.e. an HLog should only be reclaimed if it contains no data for an active memstore), I don&apos;t think it relies on this MAX_SEQ_ID inside store files-- but rather on separate mechanism of what the min edit contained in each memstore is. So, probably that case is ok.&lt;/p&gt;</comment>
                            <comment id="12986907" author="ryanobjc" created="Wed, 26 Jan 2011 09:08:46 +0000"  >&lt;p&gt;if we avoid the skip behaviour, which isnt optimization, we will&lt;br/&gt;
introduce duplicate KVs into the HFiles, which for people who are&lt;br/&gt;
depending on the version count to keep a reasonable, important, number&lt;br/&gt;
of versions will be trouble for them.&lt;/p&gt;

&lt;p&gt;why not just nab the largest seqid during flush and provide it at the&lt;br/&gt;
end?  The footer is written out at close() time, and we have time&lt;br/&gt;
between the last KV being appened and close() being called to add a&lt;br/&gt;
correct SEQ_ID.&lt;/p&gt;</comment>
                            <comment id="12986911" author="kannanm" created="Wed, 26 Jan 2011 09:13:44 +0000"  >&lt;p&gt;Is the last seq id readily available from the memstore KVs or is already stashed away somewhere? I agree that that would be the cleanest/best fix.&lt;/p&gt;

&lt;p&gt;(Happy to accept  a patch if you want to post one up. Else, I&apos;ll look further on this  tomorrow morning).&lt;/p&gt;</comment>
                            <comment id="12986921" author="ryanobjc" created="Wed, 26 Jan 2011 09:26:43 +0000"  >&lt;p&gt;In HRegion.internalFlushCache we have this logic:&lt;/p&gt;

&lt;p&gt;    this.updatesLock.writeLock().lock();&lt;br/&gt;
    final long currentMemStoreSize = this.memstoreSize.get();&lt;br/&gt;
    List&amp;lt;StoreFlusher&amp;gt; storeFlushers = new&lt;br/&gt;
ArrayList&amp;lt;StoreFlusher&amp;gt;(stores.size());&lt;br/&gt;
    try {&lt;br/&gt;
      sequenceId = (wal == null)? myseqid: wal.startCacheFlush();&lt;br/&gt;
      completeSequenceId = this.getCompleteCacheFlushSequenceId(sequenceId);&lt;/p&gt;

&lt;p&gt;      for (Store s : stores.values()) &lt;/p&gt;
{
        storeFlushers.add(s.getStoreFlusher(completeSequenceId));
      }

&lt;p&gt;      // prepare flush (take a snapshot)&lt;br/&gt;
      for (StoreFlusher flusher : storeFlushers) &lt;/p&gt;
{
        flusher.prepare();
      }
&lt;p&gt;    } finally &lt;/p&gt;
{
      this.updatesLock.writeLock().unlock();
    }

&lt;p&gt;We take a write lock, no more puts/deletes/whatever can be done to this hregion.&lt;/p&gt;

&lt;p&gt;we then grab a seqid (wal.startCacheFlush).  We now snapshot everything.&lt;/p&gt;

&lt;p&gt;we then release the update lock and mutations can happen to the region again.&lt;/p&gt;

&lt;p&gt;The flush sequence id should lie exactly between the snapshot and the memstore.&lt;/p&gt;

&lt;p&gt;Given this code, I&apos;m not sure how to explain what you are seeing...&lt;br/&gt;
But this logic seems spot on and correct.&lt;/p&gt;

&lt;p&gt;On Wed, Jan 26, 2011 at 1:14 AM, Kannan Muthukkaruppan (JIRA)&lt;/p&gt;
</comment>
                            <comment id="12987027" author="kannanm" created="Wed, 26 Jan 2011 14:57:12 +0000"  >&lt;p&gt;Ryan: Your observation seems quite correct! So the outstanding puts case I was worried about shouldn&apos;t happen because of the updatesLock.&lt;/p&gt;

&lt;p&gt;So the mystery thickens &amp;#8211; I&apos;ll need to dig further as to why these edits got skipped during replay. The edits in question were definitely present in the recovered.edits file and had older sequence number than the HFile&apos;s MAX_SEQ_ID.&lt;/p&gt;</comment>
                            <comment id="12987083" author="kannanm" created="Wed, 26 Jan 2011 16:28:43 +0000"  >&lt;p&gt;Dug some more... the bad MAX_SEQ_ID that the HFile got stamped with corresponds to an edit that looks special.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;lt;METAROW/METAFAMILY:/1296023308559/Put/vlen=17; &amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Another interesting thing is that in the  .logs, this seq id  appears out of order...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
...
#1081, pos=6403894 kannan1/kannan1,,1296023211936.dfe45b39b1d8c360b8e7f554b576ab17./71170147=[#edits: 18 = &amp;lt;0992bdb67af8d2b2172618496243433c:287686/actions:0\/1296023308555/Put/vlen=597; 0992bdb67af8d2b2172618496243433c:287686/actions:1/1296023308555/Put/vlen=416; 0992bdb67af8d2b2172618496243433c:287686/actions:2\/1296023308555/Put/vlen=436; 0992bdb67af8d2b2172618496243433c:287686/actions:3/1296023308555/Put/vlen=329; 0992bdb67af8d2b2172618496243433c:287686/actions:4\/1296023308555/Put/vlen=256; 0992bdb67af8d2b2172618496243433c:287686/actions:5/1296023308555/Put/vlen=792; 0992bdb67af8d2b2172618496243433c:287686/actions:6\/1296023308555/Put/vlen=231; 0992bdb67af8d2b2172618496243433c:287686/actions:7/1296023308555/Put/vlen=790; 0992bdb67af8d2b2172618496243433c:287686/actions:8\/1296023308555/Put/vlen=800; 0992bdb67af8d2b2172618496243433c:287686/actions:9/1296023308555/Put/vlen=664; 0992bdb67af8d2b2172618496243433c:287686/actions:10\/1296023308555/Put/vlen=618; 0992bdb67af8d2b2172618496243433c:287686/actions:11/1296023308555/Put/vlen=533; 0992bdb67af8d2b2172618496243433c:287686/actions:12\/1296023308555/Put/vlen=636; 0992bdb67af8d2b2172618496243433c:287686/actions:13/1296023308555/Put/vlen=554; 0992bdb67af8d2b2172618496243433c:287686/actions:14\/1296023308555/Put/vlen=353; 0992bdb67af8d2b2172618496243433c:287686/actions:15/1296023308555/Put/vlen=307; 0992bdb67af8d2b2172618496243433c:287686/actions:16\/1296023308555/Put/vlen=410; 0992bdb67af8d2b2172618496243433c:287686/actions:17/1296023308555/Put/vlen=579; &amp;gt;]

#1082, pos=6404082 kannan1/kannan1,bbbbbbbb,1296023211937.0a3f2362b0264b0192c311f10c006d84./71169066{=[#edits: 1 = &amp;lt;METAROW/METAFAMILY:/1296023308559/Put/vlen=17; &amp;gt;]

#1083, pos=6406845 kannan1/kannan1,99999999,1296023211936.8ff83822490b1f8175dc16aaf4c170a6./71170148=[#edits: 4 = &amp;lt;a2adc743be4a350b7009af5143acfbf3:287685/actions:0/1296023308555/Put/vlen=552; a2adc743be4a35\
0b7009af5143acfbf3:287685/actions:1/1296023308555/Put/vlen=715; a2adc743be4a350b7009af5143acfbf3:287685/actions:2/1296023308555/Put/vlen=806; a2adc743be4a350b7009af5143acfbf3:287685/actions:3/1296023308555/P\
ut/vlen=296; &amp;gt;]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;The flushed file&apos;s MAX_SEQ_ID is recorded as: 71169066 (corresponding to entry #1082 above). However that edit is not a regular put  key, but this special row &amp;lt;METAROW/METAFAMILY:/1296023308559/Put/vlen=17; &amp;gt;]&lt;/p&gt;

&lt;p&gt;Also, notice out the sequence ids for edits surrounding it(entries #1081 &amp;amp; #1083) are both higher (namely, 71170147 &amp;amp; 71170148).&lt;/p&gt;

&lt;p&gt;Looks like this sequence id is being obtained outside of the updatesLock. But haven&apos;t confirmed that in the code.&lt;/p&gt;</comment>
                            <comment id="12987198" author="kannanm" created="Wed, 26 Jan 2011 20:16:31 +0000"  >&lt;p&gt;Ryan wrote: &amp;lt;&amp;lt;&amp;lt; We take a write lock, no more puts/deletes/whatever can be done to this hregion&amp;gt;&amp;gt;&amp;gt;.&lt;/p&gt;

&lt;p&gt;Interestingly, I don&apos;t see incrementColumnValue() taking the region&apos;s updatesLock, which seems like a problem too (but not the one I am running into &amp;#8211; since in my test I didn&apos;t use incrementColumnValue()).&lt;/p&gt;

&lt;p&gt;We can file the incrementColumnValue() issue as a separate JIRA (if I am not missing something obvious there).&lt;/p&gt;

&lt;p&gt;regards,&lt;br/&gt;
Kannan &lt;/p&gt;</comment>
                            <comment id="12987208" author="kannanm" created="Wed, 26 Jan 2011 20:38:56 +0000"  >&lt;p&gt;mutliPut() code path also doesn&apos;t seem to take updatesLock. I believe this is the bug I am hitting...&lt;/p&gt;</comment>
                            <comment id="12987209" author="kannanm" created="Wed, 26 Jan 2011 20:44:30 +0000"  >&lt;p&gt;Since single Put on client gets promoted to multiPut() , this bug affects both regular puts and increment column value.&lt;/p&gt;</comment>
                            <comment id="12987212" author="ryanobjc" created="Wed, 26 Jan 2011 20:55:00 +0000"  >&lt;p&gt;here is a patch adding in those locks&lt;/p&gt;</comment>
                            <comment id="12987241" author="kannanm" created="Wed, 26 Jan 2011 21:45:21 +0000"  >&lt;p&gt;You beat me to the patch Ryan &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Thanks. Patch looks good. I imported patch and am running unit tests and will also run some kill tests shortly.&lt;/p&gt;</comment>
                            <comment id="12987256" author="stack" created="Wed, 26 Jan 2011 22:11:59 +0000"  >&lt;p&gt;Marking a blocker because data loss (and since it seems like Kannan and Ryan figured it &amp;#8211; smile).  Bringing into 0.90.1.&lt;/p&gt;</comment>
                            <comment id="12987335" author="kannanm" created="Thu, 27 Jan 2011 01:32:35 +0000"  >&lt;p&gt;Unit tests ran well (I ran against 0.89) &amp;amp; so did the kill testing. I think we are good wrt to this patch.&lt;/p&gt;</comment>
                            <comment id="12987376" author="stack" created="Thu, 27 Jan 2011 04:32:50 +0000"  >&lt;p&gt;Marking patch available and assigning to Ryan.  Want to commit RR?&lt;/p&gt;</comment>
                            <comment id="12987449" author="ryanobjc" created="Thu, 27 Jan 2011 09:15:34 +0000"  >&lt;p&gt;ill commit in am&lt;/p&gt;</comment>
                            <comment id="12987688" author="tlipcon" created="Thu, 27 Jan 2011 18:18:29 +0000"  >&lt;p&gt;Any way we can build a test for this? (even if it doesn&apos;t fail reliably every time, would be good to catch bugs like this in the future)&lt;/p&gt;</comment>
                            <comment id="12987768" author="ryanobjc" created="Thu, 27 Jan 2011 21:24:45 +0000"  >&lt;p&gt;that is a good point I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3486&quot; title=&quot;investigate and fix why HBASE-3481 didn&amp;#39;t trigger unit tests&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3486&quot;&gt;&lt;del&gt;HBASE-3486&lt;/del&gt;&lt;/a&gt; to do that. We already have&lt;br/&gt;
durability unit tests, they should be fixed to fail w/o this patch.&lt;/p&gt;</comment>
                            <comment id="12987965" author="tlipcon" created="Fri, 28 Jan 2011 07:07:46 +0000"  >&lt;p&gt;BTW, think I ran into another ramification of this bug. I had a region split,&lt;br/&gt;
but even after the parent was closed, it remained in the &lt;tt&gt;lastSeqWritten&lt;/tt&gt;&lt;br/&gt;
map in HLog. So, my log shows:&lt;/p&gt;

&lt;p&gt;2011-01-27 11:34:21,784 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 5 regions(s): 1064fef659ccf634bfb6dbd24a3d3e32, 2e84570f830560a49276e236ebfb53ed, 61b89db95b24fbc6cabf6661cfbc9534, 6f219526af6c8a9e180324bd5d03a08b, d7e912ab428&lt;br/&gt;
2011-01-27 11:34:21,784 WARN org.apache.hadoop.hbase.regionserver.LogRoller: Failed to schedule flush of d7e912ab428b25dbb388edda934591a0r=null, requester=null&lt;br/&gt;
2011-01-27 11:34:45,904 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): d7e912ab428b25dbb388edda934591a0&lt;br/&gt;
2011-01-27 11:34:45,934 WARN org.apache.hadoop.hbase.regionserver.LogRoller: Failed to schedule flush of d7e912ab428b25dbb388edda934591a0r=null, requester=null&lt;br/&gt;
...&lt;br/&gt;
and I ended up with hundreds of HLogs. This was with a mixed put/ICV workload&lt;/p&gt;</comment>
                            <comment id="15016764" author="lars_francke" created="Fri, 20 Nov 2015 12:40:38 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12469468" name="HBASE-3481.txt" size="2593" author="ryanobjc" created="Wed, 26 Jan 2011 20:55:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 26 Jan 2011 08:58:50 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26880</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hmfr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100897</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>