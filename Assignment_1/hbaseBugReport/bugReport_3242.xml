<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:08:35 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3242/HBASE-3242.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3242] HLog Compactions</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3242</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently, our memstore flush algorithm is pretty trivial.  We let it grow to a flushsize and flush a region or grow to a certain log count and then flush everything below a seqid.  In certain situations, we can get big wins from being more intelligent with our memstore flush algorithm.  I suggest we look into algorithms to intelligently handle HLog compactions.  By compaction, I mean replacing existing HLogs with new HLogs created using the contents of a memstore snapshot.  Situations where we can get huge wins:&lt;/p&gt;

&lt;p&gt;1. In the incrementColumnValue case,  N HLog entries often correspond to a single memstore entry.  Although we may have large HLog files, our memstore could be relatively small.&lt;br/&gt;
2. If we have a hot region, the majority of the HLog consists of that one region and other region edits would be minuscule.&lt;/p&gt;

&lt;p&gt;In both cases, we are forced to flush a bunch of very small stores.  Its really hard for a compaction algorithm to be efficient when it has no guarantees of the approximate size of a new StoreFile, so it currently does unconditional, inefficient compactions.  Additionally, compactions &amp;amp; flushes suck because they invalidate cache entries: be it memstore or LRUcache.  If we can limit flushes to cases where we will have significant HFile output on a per-Store basis, we can get improved performance, stability, and reduced failover time.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12480169">HBASE-3242</key>
            <summary>HLog Compactions</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="8">Not A Problem</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="nspiegelberg">Nicolas Spiegelberg</reporter>
                        <labels>
                    </labels>
                <created>Wed, 17 Nov 2010 01:18:40 +0000</created>
                <updated>Sat, 19 Jul 2014 00:33:16 +0000</updated>
                            <resolved>Sat, 19 Jul 2014 00:33:16 +0000</resolved>
                                                                    <component>regionserver</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="12932758" author="nspiegelberg" created="Wed, 17 Nov 2010 01:21:31 +0000"  >&lt;p&gt;Request for comments.  If someone feels strongly about this, they can feel free to run with the idea.  I have enough on my plate in the short term.&lt;/p&gt;</comment>
                            <comment id="12932765" author="jdcryans" created="Wed, 17 Nov 2010 01:45:06 +0000"  >&lt;p&gt;I had something in mind like that for a while now, good on you for creating the jira. IMO there are huge gains in what you described, hopefully someone someday will have the time to make it happen &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12932772" author="nspiegelberg" created="Wed, 17 Nov 2010 02:03:30 +0000"  >&lt;p&gt;IRC communication below.  Highlights:&lt;/p&gt;

&lt;p&gt;1. ICV case is highest pri for a bunch of use cases.  It would be simpler to implement.  We could just address that first&lt;br/&gt;
2. Make sure we properly handle replication with any changes&lt;br/&gt;
3. &apos;compacting flush&apos; capability would be another plus while we&apos;re digging into compaction code&lt;br/&gt;
------------------------------------------------&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:21pm&amp;#93;&lt;/span&gt; tlipcon: hlog compaction? &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:22pm&amp;#93;&lt;/span&gt; nspiegelberg:   it&apos;s an idea that&apos;s been floating around in my head the past couple weeks.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:22pm&amp;#93;&lt;/span&gt; tlipcon: makes some sense&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:22pm&amp;#93;&lt;/span&gt; nspiegelberg: the BigTable paper actually mentions that it compacts logs. that got me started thinking about the idea&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:22pm&amp;#93;&lt;/span&gt; tlipcon: just gonna be tricky... all these heuristics&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:22pm&amp;#93;&lt;/span&gt; dj_ryan: being fully persisistent and high speed are hard to get&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:23pm&amp;#93;&lt;/span&gt; dj_ryan: i was thinking it might be possible to improve the speed of log reply&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:23pm&amp;#93;&lt;/span&gt; dj_ryan: in which case we could have more outstanding logs&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:23pm&amp;#93;&lt;/span&gt; nspiegelberg: well I think the purpose is to efficiently address edge cases&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:23pm&amp;#93;&lt;/span&gt; tlipcon: right, log replay and splitting are both kind of slow&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:23pm&amp;#93;&lt;/span&gt; nspiegelberg: if log entries were uniformly distributed, what we have now is perfect&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:23pm&amp;#93;&lt;/span&gt; tlipcon: nspiegelberg: I wonder how much the &quot;compacting flush&quot; would buy us&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:24pm&amp;#93;&lt;/span&gt; tlipcon: what BT calls minor compactions&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:24pm&amp;#93;&lt;/span&gt; nspiegelberg: that&apos;s another idea that jgray advocates&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:25pm&amp;#93;&lt;/span&gt; nspiegelberg: really, we can implement a trivial HLog compaction that is only usefull for ICV applications and it would be greatly beneficial for us&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:33pm&amp;#93;&lt;/span&gt; apurtell: &quot;a trivial HLog compaction that is only usefull for ICV applications&quot; &amp;#8211; seems a good start, we&apos;d find that useful and so would stumble i believe &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:35pm&amp;#93;&lt;/span&gt; nspiegelberg: we already have practical need both cases for HLog compaction, but the ICV application is definitely higher priority.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:38pm&amp;#93;&lt;/span&gt; nspiegelberg: yeah, the only thing I haven&apos;t researched is replication impact.  I imagine that we could handle HLog compactions independently on each cluster.  Then, flag the compacted HLogs and just not send them to the replica cluster.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:39pm&amp;#93;&lt;/span&gt; dj_ryan: we might want some jd input&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:39pm&amp;#93;&lt;/span&gt; dj_ryan: but basically there is a &apos;read point&apos; in a hlog for the replication sender&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:39pm&amp;#93;&lt;/span&gt; dj_ryan: so if you are compacting stuff that was already sent, we&apos;ll be ok&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:39pm&amp;#93;&lt;/span&gt; dj_ryan: and at the target, they&apos;d do similiar things i guess&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;5:40pm&amp;#93;&lt;/span&gt; nspiegelberg: definitely.  RFC.  I have migration woes right now, but I wanted to get the idea out there and have it running through ppls heads &lt;/p&gt;</comment>
                            <comment id="12970038" author="stack" created="Fri, 10 Dec 2010 01:17:32 +0000"  >&lt;p&gt;What about doing the following (I thought this was what you were describing Nicolas but it seems like you are describing something else).  When we hit N HLogs, we currently flush those MemStores that hold the oldest edits.  What if in stead, we ran through the oldest M HLogs and rewrote them all into one new HLog file dropping edits that have made it into StoreFiles: i.e. those whose seqid is &amp;lt; what is the oldest seqid on the regionserver.  We&apos;d then atomically swap out the N old HLogs, moving aside for replication or snapshotting or whatever but the HRS would now replace the N HLogs w/ the newly written HLog in its HLog accounting.  This would not be a compaction, more a cleaning process.&lt;/p&gt;</comment>
                            <comment id="12970040" author="streamy" created="Fri, 10 Dec 2010 01:23:23 +0000"  >&lt;p&gt;I think compaction and cleaning are the same thing, no?  Or at least the &lt;em&gt;primary&lt;/em&gt; point of these things is to make them smaller / evict edits rather than reducing the number of files.  Both are about rewriting the files to end up with smaller hlogs in the end.&lt;/p&gt;

&lt;p&gt;I&apos;d say both of these optimizations could be done at the same time and both seem like a good idea.&lt;/p&gt;

&lt;p&gt;The Nicolas idea is mostly beneficial for increment-type workloads where you have a lot of updates and only care about the latest version.  Stack idea should have some impact on nearly all use cases but it&apos;s not clear to me if it would be a clear win because of the added overhead.  On under-utilized clusters with extra IO available, I imagine it would be a win.  If you&apos;re at all IO bound, maybe not.&lt;/p&gt;</comment>
                            <comment id="12970041" author="streamy" created="Fri, 10 Dec 2010 01:28:46 +0000"  >&lt;p&gt;Reading my own comment, I as a little confused.  I&apos;m not saying reducing number of files is not good or what would happen, just that, the real goal is reducing the need to evict hlogs.  That is done by reducing the aggregate size of the hlogs (at least in principle... we use # of logs today but might need to tweak that with this stuff since real trade-off is about recovery time and that&apos;s largely related to size not #files).&lt;/p&gt;</comment>
                            <comment id="12970044" author="nspiegelberg" created="Fri, 10 Dec 2010 01:41:39 +0000"  >&lt;p&gt;@stack: that&apos;s exactly what I was thinking for use case #2.  by compaction, I really meant the abstract principles of unification + pruning, which is what our HFile compactions do.&lt;/p&gt;

&lt;p&gt;I disagree that this technique only would give benefits to clusters with extra IO.  If an HLog compaction can delay 10 StoreFiles from being created, that&apos;s 10 IO-heavy compactions that we&apos;re also delaying.  Also, we have the minCompactThreshold to keep StoreFile count low in exchange for IO-inefficient compactions, which would be mitigated by HLog compaction.  The main IO problem with HLog compactions vs. just creating new HFiles is compression, which we should be able to easily add to offlined HLogs.&lt;/p&gt;</comment>
                            <comment id="12970049" author="nspiegelberg" created="Fri, 10 Dec 2010 02:11:29 +0000"  >&lt;p&gt;so, talking about this internally.  I wasn&apos;t 100% thinking along Stack&apos;s lines.  My assumption is that we could support per-CF flushing of MemStore and would therefore have different seqno per-CF to prune on.  This would prevent a slow-growing CF from being flushed until it reaches a significant size.&lt;/p&gt;

&lt;p&gt;another thing to keep in mind: &lt;/p&gt;

&lt;p&gt;HFile compaction = read HFiles + merge + write new HFIles&lt;br/&gt;
HLog compaction = snapshot MemStore + prune + write aggregate HLog.&lt;/p&gt;

&lt;p&gt;so HLog compaction only adds write IO, not read IO.  All said, Karthik&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3327&quot; title=&quot;For increment workloads, retain memstores in memory after flushing them&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3327&quot;&gt;&lt;del&gt;HBASE-3327&lt;/del&gt;&lt;/a&gt; suggestion would be much easier to implement in the short term since HFile compactions would require merging the snapShot MemStore + current MemStore after compaction has finished.&lt;/p&gt;</comment>
                            <comment id="14067287" author="apurtell" created="Sat, 19 Jul 2014 00:33:16 +0000"  >&lt;p&gt;No activity for a long time&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Wed, 17 Nov 2010 01:45:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>32956</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 21 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02fjb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12118</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>