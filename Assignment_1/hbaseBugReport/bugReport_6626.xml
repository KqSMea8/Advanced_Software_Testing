<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:38:26 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6626/HBASE-6626.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6626] Add a chapter on HDFS in the troubleshooting section of the HBase reference guide.</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6626</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;I looked mainly at the major failure case, but here is what I have:&lt;/p&gt;

&lt;p&gt;New sub chapter in the existing chapter &quot;Troubleshooting and Debugging HBase&quot;: &quot;HDFS &amp;amp; HBASE&quot;&lt;/p&gt;

&lt;p&gt;1) HDFS &amp;amp; HBase&lt;br/&gt;
2) Connection related settings&lt;br/&gt;
2.1) Number of retries&lt;br/&gt;
2.2) Timeouts&lt;br/&gt;
3) Log samples&lt;/p&gt;


&lt;p&gt;1) HDFS &amp;amp; HBase&lt;br/&gt;
HBase uses HDFS to store its HFile, i.e. the core HBase files and the Write-Ahead-Logs, i.e. the files that will be used to restore the data after a crash.&lt;br/&gt;
In both cases, the reliability of HBase comes from the fact that HDFS writes the data to multiple locations. To be efficient, HBase needs the data to be available locally, hence it&apos;s highly recommended to have the HDFS datanode on the same machines as the HBase Region Servers.&lt;/p&gt;

&lt;p&gt;Detailed information on how HDFS works can be found at &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Important features are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;HBase is a client application of HDFS, i.e. uses the HDFS DFSClient class. This class can appears in HBase logs with other HDFS client related logs.&lt;/li&gt;
	&lt;li&gt;Some HDFS settings are HDFS-server-side, i.e. must be set on the HDFS side, while some other are HDFS-client-side, i.e. must be set in HBase, while some other must be set in both places.&lt;/li&gt;
	&lt;li&gt;the HDFS writes are pipelined from one datanode to another. When writing, there are communications between:&lt;/li&gt;
	&lt;li&gt;HBase and HDFS namenode, through the HDFS client classes.&lt;/li&gt;
	&lt;li&gt;HBase and HDFS datanodes, through the HDFS client classes.&lt;/li&gt;
	&lt;li&gt;HDFS datanode between themselves: issues on these communications are in HDFS logs, not HBase. HDFS writes are always local when possible. As a consequence, there should not be much write error in HBase Region Servers: they write to the local datanode. If this datanode can&apos;t replicate the blocks, it will appear in its logs, not in the region servers logs.&lt;/li&gt;
	&lt;li&gt;datanodes can be contacted through the ipc.Client interface (once again this class can shows up in HBase logs) and the data transfer interface (usually shows up as the DataNode class in the HBase logs). There are on different ports (defaults being: 50010 and 50020).&lt;/li&gt;
	&lt;li&gt;To understand exactly what&apos;s going on, you must look that the HDFS log files as well: HBase logs represent the client side.&lt;/li&gt;
	&lt;li&gt;With the default setting, HDFS needs 630s to mark a datanode as dead. For this reason, this node will still be tried by HBase or by other datanodes when writing and reading until HDFS definitively decides it&apos;s dead. This will add some extras lines in the logs. This monitoring is performed by the NameNode.&lt;/li&gt;
	&lt;li&gt;The HDFS clients (i.e. HBase using HDFS client code) don&apos;t fully rely on the NameNode, but can mark temporally a node as dead if they had an error when they tried to use it.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2) Settings for retries and timeouts&lt;br/&gt;
2.1) Retries&lt;br/&gt;
ipc.client.connect.max.retries&lt;br/&gt;
Default 10&lt;br/&gt;
Indicates the number of retries a client will make to establish a server connection. Not taken into account if the error is a SocketTimeout. In this case the number of retries is 45 (fixed on branch, &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-7932&quot; title=&quot;HA : Make client connection retries on socket time outs configurable.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-7932&quot;&gt;&lt;del&gt;HADOOP-7932&lt;/del&gt;&lt;/a&gt; or in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-7397&quot; title=&quot;Allow configurable timeouts when connecting to HDFS via java FileSystem API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-7397&quot;&gt;&lt;del&gt;HADOOP-7397&lt;/del&gt;&lt;/a&gt;). For SASL, the number of retries is hard-coded to 15. Can be increased, especially if the socket timeouts have been lowered.&lt;/p&gt;

&lt;p&gt;ipc.client.connect.max.retries.on.timeouts&lt;br/&gt;
Default 45&lt;br/&gt;
If you have &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-7932&quot; title=&quot;HA : Make client connection retries on socket time outs configurable.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-7932&quot;&gt;&lt;del&gt;HADOOP-7932&lt;/del&gt;&lt;/a&gt;, max number of retries on timeout. Counter is different than ipc.client.connect.max.retries so if you mix the socket errors you will get 55 retries with the default values. Could be lowered, once it is available. With &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-7397&quot; title=&quot;Allow configurable timeouts when connecting to HDFS via java FileSystem API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-7397&quot;&gt;&lt;del&gt;HADOOP-7397&lt;/del&gt;&lt;/a&gt; ipc.client.connect.max.retries is reused so there would be 10 tries.&lt;/p&gt;

&lt;p&gt;dfs.client.block.write.retries&lt;br/&gt;
Default 3&lt;br/&gt;
Number of tries for the client when writing a block. After a failure, will connect to the namenode a get a new location, sending the list of the datanodes already tried without success. Could be increased, especially if the socket timeouts have been lowered. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6490&quot; title=&quot;&amp;#39;dfs.client.block.write.retries&amp;#39; value could be increased in HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6490&quot;&gt;HBASE-6490&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;dfs.client.block.write.locateFollowingBlock.retries&lt;br/&gt;
Default 5&lt;br/&gt;
Number of retries to the namenode when the client got NotReplicatedYetException, i.e. the existing nodes of the files are not yet replicated to dfs.replication.min. This should not impact HBase, as dfs.replication.min is defaulted to 1.&lt;/p&gt;

&lt;p&gt;dfs.client.max.block.acquire.failures&lt;br/&gt;
Default 3&lt;br/&gt;
Number of tries to read a block from the datanodes list. In other words, if 5 datanodes are supposed to hold a block (so dfs.replication equals to 5), the client will try all these datanodes, then check the value of dfs.client.max.block.acquire.failures to see if it should retry or not. If so, it will get a new list (likely the same), and will try to reconnect again to all these 5 datanodes. COuldbe be increased, especially if the socket timeouts have been lowered.&lt;/p&gt;


&lt;p&gt;2.2) Timeouts&lt;br/&gt;
2.3.1) Heatbeats&lt;br/&gt;
dfs.heartbeat.interval&lt;br/&gt;
 Default is 3s&lt;/p&gt;

&lt;p&gt;heartbeat.recheck.interval = 300s&lt;br/&gt;
 Defaults is 300S&lt;/p&gt;

&lt;p&gt;A datanode is considered as dead when there is no heartbeat for (2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval) seconds. That&apos;s 630s.  So before the 10:30 minutes, the datanode is considered as fully available by the namenode.  After this delay, HDFS is likely to start replicating the blocks contained in the dead node to get back to the right number of replica. As a consequence, if we&apos;re too aggressive we will have a side effect here, adding workload to an already damaged cluster. For this reason it&apos;s not recommended to change these settings.&lt;/p&gt;

&lt;p&gt;As there are communications between the datanodes, and as they share these settings, these settings are both HDFS-client-side and HDFS-server-side.&lt;/p&gt;

&lt;p&gt;2.3.2) Socket timeouts&lt;br/&gt;
3 timeouts are considered in HDFS:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;connect timeout: the timeout when we tried to establish the connection&lt;/li&gt;
	&lt;li&gt;read timeout: the timeout when we read something on an already established connection&lt;/li&gt;
	&lt;li&gt;write timeout: the timeout when we try to write something on an already established connection.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;They are managed by two settings:&lt;/p&gt;

&lt;p&gt;dfs.socket.timeout&lt;br/&gt;
Default 60s&lt;/p&gt;

&lt;p&gt;dfs.datanode.socket.write.timeout&lt;br/&gt;
Default is 480s.&lt;/p&gt;

&lt;p&gt;But these setting are used:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;between the DFSClient and the datanode&lt;/li&gt;
	&lt;li&gt;between the ipc.Client and the datanodes&lt;/li&gt;
	&lt;li&gt;Between the datanodes&lt;/li&gt;
	&lt;li&gt;sometimes but not always with an extension (depending on the number of replica)&lt;/li&gt;
	&lt;li&gt;for dfs.socket.timeout as a socket connect timeout but as well as a socket read timeout.&lt;/li&gt;
	&lt;li&gt;for dfs.datanode.socket.write.timeout, when it&apos;s set to 0, a plain old java socket is created in some cases instead of a NIO.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;final calculated connect timeout can be:&lt;br/&gt;
 hard-coded to 20s for the the ipc.Client in Hadoop 1.0.3 (changed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-7397&quot; title=&quot;Allow configurable timeouts when connecting to HDFS via java FileSystem API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-7397&quot;&gt;&lt;del&gt;HADOOP-7397&lt;/del&gt;&lt;/a&gt;)&lt;br/&gt;
 dfs.socket.timeout  (ex: DataNode#DataTransfer, DataXceiver#replaceBlock)&lt;br/&gt;
 dfs.socket.timeout + 3s*#replica  (ex: DataXceiver#write, DFSClient#getFileChecksum called from FileCheckSumServlet)&lt;/p&gt;

&lt;p&gt;final read timeouts can be:&lt;br/&gt;
 dfs.socket.timeout  (DataXceiver#replaceBlock, ipc.Client from DFSClient)&lt;br/&gt;
 dfs.socket.timeout +  3s*#replica  (ex: DataNode#DataTransfer, DataXceiver#write)&lt;br/&gt;
 dfs.socket.timeout * #replica (ex: DataNode#DataTransfer)&lt;/p&gt;

&lt;p&gt;final calculated write timeouts can be:&lt;br/&gt;
 dfs.datanode.socket.write.timeout (ex DataXceiver#copyBlock/readBlock/...)&lt;br/&gt;
 dfs.datanode.socket.write.timeout + 5s*#replica) (ex DFSClient#createBlockOutputStream, DataXceiver#writeBlock)&lt;br/&gt;
 dfs.datanode.socket.write.timeout + 5s*(#replica -1) (ex: DataNode#DataTransfer. See &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-5464&quot; title=&quot;DFSClient does not treat write timeout of 0 properly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-5464&quot;&gt;&lt;del&gt;HADOOP-5464&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Hence we will often see a 69000 timeout in the logs before the datanode is marked dead/excluded. Also, setting &quot;dfs.socket.timeout&quot; to 0 does not make it wait forever, but likely 9 seconds instead of 69s for data transfer.&lt;/p&gt;


&lt;p&gt;3) Typical error logs.&lt;br/&gt;
3.1) Typical logs when all datanode for a block are dead, making the HBase recovery impossible. HBase master logs will contain, with a 0.90 HBase:&lt;br/&gt;
INFO HDFS.DFSClient: Failed to connect to /xxx50010, add to deadNodes and continue java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel&lt;span class=&quot;error&quot;&gt;&amp;#91;connection-pending remote=/region-server-1:50010&amp;#93;&lt;/span&gt;&lt;br/&gt;
=&amp;gt; The client tries to connect to a dead datanode.&lt;br/&gt;
=&amp;gt; It failed, so the client will try the next datanode in the list. Usually the list size is 3 (dfs.replication).&lt;br/&gt;
=&amp;gt; If the final list is empty, it means that all the datanodes proposed by the namenode are in our datanodes list.&lt;br/&gt;
=&amp;gt; The HDFS client clears the dead nodes list and sleeps 3 seconds (hard-coded), shallowing InterruptedException, and asks again to the namenode. This is the log line:&lt;br/&gt;
INFO HDFS.DFSClient: Could not obtain block blk_xxx from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...&lt;br/&gt;
=&amp;gt; All the locations initially given by the namenode to this client are actually dead. The client asks for a new set of locations.&lt;br/&gt;
=&amp;gt; We&apos;re very likely to have exactly the same datanode list as 3 seconds ago, except if a Datanode came back to life or if a replication has just finished.&lt;br/&gt;
=&amp;gt; After dfs.client.max.block.acquire.failures (default: 3), an exception is thrown, then logged, and we have in the logs:&lt;br/&gt;
WARN HDFS.DFSClient: DFS Read: java.io.IOException: Could not obtain block: blk_xxx file=/hbase/.logs/boxxxx,60020,xxx/xxx%3A60020.yyy&lt;br/&gt;
=&amp;gt; There is another retry, hard-coded to 2, but this is logged only once, even if the second try fails.&lt;br/&gt;
=&amp;gt; Moreover, for the second try the errors counters are not reinitialized, including the dead nodes list, so this second attempt is unlikely to succeed. It should come again with an empty node list, and throw a new java.io.IOException: Could not obtain block: blk_xxx file=/hbase/.logs/boxxxx,60020,xxx/xxx%3A60020.yyy&lt;br/&gt;
=&amp;gt; This exception will go to the final client (hbase). HBase will log it, and we will see&lt;br/&gt;
INFO wal.HLogSplitter: Got while parsing hlog HDFS://namodenode:8020//hbase/.logs/boxxxx,60020,xxx/xxx60020.yyy. Marking as corrupted java.io.IOException: Could not obtain block: blk_xxx file=/hbase/.logs/boxxxx,60020,xxx/xxx60020.yyy&lt;/p&gt;


&lt;p&gt;3.2) Typical log for write issues: the master reads the log, then wants to split it, hence writing a block:&lt;br/&gt;
INFO org.apache.hadoop.HDFS.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: 69000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel&lt;span class=&quot;error&quot;&gt;&amp;#91;connection-pending remote=/ xxx:50010&amp;#93;&lt;/span&gt;&lt;br/&gt;
=&amp;gt; We tried to connect to the dead datanode to write. Likely from the master (it does not have a datanode, so it connects to a remote datanode).&lt;br/&gt;
=&amp;gt; A region server will not have this type of error, as it connects to a local datanode to write.&lt;br/&gt;
=&amp;gt; It failed at the beginning, we cannot connect at all (i.e. not during the write itself)&lt;br/&gt;
INFO HDFS.DFSClient: Abandoning block blk_xxx&lt;br/&gt;
=&amp;gt; HBase (as a HDFS client) told to the namenode that the block is not written.&lt;br/&gt;
INFO HDFS.DFSClient: Excluding datanode xxx:50010&lt;br/&gt;
=&amp;gt; Internally in HDFS client the stream puts it in the excludedNodes list (the &quot;Excluding datanode&quot; log line ).&lt;br/&gt;
=&amp;gt; The HDFS client is going again to the namenode asking for another datanode set proposal, sending the excluded datanode list to be sure it&apos;s not trying on the same nodes again.&lt;br/&gt;
=&amp;gt; There will be 3 retries by default. If you&apos;ve lost 20% of your cluster 1% of the time the 3 attempts will fail. Setting: &quot;dfs.client.block.write.retries&quot;. If it&apos;s the case (i.e. all attempts failed), next log line is:&lt;br/&gt;
WARN HDFS.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block. And then, if it was for a split log:&lt;br/&gt;
FATAL wal.HLogSplitter: WriterThread-xxx Got while writing log entry to log (various possible stacks here)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12604213">HBASE-6626</key>
            <summary>Add a chapter on HDFS in the troubleshooting section of the HBase reference guide.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.png">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="misty">Misty Stanley-Jones</assignee>
                                    <reporter username="nkeywal">Nicolas Liochon</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 Aug 2012 13:33:27 +0000</created>
                <updated>Sat, 21 Feb 2015 23:34:50 +0000</updated>
                            <resolved>Thu, 7 Aug 2014 20:34:24 +0000</resolved>
                                    <version>0.95.2</version>
                                    <fixVersion>0.99.0</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>documentation</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                                <comments>
                            <comment id="13438847" author="stack" created="Tue, 21 Aug 2012 16:51:26 +0000"  >&lt;p&gt;Started converting to docbook.&lt;/p&gt;

&lt;p&gt;Nicolas, you are missing the &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; link below.  What did you intend to point to?&lt;/p&gt;</comment>
                            <comment id="13438849" author="stack" created="Tue, 21 Aug 2012 16:51:53 +0000"  >&lt;p&gt;Doug, you want to take on this one?&lt;/p&gt;</comment>
                            <comment id="13438858" author="nkeywal" created="Tue, 21 Aug 2012 17:02:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Nicolas, you are missing the &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; link below. What did you intend to point to?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oops. &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://www.aosabook.org/en/hdfs.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.aosabook.org/en/hdfs.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13441420" author="stack" created="Fri, 24 Aug 2012 19:21:26 +0000"  >&lt;p&gt;Assigning Doug since this gentleman said he&apos;d take this on (offline).&lt;/p&gt;</comment>
                            <comment id="13497367" author="varunsharma" created="Wed, 14 Nov 2012 19:26:33 +0000"  >&lt;p&gt;This is super useful. One thing I wanted to point out was:&lt;br/&gt;
Even though a live Region server will not have issues while trying to write WAL appends since it would go to a local DataNode, we would still wait for the local DataNode to pipeline the write. If the write is pipelined to the failed data node, this would again fail with a conservative timeout of 60 seconds or so, thus leading to possibly failed reads and writes for these other DataNodes until we mark this as a &quot;DeadNode&quot;. I am curious if reads would also depict similar behaviour ?&lt;/p&gt;

&lt;p&gt;So, I was curious on what the behaviour would be for failing pipelined writes at the DataNode and how long it would take to mark the node as Dead (I presume its the connect timeout) ? Also, how long would it be before the deadNode marking is removed ?&lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Varun&lt;/p&gt;</comment>
                            <comment id="13580500" author="jrkinley" created="Mon, 18 Feb 2013 09:00:45 +0000"  >&lt;p&gt;This is really useful. A couple of suggestions: it might be easier to digest if you describe the various HDFS communication points along the lines of the write path. For example:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Client issues Put to a region server, which writes the edit to the WAL, which appends to HLog in HDFS
	&lt;ul&gt;
		&lt;li&gt;This uses DFSClient&lt;/li&gt;
		&lt;li&gt;HLog writes are pipelined to X number of datanodes - based on replication factor&lt;/li&gt;
		&lt;li&gt;HLogs are rolled at intervals (60 minutes), which requires interaction with HDFS&lt;/li&gt;
		&lt;li&gt;Indicate which timeout and retry options apply here&lt;/li&gt;
		&lt;li&gt;...&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Same for HFiles...&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In addition to this, and the typical error messages, it would also be great to describe what to look out for in certain scenarios. For example:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;What happens when a datanode goes bad (you&apos;ve touched on this already)
	&lt;ul&gt;
		&lt;li&gt;How does this affect a region server?&lt;/li&gt;
		&lt;li&gt;What to look out for in the logs?&lt;/li&gt;
		&lt;li&gt;What configuration options apply?&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;What happens when the active namenode goes bad, and what happens during namenode failover?
	&lt;ul&gt;
		&lt;li&gt;A region server needs to communicate with the namenode when it rolls it&apos;s WAL, flushes it&apos;s store files, etc&lt;/li&gt;
		&lt;li&gt;In between, it only communicates with the datanodes to append to the WAL&lt;/li&gt;
		&lt;li&gt;What configuration options apply?&lt;/li&gt;
		&lt;li&gt;What to look out for in the logs?&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks,&lt;br/&gt;
James.&lt;/p&gt;</comment>
                            <comment id="13580533" author="nkeywal" created="Mon, 18 Feb 2013 10:37:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=varunsharma&quot; class=&quot;user-hover&quot; rel=&quot;varunsharma&quot;&gt;Varun Sharma&lt;/a&gt;&lt;br/&gt;
Sorry, I didn&apos;t see your comment before. For writes, the decision to mark a datanode as dead is taken locally. For a new write (i.e. no connection established), yes it will be the connect timeout that will be used. IIRC, it retries 3 times in this case.&lt;br/&gt;
The dead node is per client file handler, so if you open a new file, you may go to the same server and fail again.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jrkinley&quot; class=&quot;user-hover&quot; rel=&quot;jrkinley&quot;&gt;James Kinley&lt;/a&gt;&lt;br/&gt;
Yes, I agree with your comments.&lt;br/&gt;
I plan to write an integration test with a namenode failure and to document the setting from &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-3703&quot; title=&quot;Decrease the datanode failure detection time&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-3703&quot;&gt;&lt;del&gt;HDFS-3703&lt;/del&gt;&lt;/a&gt; from an HBase point of view.&lt;/p&gt;</comment>
                            <comment id="13991316" author="misty" created="Tue, 6 May 2014 23:20:30 +0000"  >&lt;p&gt;What&apos;s the status of this? It is marked as a blocker and looks like there is a lot of info in the comments.&lt;/p&gt;</comment>
                            <comment id="13992142" author="stack" created="Wed, 7 May 2014 19:27:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=misty&quot; class=&quot;user-hover&quot; rel=&quot;misty&quot;&gt;Misty Stanley-Jones&lt;/a&gt; Not done.  I don&apos;t think we need to add a &apos;chapter&apos; but rather just a &apos;section&apos; to this troubleshooting chapter: &lt;a href=&quot;http://hbase.apache.org/book.html#trouble&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hbase.apache.org/book.html#trouble&lt;/a&gt;  Want me to go over this issue and make up &apos;source material&apos; for you to shred?&lt;/p&gt;</comment>
                            <comment id="13993551" author="nkeywal" created="Fri, 9 May 2014 12:16:49 +0000"  >&lt;p&gt;Yeah, the issue is that this content is now two years old, and may not be up to date. I do remember that i tried to write something that would work for both hadoop 1 &amp;amp; hadoop 2, it may be simpler now as we just need hadoop 2. However, I guess that 1 or 2 days of works are necessary to recheck the content... I just don&apos;t have them now. Marking this as &quot;won&apos;t fix&quot; would make sense imho...&lt;/p&gt;</comment>
                            <comment id="13995674" author="misty" created="Mon, 12 May 2014 21:44:33 +0000"  >&lt;p&gt;Stack, yes please. Unless you agree with Nicholas.&lt;/p&gt;</comment>
                            <comment id="14057011" author="misty" created="Thu, 10 Jul 2014 01:42:59 +0000"  >&lt;p&gt;I made an attempt. I did not integrate the info in the comments but I did check the initial content and updated the Hadoop parameters and defaults where needed. I left a couple of the parameters out because they didn&apos;t seem to exist anymore or were marked as &apos;expert&apos; in the HDFS config docs. I would consider &apos;expert&apos; parameters for HDFS to be out of scope and possibly dangerous for HBase to recommend tweaking. WDYT?&lt;/p&gt;</comment>
                            <comment id="14057030" author="hadoopqa" created="Thu, 10 Jul 2014 01:58:04 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12654912/HBASE-6626.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12654912/HBASE-6626.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12654912&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+0 tests included&lt;/font&gt;.  The patch appears to be a documentation patch that doesn&apos;t require tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/10009//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/10009//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14057069" author="misty" created="Thu, 10 Jul 2014 03:02:05 +0000"  >&lt;p&gt;Re-generated the patch. I can apply it to the current master so I&apos;m not sure what is wrong.&lt;/p&gt;</comment>
                            <comment id="14057075" author="hadoopqa" created="Thu, 10 Jul 2014 03:11:34 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12654923/HBASE-6626.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12654923/HBASE-6626.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;br/&gt;
  ATTACHMENT ID: 12654923&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+0 tests included&lt;/font&gt;.  The patch appears to be a documentation patch that doesn&apos;t require tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/10011//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/10011//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14087025" author="misty" created="Wed, 6 Aug 2014 00:29:53 +0000"  >&lt;p&gt;Ping, can anyone please review this?&lt;/p&gt;</comment>
                            <comment id="14089578" author="jmhsieh" created="Thu, 7 Aug 2014 18:34:46 +0000"  >&lt;p&gt;I&apos;m taking a look.&lt;/p&gt;</comment>
                            <comment id="14089791" author="jmhsieh" created="Thu, 7 Aug 2014 20:33:55 +0000"  >&lt;p&gt;Lgtm misty. &lt;/p&gt;

&lt;p&gt;I&apos;m going to commit this &amp;#8211; it is an improvement on the current vacuous state.  We can add more improvements in follow up issues.&lt;/p&gt;

&lt;p&gt;committed to trunk/branch-1.  Doesn&apos;t apply cleanly to 0.98 but i think we are ok just committing to docs to branch-1 master since that is where they are pulled from these days.&lt;/p&gt;</comment>
                            <comment id="14090055" author="hudson" created="Thu, 7 Aug 2014 23:53:53 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.0 #90 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.0/90/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.0/90/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6626&quot; title=&quot;Add a chapter on HDFS in the troubleshooting section of the HBase reference guide.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6626&quot;&gt;&lt;del&gt;HBASE-6626&lt;/del&gt;&lt;/a&gt; Add a chapter on HDFS in the troubleshooting section of the HBase reference guide (Misty Stanley-Jones) (jmhsieh: rev 28e6aedf534ca3a7bfb7395544a4435fb763c772)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;src/main/docbkx/troubleshooting.xml&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14090109" author="hudson" created="Fri, 8 Aug 2014 00:49:26 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-TRUNK #5382 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/5382/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/5382/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6626&quot; title=&quot;Add a chapter on HDFS in the troubleshooting section of the HBase reference guide.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6626&quot;&gt;&lt;del&gt;HBASE-6626&lt;/del&gt;&lt;/a&gt; Add a chapter on HDFS in the troubleshooting section of the HBase reference guide (Misty Stanley-Jones) (jmhsieh: rev 5848710aa707c41d1c3ecaa1442859ab2b9f878c)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;src/main/docbkx/troubleshooting.xml&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14331243" author="enis" created="Sat, 21 Feb 2015 23:34:50 +0000"  >&lt;p&gt;Closing this issue after 0.99.0 release. &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12654923" name="HBASE-6626.patch" size="12549" author="misty" created="Thu, 10 Jul 2014 03:02:05 +0000"/>
                            <attachment id="12541780" name="troubleshooting.txt" size="1113" author="stack" created="Tue, 21 Aug 2012 16:51:26 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 21 Aug 2012 16:51:26 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>241854</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 42 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02et3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12000</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>