<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:11:40 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-10216/HBASE-10216.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-10216] Change HBase to support local compactions</title>
                <link>https://issues.apache.org/jira/browse/HBASE-10216</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;As I understand it compactions will read data from DFS and write to DFS.  This means that even when the reading occurs on the local host (because region server has a local copy) all the writing must go over the network to the other replicas.  This proposal suggests that HBase would perform much better if all the reading and writing occurred locally and did not go over the network. &lt;/p&gt;

&lt;p&gt;I propose that the DFS interface be extended to provide method that would merge files so that the merging and deleting can be performed on local data nodes with no file contents moving over the network.  The method would take a list of paths to be merged and deleted and the merged file path and an indication of a file-format-aware class that would be run on each data node to perform the merge.  The merge method provided by this merging class would be passed files open for reading for all the files to be merged and one file open for writing.  The custom class provided merge method would read all the input files and append to the output file using some standard API that would work across all DFS implementations.  The DFS would ensure that the merge had happened properly on all replicas before returning to the caller.  It could be that greater resiliency could be achieved by implementing the deletion as a separate phase that is only done after enough of the replicas had completed the merge. &lt;/p&gt;

&lt;p&gt;HBase would be changed to use the new merge method for compactions, and would provide an implementation of the merging class that works with HFiles.&lt;/p&gt;

&lt;p&gt;This proposal would require a custom code that understands the file format to be runnable by the data nodes to manage the merge.  So there would need to be a facility to load classes into DFS if there isn&apos;t such a facility already.  Or, less generally, HDFS could build in support for HFile merging.&lt;/p&gt;

&lt;p&gt;The merge method might be optional.  If the DFS implementation did not provide it a generic version that performed the merge on top of the regular DFS interfaces would be used.&lt;/p&gt;

&lt;p&gt;It may be that this method needs to be tweaked or ignored when the region server does not have a local copy data so that, as happens currently, one copy of the data moves to the region server.&lt;/p&gt;</description>
                <environment>&lt;p&gt;All&lt;/p&gt;</environment>
        <key id="12685840">HBASE-10216</key>
            <summary>Change HBase to support local compactions</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="wdavid">David Witten</reporter>
                        <labels>
                    </labels>
                <created>Fri, 20 Dec 2013 15:47:42 +0000</created>
                <updated>Tue, 17 Feb 2015 17:16:44 +0000</updated>
                                                                            <component>Compaction</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>17</watches>
                                                                <comments>
                            <comment id="13854117" author="xieliang007" created="Fri, 20 Dec 2013 16:21:06 +0000"  >&lt;p&gt;sound  crazy while i read firstly, but yep, seems reasonable.&lt;br/&gt;
seems need do lots of work at HDFS side, you need to let the accordingly data blocks allocate to the same data nodes always, then your proposal &quot;merge&quot; probably could bypass the most of the network operation. current HDFS code, however, no ganrantee all the HFile&apos;s low layer data blocks into the same nodes&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13854179" author="wdavid" created="Fri, 20 Dec 2013 17:12:31 +0000"  >&lt;p&gt;I&apos;m no HDFS expert.  But I had imagined that a data node, D, performing a merge would just do the merge with local files, then tell the name node that D has a replica for all the data blocks for the merged file.&lt;/p&gt;</comment>
                            <comment id="13854261" author="haosdent@gmail.com" created="Fri, 20 Dec 2013 17:38:56 +0000"  >&lt;p&gt;I don&apos;t think local compaction is feasible. HDFS stored hfiles as many blocks, and these blocks have a fixed size. To provide a method to merge files in hdfs may be couldn&apos;t bring outstanding improvement. In other words, hdfs local reads maybe enough for this.&lt;/p&gt;</comment>
                            <comment id="13854414" author="vrodionov" created="Fri, 20 Dec 2013 18:51:29 +0000"  >&lt;p&gt;This should be opened as  HDFS ticket. Provide API to &lt;b&gt;register&lt;/b&gt;  new file with a given path and block locations. This may benefits a lot hdfs copy as well - blocks can be copied locally and new file will be created with just one HDFS API call registerFile(Path path, BlockLocation[] locations). Compaction will be performed locally (mostly) and the coordinator of compaction will call &lt;b&gt;registerFile(Path path, BlockLocation[] locations)&lt;/b&gt; when all involved nodes are finished.&lt;/p&gt;

</comment>
                            <comment id="13957075" author="sershe" created="Tue, 1 Apr 2014 22:15:37 +0000"  >&lt;p&gt;Was the HDFS issue ever filed?&lt;/p&gt;</comment>
                            <comment id="14310953" author="wdavid" created="Sat, 7 Feb 2015 22:15:15 +0000"  >&lt;p&gt;I was wondering if any of you had any thoughts about the validity of my initial assumption that reading from a disk locally and thus incurring the overhead of seeks and disk reads is preferable to having reads done on one node which sends the the data to multiple nodes which do writes.  In other words is it better to reduce disk seeks and reads or is it better to reduce  network bandwidth and overhead?  &lt;br/&gt;
It certainly seems more in line with the philosophy of Hadoop to do local reads and writes.  It al seems true that you would want to conserve network bandwidth when working across a wide area network,  but I don&apos;t really know about the trade offs and I wondered what others thought. &lt;/p&gt;</comment>
                            <comment id="14313284" author="apurtell" created="Tue, 10 Feb 2015 00:50:34 +0000"  >&lt;p&gt;We could propose a new HDFS API that &quot;would merge files so that the merging and deleting can be performed on local data nodes with no file contents moving over the network&quot;, but does this not only push something implemented today in the HBase regionserver down into the HDFS datanodes? Could a merge as described be safely executed in parallel on multiple datanodes without coordination? No, because the result is not a 1:1 map of input block to output block. Therefore in a realistic implementation (IMHO) a single datanode would handle the merge procedure. From a block device and network perspective nothing would change.&lt;/p&gt;

&lt;p&gt;Set the above aside. We can&apos;t push something as critical to HBase as compaction down into HDFS. First, the HDFS project is unlikely to accept the idea or implement it in the first place. Even in the unlikely event that happens, we would need reimplement compaction using the new HDFS facility to take advantage of it, yet we will need to support older versions of HDFS without the new API for a while, and if the new HDFS API ever doesn&apos;t perfectly address the minutiae of HBase compaction then or going forward we would be back where we started. &lt;/p&gt;

&lt;p&gt;Let&apos;s look at the read and write aspects with an eye toward what we have today, and assuming no new HDFS API.&lt;/p&gt;

&lt;p&gt;Reads: With short circuit reads enabled, recommended for all deployments, if file blocks are available on the local datanode then block reads are fully local via a file descriptor passed over a unix domain socket, we never touch a TCP/IP socket. The probability that a block read for an HFile is local can be made very high by taking care to align region placement with block placement and/or fix up where block locality has dropped below a threshold using an existing HDFS API, see &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4755&quot; title=&quot;HBase based block placement in DFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4755&quot;&gt;HBASE-4755&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-4606&quot; title=&quot;HDFS API to move file replicas to caller&amp;#39;s location&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-4606&quot;&gt;HDFS-4606&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Writes: Writers like regionservers always contact the local datanode, assuming colocation of datanode and regionserver, as the first hop in the write pipeline. The datanode will then pipeline the write over the network to replicas, but only the second hop in the pipeline (from local datanode to first remote replica) will add contention on the local NIC, the third (from remote replica to other remote replica) will be pipelined from the remote. It&apos;s true we can initially avoid second-replica network IO by writing to a local file. Or we can have the equivalent in HDFS by setting the initial replication factor of the new file to 1.  In either case after closing the file, to make the result robust against node loss, we need to replicate all blocks of the newly written file immediately afterward. So now we are waiting for network IO and contending the NIC anyway, we have just deferred network IO until the file was completely written. We are not saving a single byte in transmission on the local NIC. We would have to add housekeeping that insures we don&apos;t delete older HFiles until the new/merged HFile is completely replicated; this makes something our business that today is transparent because we don&apos;t defer writes, when close() on the file we are writing to HDFS directly completes we know it has been fully replicated already. &lt;/p&gt;

&lt;p&gt;For us to see any significant impact, I think the proposal on this issue must be replaced with one where we flush from memstore to local files and then at some point merge locally flushed files to a compacted file on HDFS. Only then are we really saving on IO. All of those locally flushed files represent data that never leaves the local node, never crosses the network, never causes reads or writes beyond the local node. This is the benefit &lt;b&gt;and&lt;/b&gt; the nature of the data availability problem that follows: We can&apos;t consider locally flushed files as persisted data. If a node crashes before they are compacted they are lost (until the node comes back online... maybe), or if a local file is corrupted before compaction the data inside is also lost. We can only consider flushed data persisted after a completed compaction, only after the compaction result is fully replicated in HDFS. We somehow have to track all of the data in local flush files and insure it has all been compacted before deleting the WALs that contain those edits. We somehow need to detect when local flush files after node recovery are stale. Etc etc. Will the savings be worth the added complexity and additional failure modes? Maybe, but I believe Facebook published a paper on this that was inconclusive. &lt;/p&gt;</comment>
                            <comment id="14313595" author="vrodionov" created="Tue, 10 Feb 2015 05:24:18 +0000"  >&lt;ul&gt;
	&lt;li&gt;Flush MemStore to HDFS with replication 3&lt;/li&gt;
	&lt;li&gt;Compact store files and write new one with replication factor 1 ( locally, hence no network IO ), but  keep all old files which have replication factor 3.&lt;/li&gt;
	&lt;li&gt;Periodically change replication factor of store file from 1 to 3 (but not very often)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This is not a local compaction (there is a network IO when we do 1 -&amp;gt; 3 for store files), but there is no need to change HDFS API.&lt;/p&gt;

&lt;p&gt;By controlling  frequency of 1 -&amp;gt; 3 expansion we can control network overhead.  This approach makes everything a little bit more complex, of course.&lt;/p&gt;



</comment>
                            <comment id="14321639" author="wdavid" created="Sat, 14 Feb 2015 18:54:08 +0000"  >&lt;p&gt;Replying to Andrew.  Thanks for your comments.  I&apos;ve included much of your message and will reply in-line.&lt;/p&gt;

&lt;p&gt;We could propose a new HDFS API that &quot;would merge files so that the merging and deleting can be performed on local data nodes with no file contents moving over the network&quot;, but does this not only push something implemented today in the HBase regionserver down into the HDFS datanodes?&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; It is indeed a re-implementation of an existing facility.  The proposal involves a callback called on each replica to do the actual merge.  The implementation of the callback would be implemented by the HBase team.  So the policies about how merging is done and what the file format is will still be made by the HBase team.  Which files are to be merged is specified by the HDFS client and would also be decided within HBase.&lt;/p&gt;

&lt;p&gt;Could a merge as described be safely executed in parallel on multiple datanodes without coordination? No, because the result is not a 1:1 map of input block to output block. &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; As I think about it each replica can merge the input files to the output files without much coordination.  When the merge is finished it needs to notify the merge coordinator about the new blocks and files.  The name node needs to be notified about the new files and their blocks from the merge coordinator.  I agree that there isn&apos;t a design for this yet.  But the data involved in the coordination will involve block ids and file ids and not block content.  So the amount of data moving across the net is substantially reduced.  Regarding the 1:1 comment.  It would be required that the merging callback produce identical output files for a given set of input files.  This means that all replicas will produce exactly the same output.  &lt;/p&gt;

&lt;p&gt;Therefore in a realistic implementation (IMHO) a single datanode would handle the merge procedure. &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; Yes,&lt;br/&gt;
From a block device and network perspective nothing would change.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; The amount of data moving over the net would be massively reduced.&lt;/p&gt;

&lt;p&gt;Set the above aside. We can&apos;t push something as critical to HBase as compaction down into HDFS.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; HDFS would be providing a generic facility to merge files.  HDFS would not provide any policy file formats or merger behavior.  As such the important code will be retained by HBase.  If there is a significant performance improvement, without loss of reliability, HBase can&apos;t not seriously consider it.&lt;/p&gt;

&lt;p&gt;First, the HDFS project is unlikely to accept the idea or implement it in the first place. Even in the unlikely event that happens,&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; If there is a significant improvement to HBase, and other HDFS clients which do merging (Does Parquet or other higher level storage clients?).  I would think they&apos;d be eager.&lt;/p&gt;

&lt;p&gt;we would need reimplement compaction using the new HDFS facility to take advantage of it, yet we will need to support older versions of HDFS without the new API for a while,&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt;  Actually, we&apos;d need to keep it around forever.  There would be a DFS interface that HDFS would implement, but other implementations of DFS may not.  So HBase would need to keep its current implementation if the new APIs were not implemented by some DFS.  It may also be that we&apos;d want to keep the existing implementation when the current environment cares less about network IO.&lt;/p&gt;

&lt;p&gt;and if the new HDFS API ever doesn&apos;t perfectly address the minutiae of HBase compaction then or going forward we would be back where we started.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; I think the objection is handled by the callback nature of the new API.&lt;/p&gt;

&lt;p&gt;...&lt;br/&gt;
Only then are we really saving on IO.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; The original proposal saves network IO not only on the initial compactions but also on all the other compactions.&lt;br/&gt;
...&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; The performance value of the original proposal is not certain (as can be seen by repeated use of IF in my comments here).  I think it hinges on the question I asked in my last comment post: Is it better to have local reads and writes and reduce network overhead, or is it better to limit disk reads by having them only occur on one replica (as the current implementation does) and thereby reduce disk read overhead.  There may not be a simple answer to this question.  It may be that the original proposal is worse on a LAN but when replicas are geographically far away reducing network IO is worth the cost of extra local disk reads.  I&apos;m still interested in comments about this question.&lt;/p&gt;</comment>
                            <comment id="14324486" author="apurtell" created="Tue, 17 Feb 2015 17:16:44 +0000"  >&lt;blockquote&gt;
&lt;p&gt;First, the HDFS project is unlikely to accept the idea or implement it in the first place. Even in the unlikely event that happens,&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Witten&amp;#93;&lt;/span&gt; If there is a significant improvement to HBase, and other HDFS clients which do merging (Does Parquet or other higher level storage clients?). I would think they&apos;d be eager.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That is not my expectation at all. I suggest taking this idea to HDFS on an HDFS JIRA. If they take it up we have something to discuss, otherwise there&apos;s not much.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 20 Dec 2013 16:21:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>364898</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 43 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1qw8v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>365198</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>