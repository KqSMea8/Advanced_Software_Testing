<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:46:56 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-13819/HBASE-13819.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-13819] Make RPC layer CellBlock buffer a DirectByteBuffer</title>
                <link>https://issues.apache.org/jira/browse/HBASE-13819</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In RPC layer, when we make a cellBlock to put as RPC payload, we will make an on heap byte buffer (via BoundedByteBufferPool). The pool will keep upto certain number of buffers. This jira aims at testing possibility for making this buffers off heap ones. (DBB)  The advantages&lt;br/&gt;
1. Unsafe based writes to off heap is faster than that to on heap. Now we are not using unsafe based writes at all. Even if we add, DBB will be better&lt;br/&gt;
2. When Cells are backed by off heap (&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11425&quot; title=&quot;Cell/DBB end-to-end on the read-path&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11425&quot;&gt;&lt;del&gt;HBASE-11425&lt;/del&gt;&lt;/a&gt;) off heap to off heap writes will be better&lt;br/&gt;
3. When checked the code in SocketChannel impl, if we pass a HeapByteBuffer to the socket channel, it will create a temp DBB and copy data to there and only DBBs will be moved to Sockets. If we make DBB 1st hand itself, we can  avoid this one more level of copying.&lt;/p&gt;

&lt;p&gt;Will do different perf testing with changed and report back.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12834145">HBASE-13819</key>
            <summary>Make RPC layer CellBlock buffer a DirectByteBuffer</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12783345">HBASE-13291</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="anoop.hbase">Anoop Sam John</assignee>
                                    <reporter username="anoop.hbase">Anoop Sam John</reporter>
                        <labels>
                    </labels>
                <created>Mon, 1 Jun 2015 05:27:24 +0000</created>
                <updated>Thu, 24 Mar 2016 05:25:00 +0000</updated>
                            <resolved>Tue, 13 Oct 2015 02:31:38 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                    <fixVersion>1.3.0</fixVersion>
                                    <component>Scanners</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="14906090" author="anoop.hbase" created="Thu, 24 Sep 2015 09:20:07 +0000"  >&lt;p&gt;On testing this with PE tool.&lt;br/&gt;
15 GB of total table data and whole data is cached in bucket cache off heap.&lt;br/&gt;
Doing range scan with 10K range. Having 25 threads and each thread doing range scan 1000 times.&lt;br/&gt;
Avg latency each thread takes to do this scan&lt;br/&gt;
With patch  76393 ms&lt;br/&gt;
Trunk          86729	ms&lt;br/&gt;
So this is 11% gain&lt;/p&gt;</comment>
                            <comment id="14906182" author="hadoopqa" created="Thu, 24 Sep 2015 10:41:07 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12762094/HBASE-13819.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12762094/HBASE-13819.patch&lt;/a&gt;&lt;br/&gt;
  against master branch at commit 5b7894f92ba3e9ff700da1e9194ebb4774d8b71e.&lt;br/&gt;
  ATTACHMENT ID: 12762094&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0 2.7.1)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): &lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15714//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14906514" author="stack" created="Thu, 24 Sep 2015 15:39:25 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;This is &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; following up on findings from &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4956&quot; title=&quot;Control direct memory buffer consumption by HBaseClient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4956&quot;&gt;&lt;del&gt;HBASE-4956&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nice one.&lt;/p&gt;

&lt;p&gt;Suggest backport to branch-1 and 1.2 at least.&lt;/p&gt;</comment>
                            <comment id="14906538" author="ram_krish" created="Thu, 24 Sep 2015 15:50:24 +0000"  >&lt;p&gt;+1.  This was done in our branch testing when we did the POC for &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-11425&quot; title=&quot;Cell/DBB end-to-end on the read-path&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-11425&quot;&gt;&lt;del&gt;HBASE-11425&lt;/del&gt;&lt;/a&gt; and that is why fixed this &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-12845&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-12845&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="14908236" author="stack" created="Fri, 25 Sep 2015 16:00:18 +0000"  >&lt;p&gt;One thought &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; is that rather than always make the BB offheap, maybe it should be an option on construction?&lt;/p&gt;</comment>
                            <comment id="14908390" author="stack" created="Fri, 25 Sep 2015 17:48:45 +0000"  >&lt;p&gt;We can do the above suggestion in another patch when the pool is used by more than just cell block return..  Suggest that on commit you add a comment that buffers are off-heap.&lt;/p&gt;</comment>
                            <comment id="14908995" author="anoop.hbase" created="Sat, 26 Sep 2015 01:58:07 +0000"  >&lt;p&gt;Oh yes. We might be using this pool for the cell block creation for sending from client to server as well.  So in client side we will continue with on heap only. I forgot to make this option.  Will do in next version.&lt;br/&gt;
In server side let us make only off heap (for trunk)&lt;br/&gt;
About backport, do we need to make this configurable then?  Because the max direct buffer has to consider this capacity as well.  We will keep these buffers in our pool.  WIth max size of 2MB per buffer and twice the #handlers. (correct?)  May be that will be a surprise for changes in 1.x versions?&lt;/p&gt;</comment>
                            <comment id="14909002" author="stack" created="Sat, 26 Sep 2015 02:09:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;So in client side we will continue with on heap only. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We don&apos;t want to do offheap on client-side for same benefit?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In server side let us make only off heap (for trunk)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Because the max direct buffer has to consider this capacity as well. We will keep these buffers in our pool. WIth max size of 2MB per buffer and twice the #handlers. (correct?) May be that will be a surprise for changes in 1.x versions?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Enable for 1.3 with a release note? It is a nice perf benefit. So, I suppose make it configurable, yeah. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14909157" author="stack" created="Sat, 26 Sep 2015 08:10:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;May be that will be a surprise for changes in 1.x versions?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thinking on this more, aren&apos;t these direct buffers being allocated anyways &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; by the socket? So, there should be no surprise? We&apos;ll be using same amount of direct memory whether the flag is set or not? (But yeah, need it configurable but on for server-side cellblock making)&lt;/p&gt;</comment>
                            <comment id="14909305" author="anoop.hbase" created="Sat, 26 Sep 2015 15:05:03 +0000"  >&lt;p&gt;Yes these DBBs are any way allocated but thrown away after the wire transfer. But in BoundedBBPool, we will keep these buffers.  We will have by default, 2 MB max size and 2 * handlers count such buffers.  We will keep them for the life of RS. That is what I was thinking, the diff will be. wdyt?&lt;/p&gt;</comment>
                            <comment id="14909325" author="anoop.hbase" created="Sat, 26 Sep 2015 15:59:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;We don&apos;t want to do offheap on client-side for same benefit?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There is no buffer pool in client side so no change needed in patch.  Yes it will be good to get this benefit in client side also. But as I did not test it, I think not to include in this patch. We can do in a later issue after tests. Ok Stack?&lt;/p&gt;</comment>
                            <comment id="14909417" author="stack" created="Sat, 26 Sep 2015 18:51:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;We can do in a later issue after tests. Ok Stack?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Of course.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Yes these DBBs are any way allocated but thrown away after the wire transfer.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We will have by default, 2 MB max size and 2 * handlers count such buffers. We will keep them for the life of RS. That is what I was thinking, the diff will be. wdyt?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Makes sense. 1.3 with big release note with means of disabling but default is on.&lt;/p&gt;</comment>
                            <comment id="14934806" author="anoop.hbase" created="Tue, 29 Sep 2015 07:53:03 +0000"  >&lt;p&gt;Patch for version 1.3&lt;br/&gt;
There is a config option for making DBB or HBB from the pool. By default we will make DBBs.&lt;/p&gt;</comment>
                            <comment id="14934835" author="hadoopqa" created="Tue, 29 Sep 2015 08:18:57 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12764203/HBASE-13819_branch-1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12764203/HBASE-13819_branch-1.patch&lt;/a&gt;&lt;br/&gt;
  against branch-1 branch at commit 6ad6273ddaedb7a3a21ef391d4381c19a474e3b3.&lt;br/&gt;
  ATTACHMENT ID: 12764203&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 javac&lt;/font&gt;.  The patch appears to cause mvn compile goal to fail with Hadoop version 2.4.0.&lt;/p&gt;

&lt;p&gt;    Compilation errors resume:&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; COMPILATION ERROR : &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;41,6&amp;#93;&lt;/span&gt; error: constructor BoundedByteBufferPool in class BoundedByteBufferPool cannot be applied to given types;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:testCompile (default-testCompile) on project hbase-common: Compilation failure&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;41,6&amp;#93;&lt;/span&gt; error: constructor BoundedByteBufferPool in class BoundedByteBufferPool cannot be applied to given types;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; -&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; To see the full stack trace of the errors, re-run Maven with the -e switch.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Re-run Maven using the -X switch to enable full debug logging.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; For more information about the errors and possible solutions, please read the following articles:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; After correcting the problems, you can resume the build with the command&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt;   mvn &amp;lt;goals&amp;gt; -rf :hbase-common&lt;/p&gt;


&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15799//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15799//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14939601" author="anoop.hbase" created="Thu, 1 Oct 2015 09:53:53 +0000"  >&lt;p&gt;How is the branch-1 patch with the config option &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;?&lt;br/&gt;
Will commit now or to wait for our pre commits QA to be back to green?&lt;/p&gt;</comment>
                            <comment id="14939761" author="hadoopqa" created="Thu, 1 Oct 2015 12:39:04 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12764571/HBASE-13819_branch-1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12764571/HBASE-13819_branch-1.patch&lt;/a&gt;&lt;br/&gt;
  against branch-1 branch at commit 76463a36f5648f42cdcf64019a825c1e3f0c4fe1.&lt;br/&gt;
  ATTACHMENT ID: 12764571&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 2 zombie test(s): 	at org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithACL.testVisibilityLabelsForUserWithNoAuths(TestVisibilityLabelsWithACL.java:203)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15839//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14945886" author="stack" created="Tue, 6 Oct 2015 21:44:44 +0000"  >&lt;p&gt;Retry&lt;/p&gt;

&lt;p&gt;Commit I&apos;d say &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;  Needs a release note sir.&lt;/p&gt;</comment>
                            <comment id="14946078" author="hadoopqa" created="Wed, 7 Oct 2015 00:38:36 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12765268/HBASE-13819_branch-1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12765268/HBASE-13819_branch-1.patch&lt;/a&gt;&lt;br/&gt;
  against branch-1 branch at commit 0ea1f8122709302ee19279aaa438b37dac30c25b.&lt;br/&gt;
  ATTACHMENT ID: 12765268&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 hadoop versions&lt;/font&gt;. The patch compiles with all supported hadoop versions (2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.6.1 2.7.0 2.7.1)&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 protoc&lt;/font&gt;.  The applied patch does not increase the total number of protoc compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 checkstyle&lt;/font&gt;.  The applied patch does not increase the total number of checkstyle errors&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any  new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 lineLengths&lt;/font&gt;.  The patch does not introduce lines longer than 100&lt;/p&gt;

&lt;p&gt;  &lt;font color=&quot;green&quot;&gt;+1 site&lt;/font&gt;.  The mvn post-site goal succeeds with this patch.&lt;/p&gt;

&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;     &lt;font color=&quot;red&quot;&gt;-1 core zombie tests&lt;/font&gt;.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.security.access.TestCellACLs.testCoveringCheck(TestCellACLs.java:402)&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//testReport/&lt;/a&gt;&lt;br/&gt;
Release Findbugs (version 2.0.3) 	warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//artifact/patchprocess/newFindbugsWarnings.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//artifact/patchprocess/newFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle Errors: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//artifact/patchprocess/checkstyle-aggregate.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//artifact/patchprocess/checkstyle-aggregate.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;  Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/15887//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14954275" author="anoop.hbase" created="Tue, 13 Oct 2015 02:31:38 +0000"  >&lt;p&gt;Pushed to branch-1 and master.&lt;br/&gt;
In master we will always make Direct ByteBuffer from pool. In 1.3 we will by default make it direct and using config &lt;b&gt;hbase.ipc.server.reservoir.direct.buffer&lt;/b&gt; this can be changed. Configure it as false to make the on heap ByteBuffers as before.&lt;/p&gt;

&lt;p&gt;Thanks for the reviews Stack and Ram.&lt;/p&gt;</comment>
                            <comment id="14954412" author="hudson" created="Tue, 13 Oct 2015 05:22:10 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.3 #255 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.3/255/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.3/255/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13819&quot; title=&quot;Make RPC layer CellBlock buffer a DirectByteBuffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13819&quot;&gt;&lt;del&gt;HBASE-13819&lt;/del&gt;&lt;/a&gt; Make RPC layer CellBlock buffer a DirectByteBuffer. (anoopsamjohn: rev 5dba2c71f7f4b48ec1d8e6b54e2efe59e77f4b77)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java&lt;/li&gt;
	&lt;li&gt;hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java&lt;/li&gt;
	&lt;li&gt;hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14954445" author="hudson" created="Tue, 13 Oct 2015 05:57:57 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-1.3-IT #229 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-1.3-IT/229/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-1.3-IT/229/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13819&quot; title=&quot;Make RPC layer CellBlock buffer a DirectByteBuffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13819&quot;&gt;&lt;del&gt;HBASE-13819&lt;/del&gt;&lt;/a&gt; Make RPC layer CellBlock buffer a DirectByteBuffer. (anoopsamjohn: rev 5dba2c71f7f4b48ec1d8e6b54e2efe59e77f4b77)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java&lt;/li&gt;
	&lt;li&gt;hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestBoundedByteBufferPool.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14954605" author="hudson" created="Tue, 13 Oct 2015 08:19:26 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-TRUNK #6900 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/6900/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/6900/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-13819&quot; title=&quot;Make RPC layer CellBlock buffer a DirectByteBuffer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-13819&quot;&gt;&lt;del&gt;HBASE-13819&lt;/del&gt;&lt;/a&gt; Make RPC layer CellBlock buffer a DirectByteBuffer. (anoopsamjohn: rev 6143b7694cc02e905b931de86462c6125ca8b3b6)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-common/src/main/java/org/apache/hadoop/hbase/io/BoundedByteBufferPool.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15207822" author="dvdreddy" created="Wed, 23 Mar 2016 03:18:03 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;We recently pulled this patch internally and are seeing some significant side effects of BoundedByteBufferPool where the system runs out of direct memory whenever there is some network congestion or some client side issues. &lt;/p&gt;

&lt;p&gt;System props used :&lt;br/&gt;
RPC Handlers = 300&lt;br/&gt;
Number of Clients ~200, Threads in each client ~20, we use asynchbase so all requests are multiplexed on single connections&lt;br/&gt;
Initial buffer size is 16 kb&lt;br/&gt;
Buffer size settles to 512 kb over time(from debug statements we put in) &lt;br/&gt;
Max number to cache (reservoir.initial.max ) is around 2048 (also tried with 4096)&lt;br/&gt;
DirectMem accounted for this (is 2048 * 512 KB + 1 GB)&lt;/p&gt;

&lt;p&gt;We took a heapdump and analysed the contents of the heap using VisualVM OQL and we found that &lt;br/&gt;
number of rpcs that were queued in the responder was around ~4000 and this leads to exhaustion of the direct buffer space, digging a little bit more deeper the responses buffers in the pendingCallsQueue in connection accounted for 3181117440 bytes, even though the real &lt;br/&gt;
response size (buffers are allocated for 512kb even when the response is small) accounted only for 84451734.0 bytes. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; suggested that since any way we are using buffer chain to create a CellBlock it would better to create a new ByteBufferOutputStream which acquires buffers from the pool instead of allocating a new one with very high moving average and removing the moving average overall and having a fixed size buffers instead ?&lt;/p&gt;

&lt;p&gt;Here are the visualVM query used &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
select sum(map(heap.objects(&apos;java.nio.DirectByteBuffer&apos;), function (a) {
   &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; x = 0;
   &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; callx = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
   forEachReferrer(function (y) { 
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (classof(y).name == &apos;org.apache.hadoop.hbase.ipc.RpcServer$Call&apos;) {
       x = -1;
       forEachReferrer(function (px) {
         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (classof(px).name == &apos;java.util.concurrent.ConcurrentLinkedDeque$Node&apos;) {
            callx = y;             
            x = 1;
         }
       }, y);
    } 
   }, a);

   &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (a.att == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp;  x == 1 &amp;amp;&amp;amp; callx.response.bufferOffset == 0 &amp;amp;&amp;amp; callx.response.remaining != 0) {
      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; callx.response.remaining
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; a.capacity
   } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;  0
   }
}))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15207917" author="anoop.hbase" created="Wed, 23 Mar 2016 05:56:37 +0000"  >&lt;p&gt;Ya we had an offline discuss abt this issue.&lt;br/&gt;
As the client is async, there are lot of requests pumped in and response are in Q.  The Q size increases.  While it is in Q, the cell block is already been made by getting BB from the pool.&lt;br/&gt;
There should be some sort of a throttling and should not allow these many responses to grow in Q.&lt;br/&gt;
Still one issue &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dvdreddy&quot; class=&quot;user-hover&quot; rel=&quot;dvdreddy&quot;&gt;deepankar&lt;/a&gt; noticed is that, we increase the avg running size of the BB pool and for them it reaches till 512 KB.  Initially it was 16KB.  The case is row reads.  So normally the response size is much lesser than this 512 KB.  Still we reserve those many for each of the BB.&lt;/p&gt;

&lt;p&gt;I was thinking abt this time back also..  And it came in some other Jira discuss as well..&lt;/p&gt;

&lt;p&gt;Actually for the response CellBlock we dont need the entire cells serialized data to be in single BB.  It can be in N BBs also.  Any way for writing the response we have a BufferChain. Within this also, we can see that the actual write to socket happens as chunks of 64KB.  So when a buffer of size say 1 MB is written, it is written in N writes to socket with 64 KB chunk size.&lt;/p&gt;

&lt;p&gt;So when we make the cell block, we can make it on N BBs rather than one. We need a new BBOS.. So there is no need to grow the BB within the BBOS and copy the old content to new one. When we run out of space in current BB, get a new one from pool.  Finally all can be returned to pool.&lt;/p&gt;

&lt;p&gt;What I wanted to see is that the pool deal with fixed sized BBs.  Say all 64 KB sized buffers. Depending on the need, the users of the pool may poll one or more BBs&lt;/p&gt;</comment>
                            <comment id="15207974" author="ram_krish" created="Wed, 23 Mar 2016 06:53:56 +0000"  >&lt;p&gt;How much is the Directmemory configured here?  Was the pool tried to work with onheap also?  &lt;br/&gt;
If the rows are like 400K to 500K how will the BB size grow?  Will the running average be 512K?  In that case also using AsynClient with this Pool will make us run out of direct memory?&lt;/p&gt;</comment>
                            <comment id="15207999" author="anoop.hbase" created="Wed, 23 Mar 2016 07:15:04 +0000"  >&lt;p&gt;5 GB max direct memory is what given I believe..  Ya as I said, if there is no controlling of the asyn reqs and if the response is on slower side, we may face issues even if it is on heap also..  As on heap size is bigger, they might not see it as such..   So that sort of a solution they need to work on for sure.&lt;br/&gt;
But here as you can see, they do have many BBs in pool. Like 2000+  And after some time run the avg running size become 512 KB.  On avg, they do not need more than 16 KB per row and so per request response..  Still we will use all these bigger sized BBs and those are permanent pinned in off heap space.&lt;br/&gt;
There is one more serious issue.  When the pool has grown to its max capacity wrt max #BBs in it, still when there are reqs for BB to pool and it can not see a free BB (all are occupied by other thread&apos;s response CellBlock), it will create a BB that too on off heap!   Ya we may not be pooling this later.   Still IMO these on demand BB creation should not be on off heap area.   And another reason why &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dvdreddy&quot; class=&quot;user-hover&quot; rel=&quot;dvdreddy&quot;&gt;deepankar&lt;/a&gt; test went into out of direct memory space.&lt;br/&gt;
So we need  correct this part also IMO..   When we make cell block, try to get a BB from pool. If not available in pool  (and pool&apos;s max BB capacity already reached), make it an on heap BB.  That is temp and will be thrown away after the socket write..&lt;br/&gt;
For this we need 1st make the BB pool with fixed sized BB&lt;/p&gt;</comment>
                            <comment id="15208009" author="dvdreddy" created="Wed, 23 Mar 2016 07:25:22 +0000"  >&lt;p&gt;What &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt; said is correct, in our case avg response sizes are around 10 -16 kb but the max size is 512kb if you run for significantly longer time the running avg goest toward 512kb. Another point its not like that there are lot of request if you have around 200 clients and lets say you have around 5 % are GCing then 10 clients with around 400 pending reqs will lead to 4000 pending requests and this leads to exhaustion of the direct memory we allocated (with buffer sizes 512 kb) but in general the overall pending response size for these 4000 requests is only 82 MB its atleast 1 - 2  orders of magnitude of space getting occupied extra.&lt;/p&gt;

&lt;p&gt;I think fixed size BBs is an excellent idea that will definitely be useful to get out of this issue.&lt;/p&gt;</comment>
                            <comment id="15209371" author="stack" created="Wed, 23 Mar 2016 22:58:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;We recently pulled this patch internally and are seeing some significant side effects of BoundedByteBufferPool where the system runs out of direct memory whenever there is some network congestion or some client side issues.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because we are not returning BB to the pool? The pool is growing w/o bound?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Buffer size settles to 512 kb over time(from debug statements we put in) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We should add these if not present at TRACE level.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;number of rpcs that were queued in the responder was around ~4000 and this leads to exhaustion of the direct buffer space, &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So 2G instead of 1G? But the pool is bounded?&lt;/p&gt;

&lt;p&gt;The responder is not keeping up? It is not moving stuff out of the server fast enough?&lt;/p&gt;

&lt;p&gt;Where is pendingCallsQueue?&lt;/p&gt;

&lt;p&gt;Did you observe the offheap size used growing? There s a metric IIRC.&lt;/p&gt;

&lt;p&gt;I am interested in leaks; how they are happening.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...and having a fixed size buffers instead&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Where would the fixed size be? In BBBP they eventually reach fixed size?&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;

</comment>
                            <comment id="15209388" author="stack" created="Wed, 23 Mar 2016 23:13:15 +0000"  >&lt;p&gt;Which Q we talking?  The is config to put bounds on the call queue where we will reject calls... &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The case is row reads. So normally the response size is much lesser than this 512 KB. Still we reserve those many for each of the BB.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, there is heuristic and we grow till we hit an average. Are we saying we grew to 512k and then afterward, all calls were 16k only? Is this a problem?&lt;/p&gt;

&lt;p&gt;Yeah, I like the idea for the BBBP made of fixed-size BBs. Move the code for BucketCache out here.&lt;/p&gt;
</comment>
                            <comment id="15209398" author="stack" created="Wed, 23 Mar 2016 23:20:26 +0000"  >&lt;p&gt;The BBBP is configurable. You can set upper bound on BBBP member size.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Still IMO these on demand BB creation should not be on off heap area. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Interesting. Why you say? Because it slower doing it offheap? What is wrong doing it offheap otherwise? Why is the offheap BB not discarded?&lt;/p&gt;

&lt;p&gt;So, I think this the issue... the server is overrun and so burst our bounds on the BBBP and when at limit, new BB&apos;s are allocated offheap... so we go outside of our expected offheap usage. Is there leaking going on here?&lt;/p&gt;

&lt;p&gt;But we have means of bounding the queues of inbound traffic.&lt;/p&gt;

&lt;p&gt;In description above though, talk is of Responder queue backed up which seems to say we are not clearing the server of finished responses fast enough....&lt;/p&gt;
</comment>
                            <comment id="15209401" author="dvdreddy" created="Wed, 23 Mar 2016 23:21:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Because we are not returning BB to the pool? The pool is growing w/o bound?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think there are no leaks in BB from the analysis on the heap dump, all the objects were accounted&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We should add these if not present at TRACE level.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry My comment was misled, by debug statements I meant by enabling TRACE (these loggings were most useful stuff in many debugging scenarios)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So 2G instead of 1G? But the pool is bounded?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;The responder is not keeping up? It is not moving stuff out of the server fast enough?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I am interested in leaks; how they are happening.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No concrete evidence that responder is not able to keep up, but the bound in pool does not help this case because we create a new BB when one is not present in the pool and occasionally (we are observing once in 2 - 3 days) there will be spew when returns to pool grows above the configured threshold.&lt;br/&gt;
From the analysis we did there are no leaks &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Where is pendingCallsQueue?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The queue per connection object RpcServer$Connection.responseQueue&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Did you observe the offheap size used growing? There s a metric IIRC.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes we saw this in the metric (hbase.regionserver.direct.MemoryUsed) &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Where would the fixed size be? In BBBP they eventually reach fixed size?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes they eventually reach fixed size, but the size is that of the larger response sizes rather than median or some smaller number&lt;/p&gt;
</comment>
                            <comment id="15209409" author="dvdreddy" created="Wed, 23 Mar 2016 23:25:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;Which Q we talking? The is config to put bounds on the call queue where we will reject calls...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The one that reject calls only accounts for incoming request sizes and there is default 2GB limit I think. The response sizes are not accounted in this I think.&lt;br/&gt;
The Queue is RpcServer$Connection.responseQueue.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yeah, there is heuristic and we grow till we hit an average. Are we saying we grew to 512k and then afterward, all calls were 16k only? Is this a problem?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In our use case our worst case response size is 512 kb (hard capped from client side) and our avg response size is between 12kb, what we observe is after 3 - 4 days of running almost always the moving avg is at 512 kb and in the heap dump all the response buffers is of size 512 kb&lt;/p&gt;</comment>
                            <comment id="15209412" author="stack" created="Wed, 23 Mar 2016 23:28:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;What Anoop Sam John said is correct, in our case avg response sizes are around 10 -16 kb but the max size is 512kb if you run for significantly longer time the running avg goest toward 512kb. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d think this a good thing, no? You are seeing some traffic that is 512kb... and the BBBP can accommodate without a resize. If you do find it a problem, change &quot;hbase.ipc.server.reservoir.max.buffer.size&quot;.  Its default is  1024 * 1024.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Another point its not like that there are lot of request if you have around 200 clients and lets say .... for these 4000 requests is only 82 MB its atleast 1 - 2 orders of magnitude of space getting occupied extra.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This sounds like we are leaking? The JVM publishes how much memory is being used in the jmx metric java.nio.type=BufferPool,name=direct  Does this grow over time?&lt;/p&gt;</comment>
                            <comment id="15209414" author="dvdreddy" created="Wed, 23 Mar 2016 23:29:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;In description above though, talk is of Responder queue backed up which seems to say we are not clearing the server of finished responses fast enough....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is it possible that the issues on client side we are not able to push the responses instead of Responder being slow ? because we do try sending the responses from the handler itself if it is possible which points somewhat the issue could be on client side ?&lt;/p&gt;</comment>
                            <comment id="15209425" author="dvdreddy" created="Wed, 23 Mar 2016 23:38:17 +0000"  >&lt;p&gt;Attached image of the metric over time. we are running bucket cache of size around 69427MB and the parameters of BBBP are 2048 max entries and max size is 1MB, trace showed current moving avg is 512 kb&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12795095/12795095_q.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt;&lt;/p&gt;</comment>
                            <comment id="15209427" author="stack" created="Wed, 23 Mar 2016 23:39:18 +0000"  >&lt;p&gt;Good to hear you don&apos;t think there leaking going on.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;No concrete evidence that responder is not able to keep up, but the bound in pool does not help this case because we create a new BB when one is not present in the pool and occasionally (we are observing once in 2 - 3 days) there will be spew when returns to pool grows above the configured threshold.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Messages saying ...           LOG.warn(&quot;Overflowed total capacity.&quot;); or ....          LOG.warn(&quot;At capacity: &quot; + countOfBuffers); ?&lt;/p&gt;

&lt;p&gt;They don&apos;t look that useful. Might be good though to report detail around the burst in traffic so could adjust sizing....&lt;/p&gt;

&lt;p&gt;What you think of Anoop&apos;s idea of the BB being allocated onheap rather than offheap if we can&apos;t get it from the pool?  The allocation would be faster...&lt;/p&gt;

&lt;p&gt;So, growing because of a burst in traffic.. but no leaking... because when the burst passes, we seem to shrink down again (if noisy about it...)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dvdreddy&quot; class=&quot;user-hover&quot; rel=&quot;dvdreddy&quot;&gt;deepankar&lt;/a&gt; Would you mind opening a new issue describing how you would like this to work? Its great that you fellas are running it and uncovering this stuff. It would be easy enough to do you up patches for you to try since it so contained. Thanks.&lt;/p&gt;
</comment>
                            <comment id="15209432" author="stack" created="Wed, 23 Mar 2016 23:42:18 +0000"  >&lt;p&gt;We do seem to be bottlenecking on responding. I&apos;m not sure if it this or client not pulling the responses fast enough. This is an area to dig in on.&lt;/p&gt;</comment>
                            <comment id="15209439" author="stack" created="Wed, 23 Mar 2016 23:45:01 +0000"  >&lt;p&gt;What would you like &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=deenarang&quot; class=&quot;user-hover&quot; rel=&quot;deenarang&quot;&gt;Deepankar Narang&lt;/a&gt;? The burst to show onheap or offheap?  Even if we did fixed size, you&apos;d still get a graph like this, right when the traffic comes on hard? (Traffic is up when you see the above spike, is it?)&lt;/p&gt;</comment>
                            <comment id="15209732" author="anoop.hbase" created="Thu, 24 Mar 2016 04:33:30 +0000"  >&lt;p&gt;Its not case of leak Stack..   &lt;br/&gt;
Ya I agree that when there is async reqs coming in and the response is some how delayed, there should be some sort of throttling. Client side can do?&lt;br/&gt;
The issue why we go out of off heap memory is because of 2 main reasons&lt;br/&gt;
1. The pool has already reached its max capacity of #BBs. And at a given point of time, all are in use.  Again other Calls ask pool for BBs for their cell block creation.  The pool happily make new BBs which are off heap with each having size of avg running length.  And all these cell blocks are tied to Call until the Responder write them to socket.  Ya we wont be keeping them in pool. But it is kept as is for loner time specially when the response Q is growing.  &lt;br/&gt;
2. Even when the response CellBlock need is very low like 12 KB or so, we waste 512 MB per response. Waste in the sense that all the 500 MB is not usable at all.  And even the new BBs which pool create on demand (These might not pooled at all as we reach max #BBs in pool) also takes 512 MB per BB.&lt;/p&gt;

&lt;p&gt;So in a simple way we can say that its really difficult for the user to predict how much max off heap size he need to give.  With &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dvdreddy&quot; class=&quot;user-hover&quot; rel=&quot;dvdreddy&quot;&gt;deepankar&lt;/a&gt; case, he is applying some calc based on the max #BBs in pool and max BB size + some additional GBs and set the max off heap size as 5 GB.  But this is going wrong..&lt;/p&gt;

&lt;p&gt;To explain it with an eg:&lt;br/&gt;
Consider one configured the max #BBs in pool as 100.  And max per item size as 1MB. Means max can have 100 MB off heap consumption by this pool..   Now consider there are lots of reqs and the response Q is big..  Say the 1st 100 response use all BBs from pool. Now again reqs are there and say there are like 100 more adding to Q..   Each one req to pool.  It makes BB off heap for those.   Means out of the pool we have made double the total max size what we thought it will take..     I agree that we wont store those all BBs in pool and ya the GC may be able to clean it also..  But for some time (untill we clear these response Q) the usage is more.&lt;/p&gt;

&lt;p&gt;And one more thing for GC is that the full GC only can clean the off heap area?   So this in other words cause more full GCs? (If we go out&lt;br/&gt;
of space in off heap area)!!!&lt;/p&gt;

&lt;p&gt;So that is why my thinking abt changing these temp BB creation when happens, those should be HBBs.&lt;/p&gt;

&lt;p&gt;We need to make pool such that we will give a BB back if it is having a free one.  When it is not having a free one and capacity is not reached, it makes a new DBB and return. If that is also not the case it wont return any.   The BBBPool will make and take back offf heap BBs only.  If it can not give, let the caller do what they want (Make on heap BB and make sure dont give back to pool)&lt;/p&gt;

&lt;p&gt;And abt fixing the size of BBs from pool..  Will write in another comment.  This is too big already &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15209736" author="dvdreddy" created="Thu, 24 Mar 2016 04:37:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;What you think of Anoop&apos;s idea of the BB being allocated onheap rather than offheap if we can&apos;t get it from the pool? The allocation would be faster...&lt;/p&gt;&lt;/blockquote&gt;


&lt;p&gt;I feel both onheap / offheap allocation could suffer from the same problem as long as we are not tight in the allocation (with less wastage compared to the anticipated response),  as an example we somewhat made this error rare by increasing the MaxDirectMemory to a higher value, but in the onheap case also if a user allocated his memory by tightly accounting stuff he could may as well face the issue of unnecessary GCs I think. &lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;deepankar Would you mind opening a new issue describing how you would like this to work?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;created a jira &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15525&quot; title=&quot;OutOfMemory could occur when using BoundedByteBufferPool during RPC bursts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15525&quot;&gt;&lt;del&gt;HBASE-15525&lt;/del&gt;&lt;/a&gt;, we  would definitely help in whatever way we can on this.&lt;/p&gt;</comment>
                            <comment id="15209739" author="dvdreddy" created="Thu, 24 Mar 2016 04:41:00 +0000"  >&lt;p&gt;I agree with you but what I feel is in the ideal scenario the increase in the heap usage should be proportional to the number of RPCs coming right ?, when we initially allocated the heap size for BBB we accounted for the  anticipated burst, but the issue that came was when the size use on the server was couple of orders of magnitude more than this (the actual response waiting to respond was around 80 MB when the total heap usage was around 3.1 GB). what do you think ?&lt;/p&gt;</comment>
                            <comment id="15209749" author="dvdreddy" created="Thu, 24 Mar 2016 04:51:00 +0000"  >&lt;p&gt;I totally agree with your idea, but I think practically it would be hard to exhaust out the offheap stuff if the allocation is proportional to the expected response size, one reason why we did not decrease the buffer size from BoundedByteBufferPool is that these large size requests are not that uncommon, they do occur occasionally and we did not want them having to allocate a full size buffer at that time with our initial assumption being with BoundedByteBufferPool there might not be any more allocations at all. May be we should try out internally by restricting max size from BBBP to a much lower value, may be its ok for large RPCs to have to create their own buffers.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And one more thing for GC is that the full GC only can clean the off heap area?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think young could also clear them, the problem is that Bits class when there is not enough offheap space calls the System.GC which tr iggers full GC I think but any way I agree this GC is wastefull&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We need to make pool such that we will give a BB back if it is having a free one. When it is not having a free one and capacity is not reached, it makes a new DBB and return&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This would be nice, we had a hot patch internally which just fails the request when you see the Bits is going to call System.GC(), this was just temporarily to stop the RegionServer from crashing.&lt;/p&gt;</comment>
                            <comment id="15209757" author="anoop.hbase" created="Thu, 24 Mar 2016 04:58:47 +0000"  >&lt;p&gt;Ya am not saying this on heap create is enough.  IMO we should fix the other problem also. The waste of off heap area for responding with a very small sized cell block. I did not add those related comment in the previous one as that was on one item and it was too big already &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;    Let me continue in next comment. Or we can switch to new Jira&lt;/p&gt;</comment>
                            <comment id="15209768" author="stack" created="Thu, 24 Mar 2016 05:10:40 +0000"  >&lt;p&gt;bq, .....what do you think ?&lt;/p&gt;

&lt;p&gt;Thinking on it, a manual config that put upper size on BBs at say 32k and that allowed 4x the amount of buffers to be created might get you over your spike.&lt;/p&gt;

&lt;p&gt;As to what the heuristic should be, so you don&apos;t have to do these configs. lets work that out in the new issue you filed.&lt;/p&gt;</comment>
                            <comment id="15209792" author="stack" created="Thu, 24 Mar 2016 05:24:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Client side can do?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Server has to do it. In this case, we did not write the client (asynchbase).&lt;/p&gt;

&lt;p&gt;Thanks for your #1 and #2. Nice summary.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;.... its really difficult for the user to predict how much max off heap size he need to give. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah. Lets fix. We could multiply the max queue size in listener * maximum request size to get max offheap buffer allocated.... Thats kinda ugly. Need the server throttling.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(If we go out of space in off heap area)!!!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. Need to bound offheap usage. What GC does when offheap up against the limit is probably not pretty.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So that is why my thinking abt changing these temp BB creation when happens, those should be HBBs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You still need to do accounting of the spike though.&lt;/p&gt;

&lt;p&gt;But yeah, probably easier taking the spike onheap.... Lets discuss in new issue.&lt;/p&gt;


</comment>
                            <comment id="15209793" author="stack" created="Thu, 24 Mar 2016 05:25:00 +0000"  >&lt;p&gt;New JIRA! This one is resolved (smlle) &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anoop.hbase&quot; class=&quot;user-hover&quot; rel=&quot;anoop.hbase&quot;&gt;Anoop Sam John&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12533914">HBASE-4956</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12762094" name="HBASE-13819.patch" size="720" author="anoop.hbase" created="Thu, 24 Sep 2015 09:20:07 +0000"/>
                            <attachment id="12765268" name="HBASE-13819_branch-1.patch" size="3373" author="stack" created="Tue, 6 Oct 2015 21:44:44 +0000"/>
                            <attachment id="12764571" name="HBASE-13819_branch-1.patch" size="3373" author="anoop.hbase" created="Thu, 1 Oct 2015 09:53:53 +0000"/>
                            <attachment id="12764203" name="HBASE-13819_branch-1.patch" size="2562" author="anoop.hbase" created="Tue, 29 Sep 2015 07:53:03 +0000"/>
                            <attachment id="12795095" name="q.png" size="75463" author="dvdreddy" created="Wed, 23 Mar 2016 23:35:54 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 24 Sep 2015 10:41:07 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            38 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2ffwf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>For master branch(2.0 version), the BoundedByteBufferPool always create Direct (off heap) ByteBuffers and return that.&lt;br/&gt;
For branch-1(1.3 version), byte default the buffers returned will be off heap. This can be changed to return on heap ByteBuffers by configuring &amp;#39;hbase.ipc.server.reservoir.direct.buffer&amp;#39; to false.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>