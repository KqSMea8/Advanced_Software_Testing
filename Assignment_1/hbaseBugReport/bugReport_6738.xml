<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:39:26 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-6738/HBASE-6738.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-6738] Too aggressive task resubmission from the distributed log manager</title>
                <link>https://issues.apache.org/jira/browse/HBASE-6738</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;With default settings for &quot;hbase.splitlog.manager.timeout&quot; =&amp;gt; 25s and &quot;hbase.splitlog.max.resubmit&quot; =&amp;gt; 3.&lt;/p&gt;

&lt;p&gt;On tests mentionned on &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-5843&quot; title=&quot;Improve HBase MTTR - Mean Time To Recover&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-5843&quot;&gt;&lt;del&gt;HBASE-5843&lt;/del&gt;&lt;/a&gt;, I have variations around this scenario, 0.94 + HDFS 1.0.3:&lt;/p&gt;

&lt;p&gt;The regionserver in charge of the split does not answer in less than 25s, so it gets interrupted but actually continues. Sometimes, we go out of the number of retry, sometimes not, sometimes we&apos;re out of retry, but the as the interrupts were ignored we finish nicely. In the mean time, the same single task is executed in parallel by multiple nodes, increasing the probability to get into race conditions.&lt;/p&gt;

&lt;p&gt;Details:&lt;br/&gt;
t0: unplug a box with DN+RS&lt;br/&gt;
t + x: other boxes are already connected, to their connection starts to dies. Nevertheless, they don&apos;t consider this node as suspect.&lt;br/&gt;
t + 180s: zookeeper -&amp;gt; master detects the node as dead. recovery start. It can be less than 180s sometimes it around 150s.&lt;br/&gt;
t + 180s: distributed split starts. There is only 1 task, it&apos;s immediately acquired by a one RS.&lt;br/&gt;
t + 205s: the RS has multiple errors when splitting, because a datanode is missing as well. The master decides to give the task to someone else. But often the task continues in the first RS. Interrupts are often ignored, as it&apos;s well stated in the code (&quot;// TODO interrupt often gets swallowed, do what else?&quot;)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
   2012-09-04 18:27:30,404 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;t + 211s: two regionsservers are processing the same task. They fight for the leases:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2012-09-04 18:27:32,004 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException:          org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on
   /hbase/TABLE/4d1c1a4695b1df8c58d13382b834332e/recovered.edits/0000000000000000037.temp owned by DFSClient_hb_rs_BOX2,60020,1346775882980 but is accessed by DFSClient_hb_rs_BOX1,60020,1346775719125
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;     They can fight like this for many files, until the tasks finally get interrupted or finished.&lt;br/&gt;
     The taks on the second box can be cancelled as well. In this case, the task is created again for a new box.&lt;br/&gt;
     The master seems to stop after 3 attemps. It can as well renounce to split the files. Sometimes the tasks were not cancelled on the RS side, so the split is finished despites what the master thinks and logs. In this case, the assignement starts. In the other, it&apos;s &quot;we&apos;ve got a problem&quot;).&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2012-09-04 18:43:52,724 INFO org.apache.hadoop.hbase.master.SplitLogManager: Skipping resubmissions of task /hbase/splitlog/hdfs%3A%2F%2FBOX1%3A9000%2Fhbase%2F.logs%2FBOX0%2C60020%2C1346776587640-splitting%2FBOX0%252C60020%252C1346776587640.1346776587832 because threshold 3 reached     
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;t + 300s: split is finished. Assignement starts&lt;br/&gt;
t + 330s: assignement is finished, regions are available again.&lt;/p&gt;


&lt;p&gt;There are a lot of subcases possible depending on the number of logs files, of region server and so on.&lt;/p&gt;

&lt;p&gt;The issues are:&lt;br/&gt;
1) it&apos;s difficult, especially in HBase but not only, to interrupt a task. The pattern is often&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 void f() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException{
  &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
     &lt;span class=&quot;code-comment&quot;&gt;// whatever &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; InterruptedException
&lt;/span&gt;  }&lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt;(InterruptedException){
    &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; InterruptedIOException();
  }
}

 &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; g(){
   &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; nbRetry= 0;  
   &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(;;)
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;{
         f();
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
      }&lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt;(IOException e){
         nbRetry++;
         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ( nbRetry &amp;gt; maxRetry) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
      }
   } 
 }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tyically shallows the interrupt. There are other variation, but this one seems to be the standard.&lt;br/&gt;
Even if we fix this in HBase, we need the other layers to be Interrupteble as well. That&apos;s not proven.&lt;/p&gt;

&lt;p&gt;2) 25s is very aggressive, considering that we have a default timeout of 180s for zookeeper. In other words, we give 180s to a regionserver before acting, but when it comes to split, it&apos;s 25s only. There may be reasons for this, but it seems dangerous, as during a failure the cluster is less available than during normal operations. We could do stuff around this, for example:&lt;br/&gt;
=&amp;gt; Obvious option: increase the timeout at each try. Something like *2.&lt;br/&gt;
=&amp;gt; Also possible: increase the initial timeout&lt;br/&gt;
=&amp;gt; check for an update instead of blindly cancelling + resubmitting.&lt;/p&gt;

&lt;p&gt;3) Globally, it seems that this retry mechanism duplicates the failure detection already in place with ZK. Would it not make sense to just hook into this existing detection mechanism, and resubmit a task if and only if we detect that the regionserver in charge died? During a failure scenario we should be much more gentle than during normal operation, not the opposite.&lt;/p&gt;</description>
                <environment>&lt;p&gt;3 nodes cluster test, but can occur as well on a much bigger one. It&apos;s all luck!&lt;/p&gt;</environment>
        <key id="12606604">HBASE-6738</key>
            <summary>Too aggressive task resubmission from the distributed log manager</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nkeywal">Nicolas Liochon</assignee>
                                    <reporter username="nkeywal">Nicolas Liochon</reporter>
                        <labels>
                    </labels>
                <created>Fri, 7 Sep 2012 15:09:11 +0000</created>
                <updated>Wed, 5 Nov 2014 04:51:29 +0000</updated>
                            <resolved>Wed, 3 Oct 2012 15:26:35 +0000</resolved>
                                    <version>0.94.1</version>
                    <version>0.95.2</version>
                                    <fixVersion>0.95.0</fixVersion>
                                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                <comments>
                            <comment id="13460730" author="nkeywal" created="Fri, 21 Sep 2012 19:15:39 +0000"  >&lt;p&gt;This is a &quot;low profile&quot; patch, in which I tried to limit the impact to a minimum. I don&apos;t know if something more ambitious should not be done (i.e. cleaning up this stuff), but...&lt;/p&gt;

&lt;p&gt;Reviews welcome. I have not tried it on a real cluster.&lt;/p&gt;</comment>
                            <comment id="13460737" author="stack" created="Fri, 21 Sep 2012 19:20:48 +0000"  >&lt;p&gt;What do you mean here: &lt;blockquote&gt;&lt;p&gt;This allows to continue if the worker cannot actually handle it,&lt;br/&gt;
+      //       for any reason.&lt;/p&gt;&lt;/blockquote&gt;&lt;/p&gt;

&lt;p&gt;This seems like a small change extending timeout while also reacting faster if server is actually gone.  I&apos;m +1 on patch.&lt;/p&gt;</comment>
                            <comment id="13460759" author="nkeywal" created="Fri, 21 Sep 2012 19:28:59 +0000"  >&lt;p&gt;Even if the worker is considered as &quot;alive&quot; (i.e. ZooKeeper says it&apos;s there), we will resubmit the task. Which is strange in a way.&lt;/p&gt;

&lt;p&gt;Previous behavior was:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;after 25 seconds, resubmit&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;New behavior is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;resubmit if the worker/region server is dead (usual ZooKeeper timeout)&lt;/li&gt;
	&lt;li&gt;or if we have no news for 2 minutes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In both cases, after 3 unsuccessful retries, stop.&lt;/p&gt;
</comment>
                            <comment id="13460799" author="hadoopqa" created="Fri, 21 Sep 2012 20:19:19 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12546078/6738.v1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12546078/6738.v1.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 3 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 hadoop2.0.  The patch compiles against the hadoop 2.0 profile.&lt;/p&gt;

&lt;p&gt;    -1 javadoc.  The javadoc tool appears to have generated 139 warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    -1 findbugs.  The patch appears to introduce 7 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;     -1 core tests.  The patch failed these unit tests:&lt;/p&gt;


&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//testReport/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//console&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/PreCommit-HBASE-Build/2917//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13461664" author="nkeywal" created="Mon, 24 Sep 2012 08:27:47 +0000"  >&lt;p&gt;I added some release notes. Will commit within 2 days if no objection.&lt;/p&gt;</comment>
                            <comment id="13468611" author="nkeywal" created="Wed, 3 Oct 2012 15:25:52 +0000"  >&lt;p&gt;Committed revision 1393537.&lt;/p&gt;</comment>
                            <comment id="13468642" author="hudson" created="Wed, 3 Oct 2012 16:08:58 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #3413 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/3413/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/3413/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt;  Too aggressive task resubmission from the distributed log manager (Revision 1393537)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13468966" author="hudson" created="Wed, 3 Oct 2012 23:18:12 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK-on-Hadoop-2.0.0 #206 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/206/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/206/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6738&quot; title=&quot;Too aggressive task resubmission from the distributed log manager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6738&quot;&gt;&lt;del&gt;HBASE-6738&lt;/del&gt;&lt;/a&gt;  Too aggressive task resubmission from the distributed log manager (Revision 1393537)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
nkeywal : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13616336" author="nkeywal" created="Thu, 28 Mar 2013 15:14:43 +0000"  >&lt;p&gt;May be we should have put this one on 0.94. Any opinion?&lt;/p&gt;</comment>
                            <comment id="13624565" author="hudson" created="Sun, 7 Apr 2013 01:18:57 +0000"  >&lt;p&gt;Integrated in HBase-0.94 #949 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94/949/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94/949/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8276&quot; title=&quot;Backport hbase-6738 to 0.94 &amp;quot;Too aggressive task resubmission from the distributed log manager&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8276&quot;&gt;&lt;del&gt;HBASE-8276&lt;/del&gt;&lt;/a&gt; Backport hbase-6738 to 0.94 &quot;Too aggressive task resubmission from the distributed log manager&quot; (Jeffrey) (Revision 1465161)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;/p&gt;</comment>
                            <comment id="13626349" author="hudson" created="Tue, 9 Apr 2013 07:33:25 +0000"  >&lt;p&gt;Integrated in HBase-0.94-security #133 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-0.94-security/133/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-0.94-security/133/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-8276&quot; title=&quot;Backport hbase-6738 to 0.94 &amp;quot;Too aggressive task resubmission from the distributed log manager&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-8276&quot;&gt;&lt;del&gt;HBASE-8276&lt;/del&gt;&lt;/a&gt; Backport hbase-6738 to 0.94 &quot;Too aggressive task resubmission from the distributed log manager&quot; (Jeffrey) (Revision 1465161)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;/p&gt;</comment>
                            <comment id="13774930" author="stack" created="Mon, 23 Sep 2013 18:30:22 +0000"  >&lt;p&gt;Marking closed.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12752969">HBASE-12430</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                                                <inwardlinks description="is required by">
                                        <issuelink>
            <issuekey id="12551766">HBASE-5843</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12546078" name="6738.v1.patch" size="15139" author="nkeywal" created="Fri, 21 Sep 2012 19:11:08 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 21 Sep 2012 19:20:48 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>240659</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 12 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i014cf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4473</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>The Split Log Manager now takes into account the state of the region server doing the split. If this region server is marked as dead (i.e. its ZooKeeper connection expires), its task is immediately resubmitted. If the region server is still in the &amp;quot;alive&amp;quot; state, then we wait for 2 minutes before resubmitting, instead of 25 seconds previously. This delay can be changed with the parameter &amp;quot;hbase.splitlog.manager.timeout&amp;quot; (milliseconds, new default since 0.96: 120000).&lt;br/&gt;
</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>