<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 18:50:36 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-1084/HBASE-1084.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-1084] Reinitializable DFS client</title>
                <link>https://issues.apache.org/jira/browse/HBASE-1084</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;HBase is the only long lived DFS client. Tasks handle DFS errors by dying. HBase daemons do not and instead depend on dfsclient error recovery capability, but that is not sufficiently developed or tested. Several issues are a result:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-846&quot; title=&quot;hbase looses its mind when hdfs fills&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-846&quot;&gt;&lt;del&gt;HBASE-846&lt;/del&gt;&lt;/a&gt;: hbase looses its mind when hdfs fills&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-879&quot; title=&quot;When dfs restarts or moves blocks around, hbase regionservers don&amp;#39;t notice&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-879&quot;&gt;&lt;del&gt;HBASE-879&lt;/del&gt;&lt;/a&gt;: When dfs restarts or moves blocks around, hbase regionservers don&apos;t notice&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-932&quot; title=&quot;Regionserver restart rethink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-932&quot;&gt;&lt;del&gt;HBASE-932&lt;/del&gt;&lt;/a&gt;: Regionserver restart&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1078&quot; title=&quot;&amp;quot;java.io.IOException: Could not obtain block&amp;quot;: allthough file is there and accessible through the dfs client&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-1078&quot;&gt;&lt;del&gt;HBASE-1078&lt;/del&gt;&lt;/a&gt;: &quot;java.io.IOException: Could not obtain block&quot;: allthough file is there and accessible through the dfs client&lt;/li&gt;
	&lt;li&gt;hlog indefinitely hung on getting new blocks from dfs on apurtell cluster&lt;/li&gt;
	&lt;li&gt;regions closed due to transient DFS problems during loaded cluster restart&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;These issues might also be related:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15&quot; title=&quot;[hbase] Could not complete hdfs write out to flush file forcing regionserver restart&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15&quot;&gt;&lt;del&gt;HBASE-15&lt;/del&gt;&lt;/a&gt;: Could not complete hdfs write out to flush file forcing regionserver restart&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-667&quot; title=&quot;Hung regionserver; hung on hdfs: writeChunk, DFSClient.java:2126, DataStreamer socketWrite&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-667&quot;&gt;&lt;del&gt;HBASE-667&lt;/del&gt;&lt;/a&gt;: Hung regionserver; hung on hdfs: writeChunk, DFSClient.java:2126, DataStreamer socketWrite&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;HBase should reinitialize the fs a few times upon catching fs exceptions, with backoff, to compensate. This can be done by making a wrapper around all fs operations that releases references to the old fs instance and makes and initializes a new instance to retry. All fs users would need to be fixed up to handle loss of state around fs wrapper invocations: hlog, memcache flusher, hstore, etc. &lt;/p&gt;

&lt;p&gt;Cases of clear unrecoverable failure (are there any?) should be excepted.&lt;/p&gt;

&lt;p&gt;Once the fs wrapper is in place, error recovery scenarios can be tested by forcing reinitialization of the fs during PE or other test cases.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12411185">HBASE-1084</key>
            <summary>Reinitializable DFS client</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="apurtell">Andrew Purtell</reporter>
                        <labels>
                    </labels>
                <created>Mon, 22 Dec 2008 19:02:40 +0000</created>
                <updated>Thu, 2 May 2013 02:29:20 +0000</updated>
                            <resolved>Sat, 22 Aug 2009 14:09:36 +0000</resolved>
                                                                    <component>io</component>
                    <component>master</component>
                    <component>regionserver</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                <comments>
                            <comment id="12658627" author="apurtell" created="Mon, 22 Dec 2008 19:06:35 +0000"  >&lt;p&gt;One complication might be lingering DFS leases. DFS provides a clearly recognizable exception for this case. Handle it by abandoning the file path and using another. &lt;/p&gt;</comment>
                            <comment id="12658814" author="bluelu" created="Tue, 23 Dec 2008 10:29:31 +0000"  >&lt;p&gt;Wouldn&apos;t it be better to directly make changes to hadoop&apos;s DFSClient, since the errors originate from there?&lt;/p&gt;

&lt;p&gt;If you open a file from the local filesystem, you also don&apos;t expect the filehandle to stop working after a certain amount of time/activity. The same should also hold for the dfsclient. Any other future applications on top on hadoop could also profit from this.&lt;/p&gt;</comment>
                            <comment id="12658856" author="stack" created="Tue, 23 Dec 2008 15:50:03 +0000"  >&lt;p&gt;Thibaut: Agreed.  We can work it out in hbase and then contrib. back to core.&lt;/p&gt;</comment>
                            <comment id="12658910" author="apurtell" created="Tue, 23 Dec 2008 17:59:26 +0000"  >&lt;p&gt;I don&apos;t disagree with Thibaut&apos;s point of view, but it&apos;s a question of how much work it would be. Is it wise to bring down HDFS local and start working on it? Maybe the problem is on the Datanode side of the connection. We can&apos;t say at this point. Hacks to dfsclient would not be accepted by upstream if they just work around a problem, so the point would then be moot, unless we&apos;re going to fix anything anywhere in the DFS plumbing. I suggest that is out of scope. Anyway, this issue is about working around what will hopefully be a temporary condition. Issues about the underlying cause(s) can be filed upstream as more is learned about the particulars of the problem(s). &lt;/p&gt;</comment>
                            <comment id="12659139" author="apurtell" created="Wed, 24 Dec 2008 18:42:49 +0000"  >&lt;p&gt;We had something exactly like this happen today:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-1087?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12659135#action_12659135&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-1087?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=12659135#action_12659135&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, then the affected region appeared to have a missing block no matter where it was reassigned. (I do not believe reassignment to the restarted regionserver was attempted, however.) A shutdown and restart of all regionservers was then necessary. DFS daemons were left alone. The newly started regionservers had no problems compacting and serving the formerly affected region.&lt;/p&gt;</comment>
                            <comment id="12677752" author="apparition" created="Sun, 1 Mar 2009 00:35:44 +0000"  >&lt;p&gt;There is  public void initialize(URI uri, Configuration conf) method at DistributedFileSystem class.&lt;br/&gt;
It initializes DFS by creating new instance of DFSClient.&lt;/p&gt;</comment>
                            <comment id="12704840" author="apurtell" created="Thu, 30 Apr 2009 23:16:46 +0000"  >&lt;p&gt;Hadoop 0.20 changes the picture substantially. Consider filing point issues against HDFS as they arise rather than try the reinitializable wrapper approach. Leaving this issue alive for reference.&lt;/p&gt;</comment>
                            <comment id="12712178" author="stack" created="Fri, 22 May 2009 17:38:27 +0000"  >&lt;p&gt;I haven&apos;t looked into trying a reopen of dfsclient.  Has anyone here looked into what would be involved reopening file on dfs exception?&lt;/p&gt;</comment>
                            <comment id="12712457" author="stack" created="Sat, 23 May 2009 20:05:21 +0000"  >&lt;p&gt;dfs.client.max.block.acquire.failures looks like it might be useful.  Could double this rather than mess with the timer.&lt;/p&gt;

&lt;p&gt;Root problem though seems to be &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-5903&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-5903&lt;/a&gt;  Lets figure out a patch for it and recommend backporting it in hbase installs on hadoop 0.19.x and 0.20.x.&lt;/p&gt;</comment>
                            <comment id="12746280" author="stack" created="Fri, 21 Aug 2009 22:09:33 +0000"  >&lt;p&gt;Can we close this because hadoop-4681 seems to have been root of most of our dfsclient problems or at least makes it so we have less need of a reinit dfsclient.&lt;/p&gt;</comment>
                            <comment id="12746303" author="streamy" created="Fri, 21 Aug 2009 22:27:02 +0000"  >&lt;p&gt;I have noticed one behavior still during testing of 0.20 on our dev cluster.&lt;/p&gt;

&lt;p&gt;If you take out the DataNode that sits on the same node as a RegionServer, I have seen it that the regionserver basically loses all contact with HDFS.  In one instance, my cluster was shutting down and trying to flush out memstores, that failed, so I lost a bunch of data.&lt;/p&gt;

&lt;p&gt;Need to test this on RC2.&lt;/p&gt;</comment>
                            <comment id="12746305" author="stack" created="Fri, 21 Aug 2009 22:31:02 +0000"  >&lt;p&gt;Could it be small cluster issue where timeout for hdfs is ten minutes by default meantime the NN keeps assigning the dead datanode as home for any new blocks allocated?&lt;/p&gt;

&lt;p&gt;We need to make recommendation for datanode lease lengths &amp;#8211; ones that better align with our other hbase timings.&lt;/p&gt;</comment>
                            <comment id="12746312" author="streamy" created="Fri, 21 Aug 2009 22:42:37 +0000"  >&lt;p&gt;I&apos;m not sure, need to test.  But in any case, is there no way for us to be smarter if NN keeps teling us dead DN but it continues to not work, can we not ask/find a different one?  I&apos;m just a little concerned because we&apos;re wide open to data loss even though HDFS is up.&lt;/p&gt;

&lt;p&gt;As far as the lease lengths... yes, we need to have recommendations for it.  We could probably safely drop it down towards where our 0.19 lease lengths were, 30-60 seconds I guess.  One warning though, if you do something like delete 25% of the total size of HDFS at once, it can cause some rather long starvation on the DNs, though since 0.20 I haven&apos;t seen it much past 10-12 seconds.&lt;/p&gt;

&lt;p&gt;On dual-core configurations under high load, you&apos;ll need to be careful as well, but this is the case for the hbase timeouts as well so should be okay.&lt;/p&gt;</comment>
                            <comment id="12746359" author="stack" created="Fri, 21 Aug 2009 23:24:36 +0000"  >&lt;p&gt;There is a pregnant suggestion over in hbase-4739 where dfsclient passes nn datanodes it knows are bad so nn won&apos;t use these when allocating blocks for new files.  Client side of code is up but not the server-side.&lt;/p&gt;
</comment>
                            <comment id="12746462" author="apurtell" created="Sat, 22 Aug 2009 14:09:36 +0000"  >&lt;p&gt;Concur, after patching for &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-127&quot; title=&quot;DFSClient block read failures cause open DFSInputStream to become unusable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-127&quot;&gt;&lt;del&gt;HDFS-127&lt;/del&gt;&lt;/a&gt; I&apos;ve never seen the DFS related issues enumerated here again.&lt;/p&gt;</comment>
                            <comment id="12746463" author="apurtell" created="Sat, 22 Aug 2009 14:11:56 +0000"  >&lt;p&gt;Let&apos;s take the tail of this to the HDFS jira.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="12412688">HBASE-1133</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Tue, 23 Dec 2008 10:29:31 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>31981</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 17 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hb87:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>99081</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>