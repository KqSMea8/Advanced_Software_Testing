<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 21:09:01 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-15967/HBASE-15967.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-15967] Metric for active ipc Readers and make default fraction of cpu count</title>
                <link>https://issues.apache.org/jira/browse/HBASE-15967</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Our ipc Readers are hard coded at 10 regardless since . Running w/ less Readers, we go faster..(e.g. 12 Readers has us doing 135k with workloadc and 6 readers has us doing 145k).. .but hard to tell what count of Readers are needed since no metric.&lt;/p&gt;

&lt;p&gt;This issue changes Readers to be 1/4 the installed CPUs or 8, whichever is the minimum, and then adds a new hbase.regionserver.ipc.runningReaders metric so you have a chance seeing whats needed.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12975829">HBASE-15967</key>
            <summary>Metric for active ipc Readers and make default fraction of cpu count</summary>
                <type id="7" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/subtask_alternate.png">Sub-task</type>
                            <parent id="12956205">HBASE-15594</parent>
                                    <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="stack">stack</assignee>
                                    <reporter username="stack">stack</reporter>
                        <labels>
                    </labels>
                <created>Sun, 5 Jun 2016 19:09:46 +0000</created>
                <updated>Thu, 16 Jun 2016 18:27:42 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                <comments>
                            <comment id="15315997" author="stack" created="Sun, 5 Jun 2016 19:42:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-2941&quot; title=&quot;port HADOOP-6713 - threading scalability for RPC reads - to HBase&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-2941&quot;&gt;&lt;del&gt;HBASE-2941&lt;/del&gt;&lt;/a&gt; added the fixed size Readers whereas, a later, alternative implementation, &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-6103&quot; title=&quot;Optimize the HBaseServer to deserialize the data for each ipc connection in parallel&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-6103&quot;&gt;&lt;del&gt;HBASE-6103&lt;/del&gt;&lt;/a&gt; added count of Readers based off CPUs.&lt;/p&gt;</comment>
                            <comment id="15316448" author="bbeaudreault" created="Mon, 6 Jun 2016 13:00:33 +0000"  >&lt;p&gt;We&apos;ve found point-in-time gauges to be non-ideal for tracking rapidly changing values like # of active handlers (or in this case readers). They are less sensitive to spikes and also makes the metric configuration dependent (a value of 8 is only interesting when compared to the known max value or cpu count). One nice thing that kafka does is provide a `NetworkProcessorAvgIdlePercent` metrics, which is tracked somewhat like so in 0.8.x: &lt;a href=&quot;https://apache.googlesource.com/kafka/+/5174df53778cb5cb2d6d86e4cec9f3185a2c85db/core/src/main/scala/kafka/network/SocketServer.scala#58&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://apache.googlesource.com/kafka/+/5174df53778cb5cb2d6d86e4cec9f3185a2c85db/core/src/main/scala/kafka/network/SocketServer.scala#58&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In 0.9 they refactored a bit and the code is slightly more complicated, but you can see some of the updated logic here &lt;a href=&quot;https://github.com/cloudera/kafka/blob/cdh5-0.9.0_2.0.1/clients/src/main/java/org/apache/kafka/common/network/Selector.java#L630&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/cloudera/kafka/blob/cdh5-0.9.0_2.0.1/clients/src/main/java/org/apache/kafka/common/network/Selector.java#L630&lt;/a&gt; and search for `selectTime` to see it updated.&lt;/p&gt;

&lt;p&gt;They provide similar metrics for both the network threads (similar to ipc readers) and request handler threads (ipc handlers). We&apos;ve added similar metrics to our internal hbase fork for tracking the RPC handler usage, but figured I&apos;d mention for discussion here since you&apos;re adding a net new metric. Thoughts?&lt;/p&gt;</comment>
                            <comment id="15317346" author="stack" created="Mon, 6 Jun 2016 21:58:11 +0000"  >&lt;p&gt;Thanks for the useful pointers &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbeaudreault&quot; class=&quot;user-hover&quot; rel=&quot;bbeaudreault&quot;&gt;Bryan Beaudreault&lt;/a&gt; Let me see if I can do similar. Our current metrics suffer another issue, one you do not mention, and that is that they are really expensive (in perf, w/ this random read loading, the most CPU is spent counting).&lt;/p&gt;</comment>
                            <comment id="15317689" author="ikeda" created="Tue, 7 Jun 2016 02:25:42 +0000"  >&lt;p&gt;If we apply the leader/followers pattern, remove the internal worker threads behind the queue, and make the reader threads work instead, then we collect the most of active threads into one pool and that might make it more clear to configure.&lt;/p&gt;</comment>
                            <comment id="15317742" author="stack" created="Tue, 7 Jun 2016 03:15:29 +0000"  >&lt;p&gt;Say more &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ikeda&quot; class=&quot;user-hover&quot; rel=&quot;ikeda&quot;&gt;Hiroshi Ikeda&lt;/a&gt; ... &lt;/p&gt;

&lt;p&gt;I think we need the workers behind the queue (the &apos;handler&apos; threads) because we want to do some scheduling... If priority rpc, it should happen above &apos;normal&apos; priority ipc. If we only have a pool of readers, and they do the parse of the request and then do the handling instead of handing it off, then we don&apos;t have opportunity to &apos;schedule&apos; (we will run much faster though!)  See related &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15971&quot; title=&quot;Regression: Random Read/WorkloadC slower in 1.x than 0.98&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15971&quot;&gt;&lt;del&gt;HBASE-15971&lt;/del&gt;&lt;/a&gt;. Thanks.&lt;/p&gt;</comment>
                            <comment id="15317777" author="stack" created="Tue, 7 Jun 2016 03:55:01 +0000"  >&lt;p&gt;Which one you want &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbeaudreault&quot; class=&quot;user-hover&quot; rel=&quot;bbeaudreault&quot;&gt;Bryan Beaudreault&lt;/a&gt;? We don&apos;t seem to have the meter in our armory (see DynamicMetricsRegistry) to do the percentage that you point to in the 0.8 abve. We have a Rate as in 0.9 pointer above. What did you fellows do? I like the idea of percentage occupancy so you can tell if you need to up Readers/Handlers. Thanks.&lt;/p&gt;</comment>
                            <comment id="15317950" author="ikeda" created="Tue, 7 Jun 2016 06:20:11 +0000"  >&lt;p&gt;I think it is enough to queue and pay attention to requests&apos; priority just when the requests are too many so that most reader threads are going to handle requests. One or some of reader threads should still remain to accept and enqueue requests until the queue is full.&lt;/p&gt;

&lt;p&gt;The priority or schedule becomes not so useful once the queue becomes full. Newly coming requests will be left on the native socket&apos;s buffer regardless of their priority. Another port should have been used for the priority purpose.&lt;/p&gt;

&lt;p&gt;I&apos;ll check &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15971&quot; title=&quot;Regression: Random Read/WorkloadC slower in 1.x than 0.98&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15971&quot;&gt;&lt;del&gt;HBASE-15971&lt;/del&gt;&lt;/a&gt; when I have time but it seems difficult for me so don&apos;t expect much.&lt;/p&gt;</comment>
                            <comment id="15318677" author="bbeaudreault" created="Tue, 7 Jun 2016 15:29:17 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stack&quot; class=&quot;user-hover&quot; rel=&quot;stack&quot;&gt;stack&lt;/a&gt;, here&apos;s a gist of the current iteration of our metrics source: &lt;a href=&quot;https://gist.github.com/bbeaudreault/bbec28c502d12148f8657aa3f9fecd4f&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/bbeaudreault/bbec28c502d12148f8657aa3f9fecd4f&lt;/a&gt;  This is still in code review so may change. We&apos;re also still performance testing. Feel free to use as needed, if you want. There&apos;s some extra complication in there, because we also wanted to be able to track rpc usage percentages by user, which we&apos;re using to detect problem clients (using more than their fair share of rpc time) and automatically put them into a separate detention queue or apply a quota, depending on the situation.&lt;/p&gt;

&lt;p&gt;We didn&apos;t use the DynamicMetricsRegistry, instead we used yammer metrics for calculating an exponentially weighted moving average and use the 1-min average. We directly hook into metrics2 by creating a MetricsSource.  At read-time (calls to getMetrics) we use yammer&apos;s EWMA#rate() function to return the accumulated nano idle time, basically converting it into nanosIdle/nanos over the time range. We divide that by the # of handlers in the pool to get an average, and subtract from 100. The moving average is updated by calls to countIdlePeriod() which is called from the RpcExecutor&lt;/p&gt;
</comment>
                            <comment id="15319657" author="hudson" created="Tue, 7 Jun 2016 23:13:21 +0000"  >&lt;p&gt;FAILURE: Integrated in HBase-Trunk_matrix #1007 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-Trunk_matrix/1007/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-Trunk_matrix/1007/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15967&quot; title=&quot;Metric for active ipc Readers and make default fraction of cpu count&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15967&quot;&gt;HBASE-15967&lt;/a&gt; Metric for active ipc Readers and make default fraction of (stack: rev 1125215aad3f5b149f3458ba7019c5920f6dca66)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSourceImpl.java&lt;/li&gt;
	&lt;li&gt;hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSource.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15319923" author="hudson" created="Wed, 8 Jun 2016 03:28:12 +0000"  >&lt;p&gt;SUCCESS: Integrated in HBase-Trunk_matrix #1008 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-Trunk_matrix/1008/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-Trunk_matrix/1008/&lt;/a&gt;)&lt;br/&gt;
Revert &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-15967&quot; title=&quot;Metric for active ipc Readers and make default fraction of cpu count&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-15967&quot;&gt;HBASE-15967&lt;/a&gt; Metric for active ipc Readers and make default (stack: rev 6d5a25935e5ce983e14eff576a699ed1948566d2)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSourceImpl.java&lt;/li&gt;
	&lt;li&gt;hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java&lt;/li&gt;
	&lt;li&gt;hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/ipc/MetricsHBaseServerSource.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15330955" author="stack" created="Wed, 15 Jun 2016 00:56:34 +0000"  >&lt;p&gt;I am interested in pursuing this idea Hiroshi. If we handle the total request on the Reader thread, we go faster. Lets take it up over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14479&quot; title=&quot;Apply the Leader/Followers pattern to RpcServer&amp;#39;s Reader&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14479&quot;&gt;HBASE-14479&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15333137" author="stack" created="Thu, 16 Jun 2016 05:33:27 +0000"  >&lt;p&gt;I updated the tail of &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-14479&quot; title=&quot;Apply the Leader/Followers pattern to RpcServer&amp;#39;s Reader&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-14479&quot;&gt;HBASE-14479&lt;/a&gt; with what we should be doing going forward. Meantime, hardcoded Reader count is just wrong. Let me fix that in this issue.&lt;/p&gt;</comment>
                            <comment id="15333178" author="stack" created="Thu, 16 Jun 2016 06:04:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbeaudreault&quot; class=&quot;user-hover&quot; rel=&quot;bbeaudreault&quot;&gt;Bryan Beaudreault&lt;/a&gt;, rereading your comment and patch, you are suggesting then I do something like getIdlePctOfHandlerPool for the reader metric? (Remembering Patrick&apos;s talk and looking at this patch, we should pull the whole thing in... )&lt;/p&gt;</comment>
                            <comment id="15333185" author="stack" created="Thu, 16 Jun 2016 06:12:24 +0000"  >&lt;p&gt;Or register the EWMATickTask thingy you have here? Thanks BB&lt;/p&gt;</comment>
                            <comment id="15334392" author="bbeaudreault" created="Thu, 16 Jun 2016 18:27:42 +0000"  >&lt;p&gt;Sort of. I&apos;m suggesting we adopt the convention of using &lt;tt&gt;IdlePercentage&lt;/tt&gt; for measuring anything of the class of handler and reader usage, instead of the past approach of using point-in-time counts of active X. The EWMATickTask is an implementation detail.  Typically this is all handled by yammer metrics&apos; top-level Meter class. But that has a bunch of overhead &amp;#8211; at the very least it calculates a 1min, 5min, and 15min averages. We wanted to avoid that overhead, so we used EWMA directly.&lt;/p&gt;

&lt;p&gt;The EWMA documentation says it expects &lt;tt&gt;tick()&lt;/tt&gt; to be called every 5sec, so we created and registered the EWMATickTask to do so.&lt;/p&gt;

&lt;p&gt;So the answer to your question is: both! It&apos;d be great if this new metric you&apos;re creating can use a similar implementation to our &lt;tt&gt;getIdlePctOfHandlerPool&lt;/tt&gt;. But in order to achieve that, if you want to use EWMA, you need something akin to the EWMATickTask (or use the heavier Meter, which would give more averaging options).&lt;/p&gt;

&lt;p&gt;If there&apos;s a more hbase-standard way than using yammer metrics to do something like this, we&apos;d be all ears.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12808214" name="HBASE-15967.master.001.patch" size="7652" author="stack" created="Sun, 5 Jun 2016 19:30:55 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Mon, 6 Jun 2016 13:00:33 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            26 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2z04v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>