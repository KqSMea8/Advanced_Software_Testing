<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 20:04:27 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-9440/HBASE-9440.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-9440] Pass blocks of KVs from HFile scanner to the StoreFileScanner and up</title>
                <link>https://issues.apache.org/jira/browse/HBASE-9440</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Currently we read KVs from an HFileScanner one-by-one and pass them up the scanner/heap tree. Many time the ranges of KVs retrieved from StoreFileScanner (by StoreScanners) and HFileScanner (by StoreFileScanner) will be non-overlapping. If chunks of KVs do not overlap we can sort entire chunks just by comparing the start/end key of the chunk. Only if chunks are overlapping do we need to sort KV by KV as we do now.&lt;/p&gt;

&lt;p&gt;I have no patch, but I wanted to float this idea. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12667004">HBASE-9440</key>
            <summary>Pass blocks of KVs from HFile scanner to the StoreFileScanner and up</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="1" iconUrl="https://issues.apache.org/jira/images/icons/statuses/open.png" description="The issue is open and ready for the assignee to start work on it.">Open</status>
                    <statusCategory id="2" key="new" colorName="blue-gray"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="lhofhansl">Lars Hofhansl</reporter>
                        <labels>
                    </labels>
                <created>Wed, 4 Sep 2013 23:42:29 +0000</created>
                <updated>Mon, 18 Jul 2016 09:20:48 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>15</watches>
                                                                <comments>
                            <comment id="13760600" author="mcorgan" created="Fri, 6 Sep 2013 20:46:06 +0000"  >&lt;p&gt;It&apos;s a somewhat advanced optimization, but I&apos;ve always hoped to see block level transfer of data like this.  Both for compactions and long scans.  For compactions it&apos;s probably quite often that all the cells in a block will remain contiguous, in which case you could save the decompression, decoding, heap logic, encoding, compression steps.  Just hand the byte[] through to the new file.  For the client case, maybe make it a setting to bring whole blocks back to the client (as soon as any part of a block is needed) and do filtering logic client-side.&lt;/p&gt;</comment>
                            <comment id="13760720" author="ndimiduk" created="Fri, 6 Sep 2013 22:54:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;For the client case, maybe make it a setting to bring whole blocks back to the client (as soon as any part of a block is needed) and do filtering logic client-side.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What&apos;s the benefit gained from the network overhead of sending the whole blocks?&lt;/p&gt;</comment>
                            <comment id="13761539" author="mcorgan" created="Sun, 8 Sep 2013 22:39:13 +0000"  >&lt;p&gt;In cases where the client wants the majority of cells in the block, you&apos;d basically be sending the same amount of data over the wire, but it could save the regionserver all the work of decompressing, decoding, iterating, reencoding, etc.  All that fine-grained cell handling is is most of the work that hbase is doing, and sometimes it&apos;s just doing it to rebuild a near replica of what was on disk anyway.&lt;/p&gt;</comment>
                            <comment id="13762033" author="ndimiduk" created="Mon, 9 Sep 2013 17:03:34 +0000"  >&lt;p&gt;Makes sense. How would you implement this &amp;#8211; provide new interfaces for &lt;tt&gt;BulkGet&lt;/tt&gt;, &lt;tt&gt;BulkScan&lt;/tt&gt; ?&lt;/p&gt;</comment>
                            <comment id="13762058" author="lhofhansl" created="Mon, 9 Sep 2013 17:25:23 +0000"  >&lt;p&gt;I have not thought about this yet. Ideally all the next(...) methods on the scanners (at least StoreScanner and StoreFileScanner) would have a version that return a sorted KeyValue[]. HFileScanner is a bit weird in that you have to call next() and then call getKeyValue to get the current KV, but if StoreFileScanner could just call this repeatedly and pass a block up, that would be good enough.&lt;br/&gt;
Next: Test HFileScanner.next followed by getKeyValue() directly, to see what the expected maximum throughput should be.&lt;/p&gt;</comment>
                            <comment id="13766223" author="lhofhansl" created="Fri, 13 Sep 2013 04:01:09 +0000"  >&lt;p&gt;Tested HFileReaderV2.ScannerV2 directly. It can scan 45m rows (1 column with a 100 byte value, everything in the blockcache) in 1.9s. So that would the theoretical maximum until we change the HFile format again.&lt;/p&gt;</comment>
                            <comment id="13766247" author="lhofhansl" created="Fri, 13 Sep 2013 05:11:21 +0000"  >&lt;p&gt;If you&apos;re curious: 8 byte keys, 4 byte column name, 2 byte column family name.&lt;/p&gt;

&lt;p&gt;The HFile I used for testing was 5.61gb.&lt;/p&gt;

&lt;p&gt;Scanning this from disk took ~40s, from the OS buffer cache: 12s, and block cache (as stated above): 1.9s.&lt;br/&gt;
(i.e. 140mb/s from disk, 467mb/s from OS cache, and 2.95gb/s from the block cache)&lt;/p&gt;</comment>
                            <comment id="13766385" author="stack" created="Fri, 13 Sep 2013 10:22:06 +0000"  >&lt;p&gt;And scan from the RS frontdoor of same file takes?&lt;/p&gt;</comment>
                            <comment id="13766817" author="lhofhansl" created="Fri, 13 Sep 2013 19:02:45 +0000"  >&lt;p&gt;From disk: 59s, block cache: 42s (didn&apos;t try just OS buffer cache)&lt;br/&gt;
This was with using a RowFilter to filter all KVs at the server. When returning the data to the client it took 103s.&lt;/p&gt;

&lt;p&gt;Take away:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;HBase is doing a reasonable job at interleaving IO and CPU (even though scanning takes 40s longer, scanning from disk increased only by 20s).&lt;/li&gt;
	&lt;li&gt;There is room for improvement even without changing the HFile format: 42s frontdoor vs. 1.9s directly from HFile.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13766822" author="lhofhansl" created="Fri, 13 Sep 2013 19:05:25 +0000"  >&lt;p&gt;Doing the same test now with a rows with 100 columns.&lt;/p&gt;</comment>
                            <comment id="13767030" author="lhofhansl" created="Fri, 13 Sep 2013 21:59:31 +0000"  >&lt;p&gt;Some more numbers: 720k rows, 50cols, 100 bytes, each, 36m kvs. 4.9gb HFile.&lt;br/&gt;
HFile directly... Disk: 28s, Block cache: 1.2s&lt;br/&gt;
RS frontdoor (RowFilter, skips to next row after first column)... Disk: ~30s, block cache: 2.6s&lt;br/&gt;
RS frontdoor (ValueFilter)... Disk: ~30s, block cache: 6s&lt;/p&gt;

&lt;p&gt;So HBase is doing something incredibly expensive for row assembly. Not sure I trust the numbers. Will double check.&lt;/p&gt;</comment>
                            <comment id="13767047" author="lhofhansl" created="Fri, 13 Sep 2013 22:13:36 +0000"  >&lt;p&gt;OK. Redid the tall table test (1 col). Disk takes ~40s (just like HFile), block cache takes 24s.&lt;br/&gt;
Not sure how I got the slow numbers before. Sorry.&lt;/p&gt;

&lt;p&gt;So when data is on disk, we&apos;re pretty close to what we can read from HFiles. When data is cached it&apos;s 24s vs. 1.9s for the all table and 6s vs 1.2s for the wide table. So a bit less impressive.&lt;/p&gt;

&lt;p&gt;Note that all this was just a single region, compacted to a single HFile.&lt;/p&gt;</comment>
                            <comment id="13767061" author="stack" created="Fri, 13 Sep 2013 22:25:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;So when data is on disk, we&apos;re pretty close to what we can read from HFiles. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mean going via front door?&lt;/p&gt;

&lt;p&gt;What should take-away be?  What we need to dig in on?  To go faster, we need to do the prefixtreeblocks and pull blocks up out of hfile?&lt;/p&gt;</comment>
                            <comment id="13767083" author="lhofhansl" created="Fri, 13 Sep 2013 22:34:39 +0000"  >&lt;p&gt;Aha, the difference was RowFilter vs ValueFilter. In the tall table case the row skipping is ineffective and gets into the way.&lt;/p&gt;</comment>
                            <comment id="13767365" author="lhofhansl" created="Sat, 14 Sep 2013 04:26:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;You mean going via front door?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sorry, yes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What should take-away be? What we need to dig in on? To go faster, we need to do the prefixtreeblocks and pull blocks up out of hfile?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not entirely sure... The data suggest that with 50 cols the best we can do is a ~5x improvement (and that is if we can pass the KVs up with &lt;b&gt;no&lt;/b&gt; overhead).&lt;/p&gt;

&lt;p&gt;For tall tables, we might want to check what the per row overhead is (is it the creation of the Result object for example?)&lt;/p&gt;

&lt;p&gt;Yes, to go faster we need to be able to scan encoded KVs and pass the up unchanged to the various heaps, to avoid all that baggage of the key for every column (0.94 and trunk still do that for the prefix encoders). We need to be able to pass KVs around that are not backed by a continuous byte[].&lt;/p&gt;</comment>
                            <comment id="13767366" author="lhofhansl" created="Sat, 14 Sep 2013 04:28:11 +0000"  >&lt;p&gt;I will also test with smaller KVs (maybe 10 bytes values)&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12989123">HBASE-16225</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 6 Sep 2013 20:46:06 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>346941</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 13 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1ntqv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>347241</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>