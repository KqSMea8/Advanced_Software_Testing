<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:13:43 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-3857/HBASE-3857.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-3857] Change the HFile Format</title>
                <link>https://issues.apache.org/jira/browse/HBASE-3857</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;In order to support &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3763&quot; title=&quot;Add Bloom Block Index Support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3763&quot;&gt;&lt;del&gt;HBASE-3763&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3856&quot; title=&quot;Build a tree structure data block index inside of the HFile&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3856&quot;&gt;&lt;del&gt;HBASE-3856&lt;/del&gt;&lt;/a&gt;, we need to change the format of the HFile. The new format proposal is attached here. Thanks for Mikhail Bautin for the documentation. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12506311">HBASE-3857</key>
            <summary>Change the HFile Format</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mikhail">Mikhail Bautin</assignee>
                                    <reporter username="liyin">Liyin Tang</reporter>
                        <labels>
                    </labels>
                <created>Thu, 5 May 2011 17:21:20 +0000</created>
                <updated>Fri, 20 Nov 2015 12:42:08 +0000</updated>
                            <resolved>Thu, 4 Aug 2011 20:06:32 +0000</resolved>
                                    <version>0.90.4</version>
                                    <fixVersion>0.92.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>25</watches>
                                                                <comments>
                            <comment id="13029455" author="stack" created="Thu, 5 May 2011 17:39:02 +0000"  >&lt;p&gt;I don&apos;t see the attachment.  Is it just me?&lt;/p&gt;</comment>
                            <comment id="13029462" author="mikhail" created="Thu, 5 May 2011 17:49:33 +0000"  >&lt;p&gt;Liyin initially added and deleted the attachment, which was just our design doc wiki page copy-and-paste into Word. I will upload another draft of the design doc shortly.&lt;/p&gt;</comment>
                            <comment id="13029579" author="mikhail" created="Thu, 5 May 2011 21:15:03 +0000"  >&lt;p&gt;This is our initial draft of the HFile format version 2 design. The implementation is almost complete and entering the testing phase.&lt;/p&gt;</comment>
                            <comment id="13029615" author="stack" created="Thu, 5 May 2011 22:20:03 +0000"  >&lt;p&gt;Design looks excellent.&lt;/p&gt;

&lt;p&gt;A few comments:&lt;/p&gt;

&lt;p&gt;+ It looks like it will be self-migrating in that it can read version1 hfiles.  Thats great.&lt;br/&gt;
+ You say &quot;Block!type,!a!sequence!of!bytes!equivalent!to!version!1&apos;s!&quot;magic!records&quot;  Is this the case?  The magic was supposed to be a sequence you could search to pick up the parse again after hitting a bad patch of corrupted data.  You seem to instead start blocks with a type?&lt;br/&gt;
+ How are blocks sized now?  Are we still cutting blocks off at first KV boundary after we go past configured hfile block size &amp;#8211; e.g. 64k &amp;#8211; or instead, is the block cutoff instead determined by fill of the bloom filter array or the index?&lt;br/&gt;
+ I think I know what the following refers to in the diagram, &quot;Version!2!root index,!stored!in!the!data!block!index!section!of!the!file&quot; &amp;#8211; its kept in the &apos;load-on-open section&apos;, right?&lt;br/&gt;
+ Can we have example of how root, intermediate and leaf indices interrelate?  Whats in the root, intermediates, and leaf indices?  Are intermediates optional?  At what boundary do they cut in?  Leaf indices are optional too?  What are these? indices into the data block?&lt;br/&gt;
+ &#8226; Offset!(long)!&lt;br/&gt;
o For this description &quot;This!offset!may!point!to!a!data!block!or!to!a!deeper?level!index!block.!&lt;br/&gt;
&#8226; On?disk!size!(int)!&lt;br/&gt;
&#8226; Key!(a!serialized!byte!array)!&lt;br/&gt;
o Key!(VInt)!&lt;br/&gt;
o Key!bytes&quot;&lt;/p&gt;

&lt;p&gt;You are using vint specifying key size.  We didn&apos;t do that in v1?  You have a good implementation (was costly IIRC using hadoops&apos;).&lt;/p&gt;

&lt;p&gt;+ Is a &apos;!root!index!bloc&apos; same as a &apos;Root Data Index&apos; (from the diagram?)&lt;br/&gt;
+ &quot;&#8226; entryOffsets:!the!&#8220;secondary!index&#8221; of!offsets!of!entries!in!the!block,!to!&lt;br/&gt;
facilitate!a!quick!binary!search!on!the!key!(numEntries-int!values)&quot;&lt;/p&gt;

&lt;p&gt;Is this worth the bother?  A binary search of in-memory data structure?  How many entries are you thinking there will be in these blocks?&lt;/p&gt;


&lt;p&gt;+1&lt;/p&gt;
</comment>
                            <comment id="13029739" author="stack" created="Fri, 6 May 2011 04:30:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;&quot;&#8226; entryOffsets:!the!&#8220;secondary!index&#8221; of!offsets!of!entries!in!the!block,!to!&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;facilitate!a!quick!binary!search!on!the!key!(numEntries-int!values)&quot; Is this worth the bother? A binary search of in-memory data structure? How many entries are you thinking there will be in these blocks?&lt;/p&gt;

&lt;p&gt;nvm on the above.  I see your comment over in &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3855&quot; title=&quot;Performance degradation of memstore because reseek is linear&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3855&quot;&gt;&lt;del&gt;HBASE-3855&lt;/del&gt;&lt;/a&gt; on how you want to binary search seek/reseeking.&lt;/p&gt;</comment>
                            <comment id="13031489" author="liyin" created="Wed, 11 May 2011 00:41:33 +0000"  >&lt;p&gt;Let me try to answer this&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
1) When writing the HFile, the writer will hold the current inline block index into memory. If this inline block index is larger than a threshold (default 128KB), it will flush the index into disk. The benefit here is to avoid hold all the block index during the write operation.&lt;/p&gt;

&lt;p&gt;2) But after writing all the data block and the block index is still smaller a threshold number, there will be only one level of block index, which is the root level and it is the same as before. So multi-level block index is totally optional based on block index size. &lt;/p&gt;

&lt;p&gt;3) After flushing the each of inline block index onto disk, the Writer will generate its parent level of block index on the fly, which will point to the offset of these inline block index. It will be called intermediate level. So only the leaf level of block index will point to the data block, other level block index will point to its children level block index. Also for each level of block index, it will be partitioned into blocks as well. Each index block is a chunk of block index, which will have the same format of data block.&lt;/p&gt;

&lt;p&gt;4) If the intermediate level of block is still larger than the threshold (128 KB), it will generate its parent level. Keep doing this until the current level of block index is smaller than the threshold. The last one is called root level block index, which is smaller than the threshold. It works like generating a balanced tree from bottom to up until the root level is smaller than the threshold.&lt;/p&gt;

&lt;p&gt;5) So when reading the file, it will load back the root level of block index and hold it into memory. When Reader calls seekTo a key, it will lookup the tree from root to leaf to find out the data block contains this key. All the index block will be cached during the searching.&lt;/p&gt;


&lt;p&gt;Please let me know there is any concerns about the block index&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Thanks a lot&lt;/p&gt;</comment>
                            <comment id="13032656" author="mikhail" created="Thu, 12 May 2011 20:38:18 +0000"  >&lt;p&gt;I will try to answer the rest of the questions:&lt;/p&gt;

&lt;p&gt;&amp;gt; + You say &quot;Block!type,!a!sequence!of!bytes!equivalent!to!version!1&apos;s!&quot;magic!records&quot; Is this the case? The magic was supposed to be a sequence you could search to pick up the parse again after hitting a bad patch of corrupted data. You seem to instead start blocks with a type?&lt;/p&gt;

&lt;p&gt;    In our design, the magic record a serialized representation of the block type. &lt;br/&gt;
    I did not see any logic that searches for a magic record after hitting a block &lt;br/&gt;
    of bad data in version 1, so I did not implement it in version 2. I am not sure&lt;br/&gt;
    what are the specific data corruption cases this might help fix.&lt;/p&gt;

&lt;p&gt;&#8232;&amp;gt; + How are blocks sized now? Are we still cutting blocks off at first KV boundary after we go past configured hfile block size &#8211; e.g. 64k &#8211; or instead, is the block cutoff instead determined by fill of the bloom filter array or the index?&lt;/p&gt;

&lt;p&gt;    The blocks are sized the same way as before. Block cutoff happens independently&lt;br/&gt;
    for regular data blocks and for inline blocks (Bloom blocks and leaf data index&lt;br/&gt;
    blocks). When a normal data block fills up, we give all registered &quot;inline&lt;br/&gt;
    block writers&quot; a chance to insert their next block into the stream. The Bloom&lt;br/&gt;
    filter writer has an ability to queue filled-up blocks until its next chance to&lt;br/&gt;
    write them, and block index writer&apos;s chunks can only fill up on data block&lt;br/&gt;
    boundary.&lt;/p&gt;

&lt;p&gt;&#8232;&amp;gt; + I think I know what the following refers to in the diagram, &quot;Version!2!root index,!stored!in!the!data!block!index!section!of!the!file&quot; &#8211; its kept in the &apos;load-on-open section&apos;, right?&lt;/p&gt;

&lt;p&gt;    This should have been &quot;Version 2 root index, stored in the load-on-open section&lt;br/&gt;
    of the file&quot;. Thanks for catching this. I will fix this in the spec.&lt;/p&gt;

&lt;p&gt;&#8232;&amp;gt; + &#8226; Offset!(long)&lt;span class=&quot;error&quot;&gt;Unable to render embedded object: File (&#8232;&amp;gt; o For this description &amp;quot;This!offset!may!point!to!a!data!block!or!to!a!deeper?level!index!block.) not found.&lt;/span&gt;&#8232;&amp;gt; &#8226; On?disk!size!(int)&lt;span class=&quot;error&quot;&gt;Unable to render embedded object: File (&#8232;&amp;gt; &#8226; Key) not found.&lt;/span&gt;(a!serialized!byte!array)&lt;span class=&quot;error&quot;&gt;Unable to render embedded object: File (&#8232;&amp;gt; o Key) not found.&lt;/span&gt;(VInt)!&#8232;&amp;gt; o Key!bytes&quot;&lt;br/&gt;
&amp;gt; You are using vint specifying key size. We didn&apos;t do that in v1? You have a good implementation (was costly IIRC using hadoops&apos;).&lt;/p&gt;

&lt;p&gt;    Actually, version 1 already uses VInt to store the block index, because it uses&lt;br/&gt;
    Bytes.writeByteArray, which stores the length as a VInt. We decided to keep the&lt;br/&gt;
    root-level block index format similar to the version 1 block index format, since&lt;br/&gt;
    it gets de-serialized into a byte[][], a long[], and an int[] anyway.&lt;/p&gt;

&lt;p&gt;&amp;gt; + Is a &apos;!root!index!bloc&apos; same as a &apos;Root Data Index&apos; (from the diagram?)&lt;/p&gt;

&lt;p&gt;    The Root Data Index is one particular instance of a root index block. We use the&lt;br/&gt;
    same &quot;root index block&quot; format for the data index root level, meta index&lt;br/&gt;
    (which is always single-level), and Bloom index (also single-level). For&lt;br/&gt;
    intermediate and leaf-level blocks we use another &quot;non-root index block&quot; format &lt;br/&gt;
    that allows to do binary search of the serialized data structure.&lt;/p&gt;

&lt;p&gt;&#8232;&amp;gt; + &quot;&#8226; entryOffsets:!the!&#8220;secondary!index&#8221; of!offsets!of!entries!in!the!block,!to!&#8232;facilitate!a!quick!binary!search!on!the!key!(numEntries-int!values)&quot;&lt;br/&gt;
&amp;gt; Is this worth the bother? A binary search of in-memory data structure? How many entries are you thinking there will be in these blocks?&lt;/p&gt;

&lt;p&gt;    After discussing this with Nicolas, we decided not to change the data block&lt;br/&gt;
    format, because in our case there are somewhere between 10-500 key/value pairs&lt;br/&gt;
    per data block, so binary search does not offer much benefit compared to the&lt;br/&gt;
    current linear search, and the read time is dominated by input/output anyway. &lt;/p&gt;

&lt;p&gt;Hope this helps. Please let me know if you have any further questions/concerns about&lt;br/&gt;
the HFile format v2.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;br/&gt;
--Mikhail&lt;/p&gt;</comment>
                            <comment id="13033270" author="stack" created="Fri, 13 May 2011 20:40:29 +0000"  >&lt;p&gt;Thanks lads for the answers.  Helps.&lt;/p&gt;

&lt;p&gt;High level, do you lads think this format &apos;brittle&apos;?  Will corruption or an error logic writing any piece of the file render the file as a whole unreadable or large swaths of the file unreadable? A corrupt root index makes the file totally unreadable I suppose.   A corrupt intermediate index will render all subbranches unreadable?  So, this file format seems more &apos;brittle&apos; than V1 because of the chaining between the index parts (root to intermediate, etc.)?  What do you think?  Its unavoidable I suppose if we want the nice feature that Liyin describes where we dump out index as we cross over an index size threshold (And yes Mikhail, in V1, there is not code that makes use of the &apos;magic&apos; to skip bad bits of the file.  And does &apos;magic&apos; for a parser to pick up the parse again even make sense on a filesystem that is checksummed?  Or, in your words &apos; I am not sure what are the specific data corruption cases &lt;span class=&quot;error&quot;&gt;&amp;#91;magic&amp;#93;&lt;/span&gt; might help fix.&apos;)&lt;/p&gt;

&lt;p&gt;@Mikhail&lt;/p&gt;

&lt;p&gt;I forgot we were vint&apos;ing already.  Its probably not a bad idea having root keep same format as old v1 index.&lt;/p&gt;

</comment>
                            <comment id="13033295" author="mikhail" created="Fri, 13 May 2011 21:08:35 +0000"  >&lt;p&gt;Hi St.Ack,&lt;/p&gt;

&lt;p&gt;Thank you for all the feedback! &lt;/p&gt;

&lt;p&gt;To scan an HFile in the new format we don&apos;t even need the root index. Each block is self-sufficient in that the header contains all the information necessary to decode the block, except the compression type, which is found in the trailer. We could create an &quot;HFile fix&quot; tool that would rebuild the block index if necessary. In HFile format v1, however, if the block index is corrupt, we would not be able to read any data blocks at all. So I don&apos;t see how HFile format v2 is more brittle than v1.&lt;/p&gt;

&lt;p&gt;Implementation update: a load test (org.apache.hadoop.hbase.manual.HBaseTest) is successfully running on a 5-node cluster, and I see some 2-level indexes being created with 5-15 root-level entries so far (with the max index block size set to 128K), as well as some compound ROW Bloom filters.&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
--Mikhail&lt;/p&gt;</comment>
                            <comment id="13033322" author="stack" created="Fri, 13 May 2011 21:40:29 +0000"  >&lt;p&gt;@Mikhail cool  I like the bit about each block being &apos;self-sufficient&apos;.&lt;/p&gt;</comment>
                            <comment id="13042596" author="jasonrutherglen" created="Thu, 2 Jun 2011 04:01:52 +0000"  >&lt;p&gt;I think the FST created in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2792&quot; title=&quot;Add a simple FST impl to Lucene&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-2792&quot;&gt;&lt;del&gt;LUCENE-2792&lt;/del&gt;&lt;/a&gt; could be used to compress the rowids in the HFile while simultaneously enabling fast lookup.  &lt;/p&gt;</comment>
                            <comment id="13050852" author="mikhail" created="Fri, 17 Jun 2011 01:55:34 +0000"  >&lt;p&gt;Uploading a new draft of the HFile format version 2 design document, updated to match the current implementation.&lt;/p&gt;</comment>
                            <comment id="13050893" author="stack" created="Fri, 17 Jun 2011 04:13:09 +0000"  >&lt;p&gt;@Mikhail Did much change?  Thanks.&lt;/p&gt;</comment>
                            <comment id="13055794" author="mikhail" created="Mon, 27 Jun 2011 22:01:15 +0000"  >&lt;p&gt;@Stack: there were no significant changes to the spec, just cleanup after the implementation is complete and working. Is it safe to assume that the file format itself is OK for production deployment, i.e. that we will not have to write migration software from HFile format v2 to an HFile format v2.1 as a result of review comments? I will get the patch out for review this week.&lt;/p&gt;</comment>
                            <comment id="13056195" author="stack" created="Mon, 27 Jun 2011 23:25:50 +0000"  >&lt;p&gt;@Mikhail When you put up the patch I can see if migration is needed (IIRC, your design had it that you could deal with both old and new hfiles).  Good on you.&lt;/p&gt;</comment>
                            <comment id="13056201" author="mikhail" created="Mon, 27 Jun 2011 23:35:17 +0000"  >&lt;p&gt;@Stack: no separate migration is needed to go from HFile v1 to HFile v2, because the new code can read both formats. What we are concerned about is that if we deploy HFile v2 in production and then get review comments that require a format change, we might have to migrate from HFile-v2-as-it-currently-is to HFile-v2-final-with-all-comments-addressed.&lt;/p&gt;</comment>
                            <comment id="13056202" author="stack" created="Mon, 27 Jun 2011 23:39:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;What we are concerned about is that if we deploy HFile v2 in production and then get review comments that require a format change, we might have to migrate from HFile-v2-as-it-currently-is to HFile-v2-final-with-all-comments-addressed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How can we help with that?  If you post the code before you roll it out, my guess is that you&apos;d get some outside reviewers.&lt;/p&gt;</comment>
                            <comment id="13056203" author="mikhail" created="Mon, 27 Jun 2011 23:42:04 +0000"  >&lt;p&gt;I agree. I guess it might be difficult to say that the new format is OK based on the spec only. I am working on getting the patch out asap.&lt;/p&gt;</comment>
                            <comment id="13065620" author="mikhail" created="Thu, 14 Jul 2011 23:52:38 +0000"  >&lt;p&gt;I have uploaded a diff at &lt;a href=&quot;https://reviews.apache.org/r/1134/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/1134/&lt;/a&gt; containing HFile v2 changes, multi-level block indexes, and compound Bloom filters. The patch can be downloaded at &lt;a href=&quot;https://reviews.apache.org/r/1134/diff/raw/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/1134/diff/raw/&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13066128" author="mikhail" created="Fri, 15 Jul 2011 18:47:57 +0000"  >&lt;p&gt;This is similar to &lt;a href=&quot;https://reviews.apache.org/r/1134/diff/raw/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/1134/diff/raw/&lt;/a&gt;, but more complete, as ReviewBoard seems to be removing binary files.&lt;/p&gt;

&lt;p&gt;(Sorry for my previous Submit patch / Cancel patch spam on this JIRA.)&lt;/p&gt;</comment>
                            <comment id="13068151" author="stack" created="Wed, 20 Jul 2011 04:22:54 +0000"  >&lt;p&gt;@Mikhail I&apos;ll finish my skim through your quality patch this evening but meantime here are a few things:&lt;/p&gt;

&lt;p&gt;1. Mind if I run a vote up on dev on committing this to trunk?  (My sense is that some would rather it go in after 0.92 branch is cut but I&apos;m thinking it should be included in 0.92, our next major release; its a radical change but the quality of the contribution, the tests included, the careful consideration given to migrating in-place, and that you fellas have been running this in house make me think it low risk pulling this in now.  It looks like Andrew agrees with me).&lt;br/&gt;
2. Would you mind attaching the src document for your pdf design?  It looks like its word.  My thinking is that we could use the openoffice saveas docbook to get a rough cut at a docbook section on this new format that we could work over to include in our site documentation.&lt;/p&gt;

&lt;p&gt;This stuff is excellent. &lt;/p&gt;</comment>
                            <comment id="13069377" author="stack" created="Fri, 22 Jul 2011 04:41:06 +0000"  >&lt;p&gt;Resolve hbase-3417 when this goes in.  This subsumes it.&lt;/p&gt;</comment>
                            <comment id="13071423" author="mikhail" created="Tue, 26 Jul 2011 23:18:42 +0000"  >&lt;p&gt;Adding a new patch, which should be identical to &lt;a href=&quot;https://reviews.apache.org/r/1134/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/1134/&lt;/a&gt; (except for a couple whitespace/comment fixes). Please download the patch from here and not from ReviewBoard, because ReviewBoard seems to ignore binary changes created by git format-patch.&lt;/p&gt;</comment>
                            <comment id="13071467" author="stack" created="Wed, 27 Jul 2011 01:15:39 +0000"  >&lt;p&gt;@Mikhail Thanks for updated patch.  Do you have a couple of answers for my questions above at &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857?focusedCommentId=13068151&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13068151?&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HBASE-3857?focusedCommentId=13068151&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13068151?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="13071473" author="mikhail" created="Wed, 27 Jul 2011 01:27:43 +0000"  >&lt;p&gt;@Michael:&lt;/p&gt;

&lt;p&gt;1. From my conversations with Nicolas and Jonathan I got a sense that HFile v2 would be part of 0.94, but if we can get it into 0.92, that would be even better, because I think in that case we will have to do less merging further down the road. I am also doing some load-testing of the patch on a 5-node cluster this week.&lt;br/&gt;
2. I will attach the spec document in the OpenOffice format later tonight (as soon as I can download OpenOffice and convert the doc).&lt;/p&gt;

&lt;p&gt;Thank you!&lt;br/&gt;
--Mikhail&lt;/p&gt;</comment>
                            <comment id="13071474" author="yuzhihong@gmail.com" created="Wed, 27 Jul 2011 01:33:29 +0000"  >&lt;p&gt;+1 on putting this into 0.92&lt;/p&gt;</comment>
                            <comment id="13071503" author="mikhail" created="Wed, 27 Jul 2011 04:20:39 +0000"  >&lt;p&gt;Attaching the HFile v2 spec in the OpenOffice format.&lt;/p&gt;</comment>
                            <comment id="13072027" author="stack" created="Wed, 27 Jul 2011 21:26:17 +0000"  >&lt;p&gt;@Mikhail You see Todd&apos;s comments.  Will you have a chance to work on them?  Let us know if not and someone of us will take a crack at it.  Thanks.&lt;/p&gt;</comment>
                            <comment id="13072033" author="mikhail" created="Wed, 27 Jul 2011 21:35:29 +0000"  >&lt;p&gt;@Todd: thank you for your comments!&lt;br/&gt;
@Michael: I will try to make changes and post another version of the patch tonight or tomorrow.&lt;/p&gt;

&lt;p&gt;By the way, I&apos;ve been running an automated load test of the trunk with the patch applied on a 5-node cluster, and it seems to work fine. Here are some stats, FWIW, although without the details of this particular load test the following is probably incomprehensible. I guess we will open-source the load test later &amp;#8211; need to talk about this with Nicolas and other folks. It&apos;s basically a bunch of random insertions and random full-row reads.&lt;/p&gt;

&lt;p&gt;11/07/27 14:30:13 INFO utils.MultiThreadedAction: &lt;span class=&quot;error&quot;&gt;&amp;#91;W:20&amp;#93;&lt;/span&gt; Keys = 54313718, cols = 2B, time = 17:15:04 Overall: &lt;span class=&quot;error&quot;&gt;&amp;#91;keys/s = 874, latency = 22 ms&amp;#93;&lt;/span&gt; Current: &lt;span class=&quot;error&quot;&gt;&amp;#91;keys/s = 1323, latency = 14 ms&amp;#93;&lt;/span&gt;&lt;br/&gt;
11/07/27 14:30:13 INFO utils.MultiThreadedAction: &lt;span class=&quot;error&quot;&gt;&amp;#91;R:20&amp;#93;&lt;/span&gt; Keys = 100115674, cols = 5B, time = 17:15:04 Overall: &lt;span class=&quot;error&quot;&gt;&amp;#91;keys/s = 1612, latency = 12 ms&amp;#93;&lt;/span&gt; Current: &lt;span class=&quot;error&quot;&gt;&amp;#91;keys/s = 410, latency = 49 ms&amp;#93;&lt;/span&gt;, verified = 50061516&lt;/p&gt;</comment>
                            <comment id="13073637" author="mikhail" created="Mon, 1 Aug 2011 18:11:07 +0000"  >&lt;p&gt;Here is a new version of the HFile v2 patch, addressing Todd&apos;s comments. This is intended to be applied using &quot;git apply&quot; because of the binary file needed for TestHFileReaderV1. Please use this patch instead of the one you may download from ReviewBoard, because ReviewBoard does not include the binary file into the downloaded patch for some reason.&lt;/p&gt;</comment>
                            <comment id="13073814" author="mikhail" created="Mon, 1 Aug 2011 21:59:25 +0000"  >&lt;p&gt;The new version of the patch is successfully passing the randomized load test as well.&lt;/p&gt;

&lt;p&gt;I am trying to compare unit test results between r1152122 and the same version with the patch applied.&lt;/p&gt;

&lt;p&gt;Without the patch:&lt;br/&gt;
2011-08-01_10_55_23 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-4144&quot; title=&quot; RS does not abort if the initialization of RS fails&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-4144&quot;&gt;&lt;del&gt;HBASE-4144&lt;/del&gt;&lt;/a&gt;  RS does not abort if th | tests: 812, fail: 4, err: 14, skip: 9, time: 4791.1, failed: FullLogReconstruction, DistributedLogSplitting, SplitTransactionOnCluster, ScannerTimeout, MasterFailover, MultiParallel&lt;/p&gt;

&lt;p&gt;With the patch:&lt;br/&gt;
2011-08-01_11_34_28 commit: review_hfile-v2-r1152122-2011_08_01 | tests: 845, fail: 5, err: 14, skip: 9, time: 5426.5, failed: FullLogReconstruction, DistributedLogSplitting, ServerCustomProtocol, Replication, SplitTransactionOnCluster, ScannerTimeout, MasterFailover, MultiParallel&lt;/p&gt;

&lt;p&gt;Looking at the ServerCustomProtocol (the only test that has failures with the patch but not without), I see that this is a quite frequent intermittent test failure in my automated runs of HBase trunk tests.&lt;/p&gt;

&lt;p&gt;Any advice about what version I should select as the baseline for my unit test run? The only trunk version that I managed to get a clean unit test run with was r1147350, and my patch against that passed all unit tests as well, but I think some changes were introduced since then that made the patch not apply cleanly, so I rebased the patch to a more recent version (and got all the test failures of that version). &lt;/p&gt;

&lt;p&gt;Notwithstanding all the above, I believe the patch is currently in a stable state and can be committed, and I will confirm that once a few more unit test runs complete.&lt;/p&gt;</comment>
                            <comment id="13073819" author="yuzhihong@gmail.com" created="Mon, 1 Aug 2011 22:06:19 +0000"  >&lt;p&gt;@Mikhail:&lt;br/&gt;
Thanks for your effort of bringing this closer to checkin.&lt;/p&gt;

&lt;p&gt;Minor note: Replication was new in your second list of unit tests above.&lt;/p&gt;</comment>
                            <comment id="13073880" author="mikhail" created="Mon, 1 Aug 2011 22:44:29 +0000"  >&lt;p&gt;@Ted:&lt;/p&gt;

&lt;p&gt;Yes, you are right, TestReplication also failed with the patch applied while it did not fail without the patch. The failure message was as follows:&lt;/p&gt;

&lt;p&gt;java.lang.AssertionError: Waited too much time for queueFailover replication&lt;/p&gt;

&lt;p&gt;Looking at my archive of unit test results for the trunk, TestReplication failure shows up in 67 cases out of 291 total runs of the test suite, so I suspect it is a highly problematic unit test. I will look into this a bit more.&lt;/p&gt;

&lt;p&gt;However, my question is the following: with the extremely agile development process and a non-trivial number of routine unit test failures in the trunk, what is the accepted approach of testing new patches? How do people select the right revision to test their patch against? Is it always the trunk, and are the existing unit test failures in the trunk ignored (which is bad because this may mask new bugs), or is there a different approach?&lt;/p&gt;</comment>
                            <comment id="13073982" author="jdcryans" created="Mon, 1 Aug 2011 22:54:21 +0000"  >&lt;p&gt;The reason TestReplication sometimes fails is &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3515&quot; title=&quot;[replication] ReplicationSource can miss a log after RS comes out of GC&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3515&quot;&gt;&lt;del&gt;HBASE-3515&lt;/del&gt;&lt;/a&gt;, if you see that stack trace then you can disregard. If not, that&apos;s a new bug.&lt;/p&gt;</comment>
                            <comment id="13076009" author="yuzhihong@gmail.com" created="Tue, 2 Aug 2011 02:42:18 +0000"  >&lt;p&gt;@Mikhail:&lt;br/&gt;
If 2011-08-01_11_34_28 test failures match those in TRUNK build 2067 (18 failures), that would give us confidence of not introducing regression.&lt;/p&gt;</comment>
                            <comment id="13078565" author="mikhail" created="Wed, 3 Aug 2011 02:53:45 +0000"  >&lt;p&gt;A new version of the patch that passes all unit tests when applied on top of r1153300 (thanks to J-D&apos;s fix). This patch is intended to be applied using git apply &amp;lt;patch_file&amp;gt; to correctly create binary files.&lt;/p&gt;

&lt;p&gt;Nicolas &amp;#8211; you mentioned some other sanity-checking that could be done before committing this?&lt;/p&gt;</comment>
                            <comment id="13078886" author="yuzhihong@gmail.com" created="Wed, 3 Aug 2011 17:57:11 +0000"  >&lt;p&gt;I got one test failure for 0001-review_hfile-v2-r1153300-git-1152532-2011_08_02_19_4.patch:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Failed tests:   testGzipCompression(org.apache.hadoop.hbase.io.hfile.TestHFileBlock): expected:&amp;lt;...0\x00\x00\x00\x00\x0[0]\xED\xC3\xC1\x11\x00...&amp;gt; but was:&amp;lt;...0\x00\x00\x00\x00\x0[3]\xED\xC3\xC1\x11\x00...&amp;gt;
    at org.junit.Assert.assertEquals(Assert.java:123)
    at org.junit.Assert.assertEquals(Assert.java:145)
    at org.apache.hadoop.hbase.io.hfile.TestHFileBlock.testGzipCompression(TestHFileBlock.java:126)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13078960" author="mikhail" created="Wed, 3 Aug 2011 19:50:15 +0000"  >&lt;p&gt;Addressing the issue with TestHFileBlock reported by Ted. It turns out there is an &quot;OS&quot; field inside the gzip header which might take different values depending on the OS and configuration. I have changed the unit test to always set that field to the same value before comparing.&lt;/p&gt;</comment>
                            <comment id="13078979" author="mikhail" created="Wed, 3 Aug 2011 20:09:31 +0000"  >&lt;p&gt;TL;DR version: Can we commit this patch?&lt;/p&gt;

&lt;p&gt;@Nicolas: you mentioned some last-minute sanity checking you wanted to do.&lt;/p&gt;

&lt;p&gt;Here are some recent test results with and without the patch. I am running the tests against the stable revision 1153300. All the sporadically failing unit tests with the patch applied (TestDistributedLogSplitting, TestHRegion, TestServerCustomProtocol) also intermittently fail without the patch. The failures look similar for the two versions of code, as shown below. The most recent version of the patch is also passing automated randomized read/write load testing. Given all of that, I think it is safe to assume that the patch does not introduce a regression. &lt;/p&gt;

&lt;p&gt;Note: some of these test failures might be problems with my test setup.&lt;/p&gt;

&lt;p&gt;TestServerCustomProtocol: java.lang.AssertionError: Results should contain region test,,1312378402385.4b887abff65a2e74eab86c859d255c89. for row &apos;aaa&apos;&lt;br/&gt;
TestDistirbutedLogSplitting:&lt;br/&gt;
    testThreeRSAbort(org.apache.hadoop.hbase.master.TestDistributedLogSplitting)  Time elapsed: 95.921 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!&lt;br/&gt;
    java.lang.AssertionError: &lt;br/&gt;
        . . .&lt;br/&gt;
        at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testThreeRSAbort&lt;br/&gt;
TestHRegion:&lt;br/&gt;
    testWritesWhileGetting(org.apache.hadoop.hbase.regionserver.TestHRegion)  Time elapsed: 0.398 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!&lt;br/&gt;
    junit.framework.AssertionFailedError: expected:&amp;lt;\x00\x00\x008&amp;gt; but was:&amp;lt;\x00\x00\x006&amp;gt;&lt;br/&gt;
        at org.apache.hadoop.hbase.HBaseTestCase.assertEquals(HBaseTestCase.java:691)&lt;br/&gt;
        at org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting(TestHRegion.java:2759)&lt;/p&gt;

&lt;p&gt;With the patch:&lt;/p&gt;

&lt;p&gt;2011-08-03_02_58_34 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 2, err: 0, skip: 9, time: 4784.9, failed: DistributedLogSplitting, HRegion&lt;br/&gt;
2011-08-03_03_24_45 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 0, err: 0, skip: 9, time: 4544.1&lt;br/&gt;
2011-08-03_05_00_32 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 1, err: 0, skip: 9, time: 4735.5, failed: HRegion&lt;br/&gt;
2011-08-03_05_21_43 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 1, err: 0, skip: 9, time: 4485.4, failed: ServerCustomProtocol&lt;br/&gt;
2011-08-03_07_00_19 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 0, err: 0, skip: 9, time: 4624.5&lt;br/&gt;
2011-08-03_07_14_08 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 1, err: 0, skip: 9, time: 4687.8, failed: ServerCustomProtocol&lt;br/&gt;
2011-08-03_09_08_22 commit: review_hfile-v2-r1153300-2011_08_02 | tests: 870, fail: 1, err: 0, skip: 9, time: 4711.4, failed: ServerCustomProtocol&lt;/p&gt;

&lt;p&gt;Without the patch:&lt;br/&gt;
2011-08-02_19_23_26 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 0, err: 0, skip: 9, time: 4496.4&lt;br/&gt;
2011-08-02_21_19_54 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 0, err: 0, skip: 9, time: 4520.2&lt;br/&gt;
2011-08-02_23_15_58 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 0, err: 0, skip: 9, time: 4438.1&lt;br/&gt;
2011-08-03_01_10_12 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 0, err: 0, skip: 9, time: 4503.5&lt;br/&gt;
2011-08-03_03_05_10 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 1, err: 0, skip: 9, time: 4550.1, failed: HRegion&lt;br/&gt;
2011-08-03_05_00_26 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 836, fail: 1, err: 0, skip: 9, time: 4418.0, failed: ServerCustomProtocol&lt;br/&gt;
2011-08-03_07_05_57 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 1, err: 0, skip: 9, time: 4443.9, failed: Replication&lt;br/&gt;
2011-08-03_08_59_20 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 2, err: 0, skip: 9, time: 4427.2, failed: CoprocessorEndpoint, DistributedLogSplitting&lt;br/&gt;
2011-08-03_10_52_41 commit: &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3065&quot; title=&quot;Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3065&quot;&gt;&lt;del&gt;HBASE-3065&lt;/del&gt;&lt;/a&gt; don&apos;t prepend MAGIC if d | tests: 837, fail: 0, err: 0, skip: 9, time: 4483.8&lt;/p&gt;</comment>
                            <comment id="13078980" author="yuzhihong@gmail.com" created="Wed, 3 Aug 2011 20:11:45 +0000"  >&lt;p&gt;Committed to TRUNK.&lt;br/&gt;
I moved the binary file to src/test/resources/org/apache/hadoop/hbase/8e8ab58dcf39412da19833fcd8f687ac&lt;/p&gt;

&lt;p&gt;Thanks for the great work, Mikhail.&lt;/p&gt;</comment>
                            <comment id="13078988" author="hudson" created="Wed, 3 Aug 2011 20:22:05 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #2076 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2076/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2076/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; New files&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; added the binary V1 HFile&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt;  Change the HFile Format (Mikhail &amp;amp; Liyin)&lt;/p&gt;

&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/BloomFilterWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/8e8ab58dcf39412da19833fcd8f687ac&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/SimpleBlockCache.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFilePerformance.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/HFilePerformanceEvaluation.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/RandomSeek.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/Bytes.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestReseekTo.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCachedBlockQueue.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/HServerLoad.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/CachedBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockCache.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileSeek.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/BloomFilter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStoreFile.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/TestHalfStoreFileReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerMetrics.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/Hash.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/resources/hbase-default.xml&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFile.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestLruBlockCache.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestFSErrorsExposed.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFiles.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/util/TestByteBloomFilter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/CompressionTest.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/Store.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13079102" author="hudson" created="Wed, 3 Aug 2011 22:54:20 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #2077 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2077/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2077/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; binary HFile should be under io/hfile&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; relocated binary HFile&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; New test classes.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; New files&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt; more new files&lt;/p&gt;

&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/io/hfile/8e8ab58dcf39412da19833fcd8f687ac&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/io/file/8e8ab58dcf39412da19833fcd8f687ac&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/io/hfile&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/8e8ab58dcf39412da19833fcd8f687ac&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/io&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/io/file/8e8ab58dcf39412da19833fcd8f687ac&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/resources/org/apache/hadoop/hbase/io/file&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/util/TestIdLock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompoundBloomFilter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockIndex.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestFixedFileTrailer.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV2.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileReaderV1.java&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestCacheOnWrite.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/IdLock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/DynamicByteBloomFilter.java&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/CompoundBloomFilterBase.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/BloomFilterFactory.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/DoubleOutputStream.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/CompoundBloomFilter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/InlineBlockWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileReader.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/AbstractHFileWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/BloomFilterBase.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/util/CompoundBloomFilterWriter.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13079163" author="mikhail" created="Thu, 4 Aug 2011 01:23:29 +0000"  >&lt;p&gt;Attaching the unit test fix (TestHFileBlock.testBlockHeapSize) from &lt;a href=&quot;https://reviews.apache.org/r/1282/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://reviews.apache.org/r/1282/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;@Ted: could you please commit this fix? Thanks!&lt;/p&gt;</comment>
                            <comment id="13079166" author="yuzhihong@gmail.com" created="Thu, 4 Aug 2011 01:31:22 +0000"  >&lt;p&gt;@Mikhail:&lt;br/&gt;
Test fix committed.&lt;/p&gt;

&lt;p&gt;Thanks for the follow-up.&lt;/p&gt;</comment>
                            <comment id="13079197" author="hudson" created="Thu, 4 Aug 2011 03:44:41 +0000"  >&lt;p&gt;Integrated in HBase-TRUNK #2078 (See &lt;a href=&quot;https://builds.apache.org/job/HBase-TRUNK/2078/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://builds.apache.org/job/HBase-TRUNK/2078/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HBASE-3857&quot; title=&quot;Change the HFile Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HBASE-3857&quot;&gt;&lt;del&gt;HBASE-3857&lt;/del&gt;&lt;/a&gt;  Fix TestHFileBlock.testBlockHeapSize test failure (Mikhail)&lt;/p&gt;

&lt;p&gt;tedyu : &lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hbase/trunk/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hbase/trunk/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13079557" author="yuzhihong@gmail.com" created="Thu, 4 Aug 2011 20:06:32 +0000"  >&lt;p&gt;Recent test failures in TRUNK don&apos;t seem to be related to this feature.&lt;/p&gt;</comment>
                            <comment id="13079567" author="mikhail" created="Thu, 4 Aug 2011 20:18:44 +0000"  >&lt;p&gt;A one-line patch to CHANGES.txt (not sure if this is required). I will also update the release note.&lt;/p&gt;</comment>
                            <comment id="13164613" author="jdcryans" created="Wed, 7 Dec 2011 18:52:10 +0000"  >&lt;p&gt;Any reason why this patch is removing compaction and flush queue sizes?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.metrics.compactionQueueSize.set(compactSplitThread
-        .getCompactionQueueSize());
-    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.metrics.flushQueueSize.set(cacheFlusher
-        .getFlushQueueSize());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If it was intentional, there&apos;s a bunch of dead code that also needs to be removed like those methods that were called. If it wasn&apos;t, meaning there&apos;s currently no way in 0.92 to get the compaction queue size, then this would be sufficient for me to kill the RC.&lt;/p&gt;</comment>
                            <comment id="13164618" author="zhihyu@ebaysf.com" created="Wed, 7 Dec 2011 18:55:31 +0000"  >&lt;p&gt;@J-D:&lt;br/&gt;
Nice catch.&lt;/p&gt;

&lt;p&gt;We should open another JIRA to deal with queue sizes.&lt;/p&gt;</comment>
                            <comment id="13164619" author="jdcryans" created="Wed, 7 Dec 2011 18:58:38 +0000"  >&lt;p&gt;Yeah I just want to verify first what&apos;s the situation, maybe I&apos;m missing something.&lt;/p&gt;</comment>
                            <comment id="15017170" author="lars_francke" created="Fri, 20 Nov 2015 12:42:08 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12506310">HBASE-3856</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12503884">HBASE-3763</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12489393" name="0001-Adding-release-notes-for-HBASE-3857.patch" size="781" author="mikhail" created="Thu, 4 Aug 2011 20:18:44 +0000"/>
                            <attachment id="12489289" name="0001-Fix-TestHFileBlock.testBlockHeapSize.patch" size="1510" author="mikhail" created="Thu, 4 Aug 2011 01:23:29 +0000"/>
                            <attachment id="12486659" name="0001-review_hfile-v2-r1144693_2011-07-15_11_14_44.patch" size="726094" author="mikhail" created="Fri, 15 Jul 2011 18:47:57 +0000"/>
                            <attachment id="12487913" name="0001-review_hfile-v2-r1147350_2011-07-26_11_55_59.patch" size="728469" author="mikhail" created="Tue, 26 Jul 2011 23:18:42 +0000"/>
                            <attachment id="12488426" name="0001-review_hfile-v2-r1152122-2011_08_01_03_18_00.patch" size="732839" author="mikhail" created="Mon, 1 Aug 2011 18:11:07 +0000"/>
                            <attachment id="12489152" name="0001-review_hfile-v2-r1153300-git-1152532-2011_08_02_19_4.patch" size="733327" author="mikhail" created="Wed, 3 Aug 2011 02:53:45 +0000"/>
                            <attachment id="12489236" name="0001-review_hfile-v2-r1153300-git-1152532-2011_08_03_12_4.patch" size="734131" author="mikhail" created="Wed, 3 Aug 2011 19:50:15 +0000"/>
                            <attachment id="12478329" name="hfile_format_v2_design_draft_0.1.pdf" size="426540" author="mikhail" created="Thu, 5 May 2011 21:15:03 +0000"/>
                            <attachment id="12482886" name="hfile_format_v2_design_draft_0.3.pdf" size="429600" author="mikhail" created="Fri, 17 Jun 2011 01:55:34 +0000"/>
                            <attachment id="12487932" name="hfile_format_v2_design_draft_0.4.odt" size="281219" author="mikhail" created="Wed, 27 Jul 2011 04:20:39 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Thu, 5 May 2011 17:39:02 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>33237</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i07xrr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>44238</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>HFile format version 2, multi-level block indexes, and compound Bloom filters.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>