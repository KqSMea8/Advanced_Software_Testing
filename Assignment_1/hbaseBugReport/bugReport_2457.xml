<!-- 
RSS generated by JIRA (6.3.4#6332-sha1:51bc225ef474afe3128b2f66878477f322397b16) at Fri Dec 16 19:02:04 UTC 2016

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/HBASE-2457/HBASE-2457.xml?field=key&amp;field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>6.3.4</version>
        <build-number>6332</build-number>
        <build-date>15-08-2014</build-date>
    </build-info>

<item>
            <title>[HBASE-2457] RS gets stuck compacting region ad infinitum</title>
                <link>https://issues.apache.org/jira/browse/HBASE-2457</link>
                <project id="12310753" key="HBASE">HBase</project>
                    <description>&lt;p&gt;Testing 0.20_pre_durability@934643, I ended up in a state where one region server got stuck compacting a single region over and over again forever. This was with a special config with very low flush threshold in order to stress test flush/compact code.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12462210">HBASE-2457</key>
            <summary>RS gets stuck compacting region ad infinitum</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.png">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="tlipcon">Todd Lipcon</reporter>
                        <labels>
                    </labels>
                <created>Fri, 16 Apr 2010 05:04:08 +0000</created>
                <updated>Fri, 20 Nov 2015 12:42:11 +0000</updated>
                            <resolved>Sat, 17 Apr 2010 19:49:56 +0000</resolved>
                                    <version>0.20.4</version>
                                    <fixVersion>0.90.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                <comments>
                            <comment id="12857672" author="tlipcon" created="Fri, 16 Apr 2010 05:05:27 +0000"  >&lt;p&gt;Here&apos;s the full log and stack of the stuck RS&lt;/p&gt;</comment>
                            <comment id="12857675" author="tlipcon" created="Fri, 16 Apr 2010 05:08:37 +0000"  >&lt;p&gt;Audit logs are just showing the &quot;stuck&quot; RS doing this:&lt;/p&gt;

&lt;p&gt;2010-04-15 22:08:00,574 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=listStatus  src=/hbase/test1/1736416594/actions  dst=null        perm=null&lt;br/&gt;
2010-04-15 22:08:00,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=mkdirs      src=/hbase/test1/compaction.dir/1736416594   dst=null        perm=todd:supergroup:rwxr-xr-x&lt;br/&gt;
2010-04-15 22:08:00,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=delete      src=/hbase/test1/compaction.dir/1736416594   dst=null        perm=null&lt;br/&gt;
2010-04-15 22:08:01,575 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=todd,todd,adm,git   ip=/192.168.42.43       cmd=listStatus  src=/hbase/test1/1736416594/actions     dst=null        perm=null&lt;/p&gt;

&lt;p&gt;(ie a noop compaction)&lt;/p&gt;</comment>
                            <comment id="12857678" author="tlipcon" created="Fri, 16 Apr 2010 05:24:39 +0000"  >&lt;p&gt;Managed to use the log level servlet to get debug logs of the no-op compactions:&lt;/p&gt;

&lt;p&gt;2010-04-15 22:24:05,594 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region test1,4993900000,1271390277054 has too many store files, putting it back at the end of the flush queue.&lt;br/&gt;
2010-04-15 22:24:05,594 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region test1,4993900000,1271390277054/1736416594 because: regionserver/192.168.42.43:60020.cacheFlusher&lt;br/&gt;
2010-04-15 22:24:05,594 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region test1,4993900000,1271390277054&lt;br/&gt;
2010-04-15 22:24:05,597 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of 1 file; compaction size of actions: 231.5m; Skipped 7 files, size: 242232921&lt;br/&gt;
2010-04-15 22:24:05,598 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region test1,4993900000,1271390277054 in 0sec&lt;/p&gt;</comment>
                            <comment id="12857682" author="tlipcon" created="Fri, 16 Apr 2010 05:44:48 +0000"  >&lt;p&gt;Looks like the compaction code is basically deciding that there&apos;s nothing worth compacting here. Here are the files in the order that the compaction code is looking at them (oldest to newest) with their sizes&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;7876304170849844721 2010-04-16 03:58:04
-rw-r--r--   3 todd supergroup  134676288 2010-04-15 20:58 /hbase/test1/1736416594/actions/7876304170849844721
3289606064411356106 2010-04-16 03:59:43
-rw-r--r--   3 todd supergroup   61309324 2010-04-15 20:59 /hbase/test1/1736416594/actions/3289606064411356106
4995952634622872563 2010-04-16 04:00:42
-rw-r--r--   3 todd supergroup   24381446 2010-04-15 21:00 /hbase/test1/1736416594/actions/4995952634622872563
6876448782185582993 2010-04-16 04:01:03
-rw-r--r--   3 todd supergroup   12103493 2010-04-15 21:01 /hbase/test1/1736416594/actions/6876448782185582993
7381870009831588255 2010-04-16 04:01:18
-rw-r--r--   3 todd supergroup    5855317 2010-04-15 21:01 /hbase/test1/1736416594/actions/7381870009831588255
5274234463019618354 2010-04-16 04:01:23
-rw-r--r--   3 todd supergroup    2712677 2010-04-15 21:01 /hbase/test1/1736416594/actions/5274234463019618354
3688488928995595533 2010-04-16 04:01:32
-rw-r--r--   3 todd supergroup    1194376 2010-04-15 21:01 /hbase/test1/1736416594/actions/3688488928995595533
5321716733066884905 2010-04-16 04:01:33
-rw-r--r--   3 todd supergroup     532824 2010-04-15 21:01 /hbase/test1/1736416594/actions/5321716733066884905
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12857683" author="stack" created="Fri, 16 Apr 2010 05:52:55 +0000"  >&lt;p&gt;Here is the code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!majorcompaction &amp;amp;&amp;amp; !references) {
        &lt;span class=&quot;code-comment&quot;&gt;// Here we select files &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; incremental compaction.  
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// The rule is: &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the largest(oldest) one is more than twice the 
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// size of the second, skip the largest, and &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt; to next...,
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// until we meet the compactionThreshold limit.
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (point = 0; point &amp;lt; countOfFiles - 1; point++) {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((fileSizes[point] &amp;lt; fileSizes[point + 1] * 2) &amp;amp;&amp;amp; 
               (countOfFiles - point) &amp;lt;= maxFilesToCompact) {
            &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
          }
          skipped += fileSizes[point];
        }
        filesToCompact = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;StoreFile&amp;gt;(filesToCompact.subList(point,
          countOfFiles));
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Todd did a listing of the Store:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-rw-r--r--   3 todd supergroup  134676288 2010-04-15 20:58 /hbase/test1/1736416594/actions/7876304170849844721
-rw-r--r--   3 todd supergroup   61309324 2010-04-15 20:59 /hbase/test1/1736416594/actions/3289606064411356106
-rw-r--r--   3 todd supergroup   24381446 2010-04-15 21:00 /hbase/test1/1736416594/actions/4995952634622872563
-rw-r--r--   3 todd supergroup    1194376 2010-04-15 21:01 /hbase/test1/1736416594/actions/3688488928995595533
-rw-r--r--   3 todd supergroup    2712677 2010-04-15 21:01 /hbase/test1/1736416594/actions/5274234463019618354
-rw-r--r--   3 todd supergroup     532824 2010-04-15 21:01 /hbase/test1/1736416594/actions/5321716733066884905
-rw-r--r--   3 todd supergroup   12103493 2010-04-15 21:01 /hbase/test1/1736416594/actions/6876448782185582993
-rw-r--r--   3 todd supergroup    5855317 2010-04-15 21:01 /hbase/test1/1736416594/actions/7381870009831588255
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you trace, the olders is &amp;gt; 2 * the next oldest, and so on.  Some of the times are same so not sure how that plays out.... (the above is not strictly ordered ... those of same time may not be proper chronological order).&lt;/p&gt;
</comment>
                            <comment id="12857697" author="stack" created="Fri, 16 Apr 2010 06:45:02 +0000"  >&lt;p&gt;Ugly suggested bandaid so can cut RC: &lt;a href=&quot;http://pastie.org/922715&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://pastie.org/922715&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="12857705" author="tlipcon" created="Fri, 16 Apr 2010 06:57:14 +0000"  >&lt;p&gt;This issue got me thinking about why the heuristic is the way it is, and I don&apos;t quite follow.&lt;/p&gt;

&lt;p&gt;I stepped away for a minute and tried to come up with a cost model for why it is we do minor compactions. Let me run this by everyone:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The cost of a doing compaction is the cost of reading all of the files plus the cost of writing the newly compacted file. The new file might be a bit smaller than the sum of the originals, so the total cost = sum(compacted file size) + writes cost factor * reduction factor * sum(compacted file size). Let&apos;s call this (1 + WF)*sum(file sizes)&lt;/li&gt;
	&lt;li&gt;The cost of &lt;b&gt;not&lt;/b&gt; doing a compaction is just that reads are slower because we have to seek more. Let&apos;s define R as some unknown factor which describes how important read performance is to us, and S the seek time for each additional store. So the cost of not doing the compaction is R*S*num stores.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So if we combine these, we want to basically minimize (1 + WF)*size(files to compact) - R*S*count(files to compact).&lt;/p&gt;

&lt;p&gt;So here&apos;s a simple algorithm to minimize that:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;sort files by increasing size
for each file:
  if (1+WF)*size(file) &amp;lt; R*S:
    add file to compaction set
  else:
    break
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you rearrange that inequality, you can make it: if size(file) &amp;lt; (R*S)/(1+WF). Basically, what this is saying is that we should just make file size the heuristic, and tune the minimum file size based on the ratio between how much we care about having a small number of stores vs saving sequential IO on compactions.&lt;/p&gt;

&lt;p&gt;The one flaw if you look closely is that we can&apos;t actually sort by increasing size and compact some set of the smallest ones, because we have to only compact a set of files that are contiguous in the sequence. I think we can slightly tweak the algorithm, though, to optimize the same objective but take that restriction into account.&lt;/p&gt;</comment>
                            <comment id="12857889" author="streamy" created="Fri, 16 Apr 2010 17:13:44 +0000"  >&lt;p&gt;Great stuff todd.  We&apos;ve been doing a lot of discussion of these same heuristics recently.  Some recent commits have actually changed some things that will impact what we can/can&apos;t do when deciding which files to compact.  For one, deletes are no longer processed during minor compactions.  We&apos;re also not using (to my knowledge) relative ages of storefiles right now for anything, so I&apos;m not sure it still holds that we can only compact neighbors.&lt;/p&gt;

&lt;p&gt;I plan on working at this stuff during the hackathon.&lt;/p&gt;</comment>
                            <comment id="12857895" author="streamy" created="Fri, 16 Apr 2010 17:22:43 +0000"  >&lt;p&gt;I&apos;m +1 to commit the bandaid patch until we can get smarter.&lt;/p&gt;</comment>
                            <comment id="12857905" author="stack" created="Fri, 16 Apr 2010 17:40:55 +0000"  >&lt;p&gt;Thanks Todd.  I like your cut at a better algorithm.  As per Jon, it may be the case that the contiguous-only no longer applies after yesterdays commit that leaves deletes in files until major compaction.  Also, anything that improves our i/o profile is ++ultra super important.&lt;/p&gt;

&lt;p&gt;So, I suggest that we move the compaction review out of this issue into a new blocker against 0.20.5.  This new issue would be about trying your suggestion and breaking out compaction so it standalone/testable.  You&apos;re interested in this one Jon?&lt;/p&gt;

&lt;p&gt;For this issue, for 0.20.4, I suggest we apply the band-aid.  For the super pathological case you turned up above, the band-aid will mean we only do two files at a time compressing &amp;#8211; way sub-optimal but at least we wouldn&apos;t be stuck.&lt;/p&gt;</comment>
                            <comment id="12857907" author="tlipcon" created="Fri, 16 Apr 2010 17:46:02 +0000"  >&lt;p&gt;I thought about this a bit on the train this morning (guess this long commute is good for something!)&lt;/p&gt;

&lt;p&gt;The real root of this problem is that we are getting &quot;noop&quot; compactions when we are blocked on a compaction happening. Rather than changing the compaction algorithm like in stack&apos;s patch, could we simply make it so that it checks whether there are currently flushes blocked on a compaction, and if that&apos;s the case, force one to happen?&lt;/p&gt;</comment>
                            <comment id="12858092" author="stack" created="Sat, 17 Apr 2010 04:31:42 +0000"  >&lt;p&gt;.bq ...could we simply make it so that it checks whether there are currently flushes blocked on a compaction, and if that&apos;s the case, force one to happen?&lt;/p&gt;

&lt;p&gt;Thats not easy given current structure.&lt;/p&gt;

&lt;p&gt;Here&apos;s a second cut at a band-aid.  This differs from first in that we&apos;ll compact the last 4 files in the list at a minimum.  Review please.  I&apos;m testing it now.&lt;/p&gt;</comment>
                            <comment id="12858094" author="stack" created="Sat, 17 Apr 2010 04:33:21 +0000"  >&lt;p&gt;@Todd BTW, that posted log is crazy.  Flush at 2MB!&lt;/p&gt;</comment>
                            <comment id="12858160" author="stack" created="Sat, 17 Apr 2010 16:12:37 +0000"  >&lt;p&gt;All tests w/ this patch in place.  I&apos;ve been running it overnight under cluster loading and nothing untoward.  We might be compacting a bit more aggressively now doing the last 4 files in list if &amp;gt; 4 but that is probably for the better.&lt;/p&gt;</comment>
                            <comment id="12858176" author="stack" created="Sat, 17 Apr 2010 19:49:56 +0000"  >&lt;p&gt;Ok.  I applied the bandaid so I can make an RC.  On commit I changed it so we compact the last 3 files rather than the last 4 to be a little more conservative.  I did not apply to TRUNK.&lt;/p&gt;</comment>
                            <comment id="12858523" author="apurtell" created="Mon, 19 Apr 2010 14:46:59 +0000"  >
&lt;p&gt;   [[ Old comment, sent by email on Fri, 16 Apr 2010 17:45:20 +0000 ]]&lt;/p&gt;

&lt;p&gt;+1 on Stack&apos;s strategy to get a RC out the door (today?)&lt;/p&gt;

</comment>
                            <comment id="12866877" author="stack" created="Wed, 12 May 2010 23:52:54 +0000"  >&lt;p&gt;Marking these as fixed against 0.21.0 rather than against 0.20.5.&lt;/p&gt;</comment>
                            <comment id="12868424" author="stack" created="Mon, 17 May 2010 22:24:07 +0000"  >&lt;p&gt;I applied this to TRUNK.  It wasn&apos;t there.&lt;/p&gt;</comment>
                            <comment id="15017182" author="lars_francke" created="Fri, 20 Nov 2015 12:42:11 +0000"  >&lt;p&gt;This issue was closed as part of a bulk closing operation on 2015-11-20. All issues that have been resolved and where all fixVersions have been released have been closed (following discussions on the mailing list).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12442035" name="ASF.LICENSE.NOT.GRANTED--bandaid-v2.patch" size="1648" author="stack" created="Sat, 17 Apr 2010 04:32:20 +0000"/>
                            <attachment id="12441922" name="ASF.LICENSE.NOT.GRANTED--log.gz" size="94246" author="tlipcon" created="Fri, 16 Apr 2010 05:05:27 +0000"/>
                            <attachment id="12441923" name="ASF.LICENSE.NOT.GRANTED--stack" size="34384" author="tlipcon" created="Fri, 16 Apr 2010 05:05:27 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                        <customfieldname>Date of First Response</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fri, 16 Apr 2010 05:52:55 +0000</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>26313</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0hhsf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>100144</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                        <customfieldname>Time in Status</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>